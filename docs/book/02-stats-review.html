<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.53">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>2&nbsp; Basic Cuisine: A Review on Probability and Frequentist Statistical Inference – The Regression Cookbook (in development)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../book/continuous-zone.html" rel="next">
<link href="../book/01-intro.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<link href="../site_libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet">
<script src="../site_libs/htmlwidgets-1.6.4/htmlwidgets.js"></script><link href="../site_libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet">
<script src="../site_libs/datatables-binding-0.34.0/datatables.js"></script><script src="../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><link href="../site_libs/dt-core-1.13.6/css/jquery.dataTables.min.css" rel="stylesheet">
<link href="../site_libs/dt-core-1.13.6/css/jquery.dataTables.extra.css" rel="stylesheet">
<script src="../site_libs/dt-core-1.13.6/js/jquery.dataTables.min.js"></script><link href="../site_libs/crosstalk-1.2.2/css/crosstalk.min.css" rel="stylesheet">
<script src="../site_libs/crosstalk-1.2.2/js/crosstalk.min.js"></script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="../custom.css">
</head>
<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="navbar navbar-expand-lg " data-bs-theme="dark"><div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">The Regression Cookbook (in development)</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
    <a href="https://github.com/alexrod61/regression-cookbook" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.linkedin.com/sharing/share-offsite/?url=%7Curl%7C">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
</div>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../book/02-stats-review.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../img/cookbook.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none"></a>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/meet-the-team.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Meet the Team</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/privacy-policy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Website Privacy Policy</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/audience-scope.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Audience and Scope</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/01-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Getting Ready for Regression Cooking!</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/02-stats-review.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../book/continuous-zone.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Continuous Cuisine</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/03-ols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Ordinary Least-squares Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/04-gamma.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Gamma Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/05-beta.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Soup-erb Beta Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/06-parametric-survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Parametric Survival Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/07-semiparametric-survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Semiparametric Survival Regression</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../book/discrete-zone.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Discrete Cuisine</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/08-binary-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Binary Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/09-binomial-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Binomial Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/10-classical-poisson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Classical Poisson Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/11-negative-binomial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Negative Binomial Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/12-zero-inflated-poisson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Zero-Inflated Poisson Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/13-generalized-poisson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Generalized Poisson Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/14-multinomial-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Multinomial Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/15-ordinal-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Ordinal Logistic Regression</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/A-dictionary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">ML-Stats Dictionary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/B-greek-alphabet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Greek Alphabet</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/C-distributional-mind-map.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Distributional Mind Map</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/D-regression-mind-map.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Regression Mind Map</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">The recipe</h2>
   
  <ul>
<li>
<a href="#sec-basics-prob" id="toc-sec-basics-prob" class="nav-link active" data-scroll-target="#sec-basics-prob"><span class="header-section-number">2.1</span> Basics of Probability</a>
  <ul>
<li><a href="#sec-first-insights" id="toc-sec-first-insights" class="nav-link" data-scroll-target="#sec-first-insights"><span class="header-section-number">2.1.1</span> First Insights</a></li>
  <li><a href="#sec-stats-schools" id="toc-sec-stats-schools" class="nav-link" data-scroll-target="#sec-stats-schools"><span class="header-section-number">2.1.2</span> Schools of Statistical Thinking</a></li>
  <li><a href="#sec-random-variables" id="toc-sec-random-variables" class="nav-link" data-scroll-target="#sec-random-variables"><span class="header-section-number">2.1.3</span> The Random Variables</a></li>
  <li><a href="#sec-wonders-random-variables" id="toc-sec-wonders-random-variables" class="nav-link" data-scroll-target="#sec-wonders-random-variables"><span class="header-section-number">2.1.4</span> The Wonders of Generative Modelling and Probability Distributions</a></li>
  <li><a href="#sec-characterizing-prob-dist" id="toc-sec-characterizing-prob-dist" class="nav-link" data-scroll-target="#sec-characterizing-prob-dist"><span class="header-section-number">2.1.5</span> Characterizing Probability Distributions</a></li>
  <li><a href="#sec-random-samples" id="toc-sec-random-samples" class="nav-link" data-scroll-target="#sec-random-samples"><span class="header-section-number">2.1.6</span> The Rationale in Random Sampling</a></li>
  </ul>
</li>
  <li>
<a href="#sec-mle" id="toc-sec-mle" class="nav-link" data-scroll-target="#sec-mle"><span class="header-section-number">2.2</span> What is Maximum Likelihood Estimation?</a>
  <ul>
<li><a href="#sec-mle-key-concepts" id="toc-sec-mle-key-concepts" class="nav-link" data-scroll-target="#sec-mle-key-concepts"><span class="header-section-number">2.2.1</span> Key Concepts</a></li>
  <li>
<a href="#sec-mle-empirical" id="toc-sec-mle-empirical" class="nav-link" data-scroll-target="#sec-mle-empirical"><span class="header-section-number">2.2.2</span> Analytical Estimates</a>
  <ul class="collapse">
<li><a href="#demand-query" id="toc-demand-query" class="nav-link" data-scroll-target="#demand-query">Demand Query</a></li>
  <li><a href="#time-query" id="toc-time-query" class="nav-link" data-scroll-target="#time-query">Time Query</a></li>
  </ul>
</li>
  <li><a href="#sec-num-optimization" id="toc-sec-num-optimization" class="nav-link" data-scroll-target="#sec-num-optimization"><span class="header-section-number">2.2.3</span> Numerical Optimization</a></li>
  </ul>
</li>
  <li>
<a href="#sec-basics-inf" id="toc-sec-basics-inf" class="nav-link" data-scroll-target="#sec-basics-inf"><span class="header-section-number">2.3</span> Basics of Frequentist Statistical Inference</a>
  <ul>
<li><a href="#sec-hypothesis-workflow-general-settings" id="toc-sec-hypothesis-workflow-general-settings" class="nav-link" data-scroll-target="#sec-hypothesis-workflow-general-settings"><span class="header-section-number">2.3.1</span> General Settings</a></li>
  <li><a href="#sec-hypothesis-workflow-hypotheses-definitions" id="toc-sec-hypothesis-workflow-hypotheses-definitions" class="nav-link" data-scroll-target="#sec-hypothesis-workflow-hypotheses-definitions"><span class="header-section-number">2.3.2</span> Hypotheses Definitions</a></li>
  <li><a href="#sec-hypothesis-workflow-test-flavour-components" id="toc-sec-hypothesis-workflow-test-flavour-components" class="nav-link" data-scroll-target="#sec-hypothesis-workflow-test-flavour-components"><span class="header-section-number">2.3.3</span> Test Flavour and Components</a></li>
  <li><a href="#sec-hypothesis-workflow-inferential-conclusions" id="toc-sec-hypothesis-workflow-inferential-conclusions" class="nav-link" data-scroll-target="#sec-hypothesis-workflow-inferential-conclusions"><span class="header-section-number">2.3.4</span> Inferential Conclusions</a></li>
  </ul>
</li>
  <li><a href="#sec-chapter-2-summary" id="toc-sec-chapter-2-summary" class="nav-link" data-scroll-target="#sec-chapter-2-summary"><span class="header-section-number">2.4</span> Chapter Summary</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/alexrod61/regression-cookbook/edit/main/book/02-stats-review.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/alexrod61/regression-cookbook/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/alexrod61/regression-cookbook/blob/main/book/02-stats-review.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title"><span id="sec-stats-review" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><!-- Google tag (gtag.js) --><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-7PRVEBE1EF"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7PRVEBE1EF');
</script><div class="Warning">
<div class="Warning-header">
<p>The Importance of This Chapter</p>
</div>
<div class="Warning-container">
<p>This chapter will delve into the fundamentals of probability and frequentist statistical inference. Moreover, this review will be important to understanding the philosophy of modelling parameter estimation as outlined in <a href="01-intro.html#sec-ds-workflow-estimation" class="quarto-xref"><span>Section 1.2.5</span></a>. Then, we will pave the way to the rationale behind statistical inference in the <a href="../book/01-intro.html#sec-ds-workflow-results">Results</a> stage in our workflow from <a href="01-intro.html#fig-ds-workflow" class="quarto-xref">Figure&nbsp;<span>1.1</span></a>. Note that we aim to explain all these statistical and probabilistic concepts in the most possible practical way via a made-up case study throughout this chapter (while still presenting useful theoretical admonitions as outlined in <a href="01-intro.html" class="quarto-xref"><span>Chapter 1</span></a>).</p>
</div>
</div>
<div class="LO">
<div class="LO-header">
<p>Learning Objectives</p>
</div>
<div class="LO-container">
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li>Discuss why having <strong>a complete conceptual understanding of the process of statistical inference</strong> is key when conducting studies for <strong>general audiences</strong>.</li>
<li>Explain <strong>why probability is the language of statistics</strong>.</li>
<li>Recall <strong>foundational probabilistic insights</strong>.</li>
<li>Break down the differences between <strong>the two schools of statistical thinking</strong>: <strong>frequentist</strong> and <strong>Bayesian</strong>.</li>
<li>Apply the philosophy of <strong>generative modelling</strong> along with <strong>probability distributions</strong> in <strong>parameter estimation</strong>.</li>
<li>Justify using <strong>measures of central tendency</strong> and <strong>uncertainty</strong> to characterize probability distributions.</li>
<li>Illustrate how <strong>random sampling</strong> can be used in <strong>parameter estimation</strong>.</li>
<li>Describe conceptually <strong>what maximum likelihood estimation entails</strong> in a frequentist framework.</li>
<li>Formulate a <strong>maximum likelihood approach</strong> in <strong>parameter estimation</strong>.</li>
<li>Outline <strong>the process of a frequentist classical-based hypothesis testing</strong> to solve inferential inquiries.</li>
</ul>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Let us start with a relatable story!</strong></p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/panda.png" class="img-fluid figure-img" width="400"></p>
<figcaption>Image by <a href="https://pixabay.com/users/openclipart-vectors-30363/"><em>OpenClipart-Vectors</em></a> via <a href="https://pixabay.com/vectors/panda-cute-bear-blue-question-149818/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<p>Imagine you are an undergraduate engineering student. Moreover, last term, you just took and passed your first course in probability and statistics (inference included) in an industrial engineering context. Moreover, as it could happen while taking an introductory course in probability and statistics, you used to feel quite overwhelmed by the large amount of jargon and formulas one had to grasp and use regularly for primary engineering fields such as quality control in a manufacturing facility. <em>Population parameters</em>, <em>hypothesis testing</em>, <em>tests statistics</em>, <em>significance level</em>, <em><span class="math inline">\(p\)</span>-values</em>, and <em>confidence intervals</em> were appearing here and there. And to your frustration, you could never find a statistical connection between all these inferential tools! Instead, you relied on mechanistic procedures when solving assignments or exam problems.</p>
<p>For instance, when performing hypothesis testing for a two-sample <span class="math inline">\(t\)</span>-test, you struggled to reflect what the hypotheses were trying to indicate for the corresponding population parameters or how the test statistic was related to these hypotheses. Moreover, your interpretation of the resulting <span class="math inline">\(p\)</span>-value and/or confidence interval was purely mechanical with the inherent claim:</p>
<blockquote class="blockquote">
<p><em>With a significance level <span class="math inline">\(\alpha = 0.05\)</span>, we reject (<strong>or fail to reject, if that is the case</strong>) the null hypothesis in given that…</em></p>
</blockquote>
<p>Truthfully, this whole mechanical way of doing statistics is not ideal in a teaching, research or industry environment. Along the same lines, the above situation should also not happen when we learn key statistical topics for the very first time as undergraduate students. That is why we will investigate a more intuitive way of viewing probability and its crucial role in statistical inference. This matter will help us deliver more coherent storytelling (as in <a href="01-intro.html#sec-ds-workflow-storytelling" class="quarto-xref"><span>Section 1.2.8</span></a>) when presenting our results in practice during any regression analysis to our peers or stakeholders. Note that the role of probability also extends to model training (as in <a href="01-intro.html#sec-ds-workflow-estimation" class="quarto-xref"><span>Section 1.2.5</span></a>) when it comes to supervised learning and not just regarding statistical inference.</p>
<p>Having said all this, it is time to introduce a statement that is key when teaching hypothesis testing in an introductory statistical inference course:</p>
<blockquote class="blockquote">
<p><strong>In statistical inference, everything always boils down to randomness and how we can control it!</strong></p>
</blockquote>
<p>That is quite a bold statement! Nonetheless, once one starts presenting statistical topics to audiences not entirely familiar with the usual field jargon, the idea of randomness always persists across many different tools. And, of course, regression analysis is not an exception at all since it also involves inference on population parameters of interest. This is why we have allocated this chapter in the textbook to explain core probabilistic and inferential concepts to pave the way to its role in regression analysis.</p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on why we mean as a non-ideal mechanical analysis!</p>
</div>
<div class="Heads-up-container">
<p>The reader might need clarification on why the mechanical way of performing hypothesis testing is considered <strong>non-ideal</strong>, mainly when the term <strong>cookbook</strong> is used in the book’s title. The cookbook concept here actually refers to a homogenized recipe for data modelling, as seen in the workflow from <a href="01-intro.html#fig-ds-workflow" class="quarto-xref">Figure&nbsp;<span>1.1</span></a>. However, there is a crucial distinction between this and the non-ideal mechanical way of hypothesis testing.</p>
<p>On the one hand, the non-ideal mechanical way refers to <strong>the use of a tool without understanding the rationale of what this tool stands for</strong>, resulting in vacuous and standard statements that we would not be able to explain any way further, such as the statement we previously indicated:</p>
<blockquote class="blockquote">
<p><em>With a significance level <span class="math inline">\(\alpha = 0.05\)</span>, we reject (or fail to reject, if that is the case) the null hypothesis given that…</em></p>
</blockquote>
<p>What if a stakeholder of our analysis asks us in plain words what a significance level means? Why are we phrasing our conclusion on the null hypothesis and not directly on the alternative one? As a data scientist, one should be able to explain why the whole inference process yields that statement without misleading the stakeholders’ understanding. For sure, this also implicates appropriate communication skills that cater to general audiences rather than just technical ones.</p>
<p>Conversely, the data modelling workflow in <a href="01-intro.html#fig-ds-workflow" class="quarto-xref">Figure&nbsp;<span>1.1</span></a> involves stages that necessitate a comprehensive and precise understanding of our analysis. Progressing to the next stage (without a complete grasp of the current one) risks perpetuating false insights, potentially leading to faulty data storytelling of the entire analysis.</p>
</div>
</div>
<p>Specifically, this chapter will review the following:</p>
<ul>
<li>The role of <em>random variables</em> and <em>probability distributions</em> and the governance of <em>population (or system) parameters</em> (i.e., the so-called Greek letters we usually see in statistical inference and regression analysis). <a href="#sec-basics-prob" class="quarto-xref"><span>Section 2.1</span></a> will explore these topics more in detail while connecting them to the subsequent inferential terrain under a <em>frequentist context</em>.</li>
<li>When delving into supervised learning and regression analysis, we might wonder how randomness is incorporated into <em>model fitting</em> (i.e., <em>parameter estimation</em>). That is quite a fascinating aspect, implemented via a crucial statistical tool known as <em>maximum likelihood estimation</em>. This tool is heavily related to the concept of <em>loss function</em> in supervised learning. <a href="#sec-mle" class="quarto-xref"><span>Section 2.2</span></a> will explore these matters in more detail and how the idea of a <em>random sample</em> is connected to this estimation tool.</li>
<li>
<a href="#sec-basics-inf" class="quarto-xref"><span>Section 2.3</span></a> will explore the basics of <em>hypothesis testing</em> and its intrinsic components such as <em>null</em> and <em>alternative hypotheses</em>, <em>type I</em> and <em>type II</em> errors, <em>significance level</em>, <em>power</em>, <em>observed effect</em>, <em>standard error</em>, <em>test statistic</em>, <em>critical value</em>, <em><span class="math inline">\(p\)</span>-value</em>, and <em>confidence interval</em>.</li>
</ul>
<p>Without further ado, let us start with reviewing core concepts in probability via quite a tasty example.</p>
<section id="sec-basics-prob" class="level2" data-number="2.1"><h2 data-number="2.1" class="anchored" data-anchor-id="sec-basics-prob">
<span class="header-section-number">2.1</span> Basics of Probability</h2>
<p>In terms of regression analysis (either on an <strong>inferential</strong> or <strong>predictive</strong> framework), probability can be viewed as the solid foundation on which more complex tools, including estimation and hypothesis testing, are built upon. Having said that, let us scaffold across all the necessary probabilistic concepts that will allow us to move forward into these more complex tools.</p>
<section id="sec-first-insights" class="level3" data-number="2.1.1"><h3 data-number="2.1.1" class="anchored" data-anchor-id="sec-first-insights">
<span class="header-section-number">2.1.1</span> First Insights</h3>
<p>To start building up our solid probabilistic foundation, we assume our data is coming from a given population or system of interest. Moreover, the population or system is assumed to be governed by parameters which, as data scientists or researchers, they are of our best interest to study. That said, the terms population and parameter will pave the way to our first <strong>statistical definitions</strong>.</p>
<div id="Definition-population" class="definition">
<div class="definition-header">
<p>Definition of population</p>
</div>
<div class="definition-container">
<p>It is a <strong>whole collection of individuals or items</strong> that share <strong>distinctive attributes</strong>. As data scientists or researchers, we are interested in studying these attributes, which we assume are <strong>governed</strong> by parameters. In practice, we must be <strong>as specific as possible</strong> when defining our given population such that we would frame our entire data modelling process since its very early stages. Examples of a population could be the following:</p>
<ul>
<li><em>Children between the ages of 5 and 10 years old in states of the American West Coast.</em></li>
<li><em>Customers of musical vinyl records in the Canadian provinces of British Columbia and Alberta.</em></li>
<li><em>Avocado trees grown in the Mexican state of Michoacán.</em></li>
<li><em>Adult giant pandas in the Southwestern Chinese province of Sichuan.</em></li>
<li><em>Mature açaí palm trees from the Brazilian Amazonian jungle.</em></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/lego.jpg" class="img-fluid figure-img" width="550"></p>
<figcaption>Image by <a href="https://pixabay.com/users/eak_kkk-907811/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1044891"><em>Eak K.</em></a> via <a href="https://pixabay.com/photos/lego-toys-figurines-crowd-many-1044891/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<p>Note that the term population could be exchanged for the term <strong>system</strong>, given that certain contexts do not particularly refer to individuals or items. Instead, these contexts could refer to processes whose attributes are also governed by parameters. Examples of a system could be the following:</p>
<ul>
<li><em>The production of cellular phones from a given model in a set of manufacturing facilities.</em></li>
<li><em>The sale process in the Vancouver franchises of a well-known ice cream parlour.</em></li>
<li><em>The transit cycle during rush hours on weekdays in the twelve lines of Mexico City’s subway.</em></li>
</ul>
</div>
</div>
<div id="Definition-parameter" class="definition">
<div class="definition-header">
<p>Definition of parameter</p>
</div>
<div class="definition-container">
<p>It is a characteristic (<strong>numerical</strong> or even <strong>non-numerical</strong>, such as a <strong>distinctive category</strong>) that <strong>summarizes</strong> the state of our population or system of interest. Examples of a population parameter can be described as follows:</p>
<ul>
<li><em>The average weight of children between the ages of 5 and 10 years old in states of the American west coast (<strong>numerical</strong>).</em></li>
<li><em>The variability in the height of the mature açaí palm trees from the Brazilian Amazonian jungle (<strong>numerical</strong>).</em></li>
<li><em>The proportion of defective items in the production of cellular phones in a set of manufacturing facilities (<strong>numerical</strong>).</em></li>
<li><em>The average customer waiting time to get their order in the Vancouver franchises of a well-known ice cream parlour (<strong>numerical</strong>).</em></li>
<li><em>The most favourite pizza topping of vegetarian adults between the ages of 30 and 40 years old in Edmonton (<strong>non-numerical</strong>).</em></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/typewriter.jpg" class="img-fluid figure-img" width="450"></p>
<figcaption>Image by <a href="https://pixabay.com/users/meineresterampe-26089/"><em>meineresterampe</em></a> via <a href="https://pixabay.com/photos/typewriter-old-retro-office-1899760/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<p>Note the <strong>standard mathematical notation</strong> for population parameters are <strong>Greek letters</strong> (for more insights, you can check <a href="B-greek-alphabet.html" class="quarto-xref"><span>Appendix B</span></a>). Moreover, in practice, these population parameter(s) of interest will be <strong>unknown</strong> to the data scientist or researcher. Instead, they would use formal statistical inference to <strong>estimate</strong> them.</p>
</div>
</div>
<p>The parameter <a href="#Definition-parameter">definition</a> points out a crucial fact in investigating any given population or system:</p>
<blockquote class="blockquote">
<p><strong>Our parameter(s) of interest are usually <em>unknown</em>!</strong></p>
</blockquote>
<p>Given this fact, it would be pretty unfortunate and inconvenient if we eventually wanted to discover any significant insights about the population or system. Therefore, let us proceed to our so-called tasty example so we can dive into the need for statistical inference and why probability is our perfect ally in this parameter quest.</p>
<p>Imagine you are the owner of a large fleet of ice cream carts, around 900 to be exact. These ice cream carts operate across different parks in the following Canadian cities: <em>Vancouver</em>, <em>Victoria</em>, <em>Edmonton</em>, <em>Calgary</em>, <em>Winnipeg</em>, <em>Ottawa</em>, <em>Toronto</em>, and <em>Montréal</em>. In the past, to optimize operational costs, you decided to limit ice cream cones to only two items: <em>vanilla</em> and <em>chocolate</em> flavours, as in <a href="#fig-ice-cream" class="quarto-xref">Figure&nbsp;<span>2.1</span></a>.</p>
<div id="fig-ice-cream" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ice-cream-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/ice-cream.jpg" class="img-fluid figure-img" width="390">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ice-cream-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.1: The two flavours of the ice cream cone you sell across all your ice cream carts: <em>vanilla</em> and <em>chocolate</em>. Image by <a href="https://pixabay.com/users/tomekwalecki-13027968/"><em>tomekwalecki</em></a> via <a href="https://pixabay.com/photos/ice-cream-flavor-chocolate-vanilla-4401300/"><em>Pixabay</em></a>.
</figcaption></figure>
</div>
<p>Now, let us direct this whole case onto a more statistical and probabilistic field; suppose you have a well-defined overall population of interest for those above eight Canadian cities: <strong>children between 4 and 11 years old attending these parks during the Summer weekends</strong>. Of course, Summer time is coming this year, and you would like to know <strong>which ice cream cone flavour is the favourite one</strong> for this population (<em>and by how much!</em>). As a business owner, investigating ice cream flavour preferences would allow you to plan Summer restocks more carefully with your corresponding suppliers. Therefore, it would be essential to start collecting consumer data so the company can tackle this <strong>demand query</strong>.</p>
<p>Also, suppose there is a second query. For the sake of our case, we will call it a <strong>time query</strong>. As a critical component of demand planning, besides estimating which cone flavour is the most preferred one (<em>and by how much!</em>) for the above population of interest, the operations area is currently requiring a realistic estimation of <strong>the average waiting time from one customer to the next one in any given cart during Summer weekends</strong>. This average waiting time would allow the operations team to plan carefully how much stock each cart should have so there will not be any waste or shortage.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/clock.jpg" class="img-fluid figure-img" width="500"></p>
<figcaption>Image by <a href="https://unsplash.com/@icons8?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash"><em>Icons8 Team</em></a> via <a href="https://unsplash.com/photos/silver-bell-alarm-clock-dhZtNlvNE8M"><em>Unsplash</em></a>.</figcaption></figure>
</div>
<p>Note that the <strong>time query</strong> is related to a different population from the previous query. Therefore, we can define it as <strong>all our ice cream customers during the Summer weekends</strong> and <strong>not just</strong> all the children between 4 and 11 years old attending the parks during Summer weekends. Consequently, it is crucial to note that the nature of our queries will dictate how we define our population and our subsequent data modelling and statistical inference.</p>
<p>Summer time represents the most profitable season from a business perspective, thus solving these above two queries is a significant priority for your company. Hence, you decide to organize a meeting with your eight general managers (one per Canadian city). Finally, during the meeting with the general managers, it was decided to do the following:</p>
<ol type="1">
<li>For the <strong>demand query</strong>, a comprehensive market study will be run on the population of interest across the eight Canadian cities right before next Summer; suppose we are currently in Spring.</li>
<li>For the <strong>time query</strong>, since the operations team has not previously recorded any historical data (surprisingly!), <strong>all vendor staff</strong> from the 900 carts will start collecting data on <strong>the waiting time in seconds</strong> between each customer this upcoming Summer.</li>
</ol>
<p>When discussing study requirements for the marketing firm who would be in charge of it for the <strong>demand query</strong>, <em>Vancouver’s general manager</em> dares to state the following:</p>
<blockquote class="blockquote">
<p><em>Since we’re already planning to collect consumer data on these cities, let’s mimic a census-type study to ensure we can have the <strong>most precise</strong> results on their preferences.</em></p>
</blockquote>
<p>On the other hand, when agreeing on the specific operations protocol to start recording waiting times for all the 900 vending carts this upcoming Summer, <em>Ottawa’s general manager</em> provides a comment for further statistical food for thought:</p>
<blockquote class="blockquote">
<p><em>The operations protocol for recording waiting times in the 900 vending carts looks too cumbersome to implement straightforwardly this upcoming Summer. Why don’t we select <strong>a smaller set</strong> of waiting times between two general customers across the 900 ice cream carts in the eight cities to have a more efficient process implementation that would allow us to optimize operational costs?</em></p>
</blockquote>
<p>Bingo! <em>Ottawa’s general manager</em> just nailed the probabilistic way of making inference on our population parameter of interest for the <strong>time query</strong>. Indeed, their comment was primarily framed from a business perspective of optimizing operational costs. Still, this fact does not take away a crucial insight on which statistical inference is built: a <strong>random sample</strong> (as in its <a href="#Definition-random-sample">corresponding definition</a>). As for <em>Vancouver’s general manager</em>, their proposal is <strong>not feasible</strong>. Mimicking a census-type study might not be the most optimal decision for the <strong>demand query</strong> given the time constraint and the potential size of its target population.</p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on the use random sampling with probabilistic foundations!</p>
</div>
<div class="Heads-up-container">
<p>Let us clarify things from the start, especially from a statistical perspective:</p>
<blockquote class="blockquote">
<p><strong>Realistically, there is no cheap and efficient way to conduct a census-type study for either of the two queries.</strong></p>
</blockquote>
<p>We must rely on probabilistic <strong>random sampling</strong>, selecting two small subsets of individuals from our two populations of interest. This approach allows us to save both financial and operational resources compared to conducting a complete census. However, random sampling requires us to use various probabilistic and inferential tools to manage and report the <strong>uncertainty</strong> associated with the <strong>estimation</strong> of the corresponding population parameters, which will help us answer our initial main queries.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/sample-piece.png" class="img-fluid figure-img" width="350"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/"><em>manfredsteger</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-pixel-group-3702061/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<p>Therefore, having said all this, let us assume that in this ice cream case, the company decided to go ahead with random sampling to answer both queries.</p>
</div>
</div>
<p>Moving on to one of the core topics in this chapter, we can state that probability is viewed as the language to decode random phenomena that occur in any given population or system of interest. In our example, we have two random phenomena:</p>
<ol type="1">
<li>For the <strong>demand query</strong>, a phenomenon can be represented by the preferred ice cream cone flavour of <strong>any randomly selected child between 4 and 11 years old attending the parks of the above eight Canadian cities during the Summer weekends</strong>.</li>
<li>Regarding the <strong>time query</strong>, a phenomenon of this kind can be represented by <strong>any randomly recorded waiting time between two customers during a Summer weekend in any of the above eight Canadian cities across the 900 ice cream carts</strong>.</li>
</ol>
<p>Now, let us finally define what we mean by probability along with the inherent concept of sample space.</p>
<div id="Definition-probability" class="definition">
<div class="definition-header">
<p>Definition of probability</p>
</div>
<div class="definition-container">
<p>Let <span class="math inline">\(A\)</span> be an event of interest in a random phenomenon of a population or system of interest, whose all possible outcomes belong to a given sample space <span class="math inline">\(S\)</span>. Generally, the probability for this event <span class="math inline">\(A\)</span> happening can be mathematically depicted as <span class="math inline">\(P(A)\)</span>. Moreover, suppose we observe the random phenomenon <span class="math inline">\(n\)</span> times such as we were running some class of experiment, then <span class="math inline">\(P(A)\)</span> is defined as the following ratio:</p>
<p><span id="eq-probability"><span class="math display">\[
P(A) = \frac{\text{Number of times event $A$ is observed}}{n},
\tag{2.1}\]</span></span></p>
<p><strong>as the <span class="math inline">\(n\)</span> times we observe the random phenomenon goes to infinity</strong>.</p>
<p><a href="#eq-probability" class="quarto-xref">Equation&nbsp;<span>2.1</span></a> will always put <span class="math inline">\(P(A)\)</span> in the following numerical range:</p>
<p><span class="math display">\[
0 \leq P(A) \leq 1.
\]</span></p>
</div>
</div>
<div id="Definition-sample-space" class="definition">
<div class="definition-header">
<p>Definition of sample space</p>
</div>
<div class="definition-container">
<p>Let <span class="math inline">\(A\)</span> be an event of interest in a random phenomenon of a population or system of interest. The sample space <span class="math inline">\(S\)</span> of event <span class="math inline">\(A\)</span> denotes the set of all the possible <strong>random outcomes</strong> we might encounter every time we randomly observe <span class="math inline">\(A\)</span> such as we were running some class of experiment.<br></p>
<p>Note each of these outcomes has a determined probability associated with them. If we add up all these probabilities, the probability of the sample space <span class="math inline">\(S\)</span> will be one, i.e.,</p>
<p><span id="eq-sample-space"><span class="math display">\[
P(S) = 1.
\tag{2.2}\]</span></span></p>
</div>
</div>
</section><section id="sec-stats-schools" class="level3" data-number="2.1.2"><h3 data-number="2.1.2" class="anchored" data-anchor-id="sec-stats-schools">
<span class="header-section-number">2.1.2</span> Schools of Statistical Thinking</h3>
<p>Note the above <a href="#Definition-probability">definition</a> for the probability of an event <span class="math inline">\(A\)</span> specifically highlights the following:</p>
<blockquote class="blockquote">
<p><strong>… as the <span class="math inline">\(n\)</span> times we observe the random phenomenon goes to infinity</strong>.</p>
</blockquote>
<p>The “<em>infinity</em>” term is key when it comes to understanding the philosophy behind the frequentist school of statistical thinking in contrast to its Bayesian counterpart. In general, the frequentist way of practicing statistics in terms of probability and inference is the approach we usually learn in introductory courses, more specifically when it comes to hypothesis testing and confidence intervals which will be explored in <a href="#sec-basics-inf" class="quarto-xref"><span>Section 2.3</span></a>. That said, the Bayesian approach is another way of practicing statistical inference. Its philosophy differs in what information is used to infer our population parameters of interest. Below, we briefly define both schools of thinking.</p>
<div id="Definition-frequentist-stats" class="definition">
<div class="definition-header">
<p>Definition of frequentist statistics</p>
</div>
<div class="definition-container">
<p>This statistical school of thinking heavily relies on the <strong>frequency of events</strong> to estimate specific parameters of interest in a population or system. This frequency of events is reflected in the repetition of <span class="math inline">\(n\)</span> experiments involving a random phenomenon within this population or system.</p>
<p>Under the umbrella of this approach, we assume that our governing parameters are <strong>fixed</strong>. Note that, within the philosophy of this school of thinking, we can only make <strong>precise</strong> and <strong>accurate</strong> predictions as long as we repeat our <span class="math inline">\(n\)</span> experiments as many times as possible, i.e.,</p>
<p><span class="math display">\[
n \rightarrow \infty.
\]</span></p>
</div>
</div>
<div id="Definition-bayesian-stats" class="definition">
<div class="definition-header">
<p>Definition of Bayesian statistics</p>
</div>
<div class="definition-container">
<p>This statistical school of thinking also relies on the <strong>frequency of events</strong> to estimate specific parameters of interest in a population or system. Nevertheless, unlike frequentist statistics, Bayesian statisticians use <strong>prior knowledge</strong> on the population parameters to update their estimations on them along with the <strong>current evidence</strong> they can gather. This evidence is in the form of the repetition of <span class="math inline">\(n\)</span> experiments involving a random phenomenon. All these ingredients allow Bayesian statisticians to make inference by conducting appropriate hypothesis testings, which are designed differently from their mainstream frequentist counterpart.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/thomas-bayes.jpg" class="img-fluid figure-img" width="300"></p>
<figcaption>The unique known portrait of Reverend Thomas Bayes according to <span class="citation" data-cites="odonnell1936">O’Donnell, T. (<a href="references.html#ref-odonnell1936" role="doc-biblioref">1936</a>)</span>, even though <span class="citation" data-cites="bellhouse2004">Bellhouse (<a href="references.html#ref-bellhouse2004" role="doc-biblioref">2004</a>)</span> argues it might not be a Bayes’ portrait.</figcaption></figure>
</div>
<p>Under the umbrella of this approach, we assume that our governing parameters are <strong>random</strong>; i.e., they have their own sample space and probabilities associated to their corresponding outcomes. The statistical process of inference is heavily backed up by probability theory mostly in the form of the Bayes’ rule (named after Reverend Thomas Bayes, an English statistician from the 18th century). This rule uses our <strong>current evidence</strong> along with our <strong>prior beliefs</strong> to deliver a <strong>posterior distribution</strong> of our <strong>random</strong> parameter(s) of interest.</p>
</div>
</div>
<p>Let us put the definitions for these two schools of statistical thinking into a more concrete example. We can use the <strong>demand query</strong> from our ice cream case as a starting point. More concretely, we can dig more into a standalone population parameter such as the probability that a randomly selected child between 4 and 11 years old, attending the parks of the above eight Canadian cities during the Summer weekends, prefers the chocolate-flavoured ice cream cone over the vanilla one. Think about the following two hypothetical questions:</p>
<ol type="a">
<li>From a frequentist point of view, what is the <strong>estimated</strong> probability of <strong>preferring chocolate over vanilla</strong> after <strong>randomly</strong> surveying <span class="math inline">\(n = 100\)</span> children from our population of interest?</li>
<li>Using a Bayesian approach, suppose the marketing team has found ten <strong>prior</strong> market studies on similar children populations on their preferred ice cream flavour (between chocolate and vanilla). Therefore, along with our actual <strong>random</strong> survey of <span class="math inline">\(n = 100\)</span> children from our population of interest, what is the <strong>posterior estimation</strong> of the probability of <strong>preferring chocolate over vanilla</strong>?</li>
</ol>
<p>By comparing the above (a) and (b), we can see one characteristic in common when it comes to the estimation of the probability of preferring chocolate over vanilla: both frequentist and Bayesian approaches rely on the gathered <strong>evidence</strong> coming from the random survey of <span class="math inline">\(n = 100\)</span> children from our population of interest. On the one hand, the frequentist approach solely relies on <strong>observed data</strong> to <strong>estimate</strong> this single probability of preferring chocolate over vanilla. On the other hand, the Bayesian approach uses the observed data in conjunction with the <strong>prior</strong> knowledge provided by the ten estimated probabilities to deliver a whole posterior distribution (i.e., the <strong>posterior estimation</strong>) of the probability of preferring chocolate over vanilla.</p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on the debate between frequentist and Bayesian statistics!</p>
</div>
<div class="Heads-up-container">
<p>Even though most of us began our statistical journey in a frequentist framework, we might be tempted to state that a Bayesian paradigm for parameter estimation and inference is better than a frequentist one since the former only takes into account the observed evidence without the prior knowledge on our parameters of interest.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/argument.png" class="img-fluid figure-img" width="300"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3699345"><em>Manfred Steger</em></a> via <a href="https://pixabay.com/vectors/pixel-pixel-cells-pedagogy-3699345/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<p>In the statistical community, there could be a fascinating debate between the pros and cons of each school of thinking. That said, it is crucial to state that no paradigm is considered wrong! Instead, using a pragmatic strategy of performing statistics according to our specific context is more convenient.</p>
</div>
</div>
<div class="Tip">
<div class="Tip-header">
<p>Tip on further Bayesian and frequentist insights!</p>
</div>
<div class="Tip-container">
<p>Let us check the following two examples (aside from our ice cream case) to illustrate the above pragmatic way of doing things:</p>
<ol type="1">
<li>Take the production of cellular phones from a given model in a set of manufacturing facilities as the context. Hence, one might find a frequentist estimation of the proportion of defective items as a quicker and more efficient way to correct any given manufacturing process. That is, we will sample products from our finalized batches and check their status (<em>defective</em> or <em>non-defective</em>, our <strong>observed evidence</strong>) to deliver a proportion estimation of defective items.</li>
<li>Now, take a physician’s context. It would not make a lot of sense to study the probability that a patient develops a certain disease by only using a frequentist approach, i.e., looking at the current symptoms which account for the <strong>observed evidence</strong>. In lieu, a Bayesian approach would be more suitable to study this probability which uses the observed evidence combined with the patient’s history (i.e., the <strong>prior knowledge</strong>) to deliver our <strong>posterior belief</strong> on the disease probability.</li>
</ol>
</div>
</div>
<p>Having said all this, it is important to reiterate that the focus of this textbook is <strong>purely frequentist</strong> in regards to data modelling in regression analysis. If you would like to explore the fundamentals of the Bayesian paradigm; <span class="citation" data-cites="johnson2022">Johnson, Ott, and Dogucu (<a href="references.html#ref-johnson2022" role="doc-biblioref">2022</a>)</span> have developed an amazing textbook on the basic probability theory behind this school of statistical thinking along with a whole variety regression techniques including the parameter estimation rationale.</p>
</section><section id="sec-random-variables" class="level3" data-number="2.1.3"><h3 data-number="2.1.3" class="anchored" data-anchor-id="sec-random-variables">
<span class="header-section-number">2.1.3</span> The Random Variables</h3>
<p>As we continue our frequentist quest to review the probabilistic insights related to parameter estimation and statistical inference, we will focus on our ice cream case while providing a comprehensive array of definitions. Many of these definitions are inspired by the work of <span class="citation" data-cites="casella2024">Casella and Berger (<a href="references.html#ref-casella2024" role="doc-biblioref">2024</a>)</span> and <span class="citation" data-cites="soch2023">Soch et al. (<a href="references.html#ref-soch2023" role="doc-biblioref">2024</a>)</span>.</p>
<p>Each time we introduce a new probabilistic or statistical concept, we will apply it immediately to this ice cream case, allowing for hands-on practice that meets the learning objectives of this chapter. It is important to pay close attention to the <strong>definition</strong> and <strong>heads-up</strong> admonitions, as they are essential for fully understanding how these concepts apply to the ice cream case. On the other hand, the <strong>tip</strong> admonitions are designed to offer additional theoretical insights that may interest you, but they can be skipped if you prefer.</p>
<div id="tbl-queries-1" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-queries-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2.1: Table containing the corresponding insights to solve our <em>demand</em> and <em>time queries</em>.
</figcaption><div aria-describedby="tbl-queries-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead><tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;"><strong>Demand Query</strong></th>
<th style="text-align: center;"><strong>Time Query</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>Statement</strong></td>
<td style="text-align: center;">We would like to know <strong>which</strong> ice cream flavour is the favourite one (either <strong>chocolate</strong> or <strong>vanilla</strong>) and by <strong>how much</strong>.</td>
<td style="text-align: center;">We would like to know the <strong>average</strong> waiting time from one customer to the next one in any given ice cream cart.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Population of interest</strong></td>
<td style="text-align: center;">
<strong>Children between 4 and 11 years old</strong> attending different parks in Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during Summer weekends.</td>
<td style="text-align: center;">
<strong>All our general customer-to-customer waiting times</strong> in the different parks of Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during Summer weekends across the 900 ice cream carts.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Parameter</strong></td>
<td style="text-align: center;">
<strong>Proportion</strong> of individuals from the population of interest <strong>who prefer the chocolate flavour versus the vanilla flavour</strong>.</td>
<td style="text-align: center;">
<strong>Average</strong> waiting time from one customer to the next one.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#tbl-queries-1" class="quarto-xref">Table&nbsp;<span>2.1</span></a> presents the general statements and populations of interest derived from our two queries: <strong>demand</strong> and <strong>time</strong>. It is important to note that these general statements are based on the storytelling we initiated in <a href="#sec-first-insights" class="quarto-xref"><span>Section 2.1.1</span></a>. In practice, summarizing the overarching statistical problem is essential. This will enable us to translate the corresponding issue into a specific statement and population, from which we can define the parameters we aim to estimate later in our statistical process.</p>
<p>Now, recall that in our initial meeting with the general managers, <em>Ottawa’s general manager</em> provided valuable statistical insights regarding the foundation of a random sample. For the <strong>time query</strong>, they suggested selecting a smaller set of waiting times between two general customers across the 900 ice cream carts. We already addressed this process as sampling, more specifically random sampling in technical language.</p>
<p>Similarly, we can apply this concept to the <strong>demand query</strong> by selecting a subgroup of children aged 4 to 11 who are visiting different parks in these eight cities. Then, we can ask them about their favorite ice cream flavour, specifically whether they prefer <strong>chocolate</strong> or <strong>vanilla</strong>. It is important to note that we are not conducting any census-type studies; instead, we are carrying out two studies that heavily rely on sampling to estimate population parameters.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/board.png" class="img-fluid figure-img" width="500"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/"><em>Manfred Stege</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-teaching-to-learn-wiki-3976301/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<p>Furthermore, we want to ensure that our two groups of observations—both children and waiting times—are representative of their respective populations. So, how can we achieve this? The baseline key is through what we call <strong>simple random sampling</strong>. This process involves the following per query:</p>
<ol type="1">
<li>For the <strong>demand query</strong>, let us assume there are <span class="math inline">\(N_d\)</span> observations in our population of interest. In a <strong>simple random sampling scheme with replacement</strong>, our random sample will consist of <span class="math inline">\(n_d\)</span> observations (noting that <span class="math inline">\(n_d &lt;&lt; N_d\)</span>), each having the same probability of being selected for our estimation and inferential purposes, which is given by <span class="math inline">\(\frac{1}{N_d}\)</span>.</li>
<li>For the <strong>time query</strong>, assume there are <span class="math inline">\(N_t\)</span> observations in our population of interest. Again, in a <strong>simple random sampling scheme with replacement</strong>, our random sample will consist of <span class="math inline">\(n_t\)</span> observations (noting that <span class="math inline">\(n_t &lt;&lt; N_t\)</span>), each having the same probability of selection for estimation and inferential purposes, which is <span class="math inline">\(\frac{1}{N_t}\)</span>.</li>
</ol>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on sampling with replacement!</p>
</div>
<div class="Heads-up-container">
<p>Keep in mind that <strong>sampling with replacement</strong> means you return any specific drawn observation back to the corresponding population before the next draw.</p>
</div>
</div>
<div class="Tip">
<div class="Tip-header">
<p>Tip of further sampling techniques!</p>
</div>
<div class="Tip-container">
<p>If you want to explore additional and more complex sampling techniques besides simple random sampling, <a href="01-intro.html#sec-ds-workflow-data-collection" class="quarto-xref"><span>Section 1.2.2</span></a> provides further details and an external resource.</p>
</div>
</div>
<p>We can observe the concept of randomness reflected throughout the sampling schemes mentioned above. This aligns with what we referred to as <strong>random phenomena</strong> in both queries back in <a href="#sec-first-insights" class="quarto-xref"><span>Section 2.1.1</span></a>. Consequently, there should be a way to mathematically represent these phenomena, and the random variable is the starting point in this process.</p>
<div id="Definition-random-variable" class="definition">
<div class="definition-header">
<p>Definition of random variable</p>
</div>
<div class="definition-container">
<p>A random variable is a function where the input values correspond to real numbers assigned to events belonging to the sample space <span class="math inline">\(S\)</span>, and whose outcome is one of these real numbers after executing a given random experiment. For instance, a random variable (and its support, i.e., real numbers) is depicted with an uppercase such that</p>
<p><span class="math display">\[Y \in \mathbb{R}.\]</span></p>
</div>
</div>
<p>To begin experimenting with random variables in this ice cream case, we need to define them. It is important to be as clear as possible when defining random variables, and we should also remember to use uppercase letters as follows:</p>
<p><span class="math display">\[
\begin{align*}
D_i &amp;= \text{A favourite ice cream flavour of a randomly surveyed $i$th child} \\
&amp; \qquad \text{between 4 and 11 years old attending the parks of} \\
&amp; \qquad \text{Vancouver, Victoria, Edmonton, Calgary,} \\
&amp; \qquad \text{Winnipeg, Ottawa, Toronto, and Montréal} \\
&amp; \qquad \text{during the Summer weekends} \\
&amp; \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \text{for $i = 1, \dots, n_d.$}  \\
\\
T_j &amp;= \text{A randomly recorded $j$th waiting time in minutes between two} \\
&amp; \qquad \text{customers during a Summer weekend in any of the above} \\
&amp; \qquad \text{eight Canadian cities across the 900 ice cream carts} \\
&amp; \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \text{for $j = 1, \dots, n_t.$}  \\
\end{align*}
\]</span></p>
<p>Note that the <strong>demand query</strong> corresponds to the <span class="math inline">\(i\)</span>th random variable <span class="math inline">\(D_i\)</span>, where the subindex <span class="math inline">\(i\)</span> ranges from <span class="math inline">\(1\)</span> to <span class="math inline">\(n_d\)</span>. The term <span class="math inline">\(n_d\)</span> represents the <strong>sample size</strong> for this query and theoretically indicates the number of random variables we intend to observe from our population of interest during our sampling. On the other hand, for the <strong>time query</strong>, we have the <span class="math inline">\(j\)</span>th random variable <span class="math inline">\(T_j\)</span>, with the subindex <span class="math inline">\(j\)</span> ranging from <span class="math inline">\(1\)</span> to <span class="math inline">\(n_t\)</span>. In the context of this query, <span class="math inline">\(n_t\)</span> denotes the <strong>sample size</strong> and indicates how many random variables we plan to observe from our population of interest as part of our sampling.</p>
<p>Now, <span class="math inline">\(D_i\)</span> will require real numbers that correspond to potential outcomes derived from the specific <strong>demand sample space</strong> of ice cream flavour. It is crucial to note that a given child from our population may prefer a flavour other than chocolate or vanilla—for example, strawberry, salted caramel, or pistachio. However, we are limited by our available flavour menu as a company. Therefore, we will restrict our survey question regarding these potential <span class="math inline">\(n_d\)</span> surveyed children as follows:</p>
<p><span id="eq-random-variable-demand"><span class="math display">\[
d_i =
\begin{cases}
1 \qquad \text{The surveyed child prefers chocolate.}\\
0 \qquad \text{Otherwise.}
\end{cases}
\tag{2.3}\]</span></span></p>
<p>In the modelling associated with <a href="#eq-random-variable-demand" class="quarto-xref">Equation&nbsp;<span>2.3</span></a>, an <strong>observed random variable</strong> <span class="math inline">\(d_i\)</span> <strong>(thus, the lowercase)</strong> can only yield values of <span class="math inline">\(1\)</span> if the surveyed child prefers chocolate and <span class="math inline">\(0\)</span> <em>otherwise</em>. The term “<em>otherwise</em>” refers to any flavour other than chocolate, which, in our limited menu context, is vanilla.</p>
<p>To define the real numbers from a given <strong>waiting time sample space</strong>, associated with an <strong>observed random variable</strong> <span class="math inline">\(t_j\)</span> <strong>(thus, the lowercase)</strong> measured in minutes, we need to establish a possible range for these waiting times. It would not make sense to have observed negative waiting times in this ice cream scenario; therefore, our lower bound for this range of potential values should be <span class="math inline">\(0\)</span> minutes. However, we cannot set an upper limit on these waiting times since any ice cream vendor might need to wait for <span class="math inline">\(1, 2, 3, \ldots, 10, \ldots, 20, \ldots, 60, \ldots\)</span> minutes for the next customer to arrive. In fact, it is possible to wait for a very long time, especially on a low sales day! Thus, the range of this observed random variable can be expressed as:</p>
<p><span class="math display">\[
t_j \in [0, \infty),
\]</span></p>
<p>where the <span class="math inline">\(\infty\)</span> symbol indicates <strong>no upper bound</strong>.</p>
<p>After defining the possible values for our two random variables <span class="math inline">\(D_i\)</span> and <span class="math inline">\(T_j\)</span>, we will now classify them correctly using further probabilistic definitions as shown below.</p>
<div id="Definition-discrete-random-variables" class="definition">
<div class="definition-header">
<p>Definition of discrete random variable</p>
</div>
<div class="definition-container">
<p>Let <span class="math inline">\(Y\)</span> be a random variable whose support is <span class="math inline">\(\mathcal{Y}\)</span>. If this support <span class="math inline">\(\mathcal{Y}\)</span> corresponds to a finite set or a countably infinite set of possible values, then <span class="math inline">\(Y\)</span> is considered a discrete random variable.</p>
<p>For instance, we can encounter discrete random variables which could be classified as</p>
<ul>
<li>
<strong>binary</strong> (i.e., a finite set of two possible values),</li>
<li>
<strong>categorical</strong> (either <strong>nominal</strong> or <strong>ordinal</strong>, which have a finite set of three or more possible values), or</li>
<li>
<strong>counts</strong> (which might have a finite set or a countably infinite set of possible values as integers).</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/abacus.jpg" class="img-fluid figure-img" width="500"></p>
<figcaption>Image by <a href="https://pixabay.com/users/pexels-2286921/"><em>Pexels</em></a> via <a href="https://pixabay.com/photos/abacus-classroom-count-counter-1866497/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
</div>
</div>
<div id="Definition-continuous-random-variables" class="definition">
<div class="definition-header">
<p>Definition of continuous random variable</p>
</div>
<div class="definition-container">
<p>Let <span class="math inline">\(Y\)</span> be a random variable whose support is <span class="math inline">\(\mathcal{Y}\)</span>. If this support <span class="math inline">\(\mathcal{Y}\)</span> corresponds to an uncountably infinite set of possible values, then <span class="math inline">\(Y\)</span> is considered a continuous random variable.</p>
<p>Note a continuous random variable could be</p>
<ul>
<li>
<strong>completely unbounded</strong> (i.e., its set of possible values goes from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span> as in <span class="math inline">\(-\infty &lt; y &lt; \infty\)</span>),</li>
<li>
<strong>positively unbounded</strong> (i.e., its set of possible values goes from <span class="math inline">\(0\)</span> to <span class="math inline">\(\infty\)</span> as in <span class="math inline">\(0 \leq y &lt; \infty\)</span>),</li>
<li>
<strong>negatively unbounded</strong> (i.e., its set of possible values goes from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(0\)</span> as in <span class="math inline">\(-\infty &lt; y \leq 0\)</span>), or</li>
<li>
<strong>bounded</strong> between two values <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> (i.e., its set of possible values goes from <span class="math inline">\(a\)</span> to <span class="math inline">\(b\)</span> as in <span class="math inline">\(a \leq y \leq b\)</span>).</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/measure.jpg" class="img-fluid figure-img" width="500"></p>
<figcaption>Image by <a href="https://pixabay.com/users/arielrobin-2483349/"><em>arielrobin</em></a> via <a href="https://pixabay.com/photos/measure-yardstick-tape-ruler-1509707/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
</div>
</div>
<p>Therefore, we can classify our two random variables as follows:</p>
<ul>
<li>For the <strong>demand query</strong>, the support of <span class="math inline">\(D_i\)</span> (denoted as <span class="math inline">\(\mathcal{D}\)</span>) is a countable finite set with two possible values: <span class="math inline">\(d_i \in \{0, 1\}\)</span>, as noted by <a href="#eq-random-variable-demand" class="quarto-xref">Equation&nbsp;<span>2.3</span></a>. Therefore, <span class="math inline">\(D_i\)</span> is categorized as a <strong>binary discrete random variable</strong>.</li>
<li>For the <strong>time query</strong>, the support of <span class="math inline">\(T_j\)</span> (denoted as <span class="math inline">\(\mathcal{T}\)</span>) is positively unbounded. This results in an uncountably infinite set of values that <span class="math inline">\(T_j\)</span> can take, including (but not limited to) <span class="math inline">\(0, \dots, 0.01, \ldots, 0.02, \ldots, 0.00234, \ldots, 1, \ldots, 1.5576, \ldots\)</span> minutes. Therefore, <span class="math inline">\(T_j\)</span> is classified as a <strong>positively unbounded continuous random variable</strong>.</li>
</ul>
<p>So far, we have successfully translated our two statistical queries into proper random variables, along with clear definitions and classifications derived from our problem statements, as well as the populations of interest, as noted in <a href="#tbl-queries-1" class="quarto-xref">Table&nbsp;<span>2.1</span></a>. However, we still need to find a way to include our parameters. The upcoming section will allow us to do that.</p>
</section><section id="sec-wonders-random-variables" class="level3" data-number="2.1.4"><h3 data-number="2.1.4" class="anchored" data-anchor-id="sec-wonders-random-variables">
<span class="header-section-number">2.1.4</span> The Wonders of Generative Modelling and Probability Distributions</h3>
<p>Before exploring the wonders of generative models, let us introduce <a href="#tbl-queries-2" class="quarto-xref">Table&nbsp;<span>2.2</span></a>, an extension of <a href="#tbl-queries-1" class="quarto-xref">Table&nbsp;<span>2.1</span></a> that now includes the elements discussed in <a href="#sec-random-variables" class="quarto-xref"><span>Section 2.1.3</span></a>.</p>
<div id="tbl-queries-2" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-queries-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2.2: Table containing the corresponding insights to solve our <em>demand</em> and <em>time queries</em>.
</figcaption><div aria-describedby="tbl-queries-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead><tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;"><strong>Demand Query</strong></th>
<th style="text-align: center;"><strong>Time Query</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>Statement</strong></td>
<td style="text-align: center;">We would like to know <strong>which</strong> ice cream flavour is the favourite one (either <strong>chocolate</strong> or <strong>vanilla</strong>) and by <strong>how much</strong>.</td>
<td style="text-align: center;">We would like to know the <strong>average</strong> waiting time from one customer to the next one in any given ice cream cart.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Population of interest</strong></td>
<td style="text-align: center;">
<strong>Children between 4 and 11 years old</strong> attending different parks in Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during Summer weekends.</td>
<td style="text-align: center;">
<strong>All our general customer-to-customer waiting times</strong> in the different parks of Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during Summer weekends across the 900 ice cream carts.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Parameter</strong></td>
<td style="text-align: center;">
<strong>Proportion</strong> of individuals from the population of interest <strong>who prefer the chocolate flavour versus the vanilla flavour</strong>.</td>
<td style="text-align: center;">
<strong>Average</strong> waiting time from one customer to the next one.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Random variable</strong></td>
<td style="text-align: center;">
<span class="math inline">\(D_i\)</span> for <span class="math inline">\(i = 1, \dots, n_d\)</span>.</td>
<td style="text-align: center;">
<span class="math inline">\(T_j\)</span> for <span class="math inline">\(j = 1, \dots, n_t\)</span>.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Random variable definition</strong></td>
<td style="text-align: center;">A favourite ice cream flavour of a randomly surveyed <span class="math inline">\(i\)</span>th child between 4 and 11 years old attending the parks of Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during the Summer weekends.</td>
<td style="text-align: center;">A randomly recorded <span class="math inline">\(j\)</span>th waiting time in minutes between two customers during a Summer weekend across the 900 ice cream carts found in Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Random variable type</strong></td>
<td style="text-align: center;">Discrete and binary.</td>
<td style="text-align: center;">Continuous and positively unbounded.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Random variable support</strong></td>
<td style="text-align: center;">
<span class="math inline">\(d_i \in \{ 0, 1\}\)</span> as in <a href="#eq-random-variable-demand" class="quarto-xref">Equation&nbsp;<span>2.3</span></a>.</td>
<td style="text-align: center;"><span class="math inline">\(t_j \in [0, \infty).\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Having summarized all our probabilistic elements in <a href="#tbl-queries-2" class="quarto-xref">Table&nbsp;<span>2.2</span></a>, the parameters of interest must come into play for our data modelling game. Hence, the question is:</p>
<blockquote class="blockquote">
<p><strong>Is there any feasible way to do so via the foundations of random variables?</strong></p>
</blockquote>
<p>The answer lies in what we call a generative model, for which we have a whole toolbox corresponding to another important concept called probability distributions, as shown below.</p>
<div id="Definition-generative-model" class="definition">
<div class="definition-header">
<p>Definition of generative model</p>
</div>
<div class="definition-container">
<p>Suppose you observe some data <span class="math inline">\(y\)</span> from a population or system of interest. Moreover, let us assume this population or system is governed by <span class="math inline">\(k\)</span> parameters contained in the following vector:</p>
<p><span class="math display">\[
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T.
\]</span></p>
<p>If we state that the random variable <span class="math inline">\(Y\)</span> follows certain probability distribution <span class="math inline">\(\mathcal{D}(\cdot),\)</span> then we will have a generative model <span class="math inline">\(m\)</span> such that</p>
<p><span class="math display">\[
\text{$m$: } Y \sim \mathcal{D}(\boldsymbol{\theta}).
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/generative-model.png" class="img-fluid figure-img" width="300"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/"><em>Manfred Stege</em></a> via <a href="https://pixabay.com/vectors/laptop-games-jigsaw-puzzle-puzzle-7426707/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
</div>
</div>
<div id="Definition-probability-distribution" class="definition">
<div class="definition-header">
<p>Definition of probability distribution</p>
</div>
<div class="definition-container">
<p>When we set a random variable <span class="math inline">\(Y\)</span>, we also set a new set of <span class="math inline">\(v\)</span> possible outcomes <span class="math inline">\(\mathcal{Y} = \{ y_1, \dots, y_v\}\)</span> coming from the sample space <span class="math inline">\(S\)</span>. This new set of possible outcomes <span class="math inline">\(\mathcal{Y}\)</span> corresponds to the support of the random variable <span class="math inline">\(Y\)</span> (i.e., all the possible values that could be taken on once we execute a given random experiment involving <span class="math inline">\(Y\)</span>).</p>
<p>That said, let us suppose we have a sample space of <span class="math inline">\(u\)</span> elements defined as</p>
<p><span class="math display">\[
S = \{ s_1, \dots, s_u \},
\]</span></p>
<p>where each one of these elements has a probability assigned via a function <span class="math inline">\(P_S(\cdot)\)</span> such that</p>
<p><span class="math display">\[
P(S) = \sum_{i = 1}^u P_S(s_i) = 1.
\]</span></p>
<p>which has to satisfy <a href="#eq-sample-space" class="quarto-xref">Equation&nbsp;<span>2.2</span></a>.</p>
<p>Then, the probability distribution of <span class="math inline">\(Y\)</span>, i.e., <span class="math inline">\(P_Y(\cdot)\)</span> assigns a probability to each <strong>observed value</strong> <span class="math inline">\(Y = y_j\)</span> (with <span class="math inline">\(j = 1, \dots, v\)</span>) if and only if the outcome of the random experiment belongs to the sample space, i.e., <span class="math inline">\(s_i \in S\)</span> (for <span class="math inline">\(i = 1, \dots, u\)</span>) such that <span class="math inline">\(Y(s_i) = y_j\)</span>:</p>
<p><span class="math display">\[
P_Y(Y = y_j) = P \left( \left\{ s_i \in S : Y(s_i) = y_j \right\} \right).
\]</span></p>
</div>
</div>
<p>Since we have two different queries, we will use two instances of generative models. It is worth noting that more complex modelling could refer to a single generative model. However, for the purposes of this review chapter, we will keep it simple with via two separate generative models.</p>
<p>Now, let us introduce a specific notation for our discussion: the <strong>Greek alphabet</strong>. Greek letters are frequently used to statistically represent population parameters in <strong>modelling setups</strong>, <strong>estimation</strong>, and <strong>statistical inference</strong>. These letters will be quite useful for our parameters in this ice cream case.</p>
<div class="Tip">
<div class="Tip-header">
<p>Tip on the Greek alphabet in statistics!</p>
</div>
<div class="Tip-container">
<p>In the early stages of learning statistical modelling, including concepts such as regression analysis, it is common to feel overwhelmed by unfamiliar letters and terminology. Whenever confusion arises in any of the main chapters of this book regarding these letters, we recommend referring to the Greek alphabet found in <a href="B-greek-alphabet.html" class="quarto-xref"><span>Appendix B</span></a>. It is important to note that frequentist statistical inference primarily uses lowercase letters. With consistent practice over time, you will likely memorize most of this alphabet.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/typewriter.jpg" class="img-fluid figure-img" width="500"></p>
<figcaption>Image by <a href="https://pixabay.com/users/meineresterampe-26089/"><em>meineresterampe</em></a> via <a href="https://pixabay.com/photos/typewriter-old-retro-office-1899760/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
</div>
</div>
<p>Let us retake the row corresponding to parameters in <a href="#tbl-queries-2" class="quarto-xref">Table&nbsp;<span>2.2</span></a> and assign their Greek letters:</p>
<ul>
<li>For the <strong>demand query</strong>, we are interested in the parameter <span class="math inline">\(\pi\)</span>, which represents the proportion of individuals from the children population who prefer the chocolate flavour over the vanilla flavour. It is crucial to note that a proportion is always bounded between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, similar to how probabilities function. For instance, a proportion of <span class="math inline">\(0.2\)</span> would mean that <span class="math inline">\(20\%\)</span> of the children in our population prefer chocolate flavour over vanilla. This definition establishes our demand query parameter as follows:</li>
</ul>
<p><span class="math display">\[
\pi \in [0, 1].
\]</span></p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on the use of <span class="math inline">\(\pi\)</span>!</p>
</div>
<div class="Heads-up-container">
<p>In this textbook, unless stated otherwise, the letter <span class="math inline">\(\pi\)</span> will denote a population parameter and not the mathematical constant <span class="math inline">\(3.141592...\)</span></p>
</div>
</div>
<ul>
<li>For the <strong>time query</strong>, we are interested in the parameter <span class="math inline">\(\beta\)</span>, which represents the average waiting time in minutes from one customer to the next one in our population of interest. Unlike the above <span class="math inline">\(\pi\)</span> parameter, <span class="math inline">\(\beta\)</span> is only positively unbounded given the definition of our random variable <span class="math inline">\(T_j\)</span>. Therefore, this definition establishes our time query parameter as follows:</li>
</ul>
<p><span class="math display">\[
\beta \in (0, \infty).
\]</span></p>
<p>Having defined our parameters of interest with proper lowercase Greek letters, it is time to declare our corresponding generative models on a general basis. For the <strong>demand query</strong>, there will be a single parameter called <span class="math inline">\(\pi\)</span>, where the randomly surveyed child <span class="math inline">\(D_i\)</span> will follow the model <span class="math inline">\(m_D\)</span> such that</p>
<p><span id="eq-gen-model-demand"><span class="math display">\[
\begin{gather*}
m_D : D_i \sim \mathcal{D}_D(\pi) \\
\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \text{for $i = 1, \dots, n_d.$}
\end{gather*}
\tag{2.4}\]</span></span></p>
<p>Now, for the <strong>time query</strong>, there will also be a single parameter called <span class="math inline">\(\beta\)</span>. Thus, the randomly recorded waiting time <span class="math inline">\(T_j\)</span> will follow the model <span class="math inline">\(m_T\)</span> such that</p>
<p><span id="eq-gen-model-time"><span class="math display">\[
\begin{gather*}
m_T : T_j \sim \mathcal{D}_T(\beta) \\
\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \text{for $j = 1, \dots, n_t.$}
\end{gather*}
\tag{2.5}\]</span></span></p>
<p>Nonetheless, we might wonder the following:</p>
<blockquote class="blockquote">
<p><strong>How can we determine the corresponding distributions <span class="math inline">\(\mathcal{D}_D(\pi)\)</span> and <span class="math inline">\(\mathcal{D}_T(\beta)\)</span>?</strong></p>
</blockquote>
<p>Of course the above <a href="#Definition-probability-distribution">definition</a> of a probability distribution will come in handy to resolve this question. That said, given that we have two types of random variables (discrete and continuous), it is necessary to introduce two specific types of probability functions: probability mass function (PMF) and probability density function (PDF).</p>
<div id="Definition-probability-mass-function" class="definition">
<div class="definition-header">
<p>Definition of probability mass function (PMF)</p>
</div>
<div class="definition-container">
<p>Let <span class="math inline">\(Y\)</span> be a discrete random variable whose support is <span class="math inline">\(\mathcal{Y}\)</span>. Moreover, suppose that <span class="math inline">\(Y\)</span> has a probability distribution such that</p>
<p><span class="math display">\[
P_Y(Y = y) : \mathbb{R} \rightarrow [0, 1]
\]</span></p>
<p>where, for all <span class="math inline">\(y \notin \mathcal{Y}\)</span>, we have</p>
<p><span class="math display">\[
P_Y(Y = y) = 0
\]</span></p>
<p>and</p>
<p><span id="eq-sample-space-PMF"><span class="math display">\[
\sum_{y \in \mathcal{Y}} P_Y(Y = y) = 1.
\tag{2.6}\]</span></span></p>
<p>Then, <span class="math inline">\(P_Y(Y = y)\)</span> is considered a PMF.</p>
</div>
</div>
<p>As we have discussed throughout this ice cream case, let us begin with the <strong>demand query</strong>. We have already defined the <span class="math inline">\(i\)</span>th random variable <span class="math inline">\(D_i\)</span> as discrete and binary. In statistical literature, certain random variables in common random processes can be modelled using what we call <strong>parametric families</strong>. We refer to these tools as parametric families because they are characterized by a specific set of parameters (in our case, each query has a single-element set, such as <span class="math inline">\(\pi\)</span> or <span class="math inline">\(\beta\)</span>).</p>
<p>Moreover, we call them families since each member corresponds to a particular value of our parameter(s). For instance, in our <strong>demand query</strong>, a chosen member could be where <span class="math inline">\(\pi = 0.8\)</span> within the respective chosen parametric family to model our surveyed children. Other possible members could correspond to <span class="math inline">\(\pi = 0.2\)</span>, <span class="math inline">\(\pi = 0.4\)</span> or <span class="math inline">\(\pi = 0.6\)</span>. In fact, the number of members in our chosen parametric family is infinite in this <strong>demand query</strong>!</p>
<blockquote class="blockquote">
<p><strong>Therefore, what parametric family can we choose for our demand query?</strong></p>
</blockquote>
<p>The question above introduces a new, valuable resource that is further elaborated upon in <a href="C-distributional-mind-map.html" class="quarto-xref"><span>Appendix C</span></a>. This resource outlines the various distributions that will be utilized in this textbook. In reality, the realm of parametric families—specifically, distributions—is quite extensive, and appendix material serves as only a brief overview of the many parametric families documented in statistical literature.</p>
<div class="Tip">
<div class="Tip-header">
<p>Tip on data modelling alternatives via different parametric families!</p>
</div>
<div class="Tip-container">
<p>Any data model is simply an abstraction of reality, and different parametric families can provide various alternatives for modelling. In practice, we often need to select a specific family based on our particular inquiries and the conditions of our data. This process requires time and experience to master. Furthermore, it is important to note that different families are often interconnected!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/family.png" class="img-fluid figure-img" width="550"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/"><em>Manfred Stege</em></a> via <a href="https://pixabay.com/vectors/daycare-pixel-holding-hands-7464616/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<p>If you wish to explore the world of <strong>univariate distribution families</strong>—which are used to model a single random variable—<span class="citation" data-cites="leemis">Leemis (<a href="references.html#ref-leemis" role="doc-biblioref">n.d.</a>)</span> has created a comprehensive relational chart that covers 76 distinct probability distributions: 19 are discrete, and 57 are continuous. However, this chart does not encompass all the possible families that one might encounter in statistical literature (you can check another list <a href="https://www.math.wm.edu/~leemis/chart/UDR/about.html">at the end of this section</a>).</p>
</div>
</div>
<p>Referring back to our discussion about <a href="C-distributional-mind-map.html" class="quarto-xref"><span>Appendix C</span></a>, it is time to choose the <strong>most suitable</strong> parametric family for a discrete and a binary random variable, such as the <span class="math inline">\(i\)</span>th random variable <span class="math inline">\(D_i\)</span>. A particular case we can examine is the <strong>Bernoulli distribution</strong> (also, commonly known as a <strong>Bernoulli trial</strong>). The Bernoulli distribution applies to a discrete random variable that can take one of two values: <span class="math inline">\(0\)</span>, which we refer to as a <em>failure</em>, and <span class="math inline">\(1\)</span>, identified as a <em>success</em>. This aligns with our previous definition from <a href="#eq-random-variable-demand" class="quarto-xref">Equation&nbsp;<span>2.3</span></a>:</p>
<p><span class="math display">\[
d_i =
\begin{cases}
1 \qquad \text{The surveyed child prefers chocolate.}\\
0 \qquad \text{Otherwise.}
\end{cases}
\]</span></p>
<p>The equation above defines the chocolate preference of the <span class="math inline">\(i\)</span>th surveyed child as a <em>success</em>, while another flavour—specifically vanilla in the context of our limited menu—is categorized as a <em>failure</em>. Thus, we can denote the support as <span class="math inline">\(d_i \in \{0, 1\}\)</span>.</p>
<p>We need to define our population parameter for this <strong>demand query</strong> in the context of a Bernoulli trial, which is denoted by <span class="math inline">\(\pi \in [0, 1]\)</span>. This represents the <strong>proportion of children who prefer the chocolate flavour over the vanilla flavour</strong>. In a Bernoulli trial, this parameter refers to the probability of success. Lastly, we can specify our generative model accordingly:</p>
<p><span class="math display">\[
\begin{gather*}
m_D : D_i \sim \text{Bern}(\pi) \\
\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \text{for $i = 1, \dots, n_d.$}
\end{gather*}
\]</span></p>
<p>We must define the PMF corresponding to the above generative model. The statistical literature assigns the following PMF for the a Bernoulli trial <span class="math inline">\(D_i\)</span>:</p>
<p><span id="eq-bernoulli-pmf-demand"><span class="math display">\[
P_{D_i} \left( D_i = d_i \mid \pi \right) = \pi^{d_i} (1 - \pi)^{1 - d_i} \quad \text{for $d_i \in \{ 0, 1 \}$.}
\tag{2.7}\]</span></span></p>
<p>A further question arises regarding whether <a href="#eq-bernoulli-pmf-demand" class="quarto-xref">Equation&nbsp;<span>2.7</span></a> satisfies the condition of the total probability of the sample space defined in the <a href="#eq-sample-space-PMF" class="quarto-xref">Equation&nbsp;<span>2.6</span></a> under the <a href="#Definition-probability-mass-function">definition</a> of a PMF. This condition states that a valid PMF should result in a total probability equal to one when we sum all the probabilities produced by this function over every possible value that the random variable can take.</p>
<p>Hence, we can state <a href="#eq-bernoulli-pmf-demand" class="quarto-xref">Equation&nbsp;<span>2.7</span></a> is a proper probability distribution (i.e., all the standalone probabilities over the support of <span class="math inline">\(D_i\)</span> add up to one) given that:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-bernoulli-pmf-demand-total"><span class="math display">\[
\begin{align*}
\sum_{d_i = 0}^1 P_{D_i} \left( D_i = d_i \mid \pi \right) &amp;=  \sum_{d_i = 0}^1 \pi^{d_i} (1 - \pi)^{1 - d_i}  \\
&amp;= \underbrace{\pi^0}_{1} (1 - \pi) + \pi \underbrace{(1 - \pi)^{0}}_{1} \\
&amp;= (1 - \pi) + \pi \\
&amp;= 1. \qquad \qquad \qquad \qquad \quad \square
\end{align*}
\tag{2.8}\]</span></span></p>
<blockquote class="blockquote">
<p><strong>Indeed, this Bernoulli PMF is a proper probability distribution!</strong></p>
</blockquote>
</div>
<p>The probability distribution, obtained from <a href="#eq-bernoulli-pmf-demand-total" class="quarto-xref">Equation&nbsp;<span>2.8</span></a>, is summarized in <a href="#tbl-PMF-demand" class="quarto-xref">Table&nbsp;<span>2.3</span></a>. Note that the chocolate preference has a probability equal to <span class="math inline">\(\pi\)</span>, whereas the vanilla preference corresponds to the complement <span class="math inline">\(1 - \pi\)</span>. This probability arrangement completely fulfils the corresponding probability condition of the sample space seen in <a href="#eq-sample-space-PMF" class="quarto-xref">Equation&nbsp;<span>2.6</span></a>.</p>
<div id="tbl-PMF-demand" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-PMF-demand-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2.3: Probability distribution for the <span class="math inline">\(i\)</span>th Bernoulli trial <span class="math inline">\(D_i\)</span>.
</figcaption><div aria-describedby="tbl-PMF-demand-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<thead><tr class="header">
<th style="text-align: center;"><span class="math inline">\(d_i\)</span></th>
<th style="text-align: center;"><span class="math inline">\(P_{D_i} \left( D_i = d_i \mid \pi \right)\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1 - \pi\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\pi\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>To proceed with the <strong>time query</strong>, we need to analyze the <span class="math inline">\(j\)</span>th continuous random variable <span class="math inline">\(T_j\)</span> and subsequently work with a PDF.</p>
<div id="Definition-probability-density-function" class="definition">
<div class="definition-header">
<p>Definition of probability density function (PDF)</p>
</div>
<div class="definition-container">
<p>Let <span class="math inline">\(Y\)</span> be a continuous random variable whose support is <span class="math inline">\(\mathcal{Y}\)</span>. Furthermore, consider a function <span class="math inline">\(f_Y(y)\)</span> such that</p>
<p><span class="math display">\[
f_Y(y) : \mathbb{R} \rightarrow \mathbb{R}
\]</span></p>
<p>with</p>
<p><span id="eq-lower-bound-PDF"><span class="math display">\[
f_Y(y) \geq 0.
\tag{2.9}\]</span></span></p>
<p>Then, <span class="math inline">\(f_Y(y)\)</span> is considered a PDF if the probability of <span class="math inline">\(Y\)</span> taking on a value within the range represented by the subset <span class="math inline">\(A \subset \mathcal{Y}\)</span> is equal to</p>
<p><span class="math display">\[
P_Y(Y \in A) = \int_A f_Y(y) \mathrm{d}y
\]</span></p>
<p>with</p>
<p><span id="eq-sample-space-PDF"><span class="math display">\[
\int_{\mathcal{Y}} f_Y(y) \mathrm{d}y = 1.
\tag{2.10}\]</span></span></p>
</div>
</div>
<p>To begin our second analysis, let us examine the nature of the variable <span class="math inline">\(T_j\)</span> represented as a continuous random variable. This variable is <strong>nonnegative</strong>, meaning it is positively unbounded, as it models a waiting time. We can interpret <span class="math inline">\(T_j\)</span> as the waiting time until a specific event of interest occurs, such as when the next customer arrives at the ice cream cart. In statistical literature, this is commonly referred to as a <strong>survival time</strong>. Hence, we might wonder:</p>
<blockquote class="blockquote">
<p><strong>What is the most suitable parametric family to model a survival time?</strong></p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/question.png" class="img-fluid figure-img" width="230"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/"><em>Manfred Stege</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-emotion-confused-6230199/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<p>Well, in this case within our textbook and in general in statistical literature, there is more than one alternative to model a continuous and nonnegative survival time. <a href="C-distributional-mind-map.html" class="quarto-xref"><span>Appendix C</span></a> offers four possible ways:</p>
<ul>
<li>
<a href="C-distributional-mind-map.html#sec-exponential-distribution"><strong>Exponential.</strong></a> A random variable with a single parameter that can come in either of the following forms:
<ul>
<li>As a <strong>rate</strong> <span class="math inline">\(\lambda \in (0, \infty)\)</span>, which generally defines the mean number of events of interest per time interval or space unit.</li>
<li>As a <strong>scale</strong> <span class="math inline">\(\beta \in (0, \infty)\)</span>, which generally defines the mean time until the next event of interest occurs.</li>
</ul>
</li>
<li>
<a href="C-distributional-mind-map.html#sec-weibull-distribution"><strong>Weibull.</strong></a> A random variable that is a generalization of the Exponential distribution. Note its distributional parameters are the <strong>scale</strong> continuous parameter <span class="math inline">\(\beta \in (0, \infty)\)</span> and <strong>shape</strong> continuous parameter <span class="math inline">\(\gamma \in (0, \infty)\)</span>.</li>
<li>
<a href="C-distributional-mind-map.html#sec-gamma-distribution"><strong>Gamma</strong></a> A random variable whose distributional parameters are the <strong>shape</strong> continuous parameter <span class="math inline">\(\eta \in (0, \infty)\)</span> and <strong>scale</strong> continuous parameter <span class="math inline">\(\theta \in (0, \infty)\)</span>.</li>
<li>
<a href="C-distributional-mind-map.html#sec-lognormal-distribution"><strong>Lognormal.</strong></a> A random variable whose logarithmic transformation yields a Normal distribution. Its distributional parameters are the <strong>Normal location</strong> continuous parameter <span class="math inline">\(\mu \in (-\infty, \infty)\)</span> and <strong>Normal scale</strong> continuous parameter <span class="math inline">\(\sigma^2 \in (0, \infty)\)</span>.</li>
</ul>
<p>In our context, as summarized in the <a href="#eq-gen-model-time">corresponding</a> generative model, it is in our best interest to select a probability distribution characterized by a single parameter. Therefore, the Exponential distribution is the most suitable choice for our current <strong>time query</strong>, particularly under the scale parametrization, since we aim to estimate the waiting time between two customers.</p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on survival analysis!</p>
</div>
<div class="Heads-up-container">
<p>Although our ice cream case can be straightforwardly modelled using an Exponential distribution for our <strong>time query</strong>, by using a single population parameter which indicates a mean waiting time between two customers, it is important to stress that other distributions, such as the Weibull, Gamma, or Lognormal, are also entirely valid options. In fact, utilizing these distributions, that involve more than just a standalone parameter, can enhance the flexibility of our data modelling!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/calendar.png" class="img-fluid figure-img" width="320"></p>
<figcaption>Image by <a href="https://pixabay.com/users/toushirou_px-27788994/"><em>toushirou_px</em></a> via <a href="https://pixabay.com/vectors/calendar-deadline-icon-limit-time-7253494/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<p>Additionally, there is a specialized statistical field focused on modelling waiting times—specifically, the time until an event of interest occurs. These types of times are formally referred to as survival times, and the associated field is known as <strong>survival analysis</strong>. It is worth noting that regression analysis can be extended to this area, and <a href="06-parametric-survival.html" class="quarto-xref"><span>Chapter 6</span></a> will provide a more in-depth exploration of various parametric models that involve the Exponential, Weibull, and Lognormal distributions.</p>
</div>
</div>
<p>Since we are using an Exponential distribution, we need to establish our population parameter for this <strong>time query</strong>. As mentioned in <a href="#tbl-queries-2" class="quarto-xref">Table&nbsp;<span>2.2</span></a>, this parameter refers to the <strong>average (or mean) waiting time from one customer to the next</strong>. This corresponds to a <strong>scale parametrization</strong>, where the parameter <span class="math inline">\(\beta \in (0, \infty)\)</span> defines the mean time until the next event of interest occurs (in this case, the next customer). Therefore, we can specify our generative model as follows:</p>
<p><span class="math display">\[
\begin{gather*}
m_T : T_j \sim \text{Exponential}(\beta) \\
\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \text{for $j = 1, \dots, n_t.$}
\end{gather*}
\]</span></p>
<p>Since <span class="math inline">\(T_j\)</span> is a continuous random variable, we must define the PMF corresponding to the above generative model. The statistical literature assigns the following PDF for <span class="math inline">\(T_j\)</span>:</p>
<p><span id="eq-exponential-pdf-time"><span class="math display">\[
f_{T_j} \left(t_j \mid \beta \right) = \frac{1}{\beta} \exp \left( -\frac{t_j}{\beta} \right) \quad \text{for $t_j \in [0, \infty )$.}
\tag{2.11}\]</span></span></p>
<p>Now, we might wonder whether <a href="#eq-exponential-pdf-time" class="quarto-xref">Equation&nbsp;<span>2.11</span></a> satisfies the condition of the total probability of the sample space defined in the <a href="#eq-sample-space-PDF" class="quarto-xref">Equation&nbsp;<span>2.10</span></a> under the <a href="#Definition-probability-density-function">definition</a> of a PDF. This condition states that a valid PDF should result in a total probability equal to one when we integrate this function over all the support of <span class="math inline">\(T_j\)</span>.</p>
<p>Thus, we can state that <a href="#eq-exponential-pdf-time" class="quarto-xref">Equation&nbsp;<span>2.11</span></a> is a proper probability distribution (i.e., <a href="#eq-exponential-pdf-time" class="quarto-xref">Equation&nbsp;<span>2.11</span></a> integrates to one over the support of <span class="math inline">\(T_j\)</span>) given that:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-exponential-scale-PDF-integrates-1"><span class="math display">\[
\begin{align*}
\int_{t_j = 0}^{t_j = \infty} f_{T_j} \left(t_j \mid \beta \right) \mathrm{d}y &amp;= \int_{t_j = 0}^{t_j = \infty} \frac{1}{\beta} \exp \left( -\frac{t_j}{\beta} \right) \mathrm{d}t_j \\
&amp;= \frac{1}{\beta} \int_{t_j = 0}^{t_j = \infty} \exp \left( -\frac{t_j}{\beta} \right) \mathrm{d}t_j \\
&amp;= - \frac{\beta}{\beta} \exp \left( -\frac{t_j}{\beta} \right) \Bigg|_{t_j = 0}^{t_j = \infty} \\
&amp;= - \exp \left( -\frac{t_j}{\beta} \right) \Bigg|_{t_j = 0}^{t_j = \infty} \\
&amp;= - \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \\
&amp;= - \left( 0 - 1 \right) \\
&amp;= 1. \qquad \qquad \qquad \qquad \quad \square
\end{align*}
\tag{2.12}\]</span></span></p>
<blockquote class="blockquote">
<p><strong>Indeed, the Exponential PDF, under a scale parametrization, is a proper probability distribution!</strong></p>
</blockquote>
</div>
<p>Unlike our <strong>demand query</strong>, which features a table illustrating the PMF for <span class="math inline">\(D_i \in \{ 0, 1 \}\)</span> (see <a href="#tbl-PMF-demand" class="quarto-xref">Table&nbsp;<span>2.3</span></a>), it is not feasible to create a table for the PDF of <span class="math inline">\(T_j \in [0, \infty)\)</span> because it represents an uncountably infinite set of possible values. However, we can plot the corresponding PDF using three specific members of the <strong>Exponential parametric family</strong> as examples. <a href="#fig-exponential-family-scale-time" class="quarto-xref">Figure&nbsp;<span>2.2</span></a> presents these three example members, with scale parameters values of <span class="math inline">\(\beta = 0.25, 0.5, 1\)</span> minutes, representing waiting times through their corresponding PDFs. Based on our findings in <a href="#eq-exponential-scale-PDF-integrates-1" class="quarto-xref">Equation&nbsp;<span>2.12</span></a>, we know that the area under these three <strong>density plots</strong> equals one, indicating the total probability of the sample space. Additionally, it is important to note that as we increase the scale parameter, larger observed values <span class="math inline">\(t_j\)</span> become more probable.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-exponential-family-scale-time" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-exponential-family-scale-time-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02-stats-review_files/figure-html/fig-exponential-family-scale-time-1.png" class="img-fluid figure-img" width="1344">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-exponential-family-scale-time-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.2: Some members of the Exponential family with scale parametrization.
</figcaption></figure>
</div>
</div>
</div>
</section><section id="sec-characterizing-prob-dist" class="level3" data-number="2.1.5"><h3 data-number="2.1.5" class="anchored" data-anchor-id="sec-characterizing-prob-dist">
<span class="header-section-number">2.1.5</span> Characterizing Probability Distributions</h3>
<p>Before moving on into our distributional journey, let us update <a href="#tbl-queries-2" class="quarto-xref">Table&nbsp;<span>2.2</span></a> with the specific probability distributions, and mathematical definitions of the parameters to estimate per query.</p>
<div id="tbl-queries-3" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-queries-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2.4: Table containing the corresponding insights to solve our <em>demand</em> and <em>time queries</em>.
</figcaption><div aria-describedby="tbl-queries-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead><tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;"><strong>Demand Query</strong></th>
<th style="text-align: center;"><strong>Time Query</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>Statement</strong></td>
<td style="text-align: center;">We would like to know <strong>which</strong> ice cream flavour is the favourite one (either <strong>chocolate</strong> or <strong>vanilla</strong>) and by <strong>how much</strong>.</td>
<td style="text-align: center;">We would like to know the <strong>average</strong> waiting time from one customer to the next one in any given ice cream cart.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Population of interest</strong></td>
<td style="text-align: center;">
<strong>Children between 4 and 11 years old</strong> attending different parks in Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during Summer weekends.</td>
<td style="text-align: center;">
<strong>All our general customer-to-customer waiting times</strong> in the different parks of Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during Summer weekends across the 900 ice cream carts.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Parameter</strong></td>
<td style="text-align: center;">
<strong>Proportion</strong> of individuals from the population of interest <strong>who prefer the chocolate flavour versus the vanilla flavour</strong>.</td>
<td style="text-align: center;">
<strong>Average</strong> waiting time <strong>in minutes</strong> from one customer to the next one.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Random variable</strong></td>
<td style="text-align: center;">
<span class="math inline">\(D_i\)</span> for <span class="math inline">\(i = 1, \dots, n_d\)</span>.</td>
<td style="text-align: center;">
<span class="math inline">\(T_j\)</span> for <span class="math inline">\(j = 1, \dots, n_t\)</span>.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Random variable definition</strong></td>
<td style="text-align: center;">A favourite ice cream flavour of a randomly surveyed <span class="math inline">\(i\)</span>th child between 4 and 11 years old attending the parks of Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during the Summer weekends.</td>
<td style="text-align: center;">A randomly recorded <span class="math inline">\(j\)</span>th waiting time in minutes between two customers during a Summer weekend across the 900 ice cream carts found in Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Random variable type</strong></td>
<td style="text-align: center;">Discrete and binary.</td>
<td style="text-align: center;">Continuous and positively unbounded.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Random variable support</strong></td>
<td style="text-align: center;">
<span class="math inline">\(d_i \in \{ 0, 1\}\)</span> as in <a href="#eq-random-variable-demand" class="quarto-xref">Equation&nbsp;<span>2.3</span></a>.</td>
<td style="text-align: center;"><span class="math inline">\(t_j \in [0, \infty).\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Probability distribution</strong></td>
<td style="text-align: center;"><span class="math inline">\(D_i \sim \text{Bern}(\pi)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(T_j \sim \text{Exponential}(\beta)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>Mathematical definition of the parameter</strong></td>
<td style="text-align: center;"><span class="math display">\[\pi\]</span></td>
<td style="text-align: center;"><span class="math display">\[\beta\]</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Let us proceed, then. We have been exploring the basics of random variables, as well as the importance of generative modelling and probability distributions in addressing different data inquiries. These concepts are fundamental to understanding the population parameter setup before we actually collect data and solve these inquiries to create effective storytelling. Therefore, before we delve into those stages, however, we need to identify and explain efficient ways to summarize probability distributions. This will help us make our storytelling compelling for a general audience, as we will discuss further.</p>
<p>To continue with this example, we need to use <code>R</code> and <code>Python</code> code. Thus, we will work with some simulated populations to create the corresponding proofs of concept in this section and the subsequent ones. Let us start with our <strong>demand query</strong>. We will consider a population size of <span class="math inline">\(N_d = 2,000,000\)</span> children (whose characteristics are defined in <a href="#tbl-queries-3" class="quarto-xref">Table&nbsp;<span>2.4</span></a>). The code (in either <code>R</code> or <code>Python</code>) below assigns this value as <code>N_d</code>, along with a simulation <strong>seed</strong> to ensure our results are reproducible. Additionally, <strong>for the simulation purposes related to our generative modelling</strong>, we will assume that <span class="math inline">\(65\%\)</span> of these children in this population prefer chocolate over vanilla (i.e., <span class="math inline">\(\pi = 0.65\)</span>).</p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on real and unknown parameters!</p>
</div>
<div class="Heads-up-container">
<p>Although we are assigning a value of <span class="math inline">\(\pi = 0.65\)</span> as our true population parameter in this query, we can never know the exact value in practice unless we conduct a full census. This is why we rely on probabilistic tools, via random sampling and statistical inference, to estimate this <span class="math inline">\(\pi\)</span> in frequentist statistics.</p>
</div>
</div>
<p>Let us recall that we are assuming each child as a Bernoulli trial, where a success (denoted as <code>1</code>) indicates that the child “<em>prefers chocolate</em>.” This also reflects the flavour mapping in the code. Furthermore, instead of using a Bernoulli random number generator, we are utilizing a <a href="C-distributional-mind-map.html#sec-binomial-distribution"><strong>Binomial</strong></a> random number generator. This is because the Binomial case with parameters <span class="math inline">\(n = 1\)</span> and <span class="math inline">\(\pi\)</span> is equivalent to a Bernoulli trial with parameter <span class="math inline">\(\pi\)</span>. Hence, consider the following Binomial case:</p>
<p><span class="math display">\[
Y \sim \text{Bin}(n = 1, \pi),
\]</span></p>
<p>whose PMF is simplified as a Bernoulli given that</p>
<p><span id="eq-binomial-to-bernoulli"><span class="math display">\[
\begin{align*}
P_Y \left( Y = y \mid n = 1, \pi \right) &amp;= {1 \choose y} \pi^y (1 - \pi)^{1 - y} \\
&amp;= \underbrace{\frac{1!}{y!(1 - y)!}}_{\text{$1$ for $y \in \{ 0, 1 \}$}} \pi^y (1 - \pi)^{1 - y} \\
&amp;= \pi^y (1 - \pi)^{1 - y} \\
&amp; \qquad \qquad \qquad \qquad \qquad \text{for $y \in \{ 0, 1 \}$.}
\end{align*}
\tag{2.13}\]</span></span></p>
<p>The final output of this quick simulation, which models a population of <span class="math inline">\(N_d\)</span> children as Bernoulli trials with a probability of success <span class="math inline">\(\pi = 0.65\)</span>, consists of a data frame containing <span class="math inline">\(N_d = 2,000,000\)</span> rows, with each row representing a child and their preferred ice cream flavour: either chocolate or vanilla. It is worth noting that the outputs from both <code>R</code> and <code>Python</code> differ due to the fact that each language employs <strong>different pseudo-random number generators</strong> (even though we use the same seed). Note that <code>Python</code> additionally uses the <a href="https://pypi.org/project/numpy/">{numpy}</a> and <a href="https://pypi.org/project/pandas/">{pandas}</a> libraries.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" aria-current="page"><strong><code>R</code> Code</strong></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false"><strong><code>Python</code> Code</strong></a></li>
</ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>  <span class="co"># Seed for reproducibility</span></span>
<span></span>
<span><span class="co"># Population size</span></span>
<span><span class="va">N_d</span> <span class="op">&lt;-</span> <span class="fl">2000000</span></span>
<span></span>
<span><span class="co"># Simulate binary outcomes: 1 = chocolate, 0 = vanilla</span></span>
<span><span class="va">flavour_bin</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">rbinom</a></span><span class="op">(</span><span class="va">N_d</span>, size <span class="op">=</span> <span class="fl">1</span>, prob <span class="op">=</span> <span class="fl">0.65</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Map binary to flavour names</span></span>
<span><span class="va">flavours</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">flavour_bin</span> <span class="op">==</span> <span class="fl">1</span>, <span class="st">"chocolate"</span>, <span class="st">"vanilla"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create data frame</span></span>
<span><span class="va">children_pop</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  children_ID <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="va">N_d</span>,</span>
<span>  fav_flavour <span class="op">=</span> <span class="va">flavours</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Showing the first 100 children of the population</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">children_pop</span>, n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing libraries</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)   <span class="co"># Seed for reproducibility</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Population size</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>N_d <span class="op">=</span> <span class="dv">2000000</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate binary outcomes: 1 = chocolate, 0 = vanilla</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>flavour_bin <span class="op">=</span> np.random.binomial(n <span class="op">=</span> <span class="dv">1</span>, p <span class="op">=</span> <span class="fl">0.65</span>, size <span class="op">=</span> N_d)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Map binary to flavour names</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>flavours <span class="op">=</span> np.where(flavour_bin <span class="op">==</span> <span class="dv">1</span>, <span class="st">"chocolate"</span>, <span class="st">"vanilla"</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Create data frame</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>children_pop <span class="op">=</span> pd.DataFrame({</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">"children_ID"</span>: np.arange(<span class="dv">1</span>, N_d <span class="op">+</span> <span class="dv">1</span>),</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"fav_flavour"</span>: flavours</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Showing the first 100 children of the population</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(children_pop.head(<span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true"><strong><code>R</code> Output</strong></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false"><strong><code>Python</code> Output</strong></a></li>
</ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div class="cell">
<div class="cell-output-display">
<div class="datatables html-widget html-fill-item" id="htmlwidget-1e288bffcd72d138c3e5" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-1e288bffcd72d138c3e5">{"x":{"filter":"none","vertical":false,"data":[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],["chocolate","vanilla","chocolate","vanilla","vanilla","chocolate","chocolate","vanilla","chocolate","chocolate","vanilla","chocolate","vanilla","chocolate","chocolate","vanilla","chocolate","chocolate","chocolate","vanilla","vanilla","vanilla","chocolate","vanilla","vanilla","vanilla","chocolate","chocolate","chocolate","chocolate","vanilla","vanilla","vanilla","vanilla","chocolate","chocolate","vanilla","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","vanilla","chocolate","chocolate","vanilla","chocolate","chocolate","chocolate","chocolate","vanilla","vanilla","chocolate","vanilla","chocolate","chocolate","chocolate","vanilla","chocolate","vanilla","vanilla","vanilla","chocolate","vanilla","chocolate","vanilla","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","vanilla","chocolate","vanilla","chocolate","chocolate","vanilla","vanilla","vanilla","chocolate","chocolate","vanilla","chocolate","vanilla","chocolate","chocolate","vanilla","chocolate","chocolate","chocolate"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>children_ID<\/th>\n      <th>fav_flavour<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"ltipr","autoWidth":true,"columnDefs":[{"className":"dt-left","targets":"_all"},{"width":"200px","targets":"_all"},{"name":"children_ID","targets":0},{"name":"fav_flavour","targets":1}],"order":[],"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<div class="cell">
<div class="cell-output-display">
<div class="datatables html-widget html-fill-item" id="htmlwidget-e0826e3f260d45606ba9" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-e0826e3f260d45606ba9">{"x":{"filter":"none","vertical":false,"data":[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],["vanilla","chocolate","chocolate","chocolate","vanilla","chocolate","vanilla","vanilla","chocolate","chocolate","chocolate","vanilla","chocolate","chocolate","chocolate","vanilla","chocolate","chocolate","chocolate","chocolate","chocolate","vanilla","vanilla","chocolate","vanilla","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","vanilla","vanilla","chocolate","chocolate","chocolate","chocolate","chocolate","vanilla","chocolate","chocolate","vanilla","chocolate","chocolate","chocolate","vanilla","chocolate","chocolate","chocolate","chocolate","chocolate","vanilla","vanilla","chocolate","vanilla","chocolate","chocolate","vanilla","vanilla","chocolate","vanilla","chocolate","chocolate","chocolate","chocolate","vanilla","chocolate","vanilla","chocolate","chocolate","chocolate","chocolate","chocolate","vanilla","chocolate","vanilla","chocolate","chocolate","vanilla","vanilla","chocolate","chocolate","chocolate","chocolate","vanilla","vanilla","chocolate","vanilla","chocolate","vanilla","chocolate","chocolate","chocolate","chocolate"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>children_ID<\/th>\n      <th>fav_flavour<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"ltipr","autoWidth":true,"columnDefs":[{"className":"dt-left","targets":"_all"},{"width":"200px","targets":"_all"},{"name":"children_ID","targets":0},{"name":"fav_flavour","targets":1}],"order":[],"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
</div>
</div>
</div>
<p>In our <strong>time query</strong>, we will simulate another population consisting of <span class="math inline">\(N_t = 500,000\)</span> general customer-to-customer waiting times (as defined in <a href="#tbl-queries-3" class="quarto-xref">Table&nbsp;<span>2.4</span></a>). The code below assigns this population size to the variable <code>N_t</code>. We have already established that this class of data will be modelled using an Exponential distribution under a scale parameterization, where the parameter <span class="math inline">\(\beta\)</span> defines the mean waiting time between customers. For this query, we will assume that the population of waiting times has a true parameter value of <span class="math inline">\(\beta = 10\)</span> minutes. Therefore, the code below illustrates this generative modelling mechanism, which produces a data frame containing <span class="math inline">\(N_t = 500,000\)</span> rows, with each row representing a specific waiting time in minutes.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-3-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-1" role="tab" aria-controls="tabset-3-1" aria-selected="true"><strong><code>R</code> Code</strong></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-3-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-3-2" role="tab" aria-controls="tabset-3-2" aria-selected="false"><strong><code>Python</code> Code</strong></a></li>
</ul>
<div class="tab-content">
<div id="tabset-3-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-3-1-tab">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>  <span class="co"># Seed for reproducibility</span></span>
<span></span>
<span><span class="co"># Population size</span></span>
<span><span class="va">N_t</span> <span class="op">&lt;-</span> <span class="fl">500000</span></span>
<span></span>
<span><span class="co"># In R, 'rate' is 1 / scale and rounding to two decimal places</span></span>
<span><span class="va">waiting_times</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Exponential.html">rexp</a></span><span class="op">(</span><span class="va">N_t</span>, rate <span class="op">=</span> <span class="fl">1</span> <span class="op">/</span> <span class="fl">10</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create data frame</span></span>
<span><span class="va">waiting_pop</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  time_ID <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="va">N_t</span>,</span>
<span>  waiting_time <span class="op">=</span> <span class="va">waiting_times</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Showing the first 100 waiting times of the population</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">waiting_pop</span>, n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-3-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-3-2-tab">
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)  <span class="co"># Seed for reproducibility</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Population size</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>N_t <span class="op">=</span> <span class="dv">500000</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate waiting times</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>waiting_times <span class="op">=</span> np.<span class="bu">round</span>(np.random.exponential(scale <span class="op">=</span> <span class="dv">10</span>, size <span class="op">=</span> N_t), <span class="dv">2</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>waiting_pop <span class="op">=</span> pd.DataFrame({</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"time_ID"</span>: np.arange(<span class="dv">1</span>, N_t <span class="op">+</span> <span class="dv">1</span>),</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"waiting_time"</span>: waiting_times</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Showing the first 100 waiting times of the population</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(waiting_pop.head((<span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-4-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-1" role="tab" aria-controls="tabset-4-1" aria-selected="true"><strong><code>R</code> Output</strong></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-4-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-4-2" role="tab" aria-controls="tabset-4-2" aria-selected="false"><strong><code>Python</code> Output</strong></a></li>
</ul>
<div class="tab-content">
<div id="tabset-4-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-4-1-tab">
<div class="cell">
<div class="cell-output-display">
<div class="datatables html-widget html-fill-item" id="htmlwidget-45026dc10deabf82c3f0" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-45026dc10deabf82c3f0">{"x":{"filter":"none","vertical":false,"data":[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],[8.43,5.77,13.29,0.32,0.5600000000000001,3.17,3.14,1.45,27.26,0.29,10.05,4.8,2.81,3.77,1.88,8.5,15.63,4.79,5.91,40.41,8.43,9.66,14.85,13.48,11.69,16.06,14.97,15.71,0.32,5.98,21.68,5.07,2.6,25.97,12.29,7.91,6.29,12.55,5.89,11.29,4.2,72.11,8.460000000000001,2.26,11,22.48,13.64,5.76,27.25,13.12,0.91,3.06,10.67,3.14,9.75,18.88,5.65,25.77,10.48,10.24,10.28,2.85,15.63,0.42,0.99,0.99,2.8,2.96,9.720000000000001,9.24,16.42,16.2,25.36,15.22,3.8,2.39,4.66,0.42,3.2,6.44,5.73,2.16,44.99,18.57,6.85,14.39,17.31,12.45,14.63,15.37,0.05,11.09,3,11.92,11.15,0.67,4.81,15.7,2.6,18.57]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>time_ID<\/th>\n      <th>waiting_time<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"ltipr","autoWidth":true,"columnDefs":[{"className":"dt-left","targets":"_all"},{"width":"200px","targets":"_all"},{"name":"time_ID","targets":0},{"name":"waiting_time","targets":1}],"order":[],"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
</div>
<div id="tabset-4-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-4-2-tab">
<div class="cell">
<div class="cell-output-display">
<div class="datatables html-widget html-fill-item" id="htmlwidget-ecca0a1fe2173561076b" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-ecca0a1fe2173561076b">{"x":{"filter":"none","vertical":false,"data":[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],[11.92,3.37,2.57,8.01,12.71,5.5,39.51,11.55,6.56,4.98,4.2,13.06,5.77,0.62,5.08,13.39,2.01,1.93,7.58,7.59,10.06,18.93,12.89,9.44,12.82,3.9,4.49,2.59,3.48,9.970000000000001,0.97,5.69,5.64,6.81,5.55,3.74,5.56,22.39,28.85,6.97,9.779999999999999,1.23,3.82,5.36,20.12,2.88,6.6,42.38,7.33,9.49,1.29,17.51,9.24,7.88,4.2,3.63,5.4,11.44,20.83,7.14,11.07,8.82,9.81,11.23,18.47,0.87,14.43,2.79,2.16,8.5,1.01,21.66,9.869999999999999,12.85,0.16,9.02,8.140000000000001,1.73,1.66,11.89,3.84,11.78,8.08,4.93,25.92,18.43,4.42,0.45,3.64,5.08,12.21,53.73,4.4,14.38,8.99,11.77,1.64,5.09,2.76,4.21]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>time_ID<\/th>\n      <th>waiting_time<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"ltipr","autoWidth":true,"columnDefs":[{"className":"dt-left","targets":"_all"},{"width":"200px","targets":"_all"},{"name":"time_ID","targets":0},{"name":"waiting_time","targets":1}],"order":[],"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
</div>
</div>
</div>
<p>Imagine that the data collection and analysis for the ice cream case have progressed into the future. You have a follow-up meeting with the eight general managers, one from each Canadian city, to discuss the statements related to both <strong>demand</strong> and <strong>time queries</strong>, in relation to our populations of interest. Additionally, you have collected data from a sample of <span class="math inline">\(n_d = 500\)</span> randomly surveyed children across these eight Canadian cities. Note that the below <code>R</code> and <code>Python</code> sampling functions perform <strong>simple random sampling with replacement</strong>.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-5-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-1" role="tab" aria-controls="tabset-5-1" aria-selected="true"><strong><code>R</code> Code</strong></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-5-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-5-2" role="tab" aria-controls="tabset-5-2" aria-selected="false"><strong><code>Python</code> Code</strong></a></li>
</ul>
<div class="tab-content">
<div id="tabset-5-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-5-1-tab">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">678</span><span class="op">)</span>  <span class="co"># Seed for reproducibility</span></span>
<span></span>
<span><span class="co"># Simple random sample of 500 children with replacement</span></span>
<span><span class="va">n_d</span> <span class="op">&lt;-</span> <span class="fl">500</span></span>
<span><span class="va">children_sample</span> <span class="op">&lt;-</span> <span class="va">children_pop</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">children_pop</span><span class="op">)</span>, <span class="va">n_d</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>, <span class="op">]</span></span>
<span></span>
<span><span class="co"># Showing the first 100 sampled children</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">children_sample</span>, n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-5-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-5-2-tab">
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">678</span>)  <span class="co"># Seed for reproducibility</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple random sample of 500 children with replacement</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>n_d <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>children_sample <span class="op">=</span> children_pop.sample(n <span class="op">=</span> n_d, replace <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Showing the first 100 sampled children</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(children_sample.head(<span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-6-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-1" role="tab" aria-controls="tabset-6-1" aria-selected="true"><strong><code>R</code> Output</strong></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-6-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-6-2" role="tab" aria-controls="tabset-6-2" aria-selected="false"><strong><code>Python</code> Output</strong></a></li>
</ul>
<div class="tab-content">
<div id="tabset-6-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-6-1-tab">
<div class="cell">
<div class="cell-output-display">
<div class="datatables html-widget html-fill-item" id="htmlwidget-f6082b758eb5bd64eb3c" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-f6082b758eb5bd64eb3c">{"x":{"filter":"none","vertical":false,"data":[[1616646,1801236,1360553,1526190,1732148,1039348,1806989,968392,637313,1607009,736867,186447,289285,1544504,1371503,522588,1092586,455010,1439207,1222947,1536413,1526511,1145358,268549,459403,1436069,1102119,1914803,826653,1159955,993536,833175,908461,231599,1607949,73793,1858647,880640,1086707,151159,552123,1193866,1031954,1759217,1014067,744513,1854539,1062733,304874,669530,20289,1597889,1807279,651108,1051799,1531246,691659,1828182,888693,162665,1411329,368764,1573633,851412,520177,845696,1329026,385252,1362014,1460546,1682140,717286,942155,363118,1145155,1516006,895581,975164,1760944,1175730,1040554,1873643,645605,935967,1870019,99981,1559747,493300,71908,1929332,1604261,1806333,1221513,802902,1966557,866503,330752,410494,1670627,21412],["chocolate","chocolate","chocolate","vanilla","vanilla","vanilla","chocolate","chocolate","vanilla","chocolate","chocolate","vanilla","chocolate","chocolate","chocolate","vanilla","chocolate","vanilla","chocolate","vanilla","vanilla","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","vanilla","chocolate","vanilla","vanilla","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","vanilla","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","vanilla","vanilla","vanilla","vanilla","chocolate","vanilla","vanilla","vanilla","chocolate","vanilla","chocolate","chocolate","vanilla","vanilla","chocolate","chocolate","chocolate","chocolate","vanilla","chocolate","chocolate","chocolate","chocolate","vanilla","chocolate","chocolate","chocolate","vanilla","chocolate","chocolate","chocolate","chocolate","vanilla","chocolate","vanilla","chocolate","vanilla","vanilla","chocolate","chocolate","vanilla","chocolate","vanilla","chocolate","chocolate","chocolate","chocolate","chocolate"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>children_ID<\/th>\n      <th>fav_flavour<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"ltipr","autoWidth":true,"columnDefs":[{"className":"dt-left","targets":"_all"},{"width":"200px","targets":"_all"},{"name":"children_ID","targets":0},{"name":"fav_flavour","targets":1}],"order":[],"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
</div>
<div id="tabset-6-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-6-2-tab">
<div class="cell">
<div class="cell-output-display">
<div class="datatables html-widget html-fill-item" id="htmlwidget-70acf465c9d031f01d98" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-70acf465c9d031f01d98">{"x":{"filter":"none","vertical":false,"data":[[1746002,90495,1335651,1155391,1853431,601457,868004,1222294,796123,870296,146493,646621,1826580,1591063,1846247,176451,950902,507443,241984,853738,268587,381391,176842,1165926,526065,198173,1294057,1111614,1054796,1912741,1771848,219402,254446,1442665,1898460,637581,232011,1952424,974895,220932,153252,278880,1951004,1966772,867526,1736536,360672,1670863,1145891,1272039,1255902,844106,1931281,1780136,1360835,73979,672894,353316,929730,1036872,1911093,220592,544292,520682,46991,1992406,1005258,1494617,1338726,51290,316848,616713,779032,714625,324806,925460,1610904,1350891,764241,1383879,1583113,1398262,92391,977632,975838,1292573,1132767,34577,749551,1802852,34904,928087,1557362,781341,1576927,119216,1415104,1170482,1404221,1750972],["vanilla","vanilla","chocolate","vanilla","vanilla","vanilla","chocolate","chocolate","chocolate","vanilla","chocolate","vanilla","chocolate","chocolate","chocolate","vanilla","chocolate","vanilla","vanilla","chocolate","vanilla","vanilla","chocolate","chocolate","chocolate","vanilla","vanilla","vanilla","vanilla","chocolate","chocolate","chocolate","vanilla","chocolate","chocolate","vanilla","chocolate","chocolate","vanilla","chocolate","vanilla","chocolate","vanilla","vanilla","vanilla","vanilla","vanilla","chocolate","chocolate","vanilla","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","chocolate","vanilla","chocolate","chocolate","chocolate","vanilla","chocolate","chocolate","chocolate","chocolate","vanilla","chocolate","chocolate","vanilla","vanilla","vanilla","chocolate","chocolate","chocolate","vanilla","vanilla","vanilla","chocolate","vanilla","chocolate","vanilla","chocolate","vanilla","vanilla","vanilla","vanilla","chocolate","chocolate","vanilla","chocolate","chocolate","vanilla","chocolate","vanilla","vanilla"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>children_ID<\/th>\n      <th>fav_flavour<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"ltipr","autoWidth":true,"columnDefs":[{"className":"dt-left","targets":"_all"},{"width":"200px","targets":"_all"},{"name":"children_ID","targets":0},{"name":"fav_flavour","targets":1}],"order":[],"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
</div>
</div>
</div>
<p>Also, you have sampled data on <span class="math inline">\(n_t = 200\)</span> randomly recorded waiting times between customers across our 900 ice cream carts in the same cities.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-7-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-7-1" role="tab" aria-controls="tabset-7-1" aria-selected="true"><strong><code>R</code> Code</strong></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-7-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-7-2" role="tab" aria-controls="tabset-7-2" aria-selected="false"><strong><code>Python</code> Code</strong></a></li>
</ul>
<div class="tab-content">
<div id="tabset-7-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-7-1-tab">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">345</span><span class="op">)</span>  <span class="co"># Seed for reproducibility</span></span>
<span></span>
<span><span class="co"># Simple random sample of 200 waiting times with replacement</span></span>
<span><span class="va">n_t</span> <span class="op">&lt;-</span> <span class="fl">200</span></span>
<span><span class="va">waiting_sample</span> <span class="op">&lt;-</span> <span class="va">waiting_pop</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">waiting_pop</span><span class="op">)</span>, <span class="va">n_t</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>, <span class="op">]</span></span>
<span></span>
<span><span class="co"># Showing the first 100 sampled waiting times</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">waiting_sample</span>, n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-7-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-7-2-tab">
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">345</span>)  <span class="co"># Seed for reproducibility</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple random sample of 200 waiting times with replacement</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>n_t <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>waiting_sample <span class="op">=</span> waiting_pop.sample(n <span class="op">=</span> n_t, replace <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Showing the first 100 sampled waiting times</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(waiting_sample.head(<span class="dv">100</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-8-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-8-1" role="tab" aria-controls="tabset-8-1" aria-selected="true"><strong><code>R</code> Output</strong></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-8-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-8-2" role="tab" aria-controls="tabset-8-2" aria-selected="false"><strong><code>Python</code> Output</strong></a></li>
</ul>
<div class="tab-content">
<div id="tabset-8-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-8-1-tab">
<div class="cell">
<div class="cell-output-display">
<div class="datatables html-widget html-fill-item" id="htmlwidget-65dba6c5bcad0c57ebd0" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-65dba6c5bcad0c57ebd0">{"x":{"filter":"none","vertical":false,"data":[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100],[8.43,5.77,13.29,0.32,0.5600000000000001,3.17,3.14,1.45,27.26,0.29,10.05,4.8,2.81,3.77,1.88,8.5,15.63,4.79,5.91,40.41,8.43,9.66,14.85,13.48,11.69,16.06,14.97,15.71,0.32,5.98,21.68,5.07,2.6,25.97,12.29,7.91,6.29,12.55,5.89,11.29,4.2,72.11,8.460000000000001,2.26,11,22.48,13.64,5.76,27.25,13.12,0.91,3.06,10.67,3.14,9.75,18.88,5.65,25.77,10.48,10.24,10.28,2.85,15.63,0.42,0.99,0.99,2.8,2.96,9.720000000000001,9.24,16.42,16.2,25.36,15.22,3.8,2.39,4.66,0.42,3.2,6.44,5.73,2.16,44.99,18.57,6.85,14.39,17.31,12.45,14.63,15.37,0.05,11.09,3,11.92,11.15,0.67,4.81,15.7,2.6,18.57]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>time_ID<\/th>\n      <th>waiting_time<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"ltipr","autoWidth":true,"columnDefs":[{"className":"dt-left","targets":"_all"},{"width":"200px","targets":"_all"},{"name":"time_ID","targets":0},{"name":"waiting_time","targets":1}],"order":[],"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
</div>
<div id="tabset-8-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-8-2-tab">
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>9.6685</code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div class="datatables html-widget html-fill-item" id="htmlwidget-359a502025aee3d87688" style="width:100%;height:auto;"></div>
<script type="application/json" data-for="htmlwidget-359a502025aee3d87688">{"x":{"filter":"none","vertical":false,"data":[[331289,454609,383572,149712,326282,121387,226380,190361,138053,294660,391685,230699,422754,89368,174325,403402,438871,340991,226708,236557,138745,484961,48335,137324,152916,143214,420882,433222,199496,80245,70646,50988,129925,486910,18572,209905,243305,368774,305975,409540,313407,381821,488244,468949,410035,396272,324217,337853,398278,222875,164931,274988,206203,23021,65591,185999,134924,290765,133875,15940,345660,338001,121804,168982,438826,476454,57915,420655,450520,480200,147964,177687,300544,59775,425954,146009,32797,215230,176153,65473,169293,225027,339321,62904,310789,70314,8649,464285,306793,139440,94934,32171,373466,72809,164121,211895,136406,248280,37528,400553],[3.35,41.53,2.41,0.67,2.6,12.43,5.45,9.17,11.83,12.62,9.289999999999999,14.44,1.93,0.21,8.609999999999999,10.05,3.13,14.82,1.18,13.74,2.17,11.39,8.75,28.04,0.28,0.96,3.7,25.24,16.01,2.13,28.54,42.44,4.47,3.59,5.3,4.44,61.44,25.78,7.29,4.48,14.74,2.55,2.58,23.91,0.41,1.35,12.44,11.7,4.68,9.34,10.11,7.9,16.26,4.81,2.6,4.88,0.26,20.81,0.45,0.64,6.14,16.55,0.74,24.84,18.25,3.03,12.05,43.24,18.65,0.54,7.66,9.119999999999999,23.24,11.25,9.119999999999999,10.57,3.92,11.43,14.18,2.8,18.93,10.42,0.83,11.42,2.45,0.78,0.89,3.01,7.31,15.78,12.53,3.37,5.28,28.3,11.5,3.31,7.08,10.01,1.38,10.23]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>time_ID<\/th>\n      <th>waiting_time<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"ltipr","autoWidth":true,"columnDefs":[{"className":"dt-left","targets":"_all"},{"width":"200px","targets":"_all"},{"name":"time_ID","targets":0},{"name":"waiting_time","targets":1}],"order":[],"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
</div>
</div>
</div>
</div>
</div>
<p>In terms of the executive meeting with the eight general managers, it would not be an efficient use of time to go individually over these <span class="math inline">\(n_d = 500\)</span> and <span class="math inline">\(n_t = 200\)</span> data points along with abstract mathematical concepts such as PMFs or PDFs, as well as probabilistic definitions of random variables and parameters represented by Greek letters. Instead, there should be a more straightforward and simple way to explain how these <span class="math inline">\(n_d\)</span> and <span class="math inline">\(n_t\)</span> <strong>observed random variables</strong> behaved during our data collection process. The key to addressing this complexity lies in understanding measures of central tendency and uncertainty.</p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on population and sample-based measures of central tendency and uncertainty!</p>
</div>
<div class="Heads-up-container">
<p>When learning about measures of central tendency and uncertainty, it is best to begin with those directly related to our population(s) of interest. These concepts provide valuable insights into how any given population behaves concerning typical values and spread. Subsequently, through sampled data, we can derive estimates for these population-based measures.</p>
<p>In this section, we will focus on the population measures, while <a href="#sec-mle" class="quarto-xref"><span>Section 2.2</span></a> and <a href="#sec-basics-inf" class="quarto-xref"><span>Section 2.3</span></a> will examine the sample-based measures, which are simply the corresponding estimates. Of course, the latter measures will eventually be used in the upcoming executive meeting within the ice cream case. But, for now, let us immerse ourselves a bit into the fundamentals behind the population side of things.</p>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/meeting.png" class="img-fluid figure-img" width="400"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/"><em>Manfred Stege</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-lecture-lecturer-3976299/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<p>Our first measure to explore is related to typical values found within any given population, specifically a measure of central tendency. We can think of this measure as representing “<em>the most common value</em>” we can expect when observing a certain number of random variables that are drawn from this particular population. Let us start with its formal definition.</p>
<div id="Definition-measure-central-tendency" class="definition">
<div class="definition-header">
<p>Definition of measure of central tendency</p>
</div>
<div class="definition-container">
<p>Probabilistically, a measure of central tendency is defined as a metric that identifies a <strong>central or typical value</strong> of a given probability distribution. In other words, a measure of central tendency refers to a central or typical value that a given random variable might take when we observe various realizations of this variable over a long period.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/pulling.png" class="img-fluid figure-img" width="600"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/"><em>Manfred Stege</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-download-data-transfer-3947910/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
</div>
</div>
<p>There is more than one measure of central tendency in the statistical literature. However, for the regression models discussed in this book, we will focus on the expected value (see the <a href="#Definition-expected-value">definition</a> below), which is commonly referred to as the average or mean. You will notice that this measure is closely related to the <strong>mainstream average</strong> we use in our everyday life (to be discussed later on in this section).</p>
<div id="Definition-expected-value" class="definition">
<div class="definition-header">
<p>Definition of expected value</p>
</div>
<div class="definition-container">
<p>Let <span class="math inline">\(Y\)</span> be a random variable whose support is <span class="math inline">\(\mathcal{Y}\)</span>. In general, the expected value or mean <span class="math inline">\(\mathbb{E}(Y)\)</span> of this random variable is defined as a <strong>weighted</strong> average according to its corresponding probability distribution. In other words, this measure of central tendency <span class="math inline">\(\mathbb{E}(Y)\)</span> aims to find the middle value of this random variable by weighting all its possible values in its support <span class="math inline">\(\mathcal{Y}\)</span> as dictated by its probability distribution.</p>
<p>Given the above definition, when <span class="math inline">\(Y\)</span> is a discrete random variable whose PMF is <span class="math inline">\(P_Y(Y = y)\)</span>, then its expected value is mathematically defined as</p>
<p><span id="eq-expected-value-discrete"><span class="math display">\[
\mathbb{E}(Y) = \sum_{y \in \mathcal{Y}} y \cdot P_Y(Y = y).
\tag{2.14}\]</span></span></p>
<p>When <span class="math inline">\(Y\)</span> is a continuous random variable whose PDF is <span class="math inline">\(f_Y(y)\)</span>, its expected value is mathematically defined as</p>
<p><span id="eq-expected-value-continuous"><span class="math display">\[
\mathbb{E}(Y) = \int_{\mathcal{Y}} y \cdot f_Y(y) \mathrm{d}y.
\tag{2.15}\]</span></span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/mean.png" class="img-fluid figure-img" width="600"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/"><em>Manfred Stege</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-pixel-to-learn-goal-3674110/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
</div>
</div>
<div class="Tip">
<div class="Tip-header">
<p>Tip on further measures of central tendency!</p>
</div>
<div class="Tip-container">
<p>In addition to the expected value, there are other measures that will not be explored in this book such as:</p>
<ul>
<li>
<strong>Mode:</strong> For a discrete random variable, the mode is the outcome that corresponds to the highest probability in the PMF. In the case of a continuous random variable, the mode refers to the outcome at which the maximum value occurs in the corresponding PDF.</li>
<li>
<strong>Median:</strong> This measure primarily relates to continuous random variables. The median is the outcome for which there is a probability of <span class="math inline">\(0.5\)</span> for observing a value either greater or lesser than it.</li>
</ul>
</div>
</div>
<p>Note that the discrete random variable case in <a href="#eq-expected-value-discrete" class="quarto-xref">Equation&nbsp;<span>2.14</span></a> somehow resembles the <strong>mainstream average</strong>, which actually would assign an equal weight to each possible outcome of the random variable. On the other hand, for the above statistical definition, we use the corresponding PMF to assign these weights by possible outcome of the discrete random variable.</p>
<p>Let us exemplify this by using our <strong>demand query</strong>. Recall that the <span class="math inline">\(i\)</span>th discrete random variable (as in <a href="#tbl-queries-3" class="quarto-xref">Table&nbsp;<span>2.4</span></a>) <span class="math inline">\(D_i\)</span> is distributed as follows:</p>
<p><span class="math display">\[
D_i \sim \text{Bern}(\pi),
\]</span></p>
<p>whose PMF is defined as</p>
<p><span class="math display">\[
P_{D_i} \left( D_i = d_i \mid \pi \right) = \pi^{d_i} (1 - \pi)^{1 - d_i} \quad \text{for $d_i \in \{ 0, 1 \}$.}
\]</span></p>
<p>Now, by applying <a href="#eq-expected-value-discrete" class="quarto-xref">Equation&nbsp;<span>2.14</span></a>, note we have the following result (which is in fact the formal proof of the expected value of Bernoulli trial):</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-bernoulli-mean-demand"><span class="math display">\[
\begin{align*}
\mathbb{E}(D_i) &amp;= \sum_{d_i = 0}^1 d_i P_{D_i} \left( D_i = d_i \mid \pi \right) \\
&amp;= \sum_{d_i = 0}^1 d_i \left[ \pi^{d_i} (1 - \pi)^{1 - d_i} \right] \\
&amp;= \underbrace{(0) \left[ \pi^0 (1 - \pi) \right]}_{0} + (1) \left[ \pi (1 - \pi)^{0} \right] \\
&amp;= 0 + \pi \\
&amp;= \pi. \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
\tag{2.16}\]</span></span></p>
</div>
<p>For the specific case of a Bernoulli-type population, we have just found that its expected value (from <a href="#eq-bernoulli-mean-demand" class="quarto-xref">Equation&nbsp;<span>2.16</span></a>) is equal to the corresponding parameter we aim to estimate, which is <span class="math inline">\(\pi\)</span>. Then, you might wonder:</p>
<blockquote class="blockquote">
<p><strong>In practical terms for our ice cream company, how can I explain <span class="math inline">\(\mathbb{E}(D_i) = \pi\)</span>?</strong></p>
</blockquote>
<p>Suppose you sample a sufficiently large number of children (i.e., your sample size <span class="math inline">\(n_d \rightarrow \infty\)</span>) from a Bernoulli-type population where <span class="math inline">\(65\%\)</span> of these children prefer chocolate over vanilla (i.e., <span class="math inline">\(\pi = 0.65\)</span>). Then, you will obtain the proportion of observed random variables <span class="math inline">\(d_i\)</span> (for <span class="math inline">\(i = 1, \dots, n_d\)</span>) that correspond to <span class="math inline">\(1\)</span> (as in <a href="#eq-random-variable-demand" class="quarto-xref">Equation&nbsp;<span>2.3</span></a>) which is merely a <strong>mainstream average</strong>. Theoretically speaking, according to our proof in <a href="#eq-bernoulli-mean-demand" class="quarto-xref">Equation&nbsp;<span>2.16</span></a>, this <strong>observed proportion</strong> should converge to the true population parameter <span class="math inline">\(\pi = 0.65\)</span>.</p>
<p>Let us now explore the expected value for a continuous random variable using our <strong>time query</strong>. Unlike the previous case depicted in <a href="#eq-expected-value-discrete" class="quarto-xref">Equation&nbsp;<span>2.14</span></a>, the continuous counterpart in <a href="#eq-expected-value-continuous" class="quarto-xref">Equation&nbsp;<span>2.15</span></a> cannot utilize a summation. Instead, since we have an uncountably infinite set of possible values for a continuous random variable, we must use an integral. This integral involves the corresponding PDF, which weights all possible observed outcomes of the continuous random variable in conjunction with the differential.</p>
<p>Moving along in this query, recall the <span class="math inline">\(j\)</span>th continuous random variable (as in <a href="#tbl-queries-3" class="quarto-xref">Table&nbsp;<span>2.4</span></a>) <span class="math inline">\(T_j\)</span> is distributed as follows:</p>
<p><span class="math display">\[
T_j \sim \text{Exponential}(\beta),
\]</span></p>
<p>whose PDF is defined as</p>
<p><span class="math display">\[
f_{T_j} \left(t_j \mid \beta \right) = \frac{1}{\beta} \exp \left( -\frac{t_j}{\beta} \right) \quad \text{for $t_j \in [0, \infty )$.}
\]</span></p>
<p>Now, by applying <a href="#eq-expected-value-continuous" class="quarto-xref">Equation&nbsp;<span>2.15</span></a>, note we have the following result (which is the formal proof of the expected value of an Exponential-distributed random variable under a scale parametrization):</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-exponential-mean-time-1"><span class="math display">\[
\begin{align*}
\mathbb{E}(T_j) &amp;= \int_{t_j = 0}^{t_j = \infty} t_j f_{T_j} \left(t_j \mid \beta \right) \mathrm{d}t_j \\
&amp;= \int_{t_j = 0}^{t_j = \infty} \frac{t_j}{\beta} \exp \left( -\frac{t_j}{\beta} \right) \mathrm{d}t_j \\
&amp;= \frac{1}{\beta} \int_{t_j = 0}^{t_j = \infty} t_j \exp \left( -\frac{t_j}{\beta} \right) \mathrm{d}t_j. \\
\end{align*}
\tag{2.17}\]</span></span></p>
<p><a href="#eq-exponential-mean-time-1" class="quarto-xref">Equation&nbsp;<span>2.17</span></a> cannot be solved straightforwardly, we need to use integration by parts as follows:</p>
<p><span class="math display">\[
\begin{align*}
u &amp;= t_j &amp; &amp;\Rightarrow &amp; \mathrm{d}u &amp;= \mathrm{d}t_j \\
\mathrm{d}v &amp;= \exp \left( -\frac{t_j}{\beta} \right) \mathrm{d}t_j &amp; &amp;\Rightarrow &amp; v &amp;= -\beta \exp \left( -\frac{t_j}{\beta} \right),
\end{align*}
\]</span></p>
<p>which yields</p>
<p><span id="eq-exponential-mean-time-2"><span class="math display">\[
\begin{align*}
\mathbb{E}(T_j) &amp;= \frac{1}{\beta} \left[ u v \Bigg|_{t_j = 0}^{t_j = \infty} - \int_{t_j = 0}^{t_j = \infty} v \mathrm{d}u \right] \\
&amp;= \frac{1}{\beta} \Bigg\{ \left[ -\beta t_j \exp \left( -\frac{t_j}{\beta} \right) \right] \Bigg|_{t_j = 0}^{t_j = \infty} + \\
&amp; \qquad \beta \int_{t_j = 0}^{t_j = \infty} \exp \left( -\frac{t_j}{\beta} \right) \mathrm{d}t_j \Bigg\} \\
&amp;= \frac{1}{\beta} \Bigg\{ -\beta \Bigg[ \underbrace{\infty \times \exp(-\infty)}_{0} - \underbrace{0 \times \exp(0)}_{0} \Bigg] - \\
&amp; \qquad \beta^2 \exp \left( -\frac{t_j}{\beta} \right) \Bigg|_{t_j = 0}^{t_j = \infty} \Bigg\} \\
&amp;= \frac{1}{\beta} \left\{ -\beta (0) - \beta^2 \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \right\} \\
&amp;= \frac{1}{\beta} \left[ 0 - \beta^2 (0 - 1) \right] \\
&amp;= \frac{\beta^2}{\beta} \\
&amp;= \beta. \qquad \qquad \qquad \qquad \qquad \qquad \quad \qquad \qquad \square
\end{align*}
\tag{2.18}\]</span></span></p>
</div>
<p>Again, for the specific case of an Exponential-type population, we have just found that the its expected value (from <a href="#eq-exponential-mean-time-2" class="quarto-xref">Equation&nbsp;<span>2.18</span></a>) is equal to the corresponding parameter we aim to estimate, which is <span class="math inline">\(\beta\)</span>. This yields the following question:</p>
<blockquote class="blockquote">
<p><strong>In practical terms for our ice cream company, how can I explain <span class="math inline">\(\mathbb{E}(T_j) = \beta\)</span>?</strong></p>
</blockquote>
<p>Imagine you sample a sufficiently large number of customer-to-customer waiting times (i.e., your sample size <span class="math inline">\(n_t \rightarrow \infty\)</span>) from an Exponential-type population where the true average waiting time is <span class="math inline">\(\beta = 10\)</span> minutes. Then, you will obtain the <strong>mainstream average</strong> coming from these observed random variables <span class="math inline">\(t_j\)</span> (for <span class="math inline">\(j = 1, \dots, n_t\)</span>). Theoretically speaking, according to our proof in <a href="#eq-exponential-mean-time-2" class="quarto-xref">Equation&nbsp;<span>2.18</span></a>, this <strong>observed mainstream average</strong> should converge to the true population parameter <span class="math inline">\(\beta = 10\)</span>.</p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on the so-called mainstream average!</p>
</div>
<div class="Heads-up-container">
<p>In general, suppose that you obtain <span class="math inline">\(n\)</span> realizations <span class="math inline">\(y_k\)</span> (for <span class="math inline">\(k = 1, \dots, n\)</span>) of a given random variable <span class="math inline">\(Y\)</span>. The <strong>observed mainstream average</strong> is given by</p>
<p><span id="eq-mainstream-average"><span class="math display">\[
\bar{y} = \frac{\sum_{k = 1}^n y_k}{n}.
\tag{2.19}\]</span></span></p>
<p>For the respective classes of observed random variables in the <strong>demand</strong> and <strong>time queries</strong>, <a href="#eq-mainstream-average" class="quarto-xref">Equation&nbsp;<span>2.19</span></a> can be applied to obtain estimates of the corresponding population parameters <span class="math inline">\(\pi\)</span> and <span class="math inline">\(\beta\)</span>. The statistical rationale for using <a href="#eq-mainstream-average" class="quarto-xref">Equation&nbsp;<span>2.19</span></a> will be expanded in <a href="#sec-mle" class="quarto-xref"><span>Section 2.2</span></a>.</p>
</div>
</div>
<p>We have discussed measures of central tendency in detail, more specifically the mean, which represents typical values derived from observing a standalone random variable over a sufficiently large number of trials, denoted as <span class="math inline">\(n\)</span>. Interestingly, there is a useful probabilistic theorem that allows us to calculate expected values for general mathematical functions involving a standalone random variable. This theorem is crucial for obtaining another type of measure related to the <strong>spread</strong> of a random variable.</p>
<p>As we previously highlighted, you might choose to skip the tip admonition below; however, recall the theorem’s mathematical expressions for discrete random variables found in <a href="#eq-expected-value-discrete-function" class="quarto-xref">Equation&nbsp;<span>2.20</span></a> and for continuous random variables found in <a href="#eq-expected-value-continuous-function" class="quarto-xref">Equation&nbsp;<span>2.26</span></a>, as they will be important for the upcoming discussions about our ice cream case.</p>
<div class="Tip">
<div class="Tip-header">
<p>Tip on the Law of the Unconscious Statistician!</p>
</div>
<div class="Tip-container">
<p>The <strong>law of the unconscious statistician (LOTUS)</strong> is a particular theorem in probability theory that allows us to compute a wide variety of expected values. Let us properly define it for both discrete and continuous random variables.</p>
<div id="thm-LOTUS-theorem-discrete" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.1</strong></span> Let <span class="math inline">\(Y\)</span> be a discrete random variable whose support is <span class="math inline">\(\mathcal{Y}\)</span>. The <strong>LOTUS</strong> indicates that the expected value of a general function <span class="math inline">\(g(Y)\)</span> of this random variable <span class="math inline">\(Y\)</span> can be obtained via <span class="math inline">\(g(Y)\)</span> along with the corresponding PMF <span class="math inline">\(P_Y(Y = y)\)</span>. Hence, the expected value of <span class="math inline">\(g(Y)\)</span> can be obtained as</p>
<p><span id="eq-expected-value-discrete-function"><span class="math display">\[
\mathbb{E}\left[ g(Y) \right] = \sum_{y \in \mathcal{Y}} g(y) \cdot P_Y(Y = y).
\tag{2.20}\]</span></span></p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let us explore the rationale provided by <span class="citation" data-cites="soch2023">Soch et al. (<a href="references.html#ref-soch2023" role="doc-biblioref">2024</a>)</span>. Thus, we will rename the general function <span class="math inline">\(g(Y)\)</span> as another random variable called <span class="math inline">\(Z\)</span> such that:</p>
<p><span id="eq-rv-transformation-to-z-discrete-LOTUS"><span class="math display">\[
Z = g(Y).
\tag{2.21}\]</span></span></p>
<p>Note this function <span class="math inline">\(g(Y)\)</span> can take on equal values <span class="math inline">\(g(y_1), g(y_2), \dots\)</span> coming from different observed values <span class="math inline">\(y_1, y_2, \dots\)</span>; for example, if</p>
<p><span class="math display">\[
g(y) = y^2
\]</span></p>
<p>both</p>
<p><span class="math display">\[
y_1 = 2 \quad \text{and} \quad y_2 = -2
\]</span></p>
<p>yield</p>
<p><span class="math display">\[
g(y_1) = g(y_2) = 4.
\]</span></p>
<p>The above <a href="#eq-rv-transformation-to-z-discrete-LOTUS" class="quarto-xref">Equation&nbsp;<span>2.21</span></a> is formally called a <strong>random variable transformation</strong> from the general function of random variable <span class="math inline">\(Y\)</span>, <span class="math inline">\(g(Y)\)</span>, to a new random variable <span class="math inline">\(Z\)</span>. Having said that, when we set up a transformation of this class, there will be a <strong>support mapping</strong> from this general function <span class="math inline">\(g(Y)\)</span> to <span class="math inline">\(Z\)</span>. This will also yield a proper PMF,</p>
<p><span class="math display">\[
P_Z(Z = z) : \mathbb{R} \rightarrow [0, 1] \quad \forall z \in \mathcal{Z},
\]</span></p>
<p>given that <span class="math inline">\(g(Y)\)</span> is a random variable-based function.</p>
<p>Therefore, using the expected value definition for a discrete random variable as in <a href="#eq-expected-value-discrete" class="quarto-xref">Equation&nbsp;<span>2.14</span></a>, we have the following for <span class="math inline">\(Z\)</span>:</p>
<p><span id="eq-expected-discrete-Z-LOTUS"><span class="math display">\[
\mathbb{E}(Z) = \sum_{z \in \mathcal{Z}} z \cdot P_Z(Z = z).
\tag{2.22}\]</span></span></p>
<p>Within the support <span class="math inline">\(\mathcal{Z}\)</span>, suppose that <span class="math inline">\(z_1, z_2, \dots\)</span> are the possible different values of <span class="math inline">\(Z\)</span> corresponding to function <span class="math inline">\(g(Y)\)</span>. Then, for the <span class="math inline">\(i\)</span>th value <span class="math inline">\(z_i\)</span> in this correspondence, let <span class="math inline">\(I_i\)</span> be the collection of all <span class="math inline">\(y_j\)</span> such that</p>
<p><span id="eq-LOTUS-discrete-subset"><span class="math display">\[
g(y_j) = z_i.
\tag{2.23}\]</span></span></p>
<p>Now, let us tweak a bit the above expression from <a href="#eq-expected-discrete-Z-LOTUS" class="quarto-xref">Equation&nbsp;<span>2.22</span></a> to include this setting:</p>
<p><span id="eq-LOTUS-discrete-subset-2"><span class="math display">\[
\begin{align*}
\mathbb{E}(Z) &amp;= \sum_{z \in \mathcal{Z}} z \cdot P_Z(Z = z) \\
&amp;= \sum_{i} z_i \cdot P_{g(Y)}(Z = z_i) \\
&amp; \qquad \text{we subset the summation to all $z_i$ with $Z = g(Y)$}\\
&amp;= \sum_{i} z_i \sum_{j \in I_i} P_Y(Y = y_j). \\
\end{align*}
\tag{2.24}\]</span></span></p>
<p>The last line of <a href="#eq-LOTUS-discrete-subset-2" class="quarto-xref">Equation&nbsp;<span>2.24</span></a> maps the probabilities associated to all <span class="math inline">\(z_i\)</span> in the corresponding PMF of <span class="math inline">\(Z\)</span>, <span class="math inline">\(P_Z(\cdot)\)</span> via the function <span class="math inline">\(g(Y)\)</span>, to the original PMF of <span class="math inline">\(Y\)</span>, <span class="math inline">\(P_Y(\cdot)\)</span>, for all those <span class="math inline">\(y_j\)</span> contained in the collection <span class="math inline">\(I_i\)</span>. Given that certain values <span class="math inline">\(z_i\)</span> can be obtained with more than one value <span class="math inline">\(y_j\)</span>, such as in the above example when <span class="math inline">\(g(y) = y^2\)</span> for <span class="math inline">\(y_1 = 2\)</span> and <span class="math inline">\(y_2 = -2\)</span>, note we have a second summation of probabilities applied to the PMF of <span class="math inline">\(Y\)</span>.</p>
<p>Moving along with <a href="#eq-LOTUS-discrete-subset-2" class="quarto-xref">Equation&nbsp;<span>2.24</span></a> in conjunction with <a href="#eq-LOTUS-discrete-subset" class="quarto-xref">Equation&nbsp;<span>2.23</span></a>, we have that:</p>
<p><span id="eq-LOTUS-discrete-subset-3"><span class="math display">\[
\begin{align*}
\mathbb{E}(Z) &amp;= \sum_{i} z_i \sum_{j \in I_i} P_Y(Y = y_j) \\
&amp;= \sum_{i} \sum_{j \in I_i} z_i \cdot P_Y(Y = y_j) \\
&amp;= \sum_{i} \sum_{j \in I_i} g(y_j) \cdot P_Y(Y = y_j).
\end{align*}
\tag{2.25}\]</span></span></p>
<p>The double summation in <a href="#eq-LOTUS-discrete-subset-3" class="quarto-xref">Equation&nbsp;<span>2.25</span></a> can be summarized into a single one, given neither of the factors on the right-hand side is subindexed by <span class="math inline">\(i\)</span>. Furthermore, this standalone summation can be applied to all <span class="math inline">\(y \in \mathcal{Y}\)</span> while getting rid of the subindex <span class="math inline">\(j\)</span> in the factors on the right-hand side:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}(Z) &amp;= \sum_{i} \sum_{j \in I_i} g(y_j) \cdot P_Y(Y = y_j) \\
&amp;= \sum_{y \in \mathcal{Y}} g(y) \cdot P_Y(Y = y) \\
&amp;= \mathbb{E}\left[ g(Y) \right].
\end{align*}
\]</span></p>
<p>Therefore, we have:</p>
<p><span class="math display">\[
\mathbb{E}\left[ g(Y) \right] = \sum_{y \in \mathcal{Y}} g(y) \cdot P_Y(Y = y). \quad \square
\]</span></p>
</div>
</div>
<div id="thm-LOTUS-theorem-continuous" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.2</strong></span> Let <span class="math inline">\(Y\)</span> be a continuous random variable whose support is <span class="math inline">\(\mathcal{Y}\)</span>. The <strong>LOTUS</strong> indicates that the expected value of a general function <span class="math inline">\(g(Y)\)</span> of this random variable <span class="math inline">\(Y\)</span> can be obtained via <span class="math inline">\(g(Y)\)</span> along with the corresponding PDF <span class="math inline">\(f_Y(y)\)</span>. Thus, the expected value of <span class="math inline">\(g(Y)\)</span> can be obtained as</p>
<p><span id="eq-expected-value-continuous-function"><span class="math display">\[
\mathbb{E}\left[ g(Y) \right] = \int_{\mathcal{Y}} g(y) \cdot f_Y(y).
\tag{2.26}\]</span></span></p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let us explore the rationale provided by <span class="citation" data-cites="soch2023">Soch et al. (<a href="references.html#ref-soch2023" role="doc-biblioref">2024</a>)</span>. Hence, we will rename the general function <span class="math inline">\(g(Y)\)</span> as another random variable called <span class="math inline">\(Z\)</span> such that:</p>
<p><span id="eq-rv-transformation-to-z-continuous-LOTUS"><span class="math display">\[
Z = g(Y).
\tag{2.27}\]</span></span></p>
<p>As in the discrete LOTUS proof, the above <a href="#eq-rv-transformation-to-z-continuous-LOTUS" class="quarto-xref">Equation&nbsp;<span>2.27</span></a> is formally called a <strong>random variable transformation</strong> from the general function of random variable <span class="math inline">\(Y,\)</span> <span class="math inline">\(g(Y)\)</span>, to a new random variable <span class="math inline">\(Z\)</span>. Therefore, when we set up a transformation of this class, there will be a <strong>support mapping</strong> from this general function <span class="math inline">\(g(Y)\)</span> to <span class="math inline">\(Z\)</span>. This will also yield a proper PDF:</p>
<p><span class="math display">\[
f_Z(z) : \mathbb{R} \rightarrow [0, 1] \quad \forall z \in \mathcal{Z},
\]</span></p>
<p>given that <span class="math inline">\(g(Y)\)</span> is a random variable-based function.</p>
<p>Now, we will use the concept of the cumulative distribution function (CDF) for a continuous random variable <span class="math inline">\(Z\)</span>:</p>
<p><span id="eq-cdf-z"><span class="math display">\[
\begin{align*}
F_Z(z) &amp;= P(Z \leq z) \\
&amp;= P\left[g(Y) \leq z \right] \\
&amp;= P\left[Y \leq g^{-1}(z) \right] \\
&amp;= F_Y\left[ g^{-1}(z) \right].
\end{align*}
\tag{2.28}\]</span></span></p>
<p>A well-known Calculus result is the <strong>inverse function theorem</strong>. Assuming that</p>
<p><span class="math display">\[
z = g(y)
\]</span></p>
<p>is an invertible and differentiable function, then the inverse</p>
<p><span id="eq-inverse-z"><span class="math display">\[
y = g^{-1}(z)
\tag{2.29}\]</span></span></p>
<p>must be differentiable as in:</p>
<p><span id="eq-inv-function-theorem"><span class="math display">\[
\frac{\mathrm{d}}{\mathrm{d}z} \left[ g^{-1}(z) \right] = \frac{1}{g' \left[ g^{-1}(z) \right]}.
\tag{2.30}\]</span></span></p>
<p>Note that we differentiate <a href="#eq-inverse-z" class="quarto-xref">Equation&nbsp;<span>2.29</span></a> as follows:</p>
<p><span id="eq-derivative-inv-y"><span class="math display">\[
\frac{\mathrm{d}}{\mathrm{d}z} y = \frac{\mathrm{d}}{\mathrm{d}z} \left[ g^{-1}(z) \right].
\tag{2.31}\]</span></span></p>
<p>Then, plugging <a href="#eq-derivative-inv-y" class="quarto-xref">Equation&nbsp;<span>2.31</span></a> into <a href="#eq-inv-function-theorem" class="quarto-xref">Equation&nbsp;<span>2.30</span></a>, we obtain:</p>
<p><span id="eq-derivative-inv-y-2"><span class="math display">\[
\begin{gather*}
\frac{\mathrm{d}}{\mathrm{d}z} y = \frac{1}{g' \left[ g^{-1}(z) \right]} \\
\mathrm{d}y = \frac{1}{g' \left[ g^{-1}(z) \right]} \mathrm{d}z.
\end{gather*}
\tag{2.32}\]</span></span></p>
<p>Then, we use the property that relates the CDF <span class="math inline">\(F_Z(z)\)</span> to the PDF <span class="math inline">\(f_Z(z)\)</span>:</p>
<p><span class="math display">\[
f_Z(z) = \frac{\mathrm{d}}{\mathrm{d}z} F_Z(z).
\]</span></p>
<p>Using <a href="#eq-cdf-z" class="quarto-xref">Equation&nbsp;<span>2.28</span></a>, we have:</p>
<p><span class="math display">\[
\begin{align*}
f_Z(z) &amp;= \frac{\mathrm{d}}{\mathrm{d}z} F_Z(z) \\
&amp;= \frac{\mathrm{d}}{\mathrm{d}z} F_Y\left[ g^{-1}(z) \right] \\
&amp;= f_Y\left[ g^{-1}(z) \right] \frac{\mathrm{d}}{\mathrm{d}z} \left[ g^{-1}(z) \right].
\end{align*}
\]</span></p>
<p>Then, via <a href="#eq-inv-function-theorem" class="quarto-xref">Equation&nbsp;<span>2.30</span></a>, it follows that:</p>
<p><span id="eq-pdf-continuous-Z-LOTUS"><span class="math display">\[
f_Z(z) = f_Y\left[ g^{-1}(z) \right] \frac{1}{g' \left[ g^{-1}(z) \right]}.
\tag{2.33}\]</span></span></p>
<p>Therefore, using the expected value definition for a continuous random variable as in <a href="#eq-expected-value-continuous" class="quarto-xref">Equation&nbsp;<span>2.15</span></a>, we have for <span class="math inline">\(Z\)</span> that</p>
<p><span class="math display">\[
\mathbb{E}(Z) = \int_{\mathcal{Z}} z \cdot f_Z(z) \mathrm{d}z,
\]</span></p>
<p>which yields via <a href="#eq-pdf-continuous-Z-LOTUS" class="quarto-xref">Equation&nbsp;<span>2.33</span></a>:</p>
<p><span class="math display">\[
\mathbb{E}(Z) = \int_{\mathcal{Z}} z \cdot f_Y \left[ g^{-1}(z) \right] \frac{1}{g' \left[ g^{-1}(z) \right]} \mathrm{d}z.
\]</span></p>
<p>Using <a href="#eq-inverse-z" class="quarto-xref">Equation&nbsp;<span>2.29</span></a> and <a href="#eq-derivative-inv-y-2" class="quarto-xref">Equation&nbsp;<span>2.32</span></a>, it follows that:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}(Z) &amp;= \int_{\mathcal{Z}} z \cdot f_Y(y) \frac{1}{g' \left[ g^{-1}(z) \right]} \mathrm{d}z \\
&amp;= \int_{\mathcal{Y}} g(y) \cdot f_Y(y) \mathrm{d}y.
\end{align*}
\]</span></p>
<p>Note the last line in the above equation changes the integration limits to the support of <span class="math inline">\(Y\)</span>, given all terms end up depending on <span class="math inline">\(y\)</span> on the right-hand side.</p>
<p>Finally, given the random variable transformation from <a href="#eq-rv-transformation-to-z-continuous-LOTUS" class="quarto-xref">Equation&nbsp;<span>2.27</span></a>, we have:</p>
<p><span class="math display">\[
\mathbb{E}\left[ g(X) \right] = \int_{\mathcal{Y}} g(y) \cdot f_Y(y) \mathrm{d}y. \quad \square
\]</span></p>
</div>
</div>
</div>
</div>
<p>The previous tip was quite theoretical! With that in mind, let us move on to the next stage where we will measure the spread of a random variable. This type of measure is formally known as a measure of uncertainty. We can think of this measure as a way to quantify the dispersion of any random variable from a specific population as we continue to observe it over a given number of trials. Let us begin with its formal definition.</p>
<div id="Definition-measure-uncertainty" class="definition">
<div class="definition-header">
<p>Definition of measure of uncertainty</p>
</div>
<div class="definition-container">
<p>Probabilistically, a measure of uncertainty refers to the <strong>spread</strong> of a given random variable when we observe its different realizations in the long term. Note a <strong>larger spread</strong> indicates more variability in these realizations. On the other hand, a <strong>smaller spread</strong> denotes less variability in these realizations.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/round.png" class="img-fluid figure-img" width="600"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/"><em>Manfred Stege</em></a> via <a href="https://pixabay.com/vectors/children-teacher-rectangular-figures-7464615/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
</div>
</div>
<p>Specifically for this chapter, we will elaborate on the variance (and its derived measure called <strong>standard deviation</strong>) as a measure of uncertainty, given that it is commonly used across the different regression models in subsequent chapters. When it comes to exploratory data analysis, <a href="03-ols.html" class="quarto-xref"><span>Chapter 3</span></a> will introduce an additional measure of this class called the <strong>interquartile range (IQR)</strong>.</p>
<div id="Definition-variance" class="definition">
<div class="definition-header">
<p>Definition of variance</p>
</div>
<div class="definition-container">
<p>Let <span class="math inline">\(Y\)</span> be a discrete or continuous random variable whose support is <span class="math inline">\(\mathcal{Y}\)</span> with a mean represented by <span class="math inline">\(\mathbb{E}(Y)\)</span>. Then, the variance of <span class="math inline">\(Y\)</span> is the mean of the squared deviation from the corresponding mean as follows:</p>
<p><span id="eq-first-variance"><span class="math display">\[
\text{Var}(Y) = \mathbb{E}\left\{[ Y - \mathbb{E}(Y)]^2 \right\}. \\
\tag{2.34}\]</span></span></p>
<p>Note the expression above is equivalent to:</p>
<p><span id="eq-second-variance"><span class="math display">\[
\text{Var}(Y) = \mathbb{E} \left( Y^2 \right) - \left[ \mathbb{E}(Y) \right]^2.
\tag{2.35}\]</span></span></p>
<p>Finally, to put the <strong>spread</strong> measurement on the same units of random variable <span class="math inline">\(Y\)</span>, the <strong>standard devation</strong> of <span class="math inline">\(Y\)</span> is merely the square root of <span class="math inline">\(\text{Var}(Y)\)</span>:</p>
<p><span id="eq-sd"><span class="math display">\[
\text{sd}(Y) = \sqrt{\text{Var}(Y)}.
\tag{2.36}\]</span></span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/tablet.png" class="img-fluid figure-img" width="300"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/"><em>Manfred Stege</em></a> via <a href="https://pixabay.com/vectors/tablet-games-tetris-building-blocks-7426706/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
</div>
</div>
<p>Some of us might wonder why <a href="#eq-first-variance" class="quarto-xref">Equation&nbsp;<span>2.34</span></a> and <a href="#eq-second-variance" class="quarto-xref">Equation&nbsp;<span>2.35</span></a> are equivalent. Therefore, we provide the below tip as an optional clarification on this matter. Note the work of <span class="citation" data-cites="casella2024">Casella and Berger (<a href="references.html#ref-casella2024" role="doc-biblioref">2024</a>)</span> inspires all these insights.</p>
<div class="Tip">
<div class="Tip-header">
<p>Tip on the two mathematical expressions of the variance!</p>
</div>
<div class="Tip-container">
<p>Proving the equivalence of <a href="#eq-first-variance" class="quarto-xref">Equation&nbsp;<span>2.34</span></a> and <a href="#eq-second-variance" class="quarto-xref">Equation&nbsp;<span>2.35</span></a>, requires the introduction of some further properties of the expected value of a random variable while using the <strong>LOTUS</strong>.</p>
<div id="thm-theorem-mean-properties" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.3</strong></span> Let <span class="math inline">\(Y\)</span> be a discrete or continuous random variable. Furthermore, let <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(c\)</span> be constants. Thus, for any functions <span class="math inline">\(g_1(y)\)</span> and <span class="math inline">\(g_2(x)\)</span> whose means exist, we have that:</p>
<p><span id="eq-expected-value-properties"><span class="math display">\[
\mathbb{E}\left[ a g_1(Y) + b g_2(Y) + c \right] = a \mathbb{E}\left[ g_1(Y) \right] + b \mathbb{E}\left[ g_2(Y) \right] + c.
\tag{2.37}\]</span></span></p>
<p>Firstly, let us prove <a href="#eq-expected-value-properties" class="quarto-xref">Equation&nbsp;<span>2.37</span></a> for the discrete case.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(Y\)</span> be a discrete random variable whose support is <span class="math inline">\(\mathcal{Y}\)</span> and PMF is <span class="math inline">\(P_Y(Y = y)\)</span>. Let us apply the <strong>LOTUS</strong> as in <a href="#eq-expected-value-discrete-function" class="quarto-xref">Equation&nbsp;<span>2.20</span></a>:</p>
<p><span class="math display">\[
\mathbb{E}\left[ a g_1(Y) + b g_2(Y) + c \right] = \sum_{y \in \mathcal{Y}} \left[ a g_1(y) + b g_2(y) + c \right] \cdot P_Y(Y = y).
\]</span> We can distribute the summation across each addend as follows:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}\left[ a g_1(Y) + b g_2(Y) + c \right] &amp;= \sum_{y \in \mathcal{Y}} \left[ a g_1(y) \right] \cdot P_Y(Y = y) + \\
&amp; \qquad \sum_{y \in \mathcal{Y}} \left[ b g_2(y) \right] \cdot P_Y(Y = y) + \\
&amp; \qquad \sum_{y \in \mathcal{Y}} c \cdot P_Y(Y = y).
\end{align*}
\]</span></p>
<p>Let us take the constants out of the corresponding summations:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}\left[ a g_1(Y) + b g_2(Y) + c \right] &amp;= a \sum_{y \in \mathcal{Y}} g_1(y) \cdot P_Y(Y = y) + \\
&amp; \qquad b \sum_{y \in \mathcal{Y}} g_2(y) \cdot P_Y(Y = y) + \\
&amp; \qquad c \underbrace{\sum_{y \in \mathcal{Y}} P_Y(Y = y)}_1 \\
&amp;= a \underbrace{\sum_{y \in \mathcal{Y}} g_1(y) \cdot P_Y(Y = y)}_{\mathbb{E} \left[ g_1(Y) \right]} + \\
&amp; \qquad b \underbrace{\sum_{y \in \mathcal{Y}} g_2(y) \cdot P_Y(Y = y)}_{\mathbb{E} \left[ g_2(Y) \right]} + c.
\end{align*}
\]</span></p>
<p>For the first and second addends on the right-hand side in the above equation, let us apply the <strong>LOTUS</strong> again:</p>
<p><span class="math display">\[
\mathbb{E}\left[ a g_1(Y) + b g_2(Y) + c \right] = a \mathbb{E} \left[ g_1(Y) \right] + b \mathbb{E} \left[ g_2(Y) \right] + c. \quad \square
\]</span></p>
</div>
<p>Secondly, let us prove <a href="#eq-expected-value-properties" class="quarto-xref">Equation&nbsp;<span>2.37</span></a> for the continuous case.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(Y\)</span> be a continuous random variable whose support is <span class="math inline">\(\mathcal{Y}\)</span> and PDF is <span class="math inline">\(f_Y(y)\)</span>. Let us apply the <strong>LOTUS</strong> as in <a href="#eq-expected-value-continuous-function" class="quarto-xref">Equation&nbsp;<span>2.26</span></a>:</p>
<p><span class="math display">\[
\mathbb{E}\left[ a g_1(Y) + b g_2(Y) + c \right] = \int_{\mathcal{Y}} \left[ a g_1 (y) + b g_2(y) + c \right] \cdot f_Y(y) \mathrm{d}y.
\]</span></p>
<p>We distribute the integral on the right-hand side of the above equation:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}\left[ a g_1(Y) + b g_2(Y) + c \right] &amp;= \int_{\mathcal{Y}} \left[ a g_1 (y) \right] \cdot f_Y(y) \mathrm{d}y + \\
&amp; \qquad \int_{\mathcal{Y}} \left[ b g_2(y) \right] \cdot f_Y(y) \mathrm{d}y + \\
&amp; \qquad \int_{\mathcal{Y}} c \cdot f_Y(y) \mathrm{d}y.
\end{align*}
\]</span></p>
<p>Let us take the constants out of the corresponding integrals:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}\left[ a g_1(Y) + b g_2(Y) + c \right] &amp;= a \int_{\mathcal{Y}} g_1 (y) \cdot f_Y(y) \mathrm{d}y + \\
&amp; \qquad b \int_{\mathcal{Y}} g_2(y) \cdot f_Y(y) \mathrm{d}y + \\
&amp; \qquad c \underbrace{\int_{\mathcal{Y}} f_Y(y) \mathrm{d}y}_{1} \\
&amp;= a \underbrace{\int_{\mathcal{Y}} g_1 (y) \cdot f_Y(y) \mathrm{d}y}_{\mathbb{E} \left[ g_1(Y) \right]} + \\
&amp; \qquad b \underbrace{\int_{\mathcal{Y}} g_2(y) \cdot f_Y(y) \mathrm{d}y}_{\mathbb{E} \left[ g_2(Y) \right]} + c.
\end{align*}
\]</span></p>
<p>For the first and second addends on the right-hand side in the above equation, let us apply the <strong>LOTUS</strong> again:</p>
<p><span class="math display">\[
\mathbb{E}\left[ a g_1(Y) + b g_2(Y) + c \right] = a \mathbb{E} \left[ g_1(Y) \right] + b \mathbb{E} \left[ g_2(Y) \right] + c. \quad \square
\]</span></p>
</div>
</div>
<p>Finally, after applying some algebraic rearrangements and the expected value properties shown in <a href="#eq-expected-value-properties" class="quarto-xref">Equation&nbsp;<span>2.37</span></a>, <a href="#eq-first-variance" class="quarto-xref">Equation&nbsp;<span>2.34</span></a> and <a href="#eq-second-variance" class="quarto-xref">Equation&nbsp;<span>2.35</span></a> are equivalent as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{align*}
\text{Var}(Y) &amp;= \mathbb{E}\left\{[ Y - \mathbb{E}(Y)]^2 \right\} \\
&amp;= \mathbb{E} \left\{ Y^2 - 2Y \mathbb{E}(Y) + \left[ \mathbb{E}(Y) \right]^2 \right\} \\
&amp;= \mathbb{E} \left( Y^2 \right) - \mathbb{E} \left[ 2Y \mathbb{E}(Y) \right] + \mathbb{E} \left[ \mathbb{E}(Y) \right]^2 \\
&amp; \qquad \text{distributing the expected value operator} \\
&amp;= \mathbb{E} \left( Y^2 \right) - 2 \mathbb{E} \left[ Y \mathbb{E}(Y) \right] + \mathbb{E} \left[ \mathbb{E}(Y) \right]^2 \\
&amp; \qquad \text{since $2$ is a constant} \\
&amp;= \mathbb{E} \left( Y^2 \right) - 2 \mathbb{E}(Y) \mathbb{E} \left( Y \right) + \left[ \mathbb{E}(Y) \right]^2 \\
&amp; \qquad \text{since $\mathbb{E}(Y)$ is a constant} \\
&amp;= \mathbb{E} \left( Y^2 \right) - 2 \left[ \mathbb{E}(Y) \right]^2 + \left[ \mathbb{E}(Y) \right]^2 \\
&amp;= \mathbb{E} \left( Y^2 \right) - \left[ \mathbb{E}(Y) \right]^2.  \qquad \qquad \qquad \qquad \qquad \square
\end{align*}
\]</span></p>
</div>
</div>
</div>
<p>It is time to dig into the application of the variance as a measure of uncertainty for our ice cream case. We will start with the <strong>demand query</strong>. Let us remember that the <span class="math inline">\(i\)</span>th discrete random variable (as in <a href="#tbl-queries-3" class="quarto-xref">Table&nbsp;<span>2.4</span></a>) <span class="math inline">\(D_i\)</span> is distributed as follows:</p>
<p><span class="math display">\[
D_i \sim \text{Bern}(\pi),
\]</span></p>
<p>whose PMF is defined as</p>
<p><span class="math display">\[
P_{D_i} \left( D_i = d_i \mid \pi \right) = \pi^{d_i} (1 - \pi)^{1 - d_i} \quad \text{for $d_i \in \{ 0, 1 \}$.}
\]</span></p>
<p>Via <a href="#eq-expected-value-discrete" class="quarto-xref">Equation&nbsp;<span>2.14</span></a>, the LOTUS in <a href="#eq-expected-value-discrete-function" class="quarto-xref">Equation&nbsp;<span>2.20</span></a>, and <a href="#eq-second-variance" class="quarto-xref">Equation&nbsp;<span>2.35</span></a>, the variance of this Bernoulli-distributed random variable <span class="math inline">\(D_i\)</span> can be found as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{align*}
\text{Var} (D_i) &amp;= \mathbb{E} \left( D_i^2 \right) - \left[ \mathbb{E}(D_i)\right]^2 \\
&amp;= \mathbb{E} \left( D_i^2 \right) - \pi^2 \qquad \text{since $\mathbb{E}(D_i) = \pi$} \\
&amp;= \sum_{d_i = 0}^1 d_i^2 P_{D_i} \left( D_i = d_i \mid \pi \right) - \pi^2 \qquad \text{by LOTUS} \\
&amp;= \left\{ \underbrace{(0^2) \left[ \pi^0 (1 - \pi) \right]}_{0} + \underbrace{(1^2) \left[ \pi (1 - \pi)^{0} \right]}_{\pi} \right\} - \pi^2 \\
&amp;= (0 + \pi) - \pi^2 \\
&amp;= \pi - \pi^2 \\
&amp;= \pi (1 - \pi). \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
\]</span></p>
</div>
<p>Therefore, the <strong>standard deviation</strong> is given by:</p>
<p><span class="math display">\[
\text{sd}(D_i) = \sqrt{\pi (1 - \pi)}.
\]</span></p>
<p>Since the above standard deviation puts the variance on the same units of random variable <span class="math inline">\(D_i\)</span>, you might wonder:</p>
<blockquote class="blockquote">
<p><strong>In practical terms for our ice cream company, how can I explain <span class="math inline">\(\text{sd}(D_i) = \sqrt{\pi (1 - \pi)}\)</span>?</strong></p>
</blockquote>
<p>Before discussing the specific case of a Bernoulli-type population where <span class="math inline">\(65\%\)</span> of children prefer chocolate over vanilla (i.e., a parameter <span class="math inline">\(\pi = 0.65\)</span>), let us first examine the general case for <span class="math inline">\(\pi \in [0, 1]\)</span>, as shown in <a href="#fig-bernoulli-standard-deviation" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>. In this plot, the range of <span class="math inline">\(\pi\)</span> is represented on the <span class="math inline">\(x\)</span>-axis, while <span class="math inline">\(\text{sd}(D_i) = \sqrt{\pi (1 - \pi)}\)</span> is shown on the <span class="math inline">\(y\)</span>-axis. We will initially focus on three specific and increasing values of <span class="math inline">\(\pi\)</span>, which are indicated by orange vertical dashed lines:</p>
<ul>
<li>When <span class="math inline">\(\pi = 0.5\)</span>, it means that there is an <strong>equal chance</strong> of randomly obtaining either a <em>success</em> (i.e., a <span class="math inline">\(1\)</span>) or a <em>failure</em> (i.e., a <span class="math inline">\(0\)</span>). Note the standard deviation is given by <span class="math inline">\(\text{sd}(D_i) = \sqrt{(0.5)(1 - 0.5)} = 0.5\)</span>. Therefore, we state that the square root of the average squared distance from the mean is <span class="math inline">\(0.5\)</span>. Moreover, in <a href="#fig-bernoulli-standard-deviation" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>, this results in the <strong>largest spread</strong> of observed values (either <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>).</li>
<li>When <span class="math inline">\(\pi = 0.7\)</span>, the standard deviation is <span class="math inline">\(\text{sd}(D_i) = \sqrt{(0.7)(1 - 0.7)} \approx 0.46\)</span>. Therefore, we state that the square root of the average squared distance from the mean is <span class="math inline">\(0.46\)</span>. In this case, there is a <strong>higher chance</strong> of randomly getting a <em>success</em> (i.e., a <span class="math inline">\(1\)</span>) compared to a <em>failure</em> (i.e., a <span class="math inline">\(0\)</span>). As a result, there is a <strong>smaller spread</strong> of observed values around the mean of <span class="math inline">\(\pi = 0.7\)</span>, as most values in this population tend to be <span class="math inline">\(1\)</span>.</li>
<li>When <span class="math inline">\(\pi = 0.9\)</span>, the standard deviation is <span class="math inline">\(\text{sd}(D_i) = \sqrt{(0.9)(1 - 0.9)} = 0.3\)</span>. Here, the chance of obtaining a <em>success</em> (i.e., a <span class="math inline">\(1\)</span>) is <strong>even higher</strong> than the chance of a <em>failure</em> (i.e., a <span class="math inline">\(0\)</span>). Therefore, we state that the square root of the average squared distance from the mean is <span class="math inline">\(0.3\)</span>. This leads to an <strong>even smaller spread</strong> of observed values around the mean of <span class="math inline">\(\pi = 0.9\)</span>, with most values in this population also being <span class="math inline">\(1\)</span>.</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div id="fig-bernoulli-standard-deviation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-bernoulli-standard-deviation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02-stats-review_files/figure-html/fig-bernoulli-standard-deviation-1.png" class="img-fluid figure-img" width="1344">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bernoulli-standard-deviation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.3: Behaviour of the theoretical Bernoulli’s standard deviation over the range of its distributional parameter.
</figcaption></figure>
</div>
</div>
</div>
<p>Let us focus on our specific case where <span class="math inline">\(\pi = 0.65\)</span> in terms of the standard deviation, represented by the solid purple vertical line in <a href="#fig-bernoulli-standard-deviation" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>. Suppose you sample a sufficiently large number of children (i.e., your sample size <span class="math inline">\(n_d \rightarrow \infty\)</span>) from a Bernoulli-type population where <span class="math inline">\(65\%\)</span> of these children prefer chocolate over vanilla. In this scenario, the standard deviation <span class="math inline">\(\text{sd}(D_i) = \sqrt{(0.65)(1 - 0.65)} \approx 0.48\)</span> indicates a <strong>slightly smaller spread</strong> compared to a population where there is an equal 50-50 chance of preferring chocolate over vanilla. Thus, when <span class="math inline">\(\pi = 0.65\)</span>, the spread of values around the corresponding mean is slightly narrower since a slight majority leans towards preference for chocolate, which is represented by the value <span class="math inline">\(1\)</span>.</p>
<p>To address our <strong>time query</strong>, we can use a similar approach to obtain variance, but this time for a continuous random variable. Recall that the <span class="math inline">\(j\)</span>th continuous random variable, denoted as <span class="math inline">\(T_j\)</span>, is distributed as follows:</p>
<p><span class="math display">\[
T_j \sim \text{Exponential}(\beta),
\]</span></p>
<p>whose PDF is defined as</p>
<p><span class="math display">\[
f_{T_j} \left(t_j \mid \beta \right) = \frac{1}{\beta} \exp \left( -\frac{t_j}{\beta} \right) \quad \text{for $t_j \in [0, \infty )$.}
\]</span></p>
<p>Via <a href="#eq-second-variance" class="quarto-xref">Equation&nbsp;<span>2.35</span></a> and the <a href="#eq-expected-value-continuous" class="quarto-xref">Equation&nbsp;<span>2.15</span></a> of a continuous expected value, the variance of this Exponential-distributed random variable <span class="math inline">\(T_j\)</span> can be found as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-proof-exponential-scale-variance-1-time"><span class="math display">\[
\begin{align*}
\text{Var} (T_j) &amp;= \mathbb{E} \left( T_j^2 \right) - \left[ \mathbb{E}(T_j)\right]^2 \\
&amp;= \mathbb{E} \left( T_j^2 \right) - \beta^2 \qquad \text{since $\mathbb{E}(T_j) = \beta$}.
\end{align*}
\tag{2.38}\]</span></span></p>
<p>We need to find <span class="math inline">\(\mathbb{E} \left( T_j^2 \right)\)</span> from <a href="#eq-proof-exponential-scale-variance-1-time" class="quarto-xref">Equation&nbsp;<span>2.38</span></a>. Therefore, we make the following derivation via the LOTUS from <a href="#eq-expected-value-continuous-function" class="quarto-xref">Equation&nbsp;<span>2.26</span></a> when <span class="math inline">\(g(T_j) = t_j^2\)</span>:</p>
<p><span id="eq-proof-exponential-scale-variance-2-time"><span class="math display">\[
\begin{align*}
\mathbb{E} \left( T_j^2 \right) &amp;= \int_{t_j = 0}^{t_j = \infty} t_j^2 f_{T_j} \left(t_j \mid \beta \right) \mathrm{d}t_j \\
&amp;= \int_{t_j = 0}^{t_j = \infty} t_j^2 \frac{1}{\beta} \exp \left( -\frac{t_j}{\beta} \right) \mathrm{d}t_j \\
&amp;= \frac{1}{\beta} \int_{t_j = 0}^{t_j = \infty} t_j^2 \exp \left( -\frac{t_j}{\beta} \right) \mathrm{d}t_j. \\
\end{align*}
\tag{2.39}\]</span></span></p>
<p><a href="#eq-proof-exponential-scale-variance-2-time" class="quarto-xref">Equation&nbsp;<span>2.39</span></a> cannot be solved straightforwardly, we need to use integration by parts as follows:</p>
<p><span class="math display">\[
\begin{align*}
u &amp;= t_j^2 &amp; &amp;\Rightarrow &amp; \mathrm{d}u &amp;= 2t_j \mathrm{d}t_j \\
\mathrm{d}v &amp;= \exp \left( -\frac{t_j}{\beta} \right) \mathrm{d}t_j &amp; &amp;\Rightarrow &amp; v &amp;= -\beta \exp \left( -\frac{t_j}{\beta} \right),
\end{align*}
\]</span></p>
<p>which yields</p>
<p><span id="eq-proof-exponential-scale-variance-3-time"><span class="math display">\[
\begin{align*}
\mathbb{E} \left( T_j^2 \right) &amp;= \frac{1}{\beta} \left[ u v \Bigg|_{t_j = 0}^{t_j = \infty} - \int_{t_j = 0}^{t_j = \infty} v \mathrm{d}u \right] \\
&amp;= \frac{1}{\beta} \Bigg\{ \left[ -\beta t_j^2 \exp \left( -\frac{t_j}{\beta} \right) \right] \Bigg|_{t_j = 0}^{t_j = \infty} + \\
&amp; \qquad 2 \beta \int_{t_j = 0}^{t_j = \infty} t_j \exp \left( -\frac{t_j}{\beta} \right) \mathrm{d}t_j \Bigg\} \\
&amp;= \frac{1}{\beta} \Bigg\{ -\beta \Bigg[ \underbrace{\infty \times \exp(-\infty)}_{0} - \underbrace{0 \times \exp(0)}_{0} \Bigg] + \\
&amp; \qquad 2 \beta \int_{t_j = 0}^{t_j = \infty} t_j \exp \left( -\frac{t_j}{\beta} \right) \mathrm{d}t_j \Bigg\} \\
&amp;= \frac{1}{\beta} \left\{ -\beta (0) + 2 \beta \int_{t_j = 0}^{t_j = \infty} t_j \exp \left( -\frac{t_j}{\beta} \right) \mathrm{d}t_j \right\} \\
&amp;= \frac{1}{\beta} \left\{ 0 + 2 \beta \int_{t_j = 0}^{t_j = \infty} t_j \exp \left( -\frac{t_j}{\beta} \right) \mathrm{d}t_j \right\} \\
&amp;= 2 \int_{t_j = 0}^{t_j = \infty} t_j \exp \left( -\frac{t_j}{\beta} \right) \mathrm{d}t_j. \\
\end{align*}
\tag{2.40}\]</span></span></p>
<p>Again, we need to apply integration by parts to solve <a href="#eq-proof-exponential-scale-variance-3-time" class="quarto-xref">Equation&nbsp;<span>2.40</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
u &amp;= t_j &amp; &amp;\Rightarrow &amp; \mathrm{d}u &amp;= \mathrm{d}t_j \\
\mathrm{d}v &amp;= \exp \left( -\frac{t_j}{\beta} \right) \mathrm{d}t_j &amp; &amp;\Rightarrow &amp; v &amp;= -\beta \exp \left( -\frac{t_j}{\beta} \right),
\end{align*}
\]</span></p>
<p>which yields</p>
<p><span id="eq-proof-exponential-scale-variance-4-time"><span class="math display">\[
\begin{align*}
\mathbb{E} \left( T_j^2 \right) &amp;= 2 \left[ u v \Bigg|_{t_j = 0}^{t_j = \infty} - \int_{t_j = 0}^{t_j = \infty} v \mathrm{d}u \right] \\
&amp;= 2 \Bigg\{ \left[ -\beta t_j \exp \left( -\frac{t_j}{\beta} \right) \right] \Bigg|_{t_j = 0}^{t_j = \infty} + \\
&amp; \qquad \beta \int_{t_j = 0}^{t_j = \infty} \exp \left( -\frac{t_j}{\beta} \right) \mathrm{d}t_j \Bigg\} \\
&amp;= 2 \Bigg\{ -\beta \Bigg[ \underbrace{\infty \times \exp(-\infty)}_{0} - \underbrace{0 \times \exp(0)}_{0} \Bigg] - \\
&amp; \qquad \beta^2 \exp \left( -\frac{t_j}{\beta} \right) \Bigg|_{t_j = 0}^{t_j = \infty} \Bigg\} \\
&amp;= 2 \left\{ -\beta (0) - \beta^2 \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \right\} \\
&amp;= 2 \left[ 0 - \beta^2 (0 - 1) \right] \\
&amp;= 2 \beta^2.
\end{align*}
\tag{2.41}\]</span></span></p>
<p>Finally, we plug <a href="#eq-proof-exponential-scale-variance-4-time" class="quarto-xref">Equation&nbsp;<span>2.41</span></a> into <a href="#eq-proof-exponential-scale-variance-1-time" class="quarto-xref">Equation&nbsp;<span>2.38</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
\text{Var} (Y) &amp;= \mathbb{E} \left( Y^2 \right) - \beta^2 \\
&amp;= 2 \beta^2 - \beta^2 \\
&amp;= \beta^2. \qquad \qquad \square
\end{align*}
\]</span></p>
</div>
<p>Hence, the <strong>standard deviation</strong> is given by:</p>
<p><span class="math display">\[
\text{sd}(T_j) = \sqrt{\beta^2} = \beta.
\]</span></p>
<p>Since the above standard deviation puts the variance on the same units of random variable <span class="math inline">\(T_j\)</span>, you might wonder:</p>
<blockquote class="blockquote">
<p><strong>In practical terms for our ice cream company, how can I explain <span class="math inline">\(\text{sd}(T_j) = \beta\)</span>?</strong></p>
</blockquote>
<p>Let us illustrate the behavior of the standard deviation within the context of the Exponential distribution, using a scale parameterization, through <a href="#fig-exponential-standard-deviation" class="quarto-xref">Figure&nbsp;<span>2.4</span></a>. On the <span class="math inline">\(x\)</span>-axis, we have the scale parameter <span class="math inline">\(\beta\)</span>, which has a lower bound of <span class="math inline">\(0\)</span> and an upper bound truncated at <span class="math inline">\(50\)</span>. Nonetheless, according to the definition of the Exponential distribution, recall that <span class="math inline">\(\beta\)</span> can range from <span class="math inline">\(0\)</span> to <span class="math inline">\(\infty\)</span>. Interestingly, both axes are measured in minutes, as the parameter definition of <span class="math inline">\(\beta\)</span> is expressed in the units of the Exponential random variable under this scale parametrization. Additionally, <span class="math inline">\(\text{sd}(T_j)\)</span> indicates the spread of the <span class="math inline">\(T_j\)</span> in the same units. You can confirm these facts in <a href="#tbl-queries-3" class="quarto-xref">Table&nbsp;<span>2.4</span></a>.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-exponential-standard-deviation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-exponential-standard-deviation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02-stats-review_files/figure-html/fig-exponential-standard-deviation-1.png" class="img-fluid figure-img" width="1344">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-exponential-standard-deviation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.4: Behaviour of the theoretical Exponential’s standard deviation, under a scale parametrization, over a truncated range of its distributional parameter.
</figcaption></figure>
</div>
</div>
</div>
<p>Firstly, in <a href="#fig-exponential-standard-deviation" class="quarto-xref">Figure&nbsp;<span>2.4</span></a>, it is important to emphasize that the relationship between the Exponential parameter <span class="math inline">\(\beta\)</span> and the standard deviation <span class="math inline">\(\text{sd}(T_j)\)</span> is <strong>linear</strong>. We have highlighted three specific cases as vertical lines:</p>
<ul>
<li>The orange dashed line on the left indicates a small spread with <span class="math inline">\(\text{sd}(T_j) = 5\)</span> when <span class="math inline">\(\beta = 5\)</span>. This signifies that the square root of the average squared distance from the mean is 5 minutes for the waiting times corresponding to this specific population.</li>
<li>The solid purple line represents our ice cream case, where <span class="math inline">\(\beta = 10\)</span>, resulting in a larger spread with <span class="math inline">\(\text{sd}(T_j) = 10\)</span> compared to the case above. This means that the square root of the average squared distance from the mean is 10 minutes for the waiting times related to this specific population.</li>
<li>The orange dashed line on the right indicates the largest spread, compared to the two cases above, with <span class="math inline">\(\text{sd}(T_j) = 45\)</span> when <span class="math inline">\(\beta = 45\)</span>. This means that the square root of the average squared distance from the mean is 45 minutes for the waiting times corresponding to this specific population.</li>
</ul>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on a further estimation technique for the variance!</p>
</div>
<div class="Heads-up-container">
<p>Before concluding this section, it is important to note that, unlike the expected value case, we will not directly address how a sample-based tool similar to the <strong>mainstream average</strong> (i.e., <a href="#eq-mainstream-average" class="quarto-xref">Equation&nbsp;<span>2.19</span></a>) can be used to estimate the variances for our <strong>demand</strong> and <strong>time queries</strong>. Instead, we will introduce a useful property related to the estimation parameter approach discussed in <a href="#sec-mle" class="quarto-xref"><span>Section 2.2</span></a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/curious.png" class="img-fluid figure-img" width="250"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/"><em>Manfred Stege</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-emotion-fear-expression-6230192/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
</div>
</div>
</section><section id="sec-random-samples" class="level3" data-number="2.1.6"><h3 data-number="2.1.6" class="anchored" data-anchor-id="sec-random-samples">
<span class="header-section-number">2.1.6</span> The Rationale in Random Sampling</h3>
<p>In <a href="#sec-first-insights" class="quarto-xref"><span>Section 2.1.1</span></a>, we explained that <strong>random sampling</strong> allows us to save both financial and operational resources compared to conducting an entire census. This approach is particularly beneficial for estimating our population parameters for both <strong>demand</strong> and <strong>time queries</strong>. Furthermore, we highlighted that random sampling utilizes probabilistic and inferential tools to manage and report the <strong>uncertainty</strong> associated with these <strong>estimations</strong>. In this section, we will begin by examining those probabilistic tools.</p>
<p>It is important to note that random sampling is heavily based on the concept of random variables. However, mathematically expressing a set of random variables in a single expression requires the application of certain probabilistic concepts, specifically conditional probability and independence. While we will certainly need the Bayes’ rule to connect these two concepts, we will not delve deeper into this rule in the subsequent chapters of this book. Therefore, let us begin with conditional probability.</p>
<div id="Definition-conditional-probability" class="definition">
<div class="definition-header">
<p>Definition of conditional probability</p>
</div>
<div class="definition-container">
<p>Suppose you have two events of interest, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, in a random phenomenon of a population or system of interest. These two events belong to the sample space <span class="math inline">\(S\)</span>. Moreover, assume that the probability of event <span class="math inline">\(B\)</span> is such that</p>
<p><span class="math display">\[
P(B) &gt; 0,
\]</span></p>
<p>which is considered the <strong>conditioning event</strong>.</p>
<p>Hence, the conditional probability of event <span class="math inline">\(A\)</span> <strong>given</strong> event <span class="math inline">\(B\)</span> is defined as</p>
<p><span id="eq-conditional-probability"><span class="math display">\[
P(A | B) = \frac{P(A \cap B)}{P(B)},
\tag{2.42}\]</span></span></p>
<p>where <span class="math inline">\(P(A \cap B)\)</span> is read as <strong>the probability of the intersection of events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span></strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/pieces.png" class="img-fluid figure-img" width="250"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3699345"><em>Manfred Steger</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-pixel-to-learn-3674106/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
</div>
</div>
<p>How can we think about the above concept in terms of our ice cream case? Let us consider the following in terms of our <strong>demand query</strong>. For the purpose of this explanation, imagine that we have a small population of <span class="math inline">\(N_d = 20\)</span> children to sample from. We can define an event <span class="math inline">\(B\)</span> as the selection of a specific child, referred to as <strong>child B</strong>, from this group of <span class="math inline">\(N_d = 20\)</span> children. The probability of selecting child B can be calculated as follows:</p>
<p><span class="math display">\[
P(B) = \frac{1}{20}.
\]</span></p>
<p>If you choose to conduct <strong>random sampling without replacement</strong>, you will <strong>not</strong> return any sampled child back to the small population before the next draw. In this context, let us define event <span class="math inline">\(A\)</span> as the selection of a second specific child, referred to as <strong>child A</strong>. When sampling without replacement, the probability of selecting child A (given you already sampled child B) is determined by calculating the conditional probability</p>
<p><span class="math display">\[
P(A | B) = \frac{1}{19}.
\]</span></p>
<p>In the above ratio, note we have updated the sample space to just 19 children given event <span class="math inline">\(A\)</span> is conditioned on <span class="math inline">\(B\)</span>. Finally, how can we connect these two probabilities with the intersection <span class="math inline">\(P(A \cap B)\)</span> found in <a href="#eq-conditional-probability" class="quarto-xref">Equation&nbsp;<span>2.42</span></a>? We can view this intersection as the probability of a sequence of two events:</p>
<ol type="1">
<li>We sample child B from our initial pool of <span class="math inline">\(N_d = 20\)</span> children.</li>
<li>Then, from the updated pool of <span class="math inline">\(N_d - 1 = 19\)</span> children since we are sampling without replacement, we sample child A.</li>
</ol>
<p>Probabilistically, this sequence is expressed as follows:</p>
<p><span class="math display">\[
\begin{align*}
P(A \cap B) &amp;= P(B \cap A) \\
&amp;= P(B) \times P(A | B) \\
&amp;= \frac{1}{20} \times \frac{1}{19} = \frac{1}{380}.
\end{align*}
\]</span></p>
<div class="Tip">
<div class="Tip-header">
<p>Tip on the rationale behind conditional probability!</p>
</div>
<div class="Tip-container">
<p>We can delve into the rationale of <a href="#eq-conditional-probability" class="quarto-xref">Equation&nbsp;<span>2.42</span></a> by using a handy mathematical concept called <strong>cardinality</strong>, which refers to the corresponding total number of possible outcomes in a random phenomenon belonging to any given event or sample space.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(|S|\)</span> be the cardinality corresponding to the sample space in a random phenomenon. Hence, as in <a href="#eq-sample-space" class="quarto-xref">Equation&nbsp;<span>2.2</span></a>, we have that:</p>
<p><span class="math display">\[
P(S) = \frac{|S|}{|S|} = 1.
\]</span></p>
<p>Moreover, suppose that <span class="math inline">\(A\)</span> is the <strong>primary event of interest</strong> whose cardinality is represented by <span class="math inline">\(|A|\)</span>. Alternatively to <a href="#eq-probability" class="quarto-xref">Equation&nbsp;<span>2.1</span></a>, the probability of <span class="math inline">\(A\)</span> can be represented as</p>
<p><span class="math display">\[
P(A) = \frac{|A|}{|S|}.
\]</span></p>
<p>On the other hand, the cardinality of the <strong>conditioning event</strong> is</p>
<p><span id="eq-prob-card-B"><span class="math display">\[
P(B) = \frac{|B|}{|S|}.
\tag{2.43}\]</span></span></p>
<p>Now, let <span class="math inline">\(|A \cap B|\)</span> be the cardinality of the intersection between events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. Its probability can be represented as:</p>
<p><span id="eq-prob-card-A-int-B"><span class="math display">\[
P(A \cap B) = \frac{|A \cap B|}{|B|}.
\tag{2.44}\]</span></span></p>
<p>Analogous to <a href="#eq-prob-card-B" class="quarto-xref">Equation&nbsp;<span>2.43</span></a> and <a href="#eq-prob-card-A-int-B" class="quarto-xref">Equation&nbsp;<span>2.44</span></a>, we can view the conditional probability <span class="math inline">\(P(A | B)\)</span> as an updated probability of the primary event <span class="math inline">\(A\)</span> restricted to the cardinality of the conditioning event <span class="math inline">\(|B|\)</span>. This places <span class="math inline">\(|A \cap B|\)</span> in the numerator and <span class="math inline">\(|B|\)</span> in the denominator as follows:</p>
<p><span id="eq-prob-card-cond-A-B"><span class="math display">\[
P(A | B) = \frac{|A \cap B|}{|B|}.
\tag{2.45}\]</span></span></p>
<p>Therefore, we can play around with <a href="#eq-prob-card-cond-A-B" class="quarto-xref">Equation&nbsp;<span>2.45</span></a> along with <a href="#eq-prob-card-B" class="quarto-xref">Equation&nbsp;<span>2.43</span></a> and <a href="#eq-prob-card-A-int-B" class="quarto-xref">Equation&nbsp;<span>2.44</span></a> as follows:</p>
<p><span class="math display">\[
\begin{align*}
P(A | B) &amp;= \frac{|A \cap B|}{|B|} \\
&amp;= \frac{\frac{|A \cap B}{|S|}}{\frac{|B|}{|S|}} \qquad \text{dividing numerator and denominator over $|S|$} \\
&amp;= \frac{P(A \cap B)}{P(B)}. \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
\]</span></p>
</div>
</div>
</div>
<p>To connect the previous result regarding conditional probability with the concept of statistical independence, we need to utilize Bayes’ rule, which is another important result in probability theory. It is worth noting that this principle is a fundamental aspect of Bayesian statistics.</p>
<div id="Definition-Bayes-rule" class="definition">
<div class="definition-header">
<p>Definition of the Bayes’ rule</p>
</div>
<div class="definition-container">
<p>Suppose you have two events of interest, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, in a random phenomenon of a population or system of interest. From <a href="#eq-conditional-probability" class="quarto-xref">Equation&nbsp;<span>2.42</span></a>, we can state the following expression for the conditional probability of <span class="math inline">\(A\)</span> <strong>given</strong> <span class="math inline">\(B\)</span>:</p>
<p><span id="eq-cond-prob-A-given-B"><span class="math display">\[
P(A | B) = \frac{P(A \cap B)}{P(B)} \quad \text{if $P(B) &gt; 0$.}
\tag{2.46}\]</span></span></p>
<p>Note the conditional probability of <span class="math inline">\(B\)</span> given <span class="math inline">\(A\)</span> can be stated as:</p>
<p><span id="eq-cond-prob-B-given-A"><span class="math display">\[
\begin{align*}
P(B | A) &amp;= \frac{P(B \cap A)}{P(A)} \quad \text{if $P(A) &gt; 0$} \\
&amp;= \frac{P(A \cap B)}{P(A)} \quad \text{since $P(B \cap A) = P(A \cap B)$.}
\end{align*}
\tag{2.47}\]</span></span></p>
<p>Then, we can manipulate <a href="#eq-cond-prob-B-given-A" class="quarto-xref">Equation&nbsp;<span>2.47</span></a> as follows:</p>
<p><span class="math display">\[
P(A \cap B) = P(B | A) \times P(A).
\]</span></p>
<p>The above result can be plugged into <a href="#eq-cond-prob-A-given-B" class="quarto-xref">Equation&nbsp;<span>2.46</span></a>:</p>
<p><span id="eq-bayes-rule"><span class="math display">\[
\begin{align*}
P(A | B) &amp;= \frac{P(A \cap B)}{P(B)} \\
&amp;= \frac{P(B | A) \times P(A)}{P(B)}.
\end{align*}
\tag{2.48}\]</span></span></p>
<p><a href="#eq-bayes-rule" class="quarto-xref">Equation&nbsp;<span>2.48</span></a> is called the Bayes’ rule. We are basically flipping around conditional probabilities.</p>
</div>
</div>
<p>Even though this textbook has a frequentist tone, let us quickly connect <a href="#eq-bayes-rule" class="quarto-xref">Equation&nbsp;<span>2.48</span></a> with the elements mentioned in the <a href="#Definition-bayesian-stats">definition</a> of Bayesian statistics:</p>
<p><span class="math display">\[
P(A | B) = \frac{P(B | A) \times P(A)}{P(B)}.
\]</span></p>
<ul>
<li>
<span class="math inline">\(P(A | B)\)</span> is known as the <strong>posterior</strong> probability of observing a primary event <span class="math inline">\(A\)</span> given the conditioning event <span class="math inline">\(B\)</span>, which is referred to as the <strong>current evidence</strong>.</li>
<li>
<span class="math inline">\(P(B | A)\)</span> is the <strong>conditional</strong> probability of observing the <strong>current evidence</strong> represented by event <span class="math inline">\(B\)</span> given that the primary event <span class="math inline">\(A\)</span> has occurred.</li>
<li>
<span class="math inline">\(P(A)\)</span> is called the <strong>prior</strong> probability of observing the primary event <span class="math inline">\(A\)</span>.</li>
<li>
<span class="math inline">\(P(B)\)</span> represents the overall probability of observing the <strong>current evidence</strong> represented by event <span class="math inline">\(B\)</span>, without considering event <span class="math inline">\(A\)</span>.</li>
</ul>
<p>Finally, let us use all the above results to elaborate on what statistical independence is.</p>
<div id="Definition-independence" class="definition">
<div class="definition-header">
<p>Definition of independence</p>
</div>
<div class="definition-container">
<p>Suppose you have two events of interest, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, in a random phenomenon of a population or system of interest. These two events are <strong>statistically</strong> independent if event <span class="math inline">\(B\)</span> does not affect event <span class="math inline">\(A\)</span> and vice versa. Therefore, the probability of their corresponding intersection is given by:</p>
<p><span id="eq-ind-A-B"><span class="math display">\[
P(A \cap B) = P(A) \times P(B).
\tag{2.49}\]</span></span></p>
<p>Let us expand the above definition to a random variable framework:</p>
<ul>
<li>Suppose you have a set of <span class="math inline">\(n\)</span> discrete random variables <span class="math inline">\(Y_1, \dots, Y_n\)</span> whose supports are <span class="math inline">\(\mathcal{Y_1}, \dots, \mathcal{Y_n}\)</span> with PMFs <span class="math inline">\(P_{Y_1}(Y_1 = y_1), \dots, P_{Y_n}(Y_n = y_n)\)</span> respectively. That said, the joint PMF of these <span class="math inline">\(n\)</span> random variables is the multiplication of their corresponding standalone PMFs:</li>
</ul>
<p><span id="eq-joint-PMF-ind-random-variables"><span class="math display">\[
\begin{align*}
P_{Y_1, \dots, Y_n}(Y_1 = y_1, \dots, Y_n = y_n) &amp;= \prod_{i = 1}^n P_{Y_i}(Y_i = y_i) \\
&amp; \qquad \text{for all} \\
&amp; \qquad \quad y_i \in \mathcal{Y}_i, i = 1, \dots, n.
\end{align*}
\tag{2.50}\]</span></span></p>
<ul>
<li>Suppose you have a set of <span class="math inline">\(n\)</span> continuous random variables <span class="math inline">\(Y_1, \dots, Y_n\)</span> whose supports are <span class="math inline">\(\mathcal{Y_1}, \dots, \mathcal{Y_n}\)</span> with PDFs <span class="math inline">\(f_{Y_1}(y_1), \dots, f_{Y_n}(y_n)\)</span> respectively. That said, the joint PDF of these <span class="math inline">\(n\)</span> random variables is the multiplication of their corresponding standalone PDFs:</li>
</ul>
<p><span id="eq-joint-PDF-ind-random-variables"><span class="math display">\[
\begin{align*}
f_{Y_1, \dots, Y_n}(y_1, \dots, y_n) &amp;= \prod_{i = 1}^n f_{Y_i}(y_i) \\
&amp; \qquad \text{for all} \\
&amp; \qquad \quad y_i \in \mathcal{Y}_i, i = 1, \dots, n.
\end{align*}
\tag{2.51}\]</span></span></p>
</div>
</div>
<p>Let us clarify <a href="#eq-ind-A-B" class="quarto-xref">Equation&nbsp;<span>2.49</span></a> using our ice cream case in relation to our <strong>demand query</strong>. We will revisit the scenario involving a small population consisting of <span class="math inline">\(N_d = 20\)</span> children. We can define an event <span class="math inline">\(B\)</span> as the selection of a specific child, referred to as <strong>child B</strong>, from these <span class="math inline">\(N_d = 20\)</span> children. The probability of selecting child B can be calculated as follows:</p>
<p><span class="math display">\[
P(B) = \frac{1}{20}.
\]</span></p>
<p>When conducting <strong>random sampling with replacement</strong>, each sampled child is returned to the population before the next draw. Let us define event <span class="math inline">\(A\)</span> as the selection of a specific child, referred to as <strong>child A</strong>. In this type of sampling, event <span class="math inline">\(B\)</span> does not influence event <span class="math inline">\(A\)</span>; in other words, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are statistically independent. Therefore, when sampling with replacement, the probability of selecting child A, given that you have already sampled child B, can be determined by calculating the conditional probability</p>
<p><span class="math display">\[
P(A | B) = \frac{1}{20}.
\]</span></p>
<p>In the above ratio, we do not need to update the sample space. We will still have 20 children available for sampling for event <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[
P(A) = P(A | B) = \frac{1}{20}.
\]</span></p>
<p>Therefore, we can connect the probabilities <span class="math inline">\(P(A)\)</span> and <span class="math inline">\(P(B)\)</span> with the intersection <span class="math inline">\(P(A \cap B)\)</span>. Again, this intersection is the probability of a sequence of two events:</p>
<ol type="1">
<li>We sample child B from our initial pool of <span class="math inline">\(N_d = 20\)</span> children.</li>
<li>Then, using the same pool of <span class="math inline">\(N_d = 20\)</span> children since we are sampling with replacement, we sample child A.</li>
</ol>
<p>Probabilistically, this sequence is expressed as follows:</p>
<p><span class="math display">\[
\begin{align*}
P(A \cap B) &amp;= P(B \cap A) \\
&amp;= P(B) \times P(A | B) \\
&amp;= P(B) \times P(A) \\
&amp;= \frac{1}{20} \times \frac{1}{20} = \frac{1}{400}.
\end{align*}
\]</span></p>
<p>This is <a href="#eq-ind-A-B" class="quarto-xref">Equation&nbsp;<span>2.49</span></a>! The tip below theoretically delves further into this equation, but you can skip it if you prefer.</p>
<div class="Tip">
<div class="Tip-header">
<p>Tip on the rationale behind the rule of independent events!</p>
</div>
<div class="Tip-container">
<p>We can delve into the rationale of <a href="#eq-ind-A-B" class="quarto-xref">Equation&nbsp;<span>2.49</span></a> by using the Bayes’ rule from <a href="#eq-bayes-rule" class="quarto-xref">Equation&nbsp;<span>2.48</span></a> along with the basic conditional probability formula from <a href="#eq-conditional-probability" class="quarto-xref">Equation&nbsp;<span>2.42</span></a>.</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Firstly, let us assume that a given event <span class="math inline">\(B\)</span> does not affect event <span class="math inline">\(A\)</span> which can be probabilistically represented as</p>
<p><span id="eq-unconditional-prob-A-B"><span class="math display">\[
P(A | B) = P(A).
\tag{2.52}\]</span></span></p>
<p>If the statement in <a href="#eq-unconditional-prob-A-B" class="quarto-xref">Equation&nbsp;<span>2.52</span></a> holds, by using the Bayes’ rule from <a href="#eq-bayes-rule" class="quarto-xref">Equation&nbsp;<span>2.48</span></a>, we have the following manipulation for the below conditional probability formula:</p>
<p><span class="math display">\[
\begin{align*}
P(B | A) &amp;= \frac{P(B \cap A)}{P(A)} \\
&amp;= \frac{P(A \cap B)}{P(A)} \qquad \text{since $P(B \cap A) = P(A \cap B$)} \\
&amp;= \frac{P(A | B) \times P(B)}{P(A)} \qquad \text{by the Bayes' rule} \\
&amp;= \frac{P(A) \times P(B)}{P(A)} \qquad \text{since $P(A | B) = P(A)$} \\
&amp;= P(B).
\end{align*}
\]</span></p>
<p>Then, again by using the Bayes’ rule, we obtain <span class="math inline">\(P(B \cap A)\)</span> as follows:</p>
<p><span class="math display">\[
\begin{align*}
P(B \cap A) &amp;= P(B | A) \times P(A) \\
&amp;= P(B) \times P(A) \qquad \text{since $P(B | A) = P(B)$.}
\end{align*}
\]</span></p>
<p>Finally, we have that:</p>
<p><span class="math display">\[
\begin{align*}
P(A \cap B) &amp;= P(B \cap A) \\
&amp;= P(B) \times P(A) \\
&amp;= P(A) \times P(B). \qquad \square
\end{align*}
\]</span></p>
</div>
</div>
</div>
<p>It is time to outline how our <strong>random sampling with replacement</strong> (as mentioned in <a href="#sec-random-variables" class="quarto-xref"><span>Section 2.1.3</span></a>) for both <strong>demand</strong> and <strong>time queries</strong> can be mathematically represented using random variables. This representation should also involve a crucial statistical concept such as the random sample. Ultimately, we aim to construct what is known as joint probability distributions, which can take the form of either PMF or PDF depending on the type of random variables we are dealing with.</p>
<p>The previously introduced ideas by <a href="#eq-joint-PMF-ind-random-variables" class="quarto-xref">Equation&nbsp;<span>2.50</span></a> and <a href="#eq-joint-PDF-ind-random-variables" class="quarto-xref">Equation&nbsp;<span>2.51</span></a> serve as the primary steps for our discrete (Bernoulli trials as discussed by <a href="#eq-bernoulli-pmf-demand" class="quarto-xref">Equation&nbsp;<span>2.7</span></a>) and continuous (waiting times as demonstrated by <a href="#eq-exponential-pdf-time" class="quarto-xref">Equation&nbsp;<span>2.11</span></a>) random variables respectively. However, these probability distributions will require an extra tweak, as discussed below.</p>
<div id="Definition-random-sample" class="definition">
<div class="definition-header">
<p>Definition of random sample</p>
</div>
<div class="definition-container">
<p>A random sample is a collection of random variables <span class="math inline">\(Y_1, \dots, Y_n\)</span> of size <span class="math inline">\(n\)</span> coming from a given population or system of interest. Note that <strong>the most elementary definition</strong> of a random sample assumes that these <span class="math inline">\(n\)</span> random variables are mutually <strong>independent and identically distributed</strong> (which is abbreviated as <em>iid</em>).</p>
<p>The fact that these <span class="math inline">\(n\)</span> random variables are identically distributed indicates that they have the same mathematical form for their corresponding PMFs or PDFs, depending on whether they are discrete or continuous respectively. Hence, under a generative modelling approach in a population or system of interest governed by <span class="math inline">\(k\)</span> parameters contained in the vector</p>
<p><span class="math display">\[
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T,
\]</span></p>
<p>we can apply the <em>iid</em> property in an elementary random sample to obtain the following joint probability distributions:</p>
<ul>
<li>In the case of <span class="math inline">\(n\)</span> <em>iid</em> discrete random variables <span class="math inline">\(Y_1, \dots, Y_n\)</span> whose common standalone PMF is <span class="math inline">\(P_Y(Y = y | \boldsymbol{\theta})\)</span> with support <span class="math inline">\(\mathcal{Y}\)</span>, the joint PMF is mathematically expressed as</li>
</ul>
<p><span id="eq-joint-PMF-iid-random-variables"><span class="math display">\[
\begin{align*}
P_{Y_1, \dots, Y_n}(Y_1 = y_1, \dots, Y_n = y_n | \boldsymbol{\theta}) &amp;= \prod_{i = 1}^n P_Y(Y = y_i | \boldsymbol{\theta}) \\
&amp; \quad \text{for all} \\
&amp; \quad \quad y_i \in \mathcal{Y}, i = 1, \dots, n.
\end{align*}
\tag{2.53}\]</span></span></p>
<ul>
<li>In the case of <span class="math inline">\(n\)</span> <em>iid</em> continuous random variables <span class="math inline">\(Y_1, \dots, Y_n\)</span> whose common standalone PDF is <span class="math inline">\(f_Y(y | \boldsymbol{\theta})\)</span> with support <span class="math inline">\(\mathcal{Y}\)</span>, the joint PDF is mathematically expressed as</li>
</ul>
<p><span id="eq-joint-PDF-iid-random-variables"><span class="math display">\[
\begin{align*}
f_{Y_1, \dots, Y_n}(y_1, \dots, y_n | \boldsymbol{\theta}) &amp;= \prod_{i = 1}^n f_Y(y_i | \boldsymbol{\theta}) \\
&amp; \quad \text{for all} \\
&amp; \quad \quad y_i \in \mathcal{Y}, i = 1, \dots, n.
\end{align*}
\tag{2.54}\]</span></span></p>
<p>Unlike <a href="#eq-joint-PMF-ind-random-variables" class="quarto-xref">Equation&nbsp;<span>2.50</span></a> and <a href="#eq-joint-PDF-ind-random-variables" class="quarto-xref">Equation&nbsp;<span>2.51</span></a>, note that <a href="#eq-joint-PMF-iid-random-variables" class="quarto-xref">Equation&nbsp;<span>2.53</span></a> and <a href="#eq-joint-PDF-iid-random-variables" class="quarto-xref">Equation&nbsp;<span>2.54</span></a> <strong>do not</strong> indicate a subscript for <span class="math inline">\(Y\)</span> in the corresponding probability distributions since we have identically distributed random variables. Furthermore, the joint distributions are conditioned on the population parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span> which reflects our generative modelling approach.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/sample.png" class="img-fluid figure-img" width="500"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3699345"><em>Manfred Steger</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-lecture-lecture-hall-3976296/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
</div>
</div>
<p>Alright! We are ready to deliver our joint probability distributions for our ice cream case <strong>assuming sampling with replacement</strong>. Thus, in the <strong>demand query</strong> via a random sample under the <em>iid</em> property along with the standalone PMF for <span class="math inline">\(D_i\)</span></p>
<p><span class="math display">\[
P_{D_i} \left( D_i = d_i \mid \pi \right) = \pi^{d_i} (1 - \pi)^{1 - d_i} \quad \text{for $d_i \in \{ 0, 1 \}$},
\]</span></p>
<p>our joint PMF is</p>
<p><span id="eq-joint-PMF-iid-demand"><span class="math display">\[
\begin{align*}
P_{D_1, \dots, D_{n_d}} \left( D_1 = d_1, \dots, D_{n_d} = d_{n_d} | \pi \right) &amp;= \prod_{i = 1}^{n_d} P_{D_i} \left( D_i = d_i \mid \pi \right) \\
&amp;= \prod_{i = 1}^{n_d} P_D \left( D = d_i \mid \pi \right) \\
&amp; \qquad \qquad \qquad \quad \quad \text{iid} \quad \\
&amp;= \prod_{i = 1}^{n_d} \pi^{d_i} (1 - \pi)^{1 - d_i} \\
&amp; \quad \text{for all} \\
&amp; \quad \quad d_i \in \{ 0, 1 \}, \\
&amp; \qquad \quad i = 1, \dots, n_d.
\end{align*}
\tag{2.55}\]</span></span></p>
<p>It is important to clarify that <span class="math inline">\(n_d\)</span> refers to our sample size for this query. In contrast, our vector of population parameters consists of a single element: the probability of success <span class="math inline">\(\pi\)</span> (i.e., the proportion of children who prefer chocolate over vanilla from our population). Recall that, in practice, this parameter is unknown and we aim to estimate it via our <span class="math inline">\(n_d\)</span> sampled data points. In the following section, we will see that these data points are contained in the data frame <code>children_sample</code> from <a href="#sec-characterizing-prob-dist" class="quarto-xref"><span>Section 2.1.5</span></a>, which was drawn with replacement.</p>
<p>On the other hand, our <strong>time query</strong> will demand a joint PDF. Again, this probability distribution will represent our random sample <strong>assuming sampling with replacement</strong>. Therefore, under the <em>iid</em> property along with the standalone PMF for <span class="math inline">\(T_j\)</span></p>
<p><span class="math display">\[
f_{T_j} \left(t_j \mid \beta \right) = \frac{1}{\beta} \exp \left( -\frac{t_j}{\beta} \right) \quad \text{for $t_j \in [0, \infty )$},
\]</span></p>
<p>our joint PDF is</p>
<p><span id="eq-joint-PDF-iid-time"><span class="math display">\[
\begin{align*}
f_{T_1, \dots, T_{n_t}}(t_1, \dots, t_{n_t} | \beta) &amp;= \prod_{j = 1}^{n_t} f_{T_j}(t_j | \beta) \\
&amp;= \prod_{j = 1}^{n_t} f_T(t_j | \beta) \qquad \text{iid} \\
&amp;= \prod_{j = 1}^{n_t} \frac{1}{\beta} \exp \left( -\frac{t_j}{\beta} \right) \\
&amp; \quad \text{for all} \\
&amp; \quad \quad t_j \in [0, \infty), \\
&amp; \qquad \quad j = 1, \dots, n_t.
\end{align*}
\tag{2.56}\]</span></span></p>
<p>In this query, <span class="math inline">\(n_t\)</span> refers to our sample size of waiting times. Our vector of population parameters also consists of a single element: the average waiting time in minutes from one customer to the next, denoted as <span class="math inline">\(\beta\)</span>. Again, in practice, this parameter is unknown, and our goal is to estimate it using the <span class="math inline">\(n_t\)</span> sampled data points. As we will see in the upcoming section, these data points are contained in data frame <code>waiting_sample</code> from <a href="#sec-characterizing-prob-dist" class="quarto-xref"><span>Section 2.1.5</span></a>, which was drawn with replacement.</p>
</section></section><section id="sec-mle" class="level2" data-number="2.2"><h2 data-number="2.2" class="anchored" data-anchor-id="sec-mle">
<span class="header-section-number">2.2</span> What is Maximum Likelihood Estimation?</h2>
<p>Once we have introduced all the necessary probabilistic concepts to address our <strong>demand</strong> and <strong>time queries</strong> from <a href="#tbl-queries-3" class="quarto-xref">Table&nbsp;<span>2.4</span></a>, it is time to explore an intriguing statistical concept: estimating our <strong>true</strong> population parameters using the sample data in <code>children_sample</code> and <code>waiting_sample</code>. Therefore, we will focus on maximum likelihood estimation (MLE) as a fundamental <strong>frequentist</strong> tool in this process, which will also be invoked in our subsequent regression-related chapters. MLE is closely connected to the probabilistic concepts we discussed in the previous section. As a result, we will partially transition from the probabilistic realm to the realm of inference since we are merely targeting estimation in this section.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/children.png" class="img-fluid figure-img" width="550"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3699345"><em>Manfred Steger</em></a> via <a href="https://pixabay.com/vectors/children-teacher-rectangular-figures-7464610/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<section id="sec-mle-key-concepts" class="level3" data-number="2.2.1"><h3 data-number="2.2.1" class="anchored" data-anchor-id="sec-mle-key-concepts">
<span class="header-section-number">2.2.1</span> Key Concepts</h3>
<p>MLE primarily relies on a random sample of <span class="math inline">\(n\)</span> observations from the population or system of interest. In some cases, such as the ice cream example we have been discussing per query, we may only have one parameter to estimate. In this scenario, we will apply <strong>univariate MLE</strong>. Conversely, we may encounter situations, such as with regression models, that involve more than one parameter to estimate; this is referred to as <strong>multivariate MLE</strong>. Regardless of the case, MLE offers an effective method for finding estimators, which tend to behave well (asymptotically speaking, meaning when we gather a sufficiently large number of <span class="math inline">\(n\)</span> data points).</p>
<div id="Definition-estimator" class="definition">
<div class="definition-header">
<p>Definition of estimator</p>
</div>
<div class="definition-container">
<p>An estimator is a mathematical rule involving the random variables <span class="math inline">\(Y_1, \dots, Y_n\)</span> from our random sample of size <span class="math inline">\(n\)</span>. As its name says, this rule allows us to estimate our population parameter of interest.</p>
</div>
</div>
<p>In statistical practice, finding reliable estimators can be quite challenging. However, MLE offers decent performance and is not a black box approach. The entire estimation process is transparent and well-structured, as we have elaborated throughout this chapter. For example, the <strong>sample mean</strong></p>
<p><span id="eq-sample-mean-estimator"><span class="math display">\[
\bar{Y} = \frac{\sum_{i = 1}^n Y_i}{n}
\tag{2.57}\]</span></span></p>
<p>is a primary case with a very intuitive answer to estimate any given parameter (more specifically, one related to a measure of central tendency). However, this primary case is supported by statistical modelling procedures in MLE, as we will explore further in this section. Note that <a href="#eq-sample-mean-estimator" class="quarto-xref">Equation&nbsp;<span>2.57</span></a> is considered an estimator since its notation only involves random variables (i.e., uppercases). That said, let us explore an additional concept called the estimate.</p>
<div id="Definition-estimate" class="definition">
<div class="definition-header">
<p>Definition of estimate</p>
</div>
<div class="definition-container">
<p>Suppose we have an <strong>observed</strong> random sample of size <span class="math inline">\(n\)</span> with values <span class="math inline">\(y_1, \dots , y_n\)</span>. Then, we apply a given estimator mathematical rule to these <span class="math inline">\(n\)</span> observed values. Hence, this numerical computation is called an estimate of our population parameter of interest.</p>
</div>
</div>
<p>In simple terms, an estimate is the version of the estimator that is based on observed data. Therefore, an observed <strong>sample mean</strong> serves as an estimate, represented in this way using lowercase letters:</p>
<p><span id="eq-sample-mean-estimate"><span class="math display">\[
\bar{y} = \frac{\sum_{i = 1}^n y_i}{n}.
\tag{2.58}\]</span></span></p>
<p>Note that the <strong>mainstream average</strong> from <a href="#eq-mainstream-average" class="quarto-xref">Equation&nbsp;<span>2.19</span></a> is considered an estimate. Now, moving along with our key concepts, it is time to have a crucial conversation on the difference between <strong>likelihood</strong> and probability.</p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on the statistical difference between probability and likelihood!</p>
</div>
<div class="Heads-up-container">
<p>Unlike everyday language, in statistics, probability and likelihood are <strong>not the same</strong>. In general, probability refers to the chance that some outcome of interest will happen for a particular random variable. Note a probability is always bounded between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/paint.png" class="img-fluid figure-img" width="550"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3699345"><em>Manfred Steger</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-pixel-social-network-3704049/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<p>On the other hand, given some observed data, a likelihood refers to how plausible a given distributional parameter is. Furthermore, a likelihood is not bounded between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</p>
</div>
</div>
<p>Let us set aside our ice cream case and explore the difference between these two concepts using the Binomial distribution (whose specific theoretical insights can be found in <a href="C-distributional-mind-map.html#sec-binomial-distribution" class="quarto-xref"><span>Section D.2</span></a>). This distribution models the number of successes in <span class="math inline">\(n\)</span> <strong>independent Bernoulli trials</strong>, each sharing a common probability of success <span class="math inline">\(\pi\)</span> where <span class="math inline">\(\pi \in [0, 1]\)</span>. Let the random variable</p>
<p><span class="math display">\[
Y = \text{Number of successes out of $n$ independent Bernoulli trials,}
\]</span></p>
<p>which follows Binomial distribution:</p>
<p><span class="math display">\[
Y \sim \text{Bin}(n, \pi).
\]</span></p>
<p>Its PMF is</p>
<p><span id="eq-binomial-PMF"><span class="math display">\[
\begin{align*}
P_Y \left( Y = y \mid n, \pi \right) &amp;= {n \choose y} \pi^y (1 - \pi)^{n - y} \\
&amp; \qquad \qquad \qquad \text{for $y \in \{ 0, 1, \dots, n \}$.}
\end{align*}
\tag{2.59}\]</span></span></p>
<p>Term <span class="math inline">\({n \choose y}\)</span> represents the total number of combinations for <span class="math inline">\(y\)</span> successes out of <span class="math inline">\(n\)</span> trials:</p>
<p><span class="math display">\[
{n \choose y} = \frac{n!}{y!(n - y)!}.
\]</span></p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on the <span class="math inline">\(n\)</span> trials in the Binomial distribution!</p>
</div>
<div class="Heads-up-container">
<p>In our estimation framework, we use <strong>random samples consisting of <span class="math inline">\(n\)</span> observed random variables</strong>. Hence, it is important to avoid confusing this sample size with the above distributional parameter <span class="math inline">\(n\)</span>, which refers to the <strong>number of independent Bernoulli trials</strong> in a Binomial random variable.</p>
</div>
</div>
<p>To provide a practical explanation on the difference between a probability and a likelihood in statistics, we will plot the PMFs of six Binomial random variables, as shown in <a href="#fig-binomial-PMFs" class="quarto-xref">Figure&nbsp;<span>2.5</span></a>. The following key points can be highlighted in these plots:</p>
<ol type="1">
<li>Each plot illustrates a Binomial PMF with the same number of trials <span class="math inline">\(n = 10\)</span>, but the probabilities of success vary as in <span class="math inline">\(\pi = 0.3, 0.4, 0.5, 0.6, 0.7, 0.8\)</span>. This variation results in six distinct PMFs.</li>
<li>The <span class="math inline">\(x\)</span>-axis depicts the possible values that the corresponding random variable can take, <span class="math inline">\(y \in \{ 0, 1, \dots, 10 \}\)</span>.</li>
<li>The <span class="math inline">\(y\)</span>-axis displays the probability <span class="math inline">\(P_Y \left( Y = y \mid n = 10, \pi \right)\)</span>. Above each bar, we indicate the probability <span class="math inline">\(P_Y \left( Y = y \mid n = 10, \pi \right)\)</span> for each observed value of <span class="math inline">\(y\)</span>. When we sum all these probabilities, we arrive at a total of <span class="math inline">\(1\)</span>.</li>
<li>As the value of <span class="math inline">\(\pi\)</span> increases, the PMFs become more skewed to the left, indicating that larger probabilities are assigned to a greater number of successes.</li>
<li>Lastly, the red bar highlights the specific probability <span class="math inline">\(P(Y = 7 | n = 10, \pi)\)</span>. This value will be particularly important later in our proof of concept for MLE.</li>
</ol>
<div class="cell">
<div class="cell-output-display">
<div id="fig-binomial-PMFs" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-binomial-PMFs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02-stats-review_files/figure-html/fig-binomial-PMFs-1.png" class="img-fluid figure-img" width="1344">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-binomial-PMFs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.5: Probability mass functions of six Binomial distributions with the same number of trials and six different probabilities of success.
</figcaption></figure>
</div>
</div>
</div>
<p>Imagine you are interested in estimating the <strong>unknown</strong> probability of success <span class="math inline">\(\pi\)</span>, from a population that you have chosen to model as Binomial. Therefore, you draw a random sample of a standalone Binomial observation and you obtain an observed value of <span class="math inline">\(y = 7\)</span> successes out of <span class="math inline">\(n = 10\)</span> trials. Within a frequentist framework, our main objective is to estimate the value of <span class="math inline">\(\pi\)</span> that is <strong>most likely</strong> given these specific <span class="math inline">\(n = 10\)</span> trials and <span class="math inline">\(y = 7\)</span> observed successes.</p>
<p>To achieve the above, we will introduce the concept of the <strong>likelihood function</strong>.</p>
<div id="Definition-likelihood-function" class="definition">
<div class="definition-header">
<p>Definition of likelihood function</p>
</div>
<div class="definition-container">
<p>Suppose you observe some data <span class="math inline">\(y\)</span> from a population or system of interest which is governed by <span class="math inline">\(k\)</span> parameters contained in the following vector:</p>
<p><span class="math display">\[
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T.
\]</span></p>
<p>The corresponding random variable <span class="math inline">\(Y\)</span> has a given probability function <span class="math inline">\(P_Y(Y = y | \boldsymbol{\theta})\)</span>, which can be either a PMF in the discrete case or a PDF in the continuous case. Then, the likelihood function for the parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span> given the observed data <span class="math inline">\(y\)</span> is <strong>mathematically equivalent</strong> to the aforementioned probability function such that:</p>
<p><span id="eq-likelihood-function"><span class="math display">\[
L(\boldsymbol{\theta} | y) = P_Y(Y = y | \boldsymbol{\theta}).
\tag{2.60}\]</span></span></p>
<p>It is important to note that the above likelihood is in function of the parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span> and conditioned on the observed data <span class="math inline">\(y\)</span>. Additionally, <strong>in many continuous cases</strong>, the likelihood function may exceed <span class="math inline">\(1\)</span> given the definition of bounds we have already established for a PDF (see <a href="#eq-lower-bound-PDF" class="quarto-xref">Equation&nbsp;<span>2.9</span></a>).</p>
</div>
</div>
<p>Using the above definition, let us retake the PMF from <a href="#eq-binomial-PMF" class="quarto-xref">Equation&nbsp;<span>2.59</span></a> and turn into a likelihood function given <span class="math inline">\(n = 10\)</span> trials and <span class="math inline">\(y = 7\)</span> observed successes:</p>
<p><span id="eq-binomial-likelihood-y-7-10"><span class="math display">\[
\begin{align*}
L \left( \pi | y = 7, n = 10 \right) &amp;= P_Y \left( Y = 7 \mid n = 10, \pi \right) \\
&amp;= {10 \choose 7} \pi^7 (1 - \pi)^{10 - 7}.
\end{align*}
\tag{2.61}\]</span></span></p>
<p>To estimate the value of <span class="math inline">\(\pi\)</span> that is <strong>most likely</strong> given <span class="math inline">\(n = 10\)</span> trials and <span class="math inline">\(y = 7\)</span> observed successes, we need to find the value of <span class="math inline">\(\pi\)</span> that <strong>maximizes</strong> the above Binomial likelihood function. Since we know that the probability of success <span class="math inline">\(\pi\)</span> falls within the range <span class="math inline">\([0, 1]\)</span>, we can determine the value of <span class="math inline">\(\pi\)</span> that maximizes this function through trial and error by testing different values within this range. To illustrate this process more graphically, let us take a look at <a href="#fig-binomial-likelihood-y-7-10" class="quarto-xref">Figure&nbsp;<span>2.6</span></a>:</p>
<ol type="1">
<li>This plot displays the likelihood function from <a href="#eq-binomial-likelihood-y-7-10" class="quarto-xref">Equation&nbsp;<span>2.61</span></a> on the <span class="math inline">\(y\)</span>-axis, compared to the plausible range of <span class="math inline">\(\pi\)</span> on the <span class="math inline">\(x\)</span>-axis. The function is left-skewed.</li>
<li>We have highlighted the values corresponding to the red bars in five of the previous Binomial PMFs from <a href="#fig-binomial-PMFs" class="quarto-xref">Figure&nbsp;<span>2.5</span></a> with vertical red dashed lines at <span class="math inline">\(\pi = 0.3, 0.4, 0.5, 0.6, 0.8\)</span>; since a probability function is mathematically equivalent to a likelihood function.</li>
<li>The likelihood value for <span class="math inline">\(\pi = 0.7\)</span> is indicated by a solid purple vertical line. This value corresponds to the red bar in the Binomial PMF from <a href="#fig-binomial-PMFs" class="quarto-xref">Figure&nbsp;<span>2.5</span></a> when <span class="math inline">\(\pi = 0.7\)</span>. It is important to note that this value of <span class="math inline">\(\pi\)</span> <strong>maximizes this likelihood function</strong>.</li>
</ol>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on the intuition behind MLE!</p>
</div>
<div class="Heads-up-container">
<p>Retaking (3), given our observed data of <span class="math inline">\(y = 7\)</span> successes out of <span class="math inline">\(n = 10\)</span> trials, and assuming a Binomial-distributed population, we can say that the most plausible value of <span class="math inline">\(\pi\)</span> that is generating this <strong>collected evidence</strong> is <span class="math inline">\(0.7\)</span>.</p>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-binomial-likelihood-y-7-10" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-binomial-likelihood-y-7-10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="02-stats-review_files/figure-html/fig-binomial-likelihood-y-7-10-1.png" class="img-fluid figure-img" width="1344">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-binomial-likelihood-y-7-10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.6: Binomial likelihood function of the probability of success given 10 trials and 7 observed successes.
</figcaption></figure>
</div>
</div>
</div>
<p>How can we translate this MLE principle to our <strong>demand</strong> and <strong>time queries</strong> in the ice cream case? Firstly, we need to make the corresponding jump from the joint probability to a likelihood function in each query. For the <strong>demand query</strong>, via <a href="#eq-joint-PMF-iid-demand" class="quarto-xref">Equation&nbsp;<span>2.55</span></a>, the likelihood function is mathematically defined as:</p>
<p><span id="eq-likelihood-demand"><span class="math display">\[
\begin{align*}
L \left( \pi | d_1, \dots, d_{n_d} \right) &amp;= P_{D_1, \dots, D_{n_d}} \left( D_1 = d_1, \dots, D_{n_d} = d_{n_d} | \pi \right) \\
&amp;= \prod_{i = 1}^{n_d} \pi^{d_i} (1 - \pi)^{1 - d_i}.
\end{align*}
\tag{2.62}\]</span></span></p>
<p>For the <strong>time query</strong>, via <a href="#eq-joint-PDF-iid-time" class="quarto-xref">Equation&nbsp;<span>2.56</span></a>, the likelihood function is mathematically defined as:</p>
<p><span id="eq-likelihood-time"><span class="math display">\[
\begin{align*}
L \left( \beta | t_1, \dots, t_{n_t} \right) &amp;= f_{T_1, \dots, T_{n_t}}(t_1, \dots, t_{n_t} | \beta) \\
&amp;= \prod_{j = 1}^{n_t} \frac{1}{\beta} \exp \left( -\frac{t_j}{\beta} \right).
\end{align*}
\tag{2.63}\]</span></span></p>
<p>To estimate the parameters <span class="math inline">\(\pi\)</span> and <span class="math inline">\(\beta\)</span> using MLE, we can pursue either an <strong>analytical</strong> or <strong>numerical optimization-based approach</strong> via our observed data, which is stored in <code>children_sample</code> and <code>waiting_sample</code>. The following two sections will elaborate on these approaches.</p>
</section><section id="sec-mle-empirical" class="level3" data-number="2.2.2"><h3 data-number="2.2.2" class="anchored" data-anchor-id="sec-mle-empirical">
<span class="header-section-number">2.2.2</span> Analytical Estimates</h3>
<p>This section will address our inquiries regarding the ice cream case, as summarized in <a href="#tbl-queries-3" class="quarto-xref">Table&nbsp;<span>2.4</span></a>, through what we refer to as an <strong>analytical MLE approach</strong>. At the end of this section, we will revisit the scenario of the follow-up meeting with the eight general managers. We will discuss how the sampled data stored in <code>children_sample</code> and <code>waiting_sample</code> can be utilized to resolve the <strong>demand</strong> and <strong>time queries</strong>, while incorporating estimates of the measures of central tendency and uncertainty introduced in <a href="#sec-characterizing-prob-dist" class="quarto-xref"><span>Section 2.1.5</span></a>. Ultimately, this process will provide the statistical rationale for the previously mentioned <strong>mainstream average</strong> from <a href="#eq-mainstream-average" class="quarto-xref">Equation&nbsp;<span>2.19</span></a> regarding the estimation of the Bernoulli and Exponential parameters.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/meeting-2.png" class="img-fluid figure-img" width="500"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/"><em>Manfred Stege</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-seminar-conference-3974170/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<p>Unlike our previous toy example involving a standalone observed Binomial random variable to estimate a probability of success <span class="math inline">\(\pi\)</span> using MLE, it is impractical to use a trial-and-error method to find the estimates of our population parameters based on sampled data from real-life cases. Additionally, the likelihood functions we might encounter in these scenarios will be even more complex than those corresponding to our previous examples with <a href="#eq-likelihood-demand" class="quarto-xref">Equation&nbsp;<span>2.62</span></a> and <a href="#eq-likelihood-time" class="quarto-xref">Equation&nbsp;<span>2.63</span></a>. Therefore, what is the primary mathematical approach to applying MLE in complex situations? The first step is to consider a fundamental method for finding the maximum value of any given function. This tool, which comes from calculus, is known as the <strong>derivative</strong>.</p>
<p>In general, we can use the first derivative <span class="math inline">\(f'(y)\)</span> to find <strong>critical points</strong> in the function <span class="math inline">\(f(y)\)</span>, which can either be a maximum or a minimum. In the context of MLE, our goal is to identify the critical point within the range of our population parameter that corresponds to a maximum. Essentially, we aim to find the estimate that makes our observed data, as indicated by the likelihood function, the most plausible (or likely!). Additionally, we need to confirm whether this critical point is indeed a maximum; further discussion in this section will address this by applying the <strong>second derivative test</strong>.</p>
<p>For the time being, let us retake the likelihood function for our <strong>demand</strong> and <strong>time queries</strong>:</p>
<p><span id="eq-likelihood-demand-2"><span class="math display">\[
L \left( \pi | d_1, \dots, d_{n_d} \right) = \prod_{i = 1}^{n_d} \pi^{d_i} (1 - \pi)^{1 - d_i},
\tag{2.64}\]</span></span></p>
<p>and</p>
<p><span id="eq-likelihood-time-2"><span class="math display">\[
L \left( \beta | t_1, \dots, t_{n_t} \right) = \prod_{j = 1}^{n_t} \frac{1}{\beta} \exp \left( -\frac{t_j}{\beta} \right).
\tag{2.65}\]</span></span></p>
<p>We can initiate the calculus-based process described above, which will involve the following steps:</p>
<ol type="1">
<li>Calculate the first partial derivative of either <a href="#eq-likelihood-demand-2" class="quarto-xref">Equation&nbsp;<span>2.64</span></a> or <a href="#eq-likelihood-time-2" class="quarto-xref">Equation&nbsp;<span>2.65</span></a> <strong>with respect to the population parameter of interest (<span class="math inline">\(\pi\)</span> or <span class="math inline">\(\beta\)</span>)</strong>. This will be a partial derivative, as the right-hand side of both equations includes additional variables, such as the observed values from the corresponding random samples.</li>
<li>Set the partial derivative obtained in step (1) equal to zero and solve for the parameter of interest. This involves isolating the parameter on the left-hand side while moving the other variables to the right-hand side. This is referred to as a <strong>closed-form solution</strong>.</li>
<li>Verify that the closed-form solution is indeed a maximum by performing the second derivative test.</li>
</ol>
<p>Nevertheless, there is a distinctive characteristic in both likelihood functions: they involve a multiplication of either <span class="math inline">\(n_d\)</span> or <span class="math inline">\(n_t\)</span> factors. Consequently, performing step (1) would be quite complicated, as we would need to apply the product rule to derive these functions. To address this issue, we can turn to what is known in MLE as the <strong>log-likelihood function</strong>.</p>
<div id="Definition-log-likelihood-function" class="definition">
<div class="definition-header">
<p>Definition of log-likelihood function</p>
</div>
<div class="definition-container">
<p>Suppose you observe some data <span class="math inline">\(y\)</span> from a population or system of interest which is governed by <span class="math inline">\(k\)</span> parameters contained in the following vector:</p>
<p><span class="math display">\[
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T.
\]</span></p>
<p>The corresponding random variable <span class="math inline">\(Y\)</span> has a given probability function <span class="math inline">\(P_Y(Y = y | \boldsymbol{\theta})\)</span>, which can be either a PMF in the discrete case or a PDF in the continuous case. Moreover, the likelihood function, as described in <a href="#eq-likelihood-function" class="quarto-xref">Equation&nbsp;<span>2.60</span></a>, is defined as follows:</p>
<p><span class="math display">\[
L(\boldsymbol{\theta} | y) = P_Y(Y = y | \boldsymbol{\theta}).
\]</span></p>
<p>Then, the log-likelihood function is merely the logarithm of the above function:</p>
<p><span id="eq-log-likelihood-function"><span class="math display">\[
\log L(\boldsymbol{\theta} | y) = \log \left[ P_Y(Y = y | \boldsymbol{\theta}) \right].
\tag{2.66}\]</span></span></p>
<p>Using a log-likelihood function, which is a monotonic transformation of the likelihood function, offers the following practical advantages:</p>
<ol type="1">
<li>The logarithmic properties convert products into sums. This is particularly useful in likelihood functions for random samples that involve multiplying probability functions (and its corresponding factors).</li>
<li>When estimating parameters, calculating the derivative of a sum is easier than that of a product.</li>
<li>In many cases, the likelihood functions for observed random samples can yield very small values, which may lead to computational instability. By working on a logarithmic scale, these computations become more stable. This stability is crucial for numerical optimization methods applied to a given log-likelihood function, in cases where a closed-form solution for an estimate is not mathematically feasible.</li>
</ol>
</div>
</div>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on the <span class="math inline">\(\log(\cdot)\)</span> notation!</p>
</div>
<div class="Heads-up-container">
<p>Statistically, unless indicated otherwise, the <span class="math inline">\(\log(\cdot)\)</span> notation implicates the <strong>natural logarithm</strong> with a base of <span class="math inline">\(e = 2.71828...\)</span></p>
</div>
</div>
<p>Therefore, let us modify the above three MLE steps and expand them into four:</p>
<ol type="1">
<li>Transform the likelihood function (using either <a href="#eq-likelihood-demand-2" class="quarto-xref">Equation&nbsp;<span>2.64</span></a> or <a href="#eq-likelihood-time-2" class="quarto-xref">Equation&nbsp;<span>2.65</span></a>) into a log-likelihood function, applying the necessary <strong>logarithmic and exponent properties</strong> to simplify the resulting expression.</li>
<li>Obtain the first partial derivative of the simplified log-likelihood function with respect to the population parameter of interest (either <span class="math inline">\(\pi\)</span> or <span class="math inline">\(\beta\)</span>).</li>
<li>Set the partial derivative obtained in step (2) equal to zero and solve for the parameter of interest to find the closed-form solution.</li>
<li>Verify that the closed-form solution is indeed a maximum by conducting the second derivative test.</li>
</ol>
<p>With these steps in mind, let us formally define what MLE is before moving on to the derivations of our queries.</p>
<div id="Definition-MLE" class="definition">
<div class="definition-header">
<p>Definition of maximum likelihood estimation (MLE)</p>
</div>
<div class="definition-container">
<p>Suppose you observe some data <span class="math inline">\(y\)</span> from a population or system of interest which is governed by <span class="math inline">\(k\)</span> parameters contained in the following vector:</p>
<p><span class="math display">\[
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T.
\]</span></p>
<p>The corresponding random variable <span class="math inline">\(Y\)</span> has a given probability function <span class="math inline">\(P_Y(Y = y | \boldsymbol{\theta})\)</span>, which can be either a PMF in the discrete case or a PDF in the continuous case. Furthermore, the log-likelihood function is defined as in <a href="#eq-log-likelihood-function" class="quarto-xref">Equation&nbsp;<span>2.66</span></a>:</p>
<p><span class="math display">\[
\log L(\boldsymbol{\theta} | y) = \log \left[ P_Y(Y = y | \boldsymbol{\theta}) \right].
\]</span></p>
<p>MLE aims to find the estimate of <span class="math inline">\(\boldsymbol{\theta}\)</span> that maximizes the above log-likelihood function as in:</p>
<p><span class="math display">\[
\hat{\boldsymbol{\theta}} = \underset{\boldsymbol{\theta}}{\operatorname{arg max}} \log L(\boldsymbol{\theta} | y).
\]</span></p>
<p>In supervised learning, MLE is analogous to minimizing any given loss function during model training.</p>
</div>
</div>
<p>Now, let us apply the above four steps for both queries. For the <strong>demand query</strong>, we go as follows:</p>
<ol type="1">
<li>We will transform the likelihood function into a log-likelihood function, and apply the necessary logarithmic and exponent properties to simplify this mathematical expression. The log-likelihood function, via <a href="#eq-likelihood-demand-2" class="quarto-xref">Equation&nbsp;<span>2.64</span></a>, is given by:</li>
</ol>
<p><span id="eq-log-likelihood-demand"><span class="math display">\[
\begin{align*}
\log L \left( \pi | d_1, \dots, d_{n_d} \right) &amp;= \log \left[ \prod_{i = 1}^{n_d} \pi^{d_i} (1 - \pi)^{1 - d_i} \right] \\
&amp;= \log \left[ \pi^{\sum_{i = 1}^{n_d} d_i} (1 - \pi)^{\sum_{i = 1}^{n_d} (1 - d_i)} \right] \\
&amp;= \log \left[ \pi^{\sum_{i = 1}^{n_d} d_i} (1 - \pi)^{n_d - \sum_{i = 1}^{n_d} d_i} \right] \\
&amp;= \left( \sum_{i = 1}^{n_d} d_i \right) \log(\pi) \\
&amp; \qquad + \left( n_d - \sum_{i = 1}^{n_d} d_i \right) \log(1 - \pi). \\
\end{align*}
\tag{2.67}\]</span></span></p>
<ol start="2" type="1">
<li>We obtain the first partial derivative of the simplified log-likelihood function, from <a href="#eq-log-likelihood-demand" class="quarto-xref">Equation&nbsp;<span>2.67</span></a>, with respect to the population parameter <span class="math inline">\(\pi\)</span>:</li>
</ol>
<p><span id="eq-partial-derivative-log-likelihood-demand"><span class="math display">\[
\begin{align*}
\frac{\partial}{\partial \pi} \log L \left( \pi | d_1, \dots, d_{n_d} \right) &amp;= \frac{\sum_{i = 1}^{n_d} d_i}{\pi} - \frac{n_d - \sum_{i = 1}^{n_d} d_i}{1 - \pi}.
\end{align*}
\tag{2.68}\]</span></span></p>
<ol start="3" type="1">
<li>We set <a href="#eq-partial-derivative-log-likelihood-demand" class="quarto-xref">Equation&nbsp;<span>2.68</span></a> to zero and solve for <span class="math inline">\(\pi\)</span>. This will yield the close-form solution, and thus the estimate <span class="math inline">\(\hat{\pi}\)</span>:</li>
</ol>
<p><span class="math display">\[
\begin{gather*}
\frac{\sum_{i = 1}^{n_d} d_i}{\pi} - \frac{n_d - \sum_{i = 1}^{n_d} d_i}{1 - \pi} = 0 \\
\frac{\sum_{i = 1}^{n_d} d_i}{\pi} = \frac{n_d - \sum_{i = 1}^{n_d} d_i}{1 - \pi} \\
(1 - \pi) \sum_{i = 1}^{n_d} d_i = \pi \left( n_d - \sum_{i = 1}^{n_d} d_i \right) \\
\sum_{i = 1}^{n_d} d_i - \pi \sum_{i = 1}^{n_d} d_i = \pi n_d - \pi \sum_{i = 1}^{n_d} d_i \\
\pi n_d - \pi \sum_{i = 1}^{n_d} +  \pi \sum_{i = 1}^{n_d} = \sum_{i = 1}^{n_d} d_i \\
\pi n_d = \sum_{i = 1}^{n_d} d_i \qquad \Rightarrow \qquad \hat{\pi} = \frac{\sum_{i = 1}^{n_d} d_i}{n_d}.
\end{gather*}
\]</span></p>
<ol start="4" type="1">
<li>Is <span class="math inline">\(\hat{\pi}\)</span> truly a maximum? Let us apply second derivative test. If <span class="math display">\[\frac{\partial^2}{\partial \pi^2} L \left( \pi | d_1, \dots, d_{n_d} \right) \Bigg|_{\pi = \hat{\pi}} &lt; 0,\]</span> then <span class="math inline">\(\hat{\pi}\)</span> is indeed a maximum point:</li>
</ol>
<p><span class="math display">\[
\begin{gather*}
\frac{\partial^2}{\partial \pi^2} L \left( \pi | d_1, \dots, d_{n_d} \right) = - \frac{\sum_{i = 1}^{n_d} d_i}{\pi^2} - \frac{n_d - \sum_{i = 1}^{n_d} d_i}{(1 - \pi)^2}
\end{gather*}
\]</span></p>
<p><span class="math display">\[
\begin{align*}
\frac{\partial^2}{\partial \pi^2} L \left( \pi | d_1, \dots, d_{n_d} \right) \Bigg|_{\pi = \hat{\pi}} &amp;= - \frac{\sum_{i = 1}^{n_d} d_i}{\hat{\pi}^2} - \frac{n_d - \sum_{i = 1}^{n_d} d_i}{\left( 1 - \hat{\pi} \right)^2} \\
&amp;= - \frac{n_d \hat{\pi}}{\hat{\pi}^2} - \frac{n_d - n_d \hat{\pi}}{\left( 1 - \hat{\pi} \right)^2} \\
&amp;= - \frac{n_d}{\hat{\pi}} - \frac{n_d \left( 1 - \hat{\pi} \right)}{\left( 1 - \hat{\pi} \right)^2} \\
&amp;= - \frac{n_d}{\hat{\pi}} - \frac{n_d}{1 - \hat{\pi}} &lt; 0.
\end{align*}
\]</span></p>
<p>We pass the second derivative test given that the sample size <span class="math inline">\(n_d\)</span> and the estimate <span class="math inline">\(\hat{\pi}\)</span> will always be <strong>nonnegative</strong>. Therefore, the estimate</p>
<p><span id="eq-mle-demand"><span class="math display">\[
\hat{\pi} = \frac{\sum_{i = 1}^{n_d} d_i}{n_d},
\tag{2.69}\]</span></span></p>
<p>is indeed a truly maximum for the <strong>demand query</strong>.</p>
<p>Then, we will follow the four steps for the <strong>time query</strong>:</p>
<ol type="1">
<li>We will convert the likelihood function to a log-likelihood function and use logarithmic and exponent properties to simplify this mathematical expression. According to <a href="#eq-likelihood-time-2" class="quarto-xref">Equation&nbsp;<span>2.65</span></a>, the log-likelihood function is given as:</li>
</ol>
<p><span id="eq-log-likelihood-time"><span class="math display">\[
\begin{align*}
\log L \left( \beta | t_1, \dots, t_{n_t} \right) &amp;= \log \left[ \prod_{j = 1}^{n_t} \frac{1}{\beta} \exp \left( -\frac{t_j}{\beta} \right) \right] \\
&amp;= \log \left[ \frac{1}{\beta^{n_t}} \exp \left( -\frac{1}{\beta} \sum_{j = 1}^{n_t} t_j \right) \right] \\
&amp;= -n_t \log \left( \beta \right) + \log \left[ \exp \left( -\frac{1}{\beta} \sum_{j = 1}^{n_t} t_j \right) \right] \\
&amp;= -n_t \log \left( \beta \right) - \frac{\sum_{j = 1}^{n_t} t_j}{\beta}.
\end{align*}
\tag{2.70}\]</span></span></p>
<ol start="2" type="1">
<li>We obtain the first partial derivative of the simplified log-likelihood function, from <a href="#eq-log-likelihood-time" class="quarto-xref">Equation&nbsp;<span>2.70</span></a>, with respect to the population parameter <span class="math inline">\(\beta\)</span>:</li>
</ol>
<p><span id="eq-partial-derivative-log-likelihood-time"><span class="math display">\[
\frac{\partial}{\partial \beta} \log L \left( \beta | t_1, \dots, t_{n_t} \right) = -\frac{n_t}{\beta} + \frac{\sum_{j = 1}^{n_t} t_j}{\beta^2}.
\tag{2.71}\]</span></span></p>
<ol start="3" type="1">
<li>We set <a href="#eq-partial-derivative-log-likelihood-time" class="quarto-xref">Equation&nbsp;<span>2.71</span></a> to zero and solve for <span class="math inline">\(\beta\)</span>. This will yield the close-form solution, and thus the estimate <span class="math inline">\(\hat{\beta}\)</span>:</li>
</ol>
<p><span class="math display">\[
\begin{gather*}
-\frac{n_t}{\beta} + \frac{\sum_{j = 1}^{n_t} t_j}{\beta^2} = 0 \\
\frac{1}{\beta} \left( -n_t + \frac{\sum_{j = 1}^{n_t} t_j}{\beta} \right) = 0 \\
-n_t + \frac{\sum_{j = 1}^{n_t} t_j}{\beta} = 0 \\
n_t = \frac{\sum_{j = 1}^{n_t} t_j}{\beta} \qquad \Rightarrow \qquad \hat{\beta} = \frac{\sum_{j = 1}^{n_t} t_j}{n_t}.
\end{gather*}
\]</span></p>
<ol start="4" type="1">
<li>Is <span class="math inline">\(\hat{\beta}\)</span> truly a maximum? Let us apply second derivative test. If <span class="math display">\[\frac{\partial^2}{\partial \beta^2} L \left( \beta | t_1, \dots, t_{n_t} \right) \Bigg|_{\beta = \hat{\beta}} &lt; 0,\]</span> then <span class="math inline">\(\hat{\pi}\)</span> is indeed a maximum point:</li>
</ol>
<p><span class="math display">\[
\begin{gather*}
\frac{\partial^2}{\partial \beta^2} L \left( \beta | t_1, \dots, t_{n_t} \right) = \frac{n_t}{\beta^2} - \frac{2 \sum_{j = 1}^{n_t} t_j}{\beta^3}
\end{gather*}
\]</span></p>
<p><span class="math display">\[
\begin{align*}
\frac{\partial^2}{\partial \beta^2} L \left( \beta | t_1, \dots, t_{n_t} \right) \Bigg|_{\beta = \hat{\beta}} &amp;= \frac{n_t}{\hat{\beta}^2} - \frac{2 \sum_{j = 1}^{n_t} t_j}{\hat{\beta}^3} ]\\
&amp;= \frac{n_t}{\hat{\beta}^2} - \frac{2 n_t \hat{\beta}}{\hat{\beta}^3} \\
&amp;= \frac{n_t}{\hat{\beta}^2} - \frac{2 n_t}{\hat{\beta}^2} \\
&amp;= -\frac{n_t}{\hat{\beta}^2} &lt; 0.
\end{align*}
\]</span></p>
<p>We pass the second derivative test given that the sample size <span class="math inline">\(n_t\)</span> and the estimate <span class="math inline">\(\hat{\pi}\)</span> will always be <strong>nonnegative</strong>. Therefore, the estimate</p>
<p><span id="eq-mle-time"><span class="math display">\[
\hat{\beta} = \frac{\sum_{j = 1}^{n_t} t_j}{n_t},
\tag{2.72}\]</span></span></p>
<p>is indeed a truly maximum for the <strong>demand query</strong>.</p>
<p>Great! We have derived closed-form estimates using MLE for our <strong>demand</strong> and <strong>time queries</strong>, as provided by <a href="#eq-mle-demand" class="quarto-xref">Equation&nbsp;<span>2.69</span></a> and <a href="#eq-mle-time" class="quarto-xref">Equation&nbsp;<span>2.72</span></a>. You may be wondering:</p>
<blockquote class="blockquote">
<p><strong>How can I obtain the estimators based on the corresponding <a href="#Definition-estimator">definition</a>?</strong></p>
</blockquote>
<p>To obtain the estimators for these queries, all it takes is a change in notation. Specifically, we replace our lowercase letters, which represent observations, with uppercase letters that denote random variables. This approach will yield the following <strong>estimators</strong> (thus, the tilde notation on the left-hand side instead of the hat notation):</p>
<p><span id="eq-mle-demand-estimator"><span class="math display">\[
\begin{gather*}
\tilde{\pi} = \frac{\sum_{i = 1}^{n_d} D_i}{n_d}
\end{gather*}
\tag{2.73}\]</span></span></p>
<p><span id="eq-mle-time-estimator"><span class="math display">\[
\begin{gather*}
\tilde{\beta} = \frac{\sum_{j = 1}^{n_t} T_j}{n_t}.
\end{gather*}
\tag{2.74}\]</span></span></p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on the use of estimators!</p>
</div>
<div class="Heads-up-container">
<p>The above expressions (<a href="#eq-mle-demand-estimator" class="quarto-xref">Equation&nbsp;<span>2.73</span></a> and <a href="#eq-mle-time-estimator" class="quarto-xref">Equation&nbsp;<span>2.74</span></a>) establish a rule for estimating the corresponding parameter of interest, which can be applied to a given random sample obtained from these populations. We will retake them in <a href="#sec-basics-inf" class="quarto-xref"><span>Section 2.3</span></a> to elaborate on the last inferential topics of this chapter.</p>
</div>
</div>
<p>Before we proceed with our sample data found in <code>children_sample</code> and <code>waiting_sample</code>, let us revisit the concept introduced by <a href="#sec-characterizing-prob-dist" class="quarto-xref"><span>Section 2.1.5</span></a> regarding the <strong>observed mainstream average</strong> from <a href="#eq-mainstream-average" class="quarto-xref">Equation&nbsp;<span>2.19</span></a>. This average is calculated by summing all <span class="math inline">\(n\)</span> realizations <span class="math inline">\(y_k\)</span> (where <span class="math inline">\(k = 1, \dots, n\)</span>) of a specific random variable and then dividing this total by <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[
\bar{y} = \frac{\sum_{k = 1}^n y_k}{n}.
\]</span></p>
<p>For the <strong>demand query</strong>, recall the obtained estimate by MLE is given by</p>
<p><span class="math display">\[
\hat{\pi} = \frac{\sum_{i = 1}^{n_d} d_i}{n_d},
\]</span></p>
<p>with a sample size of <span class="math inline">\(n_d\)</span> children and</p>
<p><span class="math display">\[
d_i =
\begin{cases}
1 \qquad \text{The surveyed child prefers chocolate.}\\
0 \qquad \text{Otherwise.}
\end{cases}
\]</span></p>
<p>Based on the possible numerical values that <span class="math inline">\(d_i\)</span> can take, we can see that <span class="math inline">\(\hat{\pi}\)</span> is analogous to the mainstream average! Specifically, we are estimating the proportion of children in our population who prefer chocolate over vanilla. This directly addresses both the statement and the parameter we are interested in, which can be referenced in <a href="#tbl-queries-3" class="quarto-xref">Table&nbsp;<span>2.4</span></a>. Therefore, we are ready to numerically compute <span class="math inline">\(\hat{\pi}\)</span> by using the corresponding mean function from <code>R</code> and <code>Python</code> via <code>children_sample</code>.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-9-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-9-1" role="tab" aria-controls="tabset-9-1" aria-selected="true"><strong><code>R</code></strong></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-9-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-9-2" role="tab" aria-controls="tabset-9-2" aria-selected="false"><strong><code>Python</code></strong></a></li>
</ul>
<div class="tab-content">
<div id="tabset-9-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-9-1-tab">
<div class="cell">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">pi_hat_MLE</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">children_sample</span><span class="op">$</span><span class="va">fav_flavour</span> <span class="op">==</span> <span class="st">"chocolate"</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">pi_hat_MLE</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.67</code></pre>
</div>
</div>
</div>
<div id="tabset-9-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-9-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>pi_hat_MLE <span class="op">=</span> <span class="bu">round</span>((children_sample[<span class="st">"fav_flavour"</span>] <span class="op">==</span> <span class="st">"chocolate"</span>).mean(), <span class="dv">2</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pi_hat_MLE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.61</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>To address the <strong>time query</strong>, our estimate by MLE is given by</p>
<p><span class="math display">\[
\hat{\beta} = \frac{\sum_{j = 1}^{n_t} t_j}{n_t},
\]</span></p>
<p>where <span class="math inline">\(n_t\)</span> denotes the sample size of waiting times, and <span class="math inline">\(t_j\)</span> in minutes is a continuous observation that falls within the range <span class="math inline">\([0, \infty)\)</span>. This estimate is analogous to the calculation of the mainstream average. Additionally, <span class="math inline">\(\hat{\beta}\)</span> enables us to estimate the average waiting time between customers at any given ice cream cart within our population of interest, as indicated by the statement and parameter discussed by <a href="#tbl-queries-3" class="quarto-xref">Table&nbsp;<span>2.4</span></a>. That said, we can now proceed to calculate <span class="math inline">\(\hat{\beta}\)</span> numerically using the appropriate mean function in both <code>R</code> and <code>Python</code> using <code>waiting_sample</code>.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-10-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-10-1" role="tab" aria-controls="tabset-10-1" aria-selected="true"><strong><code>R</code></strong></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-10-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-10-2" role="tab" aria-controls="tabset-10-2" aria-selected="false"><strong><code>Python</code></strong></a></li>
</ul>
<div class="tab-content">
<div id="tabset-10-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-10-1-tab">
<div class="cell">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">beta_hat_MLE</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">waiting_sample</span><span class="op">$</span><span class="va">waiting_time</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">beta_hat_MLE</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 10.23</code></pre>
</div>
</div>
</div>
<div id="tabset-10-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-10-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>beta_hat_MLE <span class="op">=</span> <span class="bu">round</span>(waiting_sample[<span class="st">"waiting_time"</span>].mean(), <span class="dv">2</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(beta_hat_MLE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>9.67</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on different sample estimates!</p>
</div>
<div class="Heads-up-container">
<p>Note that the estimates <code>pi_hat_MLE</code> and <code>beta_hat_MLE</code> differ between <code>R</code> and <code>Python</code>. This discrepancy arises because, despite using the same simulation seed to generate the <code>children_sample</code> or <code>waiting_sample</code>, each programming language employs its own pseudo-random number generator.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/question.png" class="img-fluid figure-img" width="230"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/"><em>Manfred Stege</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-emotion-confused-6230199/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<p>The same applies to the simulation seeds used for generating the corresponding populations of <span class="math inline">\(N_d = 2,000,000\)</span> children and <span class="math inline">\(N_t = 500,000\)</span> general customer-to-customer waiting times.</p>
</div>
</div>
<p>Retaking the measures of central tendency and uncertain from <a href="#sec-characterizing-prob-dist" class="quarto-xref"><span>Section 2.1.5</span></a>, and applied specifically to our <strong>demand query</strong>, we have already derived the mean (or expected value) and variance (and its derived standard deviation) for the <span class="math inline">\(i\)</span>th Bernoulli-distributed random variable <span class="math inline">\(D_i\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}(D_i) = \pi
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\text{Var}(D_i) = \pi (1 - \pi) \qquad \Rightarrow \qquad \text{sd}(D_i) = \sqrt{\pi (1 - \pi)}.
\]</span></p>
<p>Since the parameter <span class="math inline">\(\pi\)</span> that governs this Bernoulli-distributed population corresponds to the theoretical mean (or expected value), we have also found its corresponding estimate, denoted as <span class="math inline">\(\hat{\mathbb{E}}(D_i)\)</span>, which is 0.67 in <code>R</code> (or 0.61 in <code>Python</code>). Now, how can we obtain the estimate for the variance, along with the derived standard deviation? We can use an important MLE result known as the <strong>invariance property</strong> to compute these measures of spread.</p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on the MLE invariance property in a Bernoulli-distributed population!</p>
</div>
<div class="Heads-up-container">
<p>The MLE invariance property states that if we already an estimate of a parameter, such as <span class="math inline">\(\hat{\pi}\)</span> in a Bernoulli-distributed population, then any monotonic transformation of parameter <span class="math inline">\(\pi\)</span> will yield a corresponding estimate. For example, consider the transformation:</p>
<p><span class="math display">\[
\begin{align*}
g(\pi) &amp;= \pi (1 - \pi) \\
&amp; = \text{Var}(D_i).
\end{align*}
\]</span></p>
<p>Using this transformation, the estimate will be:</p>
<p><span id="eq-est-variance-demand"><span class="math display">\[
\begin{align*}
g(\hat{\pi}) &amp;= \hat{\pi} (1 - \hat{\pi}) \\
&amp;= \hat{\text{Var}}(D_i).
\end{align*}
\tag{2.75}\]</span></span></p>
</div>
</div>
<p>Therefore, by using <a href="#eq-est-variance-demand" class="quarto-xref">Equation&nbsp;<span>2.75</span></a> and our <code>children_sample</code>, the estimate <span class="math inline">\(\hat{\text{Var}}(D_i)\)</span> for the variance in the Bernoulli population corresponding to the <strong>demand query</strong> is 0.22 in <code>R</code> (or 0.24 in <code>Python</code>). Consequently, the estimate of the standard deviation, namely <span class="math inline">\(\hat{\text{sd}}(D_i)\)</span>, is 0.47 in <code>R</code> (or 0.49 in <code>Python</code>).</p>
<p>For the <strong>time query</strong>, we can easily obtain the measures of central tendency and uncertainty since we have already estimated <span class="math inline">\(\beta\)</span>. As mentioned by <a href="#sec-characterizing-prob-dist" class="quarto-xref"><span>Section 2.1.5</span></a>, the mean and variance (along with the derived standard deviation) for the <span class="math inline">\(j\)</span>th Exponential-distributed random variable <span class="math inline">\(T_j\)</span> are as follows:</p>
<p><span class="math display">\[
\mathbb{E}(T_j) = \beta
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\text{Var}(T_j) = \beta \qquad \Rightarrow \qquad \text{sd}(T_j) = \sqrt{\beta}.
\]</span></p>
<p>Therefore, via <code>waiting_sample</code>, the estimates <span class="math inline">\(\hat{\mathbb{E}}(T_j)\)</span> and <span class="math inline">\(\hat{\text{Var}}(D_i)\)</span> are also 10.23 in <code>R</code> (or 9.67 in <code>Python</code>). Then, the estimate of the standard deviation, namely <span class="math inline">\(\hat{\text{sd}}(T_j)\)</span>, is 3.2 in <code>R</code> (or 3.11 in <code>Python</code>).</p>
<p>We have already completed our queries depicted in <a href="#tbl-queries-3" class="quarto-xref">Table&nbsp;<span>2.4</span></a>! Well, at least when it comes to providing <strong>point estimates</strong>, as defined below.</p>
<div id="Definition-sample" class="definition">
<div class="definition-header">
<p>Definition of point estimate</p>
</div>
<div class="definition-container">
<p>Let <span class="math inline">\(\theta\)</span> denote a population parameter of interest. Suppose you have observed a random sample of size <span class="math inline">\(n\)</span>, represented as the vector:</p>
<p><span class="math display">\[
\boldsymbol{y} = (y_1, y_2, \ldots, y_n)^T.
\]</span></p>
<p>The point estimate <span class="math inline">\(\hat{\theta}\)</span> serves as a possible value for <span class="math inline">\(\theta\)</span> and is expressed as a function of the observed random sample contained in <span class="math inline">\(\boldsymbol{y}\)</span>:</p>
<p><span class="math display">\[
\hat{\theta} = h(\boldsymbol{y}).
\]</span></p>
</div>
</div>
<p>To expand on our inquiries regarding the ice cream case, we will discuss what is known as <strong>interval estimates</strong> in <a href="#sec-basics-inf" class="quarto-xref"><span>Section 2.3</span></a>, for which the above estimated variances will play a central role. Since we are working with observed random variables in both samples, these interval estimates will help us report and control the uncertainty in our parameter estimations to our stakeholders. For now, let us clarify why MLE is essential in parameter estimation for our upcoming regression chapters.</p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on why MLE is fundamental in regression modelling!</p>
</div>
<div class="Heads-up-container">
<p>We might initially think that discussing the statistical rationale for estimating <span class="math inline">\(\pi\)</span> and <span class="math inline">\(\beta\)</span> in Bernoulli and Exponential populations, respectively, is trivial since we concluded that the <strong>sample mean</strong> (i.e., the so-called mainstream average) suffices to address the <strong>demand</strong> and <strong>time queries</strong>. However, this conclusion is far from trivial. Theoretically, for these specific populations, the sample mean provides an estimate of the corresponding distributional parameter that maximizes the likelihood of our observed data through MLE. This assertion is supported by the probabilistic tools we previously discussed in <a href="#sec-basics-prob" class="quarto-xref"><span>Section 2.1</span></a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/aha.png" class="img-fluid figure-img" width="300"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/"><em>Manfred Stege</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-emotion-confused-6230199/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<p>That said, the same above MLE paradigm will significantly influence our upcoming chapters on regression modelling. MLE is a key aspect that makes parameter estimation in many regression models, based on sample data, a transparent process rather than a black box. This paradigm allows data scientists to perform analyses on data derived from specific populations by employing a generative modelling approach. We assume that this data comes from particular distributions, which have their corresponding parameters to estimate.</p>
<p>Furthermore, as we will see later in this book, these parameters will not only pertain to measures of central tendency and uncertainty but will also include regression parameters. Of course, the MLE approaches in these models will be more complex, as we will need to estimate multiple parameters simultaneously. Still, the frequentist concept of producing the estimates that make our observed data the “<em>most plausible (or likely!)</em>” will remain crucial to our analyses. Essentially, this framework underpins the model fitting functions in both <code>R</code> and <code>Python</code>, which we will explore later in most chapters.</p>
</div>
</div>
<p>To conclude this section on <strong>analytical MLE via point estimates</strong>, let us revisit the follow-up meeting with the eight general managers regarding our ice cream case. It would be beneficial to present a clear <strong>storytelling</strong> about the sampling and estimation process for both queries. This will ensure that all the managers in the room can easily understand the process and effectively communicate the results to others within the company.</p>
<section id="demand-query" class="level4"><h4 class="anchored" data-anchor-id="demand-query">Demand Query</h4>
<blockquote class="blockquote">
<p><em>To understand the relative demand for chocolate versus vanilla flavour among children aged 4 to 11 years old attending various parks in Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during Summer weekends, we estimated the probability that a randomly selected child prefers chocolate. This was done using a maximum likelihood estimation framework based on a sample of 500 children from this population. It is important to note that each of these 500 children was independently selected, meaning the selection of one child did not influence the selection of the next.</em></p>
<p><em>In this two-flavour choice context, each surveyed child was modelled as a Bernoulli trial, where we asked whether chocolate or vanilla was their preferred flavour. The maximum likelihood approach was particularly suitable in this case because it determines the value that makes the observed set of 500 children preferences most plausible for this population. Using this random sample of children’s flavour choices, we found that the estimated population proportion preferring chocolate was 0.67 (or 0.61 via <code>Python</code>). This indicates that, based on our survey data, approximately 67% (or 61% via <code>Python</code>) of children in the sampled population chose chocolate over vanilla.</em></p>
<p><em>Random sampling was crucial in this study because it ensured that every child in the population had an equal chance of being included, thereby minimizing bias in our estimation and allowing us to generalize the results to the entire population. However, our estimation approach does have limitations. Relying solely on point estimates, like the one mentioned above, does not account for variability in preferences across different age groups, cities, or times of day. Additionally, the MLE method assumes that the observed ice cream flavor choices are independent and identically distributed, which may not hold true in real-world settings where peer influence or marketing exposure could impact preferences. Therefore, we might be interested in exploring more complex data modelling techniques, such as regression analysis, that take these variables into consideration.</em></p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/thread.png" class="img-fluid figure-img" width="350"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/"><em>Manfred Stege</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-thread-needle-diy-sew-6230155/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
</section><section id="time-query" class="level4"><h4 class="anchored" data-anchor-id="time-query">Time Query</h4>
<blockquote class="blockquote">
<p><em>In a parallel analysis, we examined the frequency of customer arrivals, focusing on the waiting time between purchases for all general customers across the 900 ice cream carts located in various parks across Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during Summer weekends. Maximum likelihood estimation was essential in this study, enabling us to estimate the parameter associated with the average waiting time across this population, which makes our observed waiting times the most plausible. Note that we modelled the waiting times as being Exponential-distributed, which is commonly used for interarrival times in statistical analysis.</em></p>
<p><em>Using a random sample of 200 waiting times between two general customers across the 900 ice cream carts in the eight cities, we estimated the average waiting time to be approximately 10.23 minutes (or 9.67 minutes via <code>Python</code>). The assumption of random sampling was crucial to prevent bias; without it, the estimates could reflect peak-hour congestion or quiet periods instead of the flow typical of an average day during Summer weekends. This random sampling ensured that every waiting time in the population had an equal opportunity to be included.</em></p>
<p><em>The interpretation of the estimated value suggests that, on average, there is an 10.23-minute gap (or 9.67-minute gap via <code>Python</code>) between one customer and the next. This information is valuable for planning service capacity and scheduling employees. However, we must acknowledge the simplifying assumptions behind this model—specifically, that arrivals occur at a constant rate over time. If real-world arrivals are affected by external factors such as the time of day, weather, or marketing campaigns, this basic Exponential model may not fully capture that complexity.</em></p>
<p><em>As with the demand query, relying on point estimates means we must be cautious: while informative, they provide only a snapshot rather than the full probabilistic picture of customer behaviour. Finally, we might be interested in exploring more complex modelling techniques, such as regression analysis, that take additional external factors into account.</em></p>
</blockquote>
</section></section><section id="sec-num-optimization" class="level3" data-number="2.2.3"><h3 data-number="2.2.3" class="anchored" data-anchor-id="sec-num-optimization">
<span class="header-section-number">2.2.3</span> Numerical Optimization</h3>
<p>Before wrapping up this entire MLE delivery so that we can move on to a review of statistical inference via hypothesis testing and confidence intervals, we will briefly elaborate on MLE with <strong>numerical optimization</strong> in this section. For the univariate example in the ice cream case per query, we can obtain a straightforward analytical solution, as demonstrated in <a href="#sec-mle-empirical" class="quarto-xref"><span>Section 2.2.2</span></a>. However, in real-life scenarios involving regression analysis, the data modelling becomes more complex as we delve into a multivariate MLE framework. Therefore, deriving analytical expressions for the estimates is not feasible.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/bot.png" class="img-fluid figure-img" width="550"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/"><em>Manfred Stege</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-techbot-teach-o-bot-3947912/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<p>An optimization method is a mathematical algorithm designed to find the maximum or minimum values of a function, subject to specific constraints or within a certain range. In statistics, these optimization methods are essential for obtaining parameter estimates. This is often achieved by maximizing likelihood functions, minimizing error terms, or finding the best fit for data. Specifically for MLE, the goal is to find the value of a parameter (or a set of parameters) that maximizes the log-likelihood function. It is important to note that working on a logarithmic scale ensures computational stability and helps avoid issues like <strong>numerical underflow</strong>, since we typically deal with very small likelihood values. The logarithmic scale transforms these small values into more manageable negative numbers.</p>
<p>Log-likelihood functions can be nonlinear and complex, so we often use various optimization methods, including:</p>
<ul>
<li>
<strong>Gradient Descent:</strong> This method iteratively moves in the direction of the steepest descent, guided by the gradient. Its origin traces back to <span class="citation" data-cites="cauchy1847">Cauchy (<a href="references.html#ref-cauchy1847" role="doc-biblioref">1847</a>)</span>.</li>
<li>
<strong>Newton-Raphson Method:</strong> This approach utilizes both the gradient and the second derivative (known as the Hessian) to accelerate the convergence of the algorithm. It goes back to <span class="citation" data-cites="newton1736">Newton and Colson (<a href="references.html#ref-newton1736" role="doc-biblioref">1736</a>)</span> and <span class="citation" data-cites="raphson1697">Raphson (<a href="references.html#ref-raphson1697" role="doc-biblioref">1697</a>)</span>.</li>
<li>
<strong>Brent’s Method:</strong> A derivative-free technique that is ideal for scalar functions defined on a bounded interval. It is particularly useful for univariate MLE cases. You can find more information in <span class="citation" data-cites="brent1973">Brent (<a href="references.html#ref-brent1973" role="doc-biblioref">1973</a>)</span>.</li>
<li>
<strong>Broyden–Fletcher–Goldfarb–Shanno (BFGS) Method:</strong> This is a quasi-Newton method employed for multivariable optimization that uses the gradient and Hessian matrix <span class="citation" data-cites="fletcher1987">(<a href="references.html#ref-fletcher1987" role="doc-biblioref">Fletcher 1987</a>)</span>. As a side note, there is an alternative version of this method called <strong>Limited-memory BFGS (L-BFGS)</strong>, which is less computationally intensive compared to BFGS <span class="citation" data-cites="liu1989">(<a href="references.html#ref-liu1989" role="doc-biblioref">Liu and Nocedal 1989</a>)</span>.</li>
</ul>
<p>Although it is possible to obtain analytical estimates for our <strong>demand</strong> and <strong>time queries</strong>, we will use numerical optimization to provide a proof of concept for the MLE ideas discussed above. For the <strong>demand query</strong>, the code provided (for either <code>R</code> or <code>Python</code>) estimates the proportion of children who prefer chocolate over vanilla (denoted as <span class="math inline">\(\hat{\pi}\)</span>) under a Binomial model. Specifically, this scenario can be represented as:</p>
<p><span class="math display">\[
Y \sim \text{Bin}(n = 1, \pi),
\]</span></p>
<p>which is equivalent to a Bernoulli random variable with parameter <span class="math inline">\(\pi\)</span> (see <a href="#eq-binomial-to-bernoulli" class="quarto-xref">Equation&nbsp;<span>2.13</span></a>).</p>
<p>First, the code below transforms the categorical variable <code>fav_flavour</code> into a binary factor, where <code>"vanilla"</code> is coded as <code>0</code> and <code>"chocolate"</code> is coded as <code>1</code>, with <code>"chocolate"</code> representing the <em>success</em> outcome in <a href="#eq-random-variable-demand" class="quarto-xref">Equation&nbsp;<span>2.3</span></a>. It then defines a function called <code>neg_log_lik_binomial</code> that calculates the negative log-likelihood for the Binomial distribution based on a candidate probability, <span class="math inline">\(\pi\)</span>, and the observed binary responses. This function penalizes invalid <span class="math inline">\(\pi\)</span> values that fall outside the interval <span class="math inline">\([0, 1]\)</span> by returning <code>Inf</code>.</p>
<p>Then, using the corresponding optimization function through Brent’s method, the code minimizes the negative log-likelihood (which is equivalent to maximizing the positive log-likelihood) to obtain the estimate <span class="math inline">\(\hat{\pi}\)</span> that best fits the observed data. The resulting estimate is stored in <code>numerical_pi_hat_MLE</code>. Notably, this output aligns with the analytical result obtained by calculating the sample mean.</p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on additional <code>Python</code> packages!</p>
</div>
<div class="Heads-up-container">
<p>The <code>R</code> version of the optimization code below does not require any additional packages beyond the base set of functions. However, the <code>Python</code> version will need the <a href="https://pypi.org/project/scipy/">{scipy}</a> <span class="citation" data-cites="scipy">(<a href="references.html#ref-scipy" role="doc-biblioref">Virtanen et al. 2020</a>)</span> package.</p>
</div>
</div>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-11-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-11-1" role="tab" aria-controls="tabset-11-1" aria-selected="true"><strong><code>R</code></strong></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-11-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-11-2" role="tab" aria-controls="tabset-11-2" aria-selected="false"><strong><code>Python</code></strong></a></li>
</ul>
<div class="tab-content">
<div id="tabset-11-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-11-1-tab">
<div class="cell">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">children_sample</span> <span class="op">&lt;-</span> <span class="va">children_sample</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span></span>
<span>    fav_flavour <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="va">fav_flavour</span>,</span>
<span>      levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"vanilla"</span>, <span class="st">"chocolate"</span><span class="op">)</span></span>
<span>    <span class="op">)</span>, <span class="co"># Sets vanilla = 0, chocolate = 1</span></span>
<span>    flavour_binary <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/integer.html">as.integer</a></span><span class="op">(</span><span class="va">fav_flavour</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="va">neg_log_lik_binomial</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">pi</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">pi</span> <span class="op">&lt;=</span> <span class="fl">0</span> <span class="op">||</span> <span class="va">pi</span> <span class="op">&gt;=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="cn">Inf</span><span class="op">)</span></span>
<span>  <span class="op">}</span>                                  <span class="co"># pi must be in [0, 1]</span></span>
<span>  <span class="va">k</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>                        <span class="co"># Total successes</span></span>
<span>  <span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>                     <span class="co"># Number of trials</span></span>
<span>  <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">k</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">pi</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">MLE_optimizer_pi</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/optim.html">optim</a></span><span class="op">(</span></span>
<span>  par <span class="op">=</span> <span class="fl">0.5</span>,                         <span class="co"># Initial guess</span></span>
<span>  fn <span class="op">=</span> <span class="va">neg_log_lik_binomial</span>,         <span class="co"># Negative log-likelihood function</span></span>
<span>  y <span class="op">=</span> <span class="va">children_sample</span><span class="op">$</span><span class="va">flavour_binary</span>,              <span class="co"># Observed data</span></span>
<span>  method <span class="op">=</span> <span class="st">"Brent"</span>,                  <span class="co"># Method for scalar optimization</span></span>
<span>  lower <span class="op">=</span> <span class="fl">0.0001</span>, upper <span class="op">=</span> <span class="fl">0.9999</span>     <span class="co"># Ensure pi stays within [0.0001, 0.9999] in the search</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">numerical_pi_hat_MLE</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">MLE_optimizer_pi</span><span class="op">$</span><span class="va">par</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">numerical_pi_hat_MLE</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.67</code></pre>
</div>
</div>
</div>
<div id="tabset-11-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-11-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> binom</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize_scalar</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>children_sample[<span class="st">'fav_flavour'</span>] <span class="op">=</span> pd.Categorical(</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    children_sample[<span class="st">'fav_flavour'</span>], categories <span class="op">=</span> [<span class="st">'vanilla'</span>, <span class="st">'chocolate'</span>], ordered <span class="op">=</span> <span class="va">True</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>children_sample[<span class="st">'flavour_binary'</span>] <span class="op">=</span> children_sample[<span class="st">'fav_flavour'</span>].cat.codes  <span class="co"># Sets vanilla = 0, chocolate = 1</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> neg_log_lik_binomial(pi, y):</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> pi <span class="op">&lt;=</span> <span class="dv">0</span> <span class="kw">or</span> pi <span class="op">&gt;=</span> <span class="dv">1</span>:</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.inf                <span class="co"># pi must be in [0, 1]</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> y.<span class="bu">sum</span>()                      <span class="co"># Total successes</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(y)                       <span class="co"># Number of trials</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>binom.logpmf(k, n, pi)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>MLE_optimizer_pi <span class="op">=</span> minimize_scalar(</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    neg_log_lik_binomial,            <span class="co"># Negative log-likelihood function</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    bounds <span class="op">=</span> (<span class="fl">0.0001</span>, <span class="fl">0.9999</span>),       <span class="co"># Ensure pi stays within [0.0001, 0.9999] in the search</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    method <span class="op">=</span> <span class="st">'bounded'</span>,              <span class="co"># Method for scalar optimization</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>    args <span class="op">=</span> (children_sample[<span class="st">'flavour_binary'</span>],)   <span class="co"># Observed data</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>numerical_pi_hat_MLE <span class="op">=</span> <span class="bu">round</span>(MLE_optimizer_pi.x, <span class="dv">2</span>)</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(numerical_pi_hat_MLE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.61</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>For the <strong>time query</strong>, the provided code (for either <code>R</code> or <code>Python</code>) estimates the scale parameter, denoted as <span class="math inline">\(\beta\)</span>, in an Exponential distribution based on a sample of waiting times stored in <code>waiting_sample</code>. It defines a function called <code>neg_log_lik_exponential</code>, which computes the negative log-likelihood of the Exponential distribution for a given <span class="math inline">\(\beta\)</span>. This function ensures that <span class="math inline">\(\beta\)</span> is always positive; if not, it returns <code>Inf</code> to penalize invalid parameter values. The log-likelihood is calculated using the appropriate PDF.</p>
<p>The corresponding optimization function utilized minimizes this negative log-likelihood using Brent’s method. The initial guess for <span class="math inline">\(\beta\)</span> is set at <code>5</code>, and the search is confined between <code>0.0001</code> and <code>1000</code> to guarantee a valid value for <span class="math inline">\(\beta\)</span>. The final result, which is the numerical estimate of <span class="math inline">\(\hat{\beta}\)</span>, is stored in <code>numerical_beta_hat_MLE</code>. This output matches the analytical result derived from calculating the sample mean.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist">
<li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-12-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-12-1" role="tab" aria-controls="tabset-12-1" aria-selected="true"><strong><code>R</code></strong></a></li>
<li class="nav-item" role="presentation"><a class="nav-link" id="tabset-12-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-12-2" role="tab" aria-controls="tabset-12-2" aria-selected="false"><strong><code>Python</code></strong></a></li>
</ul>
<div class="tab-content">
<div id="tabset-12-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-12-1-tab">
<div class="cell">
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">neg_log_lik_exponential</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">beta</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">if</span> <span class="op">(</span><span class="va">beta</span> <span class="op">&lt;=</span> <span class="fl">0</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="cn">Inf</span><span class="op">)</span></span>
<span>  <span class="op">}</span>                                  <span class="co"># beta must be positive</span></span>
<span>  <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Exponential.html">dexp</a></span><span class="op">(</span><span class="va">y</span>, rate <span class="op">=</span> <span class="fl">1</span> <span class="op">/</span> <span class="va">beta</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">MLE_optimizer_beta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/optim.html">optim</a></span><span class="op">(</span></span>
<span>  par <span class="op">=</span> <span class="fl">5</span>,                           <span class="co"># Initial guess</span></span>
<span>  fn <span class="op">=</span> <span class="va">neg_log_lik_exponential</span>,      <span class="co"># Negative log-likelihood function</span></span>
<span>  y <span class="op">=</span> <span class="va">waiting_sample</span><span class="op">$</span><span class="va">waiting_time</span>,   <span class="co"># Observed data</span></span>
<span>  method <span class="op">=</span> <span class="st">"Brent"</span>,                  <span class="co"># Method for scalar optimization</span></span>
<span>  lower <span class="op">=</span> <span class="fl">0.0001</span>, upper <span class="op">=</span> <span class="fl">1000</span>       <span class="co"># Ensure beta stays within [0.0001, 1000] in the search</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">numerical_beta_hat_MLE</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">MLE_optimizer_beta</span><span class="op">$</span><span class="va">par</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">numerical_beta_hat_MLE</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 10.23</code></pre>
</div>
</div>
</div>
<div id="tabset-12-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-12-2-tab">
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> expon</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> neg_log_lik_exponential(beta, y):</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> beta <span class="op">&lt;=</span> <span class="dv">0</span>:</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> np.inf                  <span class="co"># beta must be positive</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>(expon.logpdf(y, scale <span class="op">=</span> beta))</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>MLE_optimizer_beta <span class="op">=</span> minimize_scalar(</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    neg_log_lik_exponential,         <span class="co"># Negative log-likelihood function</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    bounds <span class="op">=</span> (<span class="fl">0.0001</span>, <span class="dv">1000</span>),         <span class="co"># Ensure beta stays within [0.0001, 1000] in the search</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    method <span class="op">=</span> <span class="st">'bounded'</span>,</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    args <span class="op">=</span> (waiting_sample[<span class="st">'waiting_time'</span>],)   <span class="co"># Pass the data as a tuple</span></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>numerical_beta_hat_MLE <span class="op">=</span> <span class="bu">round</span>(MLE_optimizer_beta.x, <span class="dv">2</span>)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(numerical_beta_hat_MLE)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>9.67</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>As we can see, a proper numerical optimization method is essential in MLE cases where analytical solutions for the log-likelihood function cannot be obtained. It is important to re-emphasize that optimization methods play a crucial role in fitting regression models in both <code>R</code> and <code>Python</code>, as MLE analytical solutions are typically not available. For example, when we use a fitting function like <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> in <code>R</code> or various fitting functions from the <code>statsmodels</code> <span class="citation" data-cites="statsmodels">(<a href="references.html#ref-statsmodels" role="doc-biblioref">Seabold and Perktold 2010</a>)</span> library in <code>Python</code>, we are actually invoking an <strong>optimizer</strong>. This optimizer employs a specific method to search for the set of parameter values that best explain the observed data according to the chosen log-likelihood function.</p>
</section></section><section id="sec-basics-inf" class="level2" data-number="2.3"><h2 data-number="2.3" class="anchored" data-anchor-id="sec-basics-inf">
<span class="header-section-number">2.3</span> Basics of Frequentist Statistical Inference</h2>
<div id="fig-ds-workflow-results-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-ds-workflow-results-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/results.png" class="img-fluid figure-img" width="1000">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ds-workflow-results-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.7: <em>Results</em> stage from the data science workflow in <a href="01-intro.html#fig-ds-workflow" class="quarto-xref">Figure&nbsp;<span>1.1</span></a>. This stage is directly followed by <em>storytelling</em> and preceded by <em>goodness of fit</em>.
</figcaption></figure>
</div>
<div id="fig-classical-hypothesis-testing-workflow" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-classical-hypothesis-testing-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/classical-hypothesis-testing-workflow.png" class="img-fluid figure-img" width="1500">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classical-hypothesis-testing-workflow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.8: A classical-based hypothesis testing workflow structured in four substages: <em>general settings</em>, <em>hypotheses definitions</em>, <em>test flavour and components</em>, and <em>inferential conclusions</em>.
</figcaption></figure>
</div>
<section id="sec-hypothesis-workflow-general-settings" class="level3" data-number="2.3.1"><h3 data-number="2.3.1" class="anchored" data-anchor-id="sec-hypothesis-workflow-general-settings">
<span class="header-section-number">2.3.1</span> General Settings</h3>
<div id="fig-hypothesis-workflow-general-settings" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-hypothesis-workflow-general-settings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/general-settings.png" class="img-fluid figure-img" width="1000">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hypothesis-workflow-general-settings-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.9: <em>General settings</em> substage from the classical-based hypothesis testing workflow in <a href="#fig-classical-hypothesis-testing-workflow" class="quarto-xref">Figure&nbsp;<span>2.8</span></a>. This substage is directly followed by the <em>hypotheses definitions</em>.
</figcaption></figure>
</div>
<p>Based on the work by <span class="citation" data-cites="soch2023">Soch et al. (<a href="references.html#ref-soch2023" role="doc-biblioref">2024</a>)</span>, let us check some key definitions.</p>
<div id="Definition-hypothesis" class="definition">
<div class="definition-header">
<p>Definition of hypothesis</p>
</div>
<div class="definition-container">
<p>Suppose you observe some data <span class="math inline">\(y\)</span> from some population(s) or system(s) of interest governed by <span class="math inline">\(k\)</span> parameters contained in the following vector:</p>
<p><span class="math display">\[
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T.
\]</span></p>
<p>Moreover, let us assume that random variable <span class="math inline">\(Y\)</span> follows certain probability distribution <span class="math inline">\(\mathcal{D}(\cdot)\)</span> in a generative model <span class="math inline">\(m\)</span> as in</p>
<p><span class="math display">\[
\text{$m$: } Y \sim \mathcal{D}(\boldsymbol{\theta}).
\]</span></p>
<p>Beginning from the fact that <span class="math inline">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span> where <span class="math inline">\(\boldsymbol{\Theta} \in \mathbb{R}^k\)</span>, a statistical hypothesis is a general statement about some parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span> in regards to specific values contained in vector <span class="math inline">\(\boldsymbol{\Theta}^*\)</span> such that</p>
<p><span class="math display">\[
\text{$H$: } \boldsymbol{\theta} \in \boldsymbol{\Theta}^* \quad \text{where} \quad \boldsymbol{\Theta}^* \subset \boldsymbol{\Theta}.
\]</span></p>
</div>
</div>
<div id="Definition-null-hypothesis" class="definition">
<div class="definition-header">
<p>Definition of null hypothesis</p>
</div>
<div class="definition-container">
<p>In a hypothesis(s) testing, a null hypothesis is denoted by <span class="math inline">\(H_0\)</span>. The whole inferential process is designed to assess the strength of the evidence in favour or against this null hypothesis. In plain words, <span class="math inline">\(H_0\)</span> is an <strong>inferential statement</strong> associated to the <strong>status quo</strong> in some population(s) or system(s) of interest, which might refer to <strong>no signal</strong> for the researcher in question.</p>
<p>Again, suppose random variable <span class="math inline">\(Y\)</span> from some population(s) or system(s) of interest is governed by <span class="math inline">\(k\)</span> parameters contained in the following vector:</p>
<p><span class="math display">\[
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T.
\]</span></p>
<p>Moreover, we assume that random variable <span class="math inline">\(Y\)</span> follows certain probability distribution <span class="math inline">\(\mathcal{D}(\cdot)\)</span> in a generative model <span class="math inline">\(m\)</span> as in</p>
<p><span class="math display">\[
\text{$m$: } Y \sim \mathcal{D}(\boldsymbol{\theta}).
\]</span></p>
<p>Let <span class="math inline">\(\boldsymbol{\Theta}_0 \subset \boldsymbol{\theta}\)</span> denote the status quo for the parameter(s) to be tested. Then, the null hypothesis is mathematically defined as</p>
<p><span id="eq-def-null-hypothesis"><span class="math display">\[
\text{$H_0$: } \boldsymbol{\theta} \in \boldsymbol{\Theta}_0 \quad \text{where} \quad \boldsymbol{\Theta}_0 \subset \boldsymbol{\theta}.
\tag{2.76}\]</span></span></p>
</div>
</div>
<div id="Definition-alternative-hypothesis" class="definition">
<div class="definition-header">
<p>Definition of alternative hypothesis</p>
</div>
<div class="definition-container">
<p>In a hypothesis testing, an alternative hypothesis is denoted by <span class="math inline">\(H_1\)</span>. This hypothesis corresponds to the <strong>complement</strong> (i.e., the <strong>opposite</strong>) of the null hypothesis <span class="math inline">\(H_0\)</span>. Since the whole inferential process is designed to assess the strength of the evidence in favour or against of <span class="math inline">\(H_0\)</span>, any inferential conclusion against <span class="math inline">\(H_0\)</span> can be worded as “<em>rejecting <span class="math inline">\(H_0\)</span> <strong>in favour</strong> of <span class="math inline">\(H_1\)</span></em>.” In plain words, <span class="math inline">\(H_1\)</span> is an <strong>inferential statement</strong> associated to a <strong>non-status quo</strong> in some population(s) or system(s) of interest, which might refer to <strong>actual signal</strong> for the researcher in question.</p>
<p>Let us assume random variable <span class="math inline">\(Y\)</span> from some population(s) or system(s) of interest is governed by <span class="math inline">\(k\)</span> parameters contained in the following vector:</p>
<p><span class="math display">\[
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T.
\]</span></p>
<p>Moreover, suppose random variable <span class="math inline">\(Y\)</span> follows certain probability distribution <span class="math inline">\(\mathcal{D}(\cdot)\)</span> in a generative model <span class="math inline">\(m\)</span> as in</p>
<p><span class="math display">\[
\text{$m$: } Y \sim \mathcal{D}(\boldsymbol{\theta}).
\]</span></p>
<p>Let <span class="math inline">\(\boldsymbol{\Theta}_0^c \subset \boldsymbol{\theta}\)</span> denote the non-status quo for the parameter(s) to be tested. Then, the alternative hypothesis is mathematically defined as</p>
<p><span id="eq-def-alternative-hypothesis"><span class="math display">\[
\text{$H_1$: } \boldsymbol{\theta} \in \boldsymbol{\Theta}_0^c \quad \text{where} \quad \boldsymbol{\Theta}_0^c \subset \boldsymbol{\theta}.
\tag{2.77}\]</span></span></p>
</div>
</div>
<div id="Definition-hypothesis-testing" class="definition">
<div class="definition-header">
<p>Definition of hypothesis testing</p>
</div>
<div class="definition-container">
<p>A hypothesis testing is the <strong>decision rule</strong> we have to apply between the null and alternative hypotheses, via our sample data, to <strong>fail to reject</strong> or <strong>reject</strong> the null hypothesis.</p>
</div>
</div>
<div id="Definition-type-I-error" class="definition">
<div class="definition-header">
<p>Definition of type I error (false positive)</p>
</div>
<div class="definition-container">
<p>Type I error is defined as <strong>incorrectly</strong> rejecting the null hypothesis <span class="math inline">\(H_0\)</span> in favour of the alternative hypothesis <span class="math inline">\(H_1\)</span> when, in fact, <strong><span class="math inline">\(H_0\)</span> is true</strong>. Analogously, this type of error is also called false positive .</p>
</div>
</div>
<div id="Definition-type-II-error" class="definition">
<div class="definition-header">
<p>Definition of type II error (false negative)</p>
</div>
<div class="definition-container">
<p>Type II error is defined as <strong>incorrectly</strong> failing to reject the null hypothesis <span class="math inline">\(H_0\)</span> in favour of the alternative hypothesis <span class="math inline">\(H_1\)</span> when, in fact, <strong><span class="math inline">\(H_0\)</span> is false</strong>. Analogously, this type of error is also called false negative . <a href="#tbl-errors" class="quarto-xref">Table&nbsp;<span>2.5</span></a> summarizes the types of inferential conclusions in function on whether <span class="math inline">\(H_0\)</span> is true or not.</p>
<div id="tbl-errors" class="hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-errors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2.5: Types of inferential conclusions in a frequentist hypothesis testing.
</figcaption><div aria-describedby="tbl-errors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-hover caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead><tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;"><strong><span class="math inline">\(H_0\)</span> is true</strong></th>
<th style="text-align: center;"><strong><span class="math inline">\(H_0\)</span> is false</strong></th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>Reject <span class="math inline">\(H_0\)</span></strong></td>
<td style="text-align: center;">Type I error (<em>False positive</em>)</td>
<td style="text-align: center;">Correct</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>Fail to reject <span class="math inline">\(H_0\)</span></strong></td>
<td style="text-align: center;">Correct</td>
<td style="text-align: center;">Type II error (<em>False negative</em>)</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</div>
</div>
<div id="Definition-significance-level" class="definition">
<div class="definition-header">
<p>Definition of significance level</p>
</div>
<div class="definition-container">
<p>The significance level <span class="math inline">\(\alpha\)</span> is defined as the conditional probability of rejecting the null hypothesis <span class="math inline">\(H_0\)</span> given that <span class="math inline">\(H_0\)</span> is true. This can be mathematically represented as</p>
<p><span class="math display">\[
P \left( \text{Reject $H_0$} | \text{$H_0$ is true} \right) = \alpha.
\]</span></p>
<p>In plain words, <span class="math inline">\(\alpha \in [0, 1]\)</span> allows us to probabilistically control for type I error since we are dealing with random variables in our inferential process. The significance level can be thought as one of the main hypothesis testing and power analysis settings.</p>
</div>
</div>
<div id="Definition-power" class="definition">
<div class="definition-header">
<p>Definition of power</p>
</div>
<div class="definition-container">
<p>The statistical power of a test <span class="math inline">\(1 -\beta\)</span> is the complement of the conditional probability <span class="math inline">\(\beta\)</span> of failing to reject the null hypothesis <span class="math inline">\(H_0\)</span> given that <span class="math inline">\(H_0\)</span> is false, which is mathematically represented as</p>
<p><span class="math display">\[
P \left( \text{Failing to reject $H_0$} | \text{$H_0$ is false} \right) = \beta;
\]</span></p>
<p>yielding</p>
<p><span class="math display">\[
\text{Power} = 1 - \beta.
\]</span></p>
<p>In plain words, <span class="math inline">\(1 - \beta \in [0, 1]\)</span> is the <strong>probabilistic ability</strong> of our hypothesis testing to detect any signal in our inferential process, <strong>if there is any</strong>. <strong>The larger the power in our power analysis, the less prone we are to commit a type II error.</strong></p>
</div>
</div>
<div id="Definition-power-analysis" class="definition">
<div class="definition-header">
<p>Definition of power analysis</p>
</div>
<div class="definition-container">
<p>Power analysis is a set of statistical tools used to compute the <strong>minimum required sample size <span class="math inline">\(n\)</span></strong> for any given inferential study. These tools require the significance level, power, and <strong>effect size</strong> (i.e., the <strong>magnitude of the signal</strong>) the researcher aims to detect via their inferential study. This analysis seeks to determine whether observed results are likely <strong>due to chance</strong> or represent a <strong>true and meaningful effect</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/blender.png" class="img-fluid figure-img" width="400"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3699345"><em>Manfred Steger</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-blended-learning-6230153/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
</div>
</div>
<p>The larger the significance level in our power analysis and hypothesis testing, the less prone we are to commit a type I error.</p>
</section><section id="sec-hypothesis-workflow-hypotheses-definitions" class="level3" data-number="2.3.2"><h3 data-number="2.3.2" class="anchored" data-anchor-id="sec-hypothesis-workflow-hypotheses-definitions">
<span class="header-section-number">2.3.2</span> Hypotheses Definitions</h3>
<div id="fig-hypothesis-workflow-hypotheses-definitions" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-hypothesis-workflow-hypotheses-definitions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/hypotheses-definitions.png" class="img-fluid figure-img" width="1000">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hypothesis-workflow-hypotheses-definitions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.10: <em>Hypotheses definitions</em> substage from the classical-based hypothesis testing workflow in <a href="#fig-classical-hypothesis-testing-workflow" class="quarto-xref">Figure&nbsp;<span>2.8</span></a>. This substage is directly preceded by <em>general settings</em> and followed by <em>test flavour and components</em>.
</figcaption></figure>
</div>
</section><section id="sec-hypothesis-workflow-test-flavour-components" class="level3" data-number="2.3.3"><h3 data-number="2.3.3" class="anchored" data-anchor-id="sec-hypothesis-workflow-test-flavour-components">
<span class="header-section-number">2.3.3</span> Test Flavour and Components</h3>
<div id="fig-hypothesis-workflow-test-flavour-components" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-hypothesis-workflow-test-flavour-components-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/test-flavour-components.png" class="img-fluid figure-img" width="1000">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hypothesis-workflow-test-flavour-components-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.11: <em>Test flavour and components</em> substage from the classical-based hypothesis testing workflow in <a href="#fig-classical-hypothesis-testing-workflow" class="quarto-xref">Figure&nbsp;<span>2.8</span></a>. This substage is directly preceded by <em>hypotheses definitions</em> and followed by <em>inferential conclusions</em>.
</figcaption></figure>
</div>
<div id="Definition-observed-effect" class="definition">
<div class="definition-header">
<p>Definition of observed effect</p>
</div>
<div class="definition-container">
<p>An observed effect is the difference between the estimate provided the <strong>observed</strong> random sample (of size <span class="math inline">\(n\)</span>, as in <span class="math inline">\(y_1, \dots, y_n\)</span>) to the hypothesized value(s) of the population parameter(s) depicted in the statistical hypotheses.</p>
</div>
</div>
<div id="Definition-standard-error" class="definition">
<div class="definition-header">
<p>Definition of standard error</p>
</div>
<div class="definition-container">
<p>The standard error allows us to quantify the extent to which an estimate coming from an <strong>observed</strong> random sample (of size <span class="math inline">\(n\)</span>, as in <span class="math inline">\(y_1, \dots, y_n\)</span>) may deviate from the expected value under the assumption that the null hypothesis is true.</p>
<p>It plays a critical role in determining whether an observed effect is likely attributable to <strong>random variation</strong> or represents a <strong>statistically significant finding</strong>. In the absence of the standard error, it would not be possible to rigorously assess the <strong>reliability</strong> or <strong>precision</strong> of an estimate.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/magnifying-glass.png" class="img-fluid figure-img" width="420"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3699345"><em>Manfred Steger</em></a> via <a href="https://pixabay.com/vectors/pixel-easter-eggs-rabbit-8574765/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
</div>
</div>
<div id="Definition-test-statistic" class="definition">
<div class="definition-header">
<p>Definition of test statistic</p>
</div>
<div class="definition-container">
<p>The test statistic is a function of the random sample of size <span class="math inline">\(n\)</span>, i.e., it is in the function of the random variables <span class="math inline">\(Y_1, \dots, Y_n\)</span>. Therefore, the test statistic will also be a random variable, whose <strong>observed value</strong> will describe how closely the probability distribution from which the random sample comes from matches the probability distribution of the null hypothesis <span class="math inline">\(H_0\)</span>.</p>
<p>More specifically, once we have obtained the observed effect and standard error from our <strong>observed</strong> random sample, we can compute the corresponding <strong>observed</strong> test statistic. This test statistic computation will be placed on the corresponding <span class="math inline">\(x\)</span>-axis of the probability distribution of <span class="math inline">\(H_0\)</span> so we can reject or fail to reject it accordingly.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/test.png" class="img-fluid figure-img" width="420"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3699345"><em>Manfred Steger</em></a> via <a href="https://pixabay.com/vectors/pixel-cells-protocol-exchange-3947913/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
</div>
</div>
</section><section id="sec-hypothesis-workflow-inferential-conclusions" class="level3" data-number="2.3.4"><h3 data-number="2.3.4" class="anchored" data-anchor-id="sec-hypothesis-workflow-inferential-conclusions">
<span class="header-section-number">2.3.4</span> Inferential Conclusions</h3>
<div id="fig-hypothesis-workflow-inferential-conclusions" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-hypothesis-workflow-inferential-conclusions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/inferential-conclusions.png" class="img-fluid figure-img" width="1000">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-hypothesis-workflow-inferential-conclusions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2.12: <em>Inferential conclusions</em> substage from the classical-based hypothesis testing workflow in <a href="#fig-classical-hypothesis-testing-workflow" class="quarto-xref">Figure&nbsp;<span>2.8</span></a>. This substage is directly preceded by <em>rest flavour and components</em> and followed by the corresponding <em>delivery significance conclusion</em> within the <em>results</em> stage of the <em>data science workflow</em> as shown in <a href="#fig-ds-workflow-results-2" class="quarto-xref">Figure&nbsp;<span>2.7</span></a>.
</figcaption></figure>
</div>
<div id="Definition-critical-value" class="definition">
<div class="definition-header">
<p>Definition of critical value</p>
</div>
<div class="definition-container">
<p>The critical value of a hypothesis testing defines the region for which we might reject <span class="math inline">\(H_0\)</span> in favour of <span class="math inline">\(H_1\)</span>. This critical value is in the function of the significance level <span class="math inline">\(\alpha\)</span> and <strong>test flavour</strong>. It is located on the corresponding <span class="math inline">\(x\)</span>-axis of the probability distribution of <span class="math inline">\(H_0\)</span>. Hence, this value acts as a threshold to decide either of the following:</p>
<ul>
<li>If the <strong>observed</strong> test statistic exceeds a given critical value, then we have enough statistical evidence to reject <span class="math inline">\(H_0\)</span> in favour of <span class="math inline">\(H_1\)</span>.</li>
<li>If the <strong>observed</strong> test statistic does not exceed a given critical value, then we have enough statistical evidence to fail to reject <span class="math inline">\(H_0\)</span>.</li>
</ul>
</div>
</div>
<div id="Definition-p-value" class="definition">
<div class="definition-header">
<p>Definition of <span class="math inline">\(p\)</span>-value</p>
</div>
<div class="definition-container">
<p>A <span class="math inline">\(p\)</span>-value refers to the probability of obtaining a test statistic just as <strong>extreme</strong> or <strong>more extreme</strong> than the <strong>observed</strong> test statistic coming from our <strong>observed</strong> random sample of size <span class="math inline">\(n\)</span>. This <span class="math inline">\(p\)</span>-value is obtained via the probability distribution of <span class="math inline">\(H_0\)</span> and the <strong>observed</strong> test statistic.</p>
<p>Alternatively to a critical value, we can reject or fail to reject the null hypothesis <span class="math inline">\(H_0\)</span> using this <span class="math inline">\(p\)</span>-value as follows:</p>
<ul>
<li>If the <span class="math inline">\(p\)</span>-value associated to the <strong>observed</strong> test statistic exceeds a given significance level <span class="math inline">\(\alpha\)</span>, then we have enough statistical evidence to reject <span class="math inline">\(H_0\)</span> in favour of <span class="math inline">\(H_1\)</span>.</li>
<li>If the <span class="math inline">\(p\)</span>-value associated to the <strong>observed</strong> test statistic does not exceed a given significance level <span class="math inline">\(\alpha\)</span>, then we have enough statistical evidence to fail to reject <span class="math inline">\(H_0\)</span>.</li>
</ul>
</div>
</div>
<div id="Definition-confidence-interval" class="definition">
<div class="definition-header">
<p>Definition of confidence interval</p>
</div>
<div class="definition-container">
<p>A confidence interval provides an estimated range of values within which the true population parameter is likely to fall, based on the sample data. It reflects the degree of uncertainty associated with the obtained estimate. For instance, a 95% confidence interval means that if the study were repeated many times using different random samples from the same population or <strong>system</strong> of interest, approximately 95% of the resulting intervals would contain the true parameter.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/interval.png" class="img-fluid figure-img" width="500"></p>
<figcaption>Image by <a href="https://pixabay.com/users/manfredsteger-1848497/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3699345"><em>Manfred Steger</em></a> via <a href="https://pixabay.com/vectors/pixel-etherpad-group-work-3683373/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
</div>
</div>
</section></section><section id="sec-chapter-2-summary" class="level2" data-number="2.4"><h2 data-number="2.4" class="anchored" data-anchor-id="sec-chapter-2-summary">
<span class="header-section-number">2.4</span> Chapter Summary</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-bellhouse2004" class="csl-entry" role="listitem">
Bellhouse, D. R. 2004. <span>“<span class="nocase">The Reverend Thomas Bayes, FRS: A Biography to Celebrate the Tercentenary of His Birth</span>.”</span> <em>Statistical Science</em> 19 (1): 3–43. <a href="https://doi.org/10.1214/088342304000000189">https://doi.org/10.1214/088342304000000189</a>.
</div>
<div id="ref-brent1973" class="csl-entry" role="listitem">
Brent, Richard P. 1973. <span>“Chapter 4: An Algorithm with Guaranteed Convergence for Finding a Zero of a Function.”</span> In <em>Algorithms for Minimization Without Derivatives</em>. Englewood Cliffs, NJ: Prentice-Hall.
</div>
<div id="ref-casella2024" class="csl-entry" role="listitem">
Casella, G., and R. Berger. 2024. <em>Statistical Inference</em>. Chapman &amp; Hall/CRC Texts in Statistical Science. CRC Press. <a href="https://www.taylorfrancis.com/books/mono/10.1201/9781003456285/statistical-inference-roger-berger-george-casella">https://www.taylorfrancis.com/books/mono/10.1201/9781003456285/statistical-inference-roger-berger-george-casella</a>.
</div>
<div id="ref-cauchy1847" class="csl-entry" role="listitem">
Cauchy, Augustin-Louis. 1847. <span>“Méthode Générale Pour La Résolution Des Systèmes d’équations Simultanées.”</span> <em>Comptes Rendus Hebdomadaires Des Séances de l’Académie Des Sciences</em> 25: 536. <a href="https://gallica.bnf.fr/ark:/12148/bpt6k32298/f548.item">https://gallica.bnf.fr/ark:/12148/bpt6k32298/f548.item</a>.
</div>
<div id="ref-fletcher1987" class="csl-entry" role="listitem">
Fletcher, Roger. 1987. <em>Practical Methods of Optimization</em>. 2nd ed. New York: John Wiley &amp; Sons.
</div>
<div id="ref-johnson2022" class="csl-entry" role="listitem">
Johnson, A. A., M. Q. Ott, and M. Dogucu. 2022. <em>Bayes Rules!: An Introduction to Applied Bayesian Modeling</em>. Chapman &amp; Hall/CRC Texts in Statistical Science. CRC Press. <a href="https://www.bayesrulesbook.com/">https://www.bayesrulesbook.com/</a>.
</div>
<div id="ref-leemis" class="csl-entry" role="listitem">
Leemis, Larry. n.d. <span>“<span>U</span>nivariate <span>D</span>istribution <span>R</span>elationship <span>C</span>hart.”</span> <a href="https://www.math.wm.edu/~leemis/chart/UDR/UDR.html" class="uri">https://www.math.wm.edu/~leemis/chart/UDR/UDR.html</a>.
</div>
<div id="ref-liu1989" class="csl-entry" role="listitem">
Liu, Dong C., and Jorge Nocedal. 1989. <span>“On the Limited Memory BFGS Method for Large Scale Optimization.”</span> <em>Mathematical Programming</em> 45: 503–28. <a href="https://doi.org/10.1007/BF01589116">https://doi.org/10.1007/BF01589116</a>.
</div>
<div id="ref-newton1736" class="csl-entry" role="listitem">
Newton, Isaac, and John Colson. 1736. <em>Methodus Fluxionum Et Serierum Infinitarum</em>. London: Printed by Henry Woodfall;; sold by John Nourse. <a href="https://www.loc.gov/item/42048007/">https://www.loc.gov/item/42048007/</a>.
</div>
<div id="ref-odonnell1936" class="csl-entry" role="listitem">
O’Donnell, T. 1936. <em><span class="nocase">History of Life Insurance in Its Formative Years. Compiled from Approved Sources by T. O’Donnell</span></em>. Chicago.
</div>
<div id="ref-raphson1697" class="csl-entry" role="listitem">
Raphson, Joseph. 1697. <em>Analysis Æquationum Universalis</em>. 2nd ed. London: Thomas Bradyll. <a href="https://doi.org/10.3931/e-rara-13516">https://doi.org/10.3931/e-rara-13516</a>.
</div>
<div id="ref-statsmodels" class="csl-entry" role="listitem">
Seabold, Skipper, and Josef Perktold. 2010. <span>“Statsmodels: Econometric and Statistical Modeling with Python.”</span> In <em>9th Python in Science Conference</em>.
</div>
<div id="ref-soch2023" class="csl-entry" role="listitem">
Soch, Joram, The Book of Statistical Proofs, Maja, Pietro Monticone, Thomas J. Faulkenberry, Alex Kipnis, Kenneth Petrykowski, et al. 2024. <span>“<span class="nocase">StatProofBook/StatProofBook.github.io: StatProofBook 2023</span>.”</span> Zenodo. <a href="https://doi.org/10.5281/zenodo.10495684">https://doi.org/10.5281/zenodo.10495684</a>.
</div>
<div id="ref-scipy" class="csl-entry" role="listitem">
Virtanen, Pauli, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, et al. 2020. <span>“<span class="nocase"><span>SciPy</span> 1.0: Fundamental Algorithms for Scientific Computing in Python</span>.”</span> <em>Nature Methods</em> 17: 261–72. <a href="https://doi.org/10.1038/s41592-019-0686-2">https://doi.org/10.1038/s41592-019-0686-2</a>.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/alexrod61\.github\.io\/regression-cookbook\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><script src="https://giscus.app/client.js" data-repo="alexrod61/regression-cookbook" data-repo-id="R_kgDOMSJoUA" data-category="General" data-category-id="" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script><input type="hidden" id="giscus-base-theme" value="light"><input type="hidden" id="giscus-alt-theme" value="dark"><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../book/01-intro.html" class="pagination-link" aria-label="Getting Ready for Regression Cooking!">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Getting Ready for Regression Cooking!</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../book/continuous-zone.html" class="pagination-link" aria-label="Continuous Cuisine">
        <span class="nav-page-text">Continuous Cuisine</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025; G. Alexi Rodríguez-Arelis, Andy Tai, and Ben Chen</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/alexrod61/regression-cookbook/edit/main/book/02-stats-review.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/alexrod61/regression-cookbook/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/alexrod61/regression-cookbook/blob/main/book/02-stats-review.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>


<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>