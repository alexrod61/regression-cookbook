<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.53">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Appendix C — Distributional Mind Map – The Regression Cookbook (in development)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../book/D-regression-mind-map.html" rel="next">
<link href="../book/B-greek-alphabet.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<script src="../site_libs/quarto-diagram/mermaid.min.js"></script><script src="../site_libs/quarto-diagram/mermaid-init.js"></script><link href="../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="../custom.css">
</head>
<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="navbar navbar-expand-lg " data-bs-theme="dark"><div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">The Regression Cookbook (in development)</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
    <a href="https://github.com/alexrod61/regression-cookbook" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://www.linkedin.com/sharing/share-offsite/?url=%7Curl%7C">
              <i class="bi bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
</div>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../book/A-dictionary.html">Appendices</a></li><li class="breadcrumb-item"><a href="../book/C-distributional-mind-map.html"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Distributional Mind Map</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../img/cookbook.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none"></a>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/privacy-policy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Website Privacy Policy</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/audience-scope.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Audience and Scope</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/01-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Getting Ready for Regression Cooking!</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/02-stats-review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../book/continuous-zone.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Continuous Cuisine</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/03-ols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Zestylicious Ordinary Least-squares Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/04-gamma.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Gamma Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/05-beta.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Soup-erb Beta Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/06-parametric-survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Crunchified Parametric Survival Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/07-semiparametric-survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Butteryfied Semiparametric Survival Regression</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../book/discrete-zone.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Discrete Cuisine</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/08-binary-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Sauce-sational Binary Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/09-binomial-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Cheesified Binomial Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/10-classical-poisson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Classical Poisson Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/11-negative-binomial.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Umami-zing Negative Binomial Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/12-zero-inflated-poisson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Spicetacular Zero-Inflated Poisson Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/13-generalized-poisson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Herbalicious Generalized Poisson Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/14-multinomial-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Picklified Multinomial Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/15-ordinal-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Tang-tastic Ordinal Logistic Regression</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/A-dictionary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">ML-Stats Dictionary</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/B-greek-alphabet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Greek Alphabet</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/C-distributional-mind-map.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Distributional Mind Map</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../book/D-regression-mind-map.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Regression Mind Map</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">The recipe</h2>
   
  <ul>
<li>
<a href="#discrete-random-variables" id="toc-discrete-random-variables" class="nav-link active" data-scroll-target="#discrete-random-variables"><span class="header-section-number">D</span> Discrete Random Variables</a>
  <ul>
<li>
<a href="#sec-bernoulli-distribution" id="toc-sec-bernoulli-distribution" class="nav-link" data-scroll-target="#sec-bernoulli-distribution"><span class="header-section-number">D.1</span> Bernoulli</a>
  <ul class="collapse">
<li><a href="#probability-mass-function" id="toc-probability-mass-function" class="nav-link" data-scroll-target="#probability-mass-function"><span class="header-section-number">D.1.1</span> Probability Mass Function</a></li>
  <li><a href="#expected-value" id="toc-expected-value" class="nav-link" data-scroll-target="#expected-value"><span class="header-section-number">D.1.2</span> Expected Value</a></li>
  <li><a href="#variance" id="toc-variance" class="nav-link" data-scroll-target="#variance"><span class="header-section-number">D.1.3</span> Variance</a></li>
  </ul>
</li>
  <li>
<a href="#sec-binomial-distribution" id="toc-sec-binomial-distribution" class="nav-link" data-scroll-target="#sec-binomial-distribution"><span class="header-section-number">D.2</span> Binomial</a>
  <ul class="collapse">
<li><a href="#probability-mass-function-1" id="toc-probability-mass-function-1" class="nav-link" data-scroll-target="#probability-mass-function-1"><span class="header-section-number">D.2.1</span> Probability Mass Function</a></li>
  <li><a href="#expected-value-1" id="toc-expected-value-1" class="nav-link" data-scroll-target="#expected-value-1"><span class="header-section-number">D.2.2</span> Expected Value</a></li>
  <li><a href="#variance-1" id="toc-variance-1" class="nav-link" data-scroll-target="#variance-1"><span class="header-section-number">D.2.3</span> Variance</a></li>
  </ul>
</li>
  <li>
<a href="#sec-negative-binomial-distribution" id="toc-sec-negative-binomial-distribution" class="nav-link" data-scroll-target="#sec-negative-binomial-distribution"><span class="header-section-number">D.3</span> Negative Binomial</a>
  <ul class="collapse">
<li><a href="#probability-mass-function-2" id="toc-probability-mass-function-2" class="nav-link" data-scroll-target="#probability-mass-function-2"><span class="header-section-number">D.3.1</span> Probability Mass Function</a></li>
  <li><a href="#expected-value-2" id="toc-expected-value-2" class="nav-link" data-scroll-target="#expected-value-2"><span class="header-section-number">D.3.2</span> Expected Value</a></li>
  <li><a href="#variance-2" id="toc-variance-2" class="nav-link" data-scroll-target="#variance-2"><span class="header-section-number">D.3.3</span> Variance</a></li>
  </ul>
</li>
  <li>
<a href="#sec-classical-poisson-distribution" id="toc-sec-classical-poisson-distribution" class="nav-link" data-scroll-target="#sec-classical-poisson-distribution"><span class="header-section-number">D.4</span> Classical Poisson</a>
  <ul class="collapse">
<li><a href="#probability-mass-function-3" id="toc-probability-mass-function-3" class="nav-link" data-scroll-target="#probability-mass-function-3"><span class="header-section-number">D.4.1</span> Probability Mass Function</a></li>
  <li><a href="#expected-value-3" id="toc-expected-value-3" class="nav-link" data-scroll-target="#expected-value-3"><span class="header-section-number">D.4.2</span> Expected Value</a></li>
  <li><a href="#variance-3" id="toc-variance-3" class="nav-link" data-scroll-target="#variance-3"><span class="header-section-number">D.4.3</span> Variance</a></li>
  </ul>
</li>
  <li>
<a href="#sec-generalized-poisson-distribution" id="toc-sec-generalized-poisson-distribution" class="nav-link" data-scroll-target="#sec-generalized-poisson-distribution"><span class="header-section-number">D.5</span> Generalized Poisson</a>
  <ul class="collapse">
<li><a href="#probability-mass-function-4" id="toc-probability-mass-function-4" class="nav-link" data-scroll-target="#probability-mass-function-4"><span class="header-section-number">D.5.1</span> Probability Mass Function</a></li>
  <li><a href="#expected-value-4" id="toc-expected-value-4" class="nav-link" data-scroll-target="#expected-value-4"><span class="header-section-number">D.5.2</span> Expected Value</a></li>
  <li><a href="#variance-4" id="toc-variance-4" class="nav-link" data-scroll-target="#variance-4"><span class="header-section-number">D.5.3</span> Variance</a></li>
  </ul>
</li>
  <li><a href="#sec-zero-inflated-poisson-distribution" id="toc-sec-zero-inflated-poisson-distribution" class="nav-link" data-scroll-target="#sec-zero-inflated-poisson-distribution"><span class="header-section-number">D.6</span> Zero-inflated Poisson</a></li>
  <li><a href="#sec-multinomial-distribution" id="toc-sec-multinomial-distribution" class="nav-link" data-scroll-target="#sec-multinomial-distribution"><span class="header-section-number">D.7</span> Multinomial</a></li>
  </ul>
</li>
  <li>
<a href="#continuous-random-variables" id="toc-continuous-random-variables" class="nav-link" data-scroll-target="#continuous-random-variables"><span class="header-section-number">E</span> Continuous Random Variables</a>
  <ul>
<li>
<a href="#sec-weibull-distribution" id="toc-sec-weibull-distribution" class="nav-link" data-scroll-target="#sec-weibull-distribution"><span class="header-section-number">E.1</span> Weibull</a>
  <ul class="collapse">
<li><a href="#probability-density-function" id="toc-probability-density-function" class="nav-link" data-scroll-target="#probability-density-function"><span class="header-section-number">E.1.1</span> Probability Density Function</a></li>
  <li><a href="#expected-value-5" id="toc-expected-value-5" class="nav-link" data-scroll-target="#expected-value-5"><span class="header-section-number">E.1.2</span> Expected Value</a></li>
  <li><a href="#variance-5" id="toc-variance-5" class="nav-link" data-scroll-target="#variance-5"><span class="header-section-number">E.1.3</span> Variance</a></li>
  </ul>
</li>
  <li><a href="#sec-lognormal-distribution" id="toc-sec-lognormal-distribution" class="nav-link" data-scroll-target="#sec-lognormal-distribution"><span class="header-section-number">E.2</span> Lognormal</a></li>
  <li>
<a href="#sec-exponential-distribution" id="toc-sec-exponential-distribution" class="nav-link" data-scroll-target="#sec-exponential-distribution"><span class="header-section-number">E.3</span> Exponential</a>
  <ul class="collapse">
<li>
<a href="#probability-density-functions" id="toc-probability-density-functions" class="nav-link" data-scroll-target="#probability-density-functions"><span class="header-section-number">E.3.1</span> Probability Density Functions</a>
  <ul class="collapse">
<li><a href="#rate-parametrization" id="toc-rate-parametrization" class="nav-link" data-scroll-target="#rate-parametrization">Rate Parametrization</a></li>
  <li><a href="#scale-parametrization" id="toc-scale-parametrization" class="nav-link" data-scroll-target="#scale-parametrization">Scale Parametrization</a></li>
  </ul>
</li>
  <li>
<a href="#expected-value-6" id="toc-expected-value-6" class="nav-link" data-scroll-target="#expected-value-6"><span class="header-section-number">E.3.2</span> Expected Value</a>
  <ul class="collapse">
<li><a href="#rate-parametrization-1" id="toc-rate-parametrization-1" class="nav-link" data-scroll-target="#rate-parametrization-1">Rate Parametrization</a></li>
  <li><a href="#scale-parametrization-1" id="toc-scale-parametrization-1" class="nav-link" data-scroll-target="#scale-parametrization-1">Scale Parametrization</a></li>
  </ul>
</li>
  <li>
<a href="#variance-6" id="toc-variance-6" class="nav-link" data-scroll-target="#variance-6"><span class="header-section-number">E.3.3</span> Variance</a>
  <ul class="collapse">
<li><a href="#rate-parametrization-2" id="toc-rate-parametrization-2" class="nav-link" data-scroll-target="#rate-parametrization-2">Rate Parametrization</a></li>
  <li><a href="#scale-parametrization-2" id="toc-scale-parametrization-2" class="nav-link" data-scroll-target="#scale-parametrization-2">Scale Parametrization</a></li>
  </ul>
</li>
  </ul>
</li>
  <li><a href="#sec-gamma-distribution" id="toc-sec-gamma-distribution" class="nav-link" data-scroll-target="#sec-gamma-distribution"><span class="header-section-number">E.4</span> Gamma</a></li>
  <li><a href="#sec-logistic-distribution" id="toc-sec-logistic-distribution" class="nav-link" data-scroll-target="#sec-logistic-distribution"><span class="header-section-number">E.5</span> Logistic</a></li>
  <li>
<a href="#sec-normal-distribution" id="toc-sec-normal-distribution" class="nav-link" data-scroll-target="#sec-normal-distribution"><span class="header-section-number">E.6</span> Normal</a>
  <ul class="collapse">
<li><a href="#probability-density-function-1" id="toc-probability-density-function-1" class="nav-link" data-scroll-target="#probability-density-function-1"><span class="header-section-number">E.6.1</span> Probability Density Function</a></li>
  </ul>
</li>
  <li><a href="#sec-beta-distribution" id="toc-sec-beta-distribution" class="nav-link" data-scroll-target="#sec-beta-distribution"><span class="header-section-number">E.7</span> Beta</a></li>
  </ul>
</li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/alexrod61/regression-cookbook/edit/main/book/C-distributional-mind-map.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/alexrod61/regression-cookbook/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/alexrod61/regression-cookbook/blob/main/book/C-distributional-mind-map.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../book/A-dictionary.html">Appendices</a></li><li class="breadcrumb-item"><a href="../book/C-distributional-mind-map.html"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Distributional Mind Map</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-distributional-mind-map" class="quarto-section-identifier">Appendix C — Distributional Mind Map</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><!-- Google tag (gtag.js) --><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-7PRVEBE1EF"></script><script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7PRVEBE1EF');
</script><p>A crucial part of the practice of regression analysis is a fair knowledge of all the different probability distributions that would allow us to identify <strong>the most suitable regression model</strong>. Let us delve into the distributional toolbox to be used in this book.</p>
<p><a href="#fig-app-distributions" class="quarto-xref">Figure&nbsp;<span>C.1</span></a> shows all those probability distributions <strong>depicted as clouds</strong>, in the form of a <strong>univariate</strong> random variable <span class="math inline">\(Y\)</span>, used to model our outcomes of interest in the regression tools explored in each of the thirteen regression chapters, i.e., we take a generative modelling approach. Note this mind map splits the outcomes of interest into two large zones: <em>discrete</em> and <em>continuous</em>. Furthermore, this mind map briefly describes a given random variable <span class="math inline">\(Y\)</span> as a quick cheat sheet regarding its <strong>support</strong> (e.g., nonnegative, bounded or unbounded) or <strong>distributional definition</strong> (e.g., success or failure, successes in <span class="math inline">\(n\)</span> trials, etc.).</p>
<div id="fig-app-distributions" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-app-distributions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"></figure><p></p>
<div>
<pre class="mermaid mermaid-js">mindmap
  root((Univariate 
    Random
    Variable Y))
    Continuous
      {{Unbounded}}
        )Normal(
        )Logistic(
      {{Nonnegative Y}}
        )Lognormal(
        )Exponential(
        )Gamma(
        )Weibull(
      {{Y is between &lt;br/&gt;0 and 1}}
        )Beta(
    Discrete
      Binary
        {{Y is a success or &lt;br/&gt;failure}}
          )Bernoulli(
      Count
        {{Y succeses in &lt;br/&gt;n trials}}
          )Binomial(
        {{Y failures &lt;br/&gt;before experiencing &lt;br/&gt;k successes}}
          )Negative Binomial(
        {{Y events in &lt;br/&gt;a fixed interval &lt;br/&gt;of time or space}}
          )Classical &lt;br/&gt;Poisson(
          )Generalized &lt;br/&gt;Poisson(
          )Zero Inflated &lt;br/&gt;Poisson(
      Categorical
        {{Y successes of a given category, &lt;br/&gt;among a set of k categories, &lt;br/&gt;in n trials}}
          )Multinomial(
</pre>
</div>
<p></p>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-app-distributions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;C.1: Probability distribution mind map depicting all univariate random variables to be used as outcomes of interest <span class="math inline">\(Y\)</span> in the modelling techniques to be explored in this book. These outcomes are split into two large zones: discrete and continuous.
</figcaption></figure>
</div>
<p>Since a given random variable (e.g., the outcome <span class="math inline">\(Y\)</span> in any of the thirteen regression models in this book) will have a probability distribution associated with it, which will define the probability arrangement of each possible value or category <span class="math inline">\(Y\)</span> could take on, we also need a way to summarize all this information via key estimated metrics called measures of central tendency and uncertainty:</p>
<ul>
<li>
<strong>Measure of central tendency:</strong> This metric refers to a <strong>central or typical value</strong> that a given random variable might take when we observe various realizations of this variable over a long period.</li>
<li>
<strong>Measure of uncertainty:</strong> This metric pertains to the <strong>spread</strong> of a random variable when we observe its different realizations in the long term. As a side note, a <strong>larger spread</strong> indicates more variability in these realizations. On the other hand, a <strong>smaller spread</strong> denotes less variability in these realizations.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/dice.jpg" class="img-fluid figure-img" width="400"></p>
<figcaption>Image by <a href="https://pixabay.com/users/pixounaut-2729335/"><em>Tobias Frick</em></a> via <a href="https://pixabay.com/photos/game-dice-board-game-craps-game-4695864/"><em>Pixabay</em></a>.</figcaption></figure>
</div>
<p>These metrics allow us to clearly communicate how the outcome <span class="math inline">\(Y\)</span> behaves in our case study, and this is heavily related to the <strong>storytelling</strong> stage from the <strong>data science workflow</strong>, as explained in <a href="01-intro.html#sec-ds-workflow-storytelling" class="quarto-xref"><span>Section 1.2.8</span></a>. More specifically, the measures of central tendency can be communicated along with our estimated regression parameters, given that these metrics are usually conditioned to our regressors of interest within our modelling framework.</p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on parameter estimation!</p>
</div>
<div class="Heads-up-container">
<p>Just as in the case of regression parameters, the measures of central tendency and uncertainty are also parameters (more specifically, belonging to a given probability distribution) that can be estimated via an observed random sample through methods such as maximum likelihood estimation (MLE). You can check further details on the MLE fundamentals in <a href="02-stats-review.html#sec-mle" class="quarto-xref"><span>Section 2.2</span></a>.</p>
</div>
</div>
<p>There are different measures of central tendency and uncertainty. Nevertheless, we will only focus on the expected value and the variance. Now, suppose <span class="math inline">\(Y\)</span> is a random variable whose support is <span class="math inline">\(\mathcal{Y}\)</span>. Furthermore, let <span class="math inline">\(g(Y)\)</span> be a general function of <span class="math inline">\(Y\)</span>.</p>
<p>By the <strong>law of the unconscious statistician (LOTUS)</strong>, when <span class="math inline">\(Y\)</span> is a discrete random variable, we have that:</p>
<p><span id="eq-app-expected-value-discrete-function"><span class="math display">\[
\mathbb{E} \left[ g(Y) \right] = \displaystyle \sum_{y \in \mathcal{Y}} g(Y) \cdot P_Y(Y = y),
\tag{C.1}\]</span></span></p>
<p>where <span class="math inline">\(P_Y(Y = y)\)</span> is the probability mass function (PMF) of <span class="math inline">\(Y\)</span>.</p>
<p>If <span class="math inline">\(Y\)</span> is a continuous random variable, by the <strong>LOTUS</strong>, the mean of function <span class="math inline">\(g(Y)\)</span> is defined as</p>
<p><span id="eq-app-expected-value-continuous-function"><span class="math display">\[
\mathbb{E} \left[ g(Y) \right] = \displaystyle \int_{\mathcal{Y}} g(Y) \cdot f_Y(y) \text{d}y,
\tag{C.2}\]</span></span></p>
<p>where <span class="math inline">\(f_Y(y)\)</span> is the probability density function (PDF) of <span class="math inline">\(Y\)</span>.</p>
<p>Note that when <span class="math inline">\(g(Y) = y\)</span> in the discrete case, <a href="#eq-app-expected-value-discrete-function" class="quarto-xref">Equation&nbsp;<span>C.1</span></a> becomes</p>
<p><span id="eq-app-expected-value-discrete"><span class="math display">\[
\mathbb{E} \left( Y \right) = \displaystyle \sum_{y \in \mathcal{Y}} y \cdot P_Y(Y = y).
\tag{C.3}\]</span></span></p>
<p>On the other hand, when <span class="math inline">\(g(Y) = y\)</span> in the continuous case, <a href="#eq-app-expected-value-continuous-function" class="quarto-xref">Equation&nbsp;<span>C.2</span></a> becomes</p>
<p><span id="eq-app-expected-value-continuous"><span class="math display">\[
\mathbb{E} \left( Y \right) = \displaystyle \int_{\mathcal{Y}} y \cdot f_Y(y) \mathrm{d}y.
\tag{C.4}\]</span></span></p>
<p>Either for a discrete or continuous case, the variance is defined as</p>
<p><span class="math display">\[
\text{Var}(Y) = \mathbb{E}\{[Y - \mathbb{E}(Y)]^2\}.
\]</span></p>
<p>After applying some algebraic rearrangements and expected value properties, the expression above is equivalent to:</p>
<p><span id="eq-app-variance"><span class="math display">\[
\text{Var}(Y) = \mathbb{E} \left( Y^2 \right) - [\mathbb{E}(Y)]^2.
\tag{C.5}\]</span></span></p>
<p>where <span class="math inline">\(\mathbb{E} \left( Y^2 \right)\)</span> can be computed either via <a href="#eq-app-expected-value-discrete-function" class="quarto-xref">Equation&nbsp;<span>C.1</span></a> or <a href="#eq-app-expected-value-continuous-function" class="quarto-xref">Equation&nbsp;<span>C.2</span></a> depending on the nature of <span class="math inline">\(Y\)</span> with <span class="math inline">\(g(Y) = y^2\)</span>.</p>
<p>Now, for each case depicted as a cloud in <a href="#fig-app-distributions" class="quarto-xref">Figure&nbsp;<span>C.1</span></a>, subsequent sections in this appendix will show elaborate on why each corresponding PMF or PDF (depending on the type of random variable, <span class="math inline">\(Y\)</span>) is a proper probability distribution (i.e., all the standalone probabilities over the support of <span class="math inline">\(Y\)</span> add up to one) along with the respective proofs of their corresponding means and variances.</p>
<section id="discrete-random-variables" class="level1" data-number="D"><h1 data-number="D">
<span class="header-section-number">D</span> Discrete Random Variables</h1>
<p>Let us recall what a discrete random variable is. This type of variable is defined to take on a set of countable possible values. In other words, these values belong to a finite set. <a href="#fig-app-distributions" class="quarto-xref">Figure&nbsp;<span>C.1</span></a> delves into the following specific probability distributions:</p>
<ul>
<li>
<strong>Bernoulli.</strong> A random variable <span class="math inline">\(Y\)</span> that can take on the values of <span class="math inline">\(0\)</span> (i.e., a failure) or <span class="math inline">\(1\)</span> (i.e., a success) where the distributional parameter is the probability of success <span class="math inline">\(\pi \in [0, 1]\)</span>. Note <span class="math inline">\(Y\)</span> is said to be <strong>binary</strong> with a support of <span class="math inline">\(y \in \{ 0, 1 \}\)</span>.</li>
<li>
<strong>Binomial.</strong> A random variable <span class="math inline">\(Y\)</span> that defines the number of independent <strong>Bernoulli trials</strong> in which we observe a success out of <span class="math inline">\(n\)</span> trials. Its distributional parameters are the probability of success <span class="math inline">\(\pi \in [0, 1]\)</span>of each Bernoulli trial along with the total number of trials <span class="math inline">\(n \in \mathbb{N}\)</span>. Note <span class="math inline">\(Y\)</span> is said to be of <strong>count</strong> type with a support of <span class="math inline">\(y \in \{ 0, 1, \dots, n \}\)</span> successes.</li>
<li>
<strong>Negative Binomial.</strong> A random variable <span class="math inline">\(Y\)</span> that defines the number of independent <strong>Bernoulli trials</strong> in which we observe a failure before experiencing <span class="math inline">\(k\)</span> successes. Its distributional parameters are the probability of success <span class="math inline">\(\pi \in [0, 1]\)</span> of each Bernoulli trial along with the number of <span class="math inline">\(k \in \{ 0, 1, 2 \dots\}\)</span> successes. Note <span class="math inline">\(Y\)</span> is said to be of <strong>count</strong> type with a support of <span class="math inline">\(y \in \{ 0, 1, 2 \dots\}\)</span> failures.</li>
<li>
<strong>Classical Poisson.</strong> A random variable <span class="math inline">\(Y\)</span> that defines the number of events occurring in a predetermined interval of time or space. Its distributional parameter is the average rate <span class="math inline">\(\lambda \in (0, \infty)\)</span> of events per this predetermined interval of time or space. Note <span class="math inline">\(Y\)</span> is said to be of <strong>count</strong> type with a support of <span class="math inline">\(y \in \{ 0, 1, 2, \dots \}\)</span> events.</li>
<li>
<strong>Generalized Poisson.</strong> As in the above <strong>classical Poisson</strong> case, it is random variable <span class="math inline">\(Y\)</span> that defines the number of events occurring in a predetermined interval of time or space. It has two distributional parameters: the average rate <span class="math inline">\(\lambda \in (0, \infty)\)</span> of events per this predetermined interval of time or space, and a dispersion parameter <span class="math inline">\(\theta \in (-1, 1)\)</span> that models the random variable’s degree of underdispersion (when <span class="math inline">\(-1 &lt; \theta &lt; 0\)</span>) and overdispersion (when <span class="math inline">\(0 &lt; \theta &lt; 1\)</span>). Note <span class="math inline">\(Y\)</span> is said to be of <strong>count</strong> type with a support of <span class="math inline">\(y \in \{ 0, 1, 2, \dots \}\)</span> events.</li>
</ul>
<p><a href="#tbl-distributions-discrete" class="quarto-xref">Table&nbsp;<span>D.1</span></a> outlines the parameter(s), support, mean, and variance for each discrete probability distribution utilized to model the target <span class="math inline">\(Y\)</span> in a specific regression tool explained in this book.</p>
<div id="tbl-distributions-discrete" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-distributions-discrete-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;D.1: Univariate discrete probability distributions for a random variable <span class="math inline">\(Y\)</span>; including parameter(s), support, mean, and variance.
</figcaption><div aria-describedby="tbl-distributions-discrete-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 24%">
</colgroup>
<thead><tr class="header">
<th style="text-align: center;">Distribution and <br> Parametrization</th>
<th style="text-align: center;">Support</th>
<th style="text-align: center;">Mean</th>
<th style="text-align: center;">Variance</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">
<strong>Bernoulli</strong> as in <br><span class="math inline">\(Y \sim \text{Bern}(\pi)\)</span> with probability of success <span class="math inline">\(\pi \in [0, 1]\)</span>
</td>
<td style="text-align: center;"><span class="math display">\[y \in \{ 0, 1 \}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\pi\]</span></td>
<td style="text-align: center;"><span class="math display">\[\pi (1 - \pi)\]</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">
<strong>Binomial</strong> as in <br><span class="math inline">\(Y \sim \text{Bin}(n, \pi)\)</span> with number of trials <span class="math inline">\(n \in \mathbb{N}\)</span> and probability of success <span class="math inline">\(\pi \in [0, 1]\)</span>
</td>
<td style="text-align: center;"><span class="math display">\[y \in \{ 0, 1, \dots, n \}\]</span></td>
<td style="text-align: center;"><span class="math display">\[n \pi\]</span></td>
<td style="text-align: center;"><span class="math display">\[n \pi (1 - \pi)\]</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">
<strong>Negative Binomial</strong> as in <br><span class="math inline">\(Y \sim \text{NegBin}(k, \pi)\)</span> with number of successes <span class="math inline">\(k \in \{ 0, 1, 2 \dots\}\)</span> and probability of success <span class="math inline">\(\pi \in [0, 1]\)</span>
</td>
<td style="text-align: center;"><span class="math display">\[y \in \{ 0, 1, 2 \dots\}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\frac{k (1 - \pi)}{\pi}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\frac{k (1 - \pi)}{\pi^2}\]</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">
<strong>Poisson</strong> as in <br><span class="math inline">\(Y \sim \text{Pois}(\lambda)\)</span> with continuous average rate <span class="math inline">\(\lambda \in (0, \infty)\)</span>
</td>
<td style="text-align: center;"><span class="math display">\[y \in \{ 0, 1, 2, \dots\}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\lambda\]</span></td>
<td style="text-align: center;"><span class="math display">\[\lambda\]</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">
<strong>Genealized Poisson</strong> as in <br><span class="math inline">\(Y \sim \text{GP}(\lambda, \theta)\)</span> with continuous average rate <span class="math inline">\(\lambda \in (0, \infty)\)</span> and dispersion parameter <span class="math inline">\(\theta \in (-1, 1)\)</span>
</td>
<td style="text-align: center;"><span class="math display">\[y \in \{ 0, 1, 2, \dots\}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\frac{\lambda}{1 - \theta}\]</span></td>
<td style="text-align: center;"><span class="math display">\[\frac{\lambda}{(1 - \theta)^2}\]</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<section id="sec-bernoulli-distribution" class="level2" data-number="D.1"><h2 data-number="D.1" class="anchored" data-anchor-id="sec-bernoulli-distribution">
<span class="header-section-number">D.1</span> Bernoulli</h2>
<p>Let <span class="math inline">\(Y\)</span> be a discrete random variable that is part of a random process or system. <span class="math inline">\(Y\)</span> can only take on the following values:</p>
<p><span id="eq-app-bernoulli-support"><span class="math display">\[
y =
\begin{cases}
1 \; \; \; \; \text{if there is a success},\\
0 \; \; \; \; \mbox{otherwise}.
\end{cases}
\tag{D.1}\]</span></span></p>
<p>Note that the support of <span class="math inline">\(Y\)</span> in <a href="#eq-app-bernoulli-support" class="quarto-xref">Equation&nbsp;<span>D.1</span></a> makes it binary with these outcomes: <span class="math inline">\(1\)</span> for <em>success</em> and <span class="math inline">\(0\)</span> for <em>failure</em>. Then, <span class="math inline">\(Y\)</span> is said to have a <strong>Bernoulli distribution</strong> with parameter <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[
Y \sim \text{Bern}(\pi).
\]</span></p>
<section id="probability-mass-function" class="level3" data-number="D.1.1"><h3 data-number="D.1.1" class="anchored" data-anchor-id="probability-mass-function">
<span class="header-section-number">D.1.1</span> Probability Mass Function</h3>
<p>The PMF of <span class="math inline">\(Y\)</span> is the following:</p>
<p><span id="eq-app-bernoulli-pmf"><span class="math display">\[
P_Y \left( Y = y \mid \pi \right) = \pi^y (1 - \pi)^{1 - y} \quad \text{for $y \in \{ 0, 1 \}$.}
\tag{D.2}\]</span></span></p>
<p>Parameter <span class="math inline">\(\pi \in [0, 1]\)</span> refers to the probability of success. We can verify <a href="#eq-app-bernoulli-pmf" class="quarto-xref">Equation&nbsp;<span>D.2</span></a> is a proper probability distribution (i.e., all the standalone probabilities over the support of <span class="math inline">\(Y\)</span> add up to one) given that:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{align*}
\sum_{y = 0}^1 P_Y \left( Y = y \mid \pi \right) &amp;=  \sum_{y = 0}^1 \pi^y (1 - \pi)^{1 - y}  \\
&amp;= \underbrace{\pi^0}_{1} (1 - \pi) + \pi \underbrace{(1 - \pi)^{0}}_{1} \\
&amp;= (1 - \pi) + \pi \\
&amp;= 1. \qquad \qquad \qquad \qquad \quad \square
\end{align*}
\]</span></p>
<blockquote class="blockquote">
<p><strong>Indeed, the Bernoulli PMF is a proper probability distribution!</strong></p>
</blockquote>
</div>
</section><section id="expected-value" class="level3" data-number="D.1.2"><h3 data-number="D.1.2" class="anchored" data-anchor-id="expected-value">
<span class="header-section-number">D.1.2</span> Expected Value</h3>
<p>Via <a href="#eq-app-expected-value-discrete" class="quarto-xref">Equation&nbsp;<span>C.3</span></a>, the expected value or mean of a Bernoulli-distributed random variable <span class="math inline">\(Y\)</span> can be found as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{align*}
\mathbb{E}(Y) &amp;= \sum_{y = 0}^1 y P_Y \left( Y = y \mid \pi \right) \\
&amp;= \sum_{y = 0}^1 y \left[ \pi^y (1 - \pi)^{1 - y} \right] \\
&amp;= \underbrace{(0) \left[ \pi^0 (1 - \pi) \right]}_{0} + (1) \left[ \pi (1 - \pi)^{0} \right] \\
&amp;= 0 + \pi \\
&amp;= \pi. \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
\]</span></p>
</div>
</section><section id="variance" class="level3" data-number="D.1.3"><h3 data-number="D.1.3" class="anchored" data-anchor-id="variance">
<span class="header-section-number">D.1.3</span> Variance</h3>
<p>Via <a href="#eq-app-variance" class="quarto-xref">Equation&nbsp;<span>C.5</span></a> and the <a href="#eq-app-expected-value-discrete" class="quarto-xref">Equation&nbsp;<span>C.3</span></a> of a discrete expected value, the variance of a Bernoulli-distributed random variable <span class="math inline">\(Y\)</span> can be found as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{align*}
\text{Var} (Y) &amp;= \mathbb{E} \left( Y^2 \right) - \left[ \mathbb{E}(Y)\right]^2 \\
&amp;= \mathbb{E} \left( Y^2 \right) - \pi^2 \qquad \text{since $\mathbb{E}(Y) = \pi$} \\
&amp;= \sum_{y = 0}^1 y^2 P_Y \left( Y = y \mid \pi \right) - \pi^2 \qquad \text{by LOTUS} \\
&amp;= \left\{ \underbrace{(0^2) \left[ \pi^0 (1 - \pi) \right]}_{0} + \underbrace{(1^2) \left[ \pi (1 - \pi)^{0} \right]}_{\pi} \right\} - \pi^2 \\
&amp;= (0 + \pi) - \pi^2 \\
&amp;= \pi - \pi^2 \\
&amp;= \pi (1 - \pi). \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
\]</span></p>
</div>
</section></section><section id="sec-binomial-distribution" class="level2" data-number="D.2"><h2 data-number="D.2" class="anchored" data-anchor-id="sec-binomial-distribution">
<span class="header-section-number">D.2</span> Binomial</h2>
<p>Suppose you execute <span class="math inline">\(n\)</span> independent <strong>Bernoulli trials</strong>, each one with a probability of success <span class="math inline">\(\pi\)</span>. Let <span class="math inline">\(Y\)</span> be the number of successes obtained within these <span class="math inline">\(n\)</span> Bernoulli trials. Then, <span class="math inline">\(Y\)</span> is said to have a <strong>Binomial distribution</strong> with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[
Y \sim \text{Bin}(n, \pi).
\]</span></p>
<section id="probability-mass-function-1" class="level3" data-number="D.2.1"><h3 data-number="D.2.1" class="anchored" data-anchor-id="probability-mass-function-1">
<span class="header-section-number">D.2.1</span> Probability Mass Function</h3>
<p>The PMF of <span class="math inline">\(Y\)</span> is the following:</p>
<p><span id="eq-app-binomial-pmf"><span class="math display">\[
\begin{align*}
P_Y \left( Y = y \mid n, \pi \right) &amp;= {n \choose y} \pi^y (1 - \pi)^{n - y} \\
&amp; \qquad \qquad \qquad \text{for $y \in \{ 0, 1, \dots, n \}$.}
\end{align*}
\tag{D.3}\]</span></span></p>
<p>Parameter <span class="math inline">\(\pi \in [0, 1]\)</span> refers to the probability of success of each Bernoulli trial and <span class="math inline">\(n \in \mathbb{N}\)</span> to the number of trials. On the other hand, the term <span class="math inline">\({n \choose y}\)</span> indicates the total number of possible combinations for <span class="math inline">\(y\)</span> successes out of our <span class="math inline">\(n\)</span> trials:</p>
<p><span id="eq-app-combination"><span class="math display">\[
{n \choose y} = \frac{n!}{y!(n - y)!}.
\tag{D.4}\]</span></span></p>
<blockquote class="blockquote">
<p><strong>How can we verify that <a href="#eq-app-binomial-pmf" class="quarto-xref">Equation&nbsp;<span>D.3</span></a> is a proper PMF (i.e., all the standalone probabilities over the support of <span class="math inline">\(Y\)</span> add up to one)?</strong></p>
</blockquote>
<p>To elaborate on this, we need to use a handy mathematical result called the <strong>binomial theorem</strong>.</p>
<div id="thm-binomial-theorem" class="theorem">
<p><span class="theorem-title"><strong>Theorem D.1 (Binomial Theorem)</strong></span> This theorem is associated to the <strong>Pascal’s identity</strong>, and it defines the pattern of coefficients in the expansion of a polynomial in the form <span class="math inline">\((u + v)^m\)</span>. More specifically, the binomial theorem indicates that if <span class="math inline">\(m\)</span> is a non-negative integer, then the polynomial <span class="math inline">\((u + v)^m\)</span> can be expanded via the following series:</p>
<p><span id="eq-app-binomial-theorem"><span class="math display">\[
\begin{align*}
(u + v)^m &amp;= u^m + {m \choose 1} u^{m - 1} v + {m \choose 2} u^{m - 2} v^2 + \dots + \\
&amp; \qquad {m \choose r} u^{m - r} v^r + \dots + \\
&amp; \qquad {m \choose m - 1} u v^{m - 1} + v^m \\
&amp;= \underbrace{{m \choose 0}}_1 u^m + {m \choose 1} u^{m - 1} v + {m \choose 2} u^{m - 2} v^2 + \dots + \\
&amp; \qquad {m \choose r} u^{m - r} v^r + \dots + \\
&amp; \qquad {m \choose m - 1} u v^{m - 1} + \underbrace{{m \choose m}}_1 v^m \\
&amp;= \sum_{i = 0}^m {m \choose i} u^{m - i} v^i.
\end{align*}
\tag{D.5}\]</span></span></p>
</div>
<div class="Tip">
<div class="Tip-header">
<p>Tip on the binomial theorem and Pascal’s identity</p>
</div>
<div class="Tip-container">
<p>Let us dig into the proof of the binomial theorem from <a href="#eq-app-binomial-theorem" class="quarto-xref">Equation&nbsp;<span>D.5</span></a>. This proof will require another important result called the <strong>Pascal’s identity</strong>. This identity states that for any integers <span class="math inline">\(m\)</span> and <span class="math inline">\(k\)</span>, with <span class="math inline">\(k \in \{ 1, \dots, m \}\)</span>, it follows that:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-pascal-dentity"><span class="math display">\[
\begin{align*}
{m \choose k - 1} + {m \choose k} &amp;= \left[ \frac{m!}{(k - 1)! (m - k + 1)!} \right] \\
&amp; \qquad + \left[ \frac{m!}{k! (m - k)!} \right] \\
&amp;= m! \biggl\{ \left[ \frac{1}{(k - 1)! (m - k + 1)!} \right] + \\
&amp; \qquad \left[ \frac{1}{k! (m - k)!} \right] \biggl\} \\
&amp;= m! \Biggl\{ \Biggr[ \frac{k}{\underbrace{k (k - 1)!}_{k!} (m - k + 1)!} \Biggr] + \\
&amp; \qquad \Biggr[ \frac{m - k + 1}{k! \underbrace{(m - k + 1)(m - k)!}_{(m - k + 1)!}} \Biggr] \Biggl\}  \\
&amp;= m! \left[ \frac{k + m - k + 1}{k! (m - k + 1)!} \right] \\
&amp;= m! \left[ \frac{m + 1}{k! (m - k + 1)!} \right] \\
&amp;= \frac{(m + 1)!}{k! (m + 1 - k)!} \\
&amp;= {m + 1 \choose k }. \qquad \qquad \qquad \qquad \square
\end{align*}
\tag{D.6}\]</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Now, we will use <strong>mathematical induction</strong> to prove the binomial theorem from <a href="#eq-app-binomial-theorem" class="quarto-xref">Equation&nbsp;<span>D.5</span></a>. Firstly, on the left-hand side of the theorem, note that when <span class="math inline">\(m = 0\)</span> we have:<br></p>
<p><span class="math display">\[
(u + v)^0 = 1.
\]</span></p>
<p>Now, when <span class="math inline">\(m = 0\)</span>, for the right-hand side of this equation, we have that<br></p>
<p><span class="math display">\[
\sum_{i = 0}^m {m \choose i} u^{m - i} v^i  = \sum_{i = 0}^0 {0 \choose i} u^i v^{i} = {0 \choose 0} u^0 v^0 = 1.
\]</span></p>
<p>Hence, the binomial theorem holds when <span class="math inline">\(m = 0\)</span>. This is what we call the <strong>base case</strong> in mathematical induction.<br></p>
<p>That said, let us proceed with the <strong>inductive hypothesis</strong>. We aim to prove that the binomial theorem<br></p>
<p><span id="eq-inductive-hyp"><span class="math display">\[
\begin{align*}
(u + v)^j &amp;= u^j + {j \choose 1} u^{j - 1} v + {j \choose 2} u^{j - 2} v^2 + \dots + \\
&amp; \qquad {j \choose r} u^{j - r} v^r + \dots + \\
&amp; \qquad {j \choose j - 1} u v^{j - 1} + v^j \\
&amp;= \underbrace{{j \choose 0}}_1 u^j + {j \choose 1} u^{j - 1} v + {j \choose 2} u^{j - 2} v^2 + \dots + \\
&amp; \qquad {j \choose r} u^{j - r} v^r + \dots + \\
&amp; \qquad {j \choose j - 1} u v^{j - 1} + \underbrace{{j \choose j}}_1 v^j \\
&amp;= \sum_{i = 0}^j {j \choose i} u^{j - i} v^i
\end{align*}
\tag{D.7}\]</span></span></p>
<p>holds when integer <span class="math inline">\(j \geq 1\)</span>. This is our inductive hypothesis.</p>
<p>Then, we pave the way to the <strong>inductive step</strong>. Let us consider the following expansion:<br></p>
<p><span id="eq-binomial-inductive-step"><span class="math display">\[
\begin{align*}
(u + v)^{j + 1} &amp;= (u + v) (u + v)^j \\
&amp;= (u + v) \times \\
&amp; \qquad \bigg[ u^j + {j \choose 1} u^{j - 1} v + {j \choose 2} u^{j - 2} v^2 + \dots + \\
&amp; \qquad {j \choose r} u^{j - r} v^r + \dots + {j \choose j - 1} u v^{j - 1} + v^j \bigg] \\
&amp;= \bigg[u^{j + 1} + {j \choose 1} u^j v + {j \choose 2} u^{j - 1} v^2 + \dots + \\
&amp; \qquad {j \choose r} u^{j - r + 1} v^r + \dots + \\
&amp; \qquad {j \choose j - 1} u^2 v^{j - 1} + u v^j \bigg] + \\
&amp; \qquad \bigg[ u^j v + {j \choose 1} u^{j - 1} v^2 + {j \choose 2} u^{j - 2} v^3 + \dots + \\
&amp; \qquad {j \choose r} u^{j - r} v^{r + 1} + \dots + \\
&amp; \qquad {j \choose j - 1} u v^j + {j \choose j} v^{j + 1} \bigg] \\
&amp;= u^{j + 1} + \left[ {j \choose 0} + {j \choose 1} \right] u^j v + \\
&amp; \qquad \left[ {j \choose 1} + {j \choose 2} \right] u^{j - 1} v^2 + \dots + \\
&amp; \qquad \left[ {j \choose r - 1} + {j \choose r} \right] u^{j - r + 1} v^r + \dots + \\
&amp; \qquad \left[ {j \choose j - 1} + {j \choose j} \right] u v^j + v^{j + 1}.
\end{align*}
\tag{D.8}\]</span></span></p>
<p>Let us plug in the Pascal’s identity from <a href="#eq-pascal-dentity" class="quarto-xref">Equation&nbsp;<span>D.6</span></a> into <a href="#eq-binomial-inductive-step" class="quarto-xref">Equation&nbsp;<span>D.8</span></a>:<br></p>
<p><span id="eq-proof-inductive-hyp"><span class="math display">\[
\begin{align*}
(u + v)^{j + 1} &amp;= u^{j + 1} + {j + 1 \choose 1} u^j v + \\
&amp; \qquad {j + 1 \choose 2} u^{j - 1} v^2 + \dots + \\
&amp; \qquad {j + 1 \choose r} u^{j - r + 1} v^r + \dots + \\
&amp; \qquad {j + 1 \choose j} u v^j + v^{j + 1} \\
&amp;= \underbrace{{j + 1 \choose 0}}_1 u^{j + 1} + {j + 1 \choose 1} u^j v + \\
&amp; \qquad {j + 1 \choose 2} u^{j - 1} v^2 + \dots + \\
&amp; \qquad {j + 1 \choose r} u^{j - r + 1} v^r + \dots + \\
&amp; \qquad {j + 1 \choose j} u v^j + \underbrace{{j + 1 \choose j + 1}}_1 v^{j + 1} \\
&amp;= \sum_{i = 0}^{j + 1} {j + 1 \choose i} u^{j + 1 - i} v^i. \qquad \quad \square
\end{align*}
\tag{D.9}\]</span></span></p>
<p>Note that the result for <span class="math inline">\(j\)</span> in <a href="#eq-inductive-hyp" class="quarto-xref">Equation&nbsp;<span>D.7</span></a> also holds for <span class="math inline">\(j + 1\)</span> in <a href="#eq-proof-inductive-hyp" class="quarto-xref">Equation&nbsp;<span>D.9</span></a>. Therefore, by induction, the binomial theorem from <a href="#eq-app-binomial-theorem" class="quarto-xref">Equation&nbsp;<span>D.5</span></a> is true for all positive integers <span class="math inline">\(m\)</span>.</p>
</div>
</div>
</div>
<p>After the above fruitful digression on the binomial theorem, let us use it to show that our Binomial PMF in <a href="#eq-app-binomial-pmf" class="quarto-xref">Equation&nbsp;<span>D.3</span></a> actually adds up to one all over the support of the random variable:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{align*}
\sum_{y = 0}^n P_Y \left( Y = y \mid n, \pi \right) &amp;= \sum_{y = 0}^n {n \choose y} \pi^y (1 - \pi)^{n - y} \\
&amp;= \sum_{y = 0}^n {n \choose y} (1 - \pi)^{n - y} \pi^y \\
&amp; \quad \qquad \text{rearranging factors.}
\end{align*}
\]</span></p>
<p>Now, by using the binomial theorem in <a href="#eq-app-binomial-theorem" class="quarto-xref">Equation&nbsp;<span>D.5</span></a>, let:</p>
<p><span class="math display">\[
\begin{gather*}
m  = n\\
i = y \\
u = 1 - \pi \\
v = \pi.
\end{gather*}
\]</span></p>
<p>The above arrangement yields the following result:</p>
<p><span id="eq-proof-binomial-PMF-adds-to-1"><span class="math display">\[
\begin{align*}
\sum_{y = 0}^n P_Y \left( Y = y \mid n, \pi \right) &amp;= (1 - \pi + \pi)^n \\
&amp;= 1^n = 1. \qquad \square
\end{align*}
\tag{D.10}\]</span></span></p>
<blockquote class="blockquote">
<p><strong>Indeed, the Binomial PMF is a proper probability distribution!</strong></p>
</blockquote>
</div>
</section><section id="expected-value-1" class="level3" data-number="D.2.2"><h3 data-number="D.2.2" class="anchored" data-anchor-id="expected-value-1">
<span class="header-section-number">D.2.2</span> Expected Value</h3>
<p>Via <a href="#eq-app-expected-value-discrete" class="quarto-xref">Equation&nbsp;<span>C.3</span></a>, the expected value or mean of a Binomial-distributed random variable <span class="math inline">\(Y\)</span> can be found as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-proof-binomial-mean-1"><span class="math display">\[
\begin{align*}
\mathbb{E}(Y) &amp;= \sum_{y = 0}^n y P_Y \left( Y = y \mid n, \pi \right) \\
&amp;= \sum_{y = 1}^n y P_Y \left( Y = y \mid n, \pi \right) \\
&amp; \quad \qquad \text{for $y = 0$, the addend is equal to zero} \\
&amp;= \sum_{y = 1}^n y \left[ {n \choose y} \pi^y (1 - \pi)^{n - y} \right] \\
&amp;= \sum_{y = 1}^n y \left[ \frac{n!}{y! (n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
&amp;= \sum_{y = 1}^n \left[ \frac{y n!}{y (y - 1)!(n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
&amp; \quad \qquad \text{in the denominator, $y! = y (y - 1)!$}\\
&amp;= \sum_{y = 1}^n \left[ \frac{n (n - 1)!}{(y - 1)!(n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
&amp; \quad \qquad \text{in the numerator, $n! = n (n - 1)!$} \\
&amp;= \sum_{y = 1}^n \left[ \frac{n (n - 1)!}{(y - 1)!(n - y)!} \pi^{y + 1 - 1} (1 - \pi)^{n - y} \right] \\
&amp; \quad \qquad \text{note $\pi^y = \pi^{y + 1 - 1}$} \\
&amp;= n \sum_{y = 1}^n \left[ \frac{(n - 1)!}{(y - 1)!(n - y)!} \pi \pi^{y - 1} (1 - \pi)^{n - y} \right] \\
&amp; \quad \qquad \text{rearranging terms} \\
&amp;= n \pi \sum_{y = 1}^n \left[ \frac{(n - 1)!}{(y - 1)!(n - y)!} \pi^{y - 1} (1 - \pi)^{n - y} \right].
\end{align*}
\tag{D.11}\]</span></span></p>
<p>Now, let us make the following variable rearrangement:</p>
<p><span class="math display">\[
\begin{gather*}
m = n - 1 \\
z = y - 1 \\
m - z = n - y.
\end{gather*}
\]</span></p>
<p>Going back to <a href="#eq-proof-binomial-mean-1" class="quarto-xref">Equation&nbsp;<span>D.11</span></a>, <strong>and applying our above variable rearrangement within the summation</strong>, we have:</p>
<p><span id="eq-proof-binomial-mean-2"><span class="math display">\[
\begin{align*}
\mathbb{E}(Y) &amp;= n \pi \sum_{z = 0}^m \left[ \frac{m!}{z!(m - z)!} \pi^{z} (1 - \pi)^{m - z} \right] \\
&amp;= n \pi \sum_{z = 0}^m \left[ {m \choose z}\pi^{z} (1 - \pi)^{m - z} \right].
\end{align*}
\tag{D.12}\]</span></span></p>
<p>Note that, in the summation of <a href="#eq-proof-binomial-mean-2" class="quarto-xref">Equation&nbsp;<span>D.12</span></a>, we encounter the PMF of a random variable <span class="math inline">\(Z\)</span> as follows:</p>
<p><span class="math display">\[
Z \sim \text{Bin}(m, \pi).
\]</span></p>
<p>Since the summation, where this Binomial PMF of <span class="math inline">\(Z\)</span> is depicted, goes from <span class="math inline">\(z = 0\)</span> to <span class="math inline">\(m\)</span>, we can apply our result from <a href="#eq-proof-binomial-PMF-adds-to-1" class="quarto-xref">Equation&nbsp;<span>D.10</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}(Y) &amp;= n \pi \underbrace{\sum_{z = 0}^m \left[ {m \choose z}\pi^{z} (1 - \pi)^{m - z} \right]}_{1} \\
&amp;= n \pi. \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
\]</span></p>
</div>
</section><section id="variance-1" class="level3" data-number="D.2.3"><h3 data-number="D.2.3" class="anchored" data-anchor-id="variance-1">
<span class="header-section-number">D.2.3</span> Variance</h3>
<p>Via <a href="#eq-app-variance" class="quarto-xref">Equation&nbsp;<span>C.5</span></a> and the <a href="#eq-app-expected-value-discrete" class="quarto-xref">Equation&nbsp;<span>C.3</span></a> of a discrete expected value, the variance of a Binomial-distributed random variable <span class="math inline">\(Y\)</span> can be found as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-proof-binomial-variance-1"><span class="math display">\[
\begin{align*}
\text{Var} (Y) &amp;= \mathbb{E} \left( Y^2 \right) - \left[ \mathbb{E}(Y)\right]^2 \\
&amp;= \mathbb{E} \left( Y^2 \right) - (n \pi)^2 \qquad \text{since $\mathbb{E}(Y) = n \pi$.}
\end{align*}
\tag{D.13}\]</span></span></p>
<p>Unlike the Bernoulli random variable, finding <span class="math inline">\(\mathbb{E} \left( Y^2 \right)\)</span> is not quite straightforward. We need to play around with the below expected value expression as follows:</p>
<p><span id="eq-proof-binomial-variance-2"><span class="math display">\[
\begin{align*}
\mathbb{E} \left( Y^2 \right) &amp;= \mathbb{E} \left[ Y (Y - 1) \right] + \mathbb{E}(Y) \\
&amp;= \mathbb{E} \left[ Y (Y - 1) \right] + n \pi \qquad \text{since $\mathbb{E}(Y) = n \pi$.}
\end{align*}
\tag{D.14}\]</span></span></p>
<p>Now, to find <span class="math inline">\(\mathbb{E} \left[ Y (Y - 1) \right]\)</span>, we make the following derivation via the LOTUS from <a href="#eq-app-expected-value-discrete-function" class="quarto-xref">Equation&nbsp;<span>C.1</span></a> when <span class="math inline">\(g(Y) = y (y - 1)\)</span>:</p>
<p><span id="eq-proof-binomial-variance-3"><span class="math display">\[
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &amp;= \sum_{y = 0}^n y (y - 1) P_Y \left( Y = y \mid n, \pi \right) \\
&amp;= \sum_{y = 2}^n y (y - 1) P_Y \left( Y = y \mid n, \pi \right) \\
&amp; \quad \qquad \text{for $y = \{0, 1\}$,} \\
&amp; \quad \qquad \text{the addends are equal to zero} \\
&amp;= \sum_{y = 2}^n y (y - 1) \left[ {n \choose y} \pi^y (1 - \pi)^{n - y} \right] \\
&amp;= \sum_{y = 2}^n y (y - 1) \left[ \frac{n!}{y! (n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
&amp;= \sum_{y = 2}^n \left[ \frac{y (y - 1) n!}{y (y - 1) (y - 2)! (n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
&amp; \quad \qquad \text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\
&amp;= \sum_{y = 2}^n \left[ \frac{n (n - 1) (n - 2)!}{(y - 2)! (n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
&amp; \quad \qquad \text{in the numerator, $n! = n (n - 1) (n - 2)!$} \\
&amp;= \sum_{y = 2}^n \left[ \frac{n (n - 1) (n - 2)!}{(y - 2)! (n - y)!} \pi^{y + 2 - 2} (1 - \pi)^{n - y} \right] \\
&amp; \quad \qquad \text{note $\pi^y = \pi^{y + 2 - 2}$} \\
&amp;= n (n - 1) \times \\
&amp; \qquad \sum_{y = 2}^n \left[ \frac{(n - 2)!}{(y - 2)! (n - y)!} \pi^2 \pi^{y - 2} (1 - \pi)^{n - y} \right] \\
&amp; \qquad \qquad \text{rearranging terms} \\
&amp;= n (n - 1) \pi^2 \times \\
&amp; \qquad \sum_{y = 2}^n \left[ \frac{(n - 2)!}{(y - 2)! (n - y)!} \pi^{y - 2} (1 - \pi)^{n - y} \right] \\
&amp; \qquad \qquad \text{rearranging terms.}
\end{align*}
\tag{D.15}\]</span></span></p>
<p>Then, we make the following variable rearrangement:</p>
<p><span class="math display">\[
\begin{gather*}
m = n - 2 \\
z = y - 2 \\
m - z = n - y.
\end{gather*}
\]</span></p>
<p>Going back to <a href="#eq-proof-binomial-variance-3" class="quarto-xref">Equation&nbsp;<span>D.15</span></a>, <strong>and applying our above variable rearrangement within the summation</strong>, we have:</p>
<p><span id="eq-proof-binomial-variance-4"><span class="math display">\[
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &amp;= n (n - 1) \pi^2 \sum_{z = 0}^m \left[ \frac{m!}{z! (m - z)!} \pi^{z} (1 - \pi)^{m - z} \right] \\
&amp;= n (n - 1) \pi^2 \sum_{z = 0}^m \left[ {m \choose z} \pi^{z} (1 - \pi)^{m - z} \right].
\end{align*}
\tag{D.16}\]</span></span></p>
<p>Note that, in the summation of <a href="#eq-proof-binomial-variance-4" class="quarto-xref">Equation&nbsp;<span>D.16</span></a>, we encounter the PMF of a random variable <span class="math inline">\(Z\)</span> as follows:</p>
<p><span class="math display">\[
Z \sim \text{Bin}(m, \pi).
\]</span></p>
<p>Since the summation, where this Binomial PMF of <span class="math inline">\(Z\)</span> is depicted, goes from <span class="math inline">\(z = 0\)</span> to <span class="math inline">\(m,\)</span> we can apply our result from <a href="#eq-proof-binomial-PMF-adds-to-1" class="quarto-xref">Equation&nbsp;<span>D.10</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &amp;= n (n - 1) \pi^2 \underbrace{\sum_{z = 0}^m \left[ {m \choose z} \pi^{z} (1 - \pi)^{m - z} \right]}_{1} \\
&amp;= n (n - 1) \pi^2.
\end{align*}
\]</span></p>
<p>Let us go back to <a href="#eq-proof-binomial-variance-2" class="quarto-xref">Equation&nbsp;<span>D.14</span></a> and plug in the above result:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E} \left( Y^2 \right) &amp;= \mathbb{E} \left[ Y (Y - 1) \right] + n \pi \\
&amp;= n (n - 1) \pi^2 + n \pi. \\
\end{align*}
\]</span></p>
<p>Finally, we plug in <span class="math inline">\(\mathbb{E} \left( Y^2 \right)\)</span> in <a href="#eq-proof-binomial-variance-1" class="quarto-xref">Equation&nbsp;<span>D.13</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
\text{Var} (Y) &amp;= \mathbb{E} \left( Y^2 \right) - (n \pi)^2 \\
&amp;= n (n - 1) \pi^2 + n \pi - n^2 \pi^2 \\
&amp;= n^2 \pi^2 - n \pi^2 + n \pi - n^2 \pi^2 \\
&amp;= n \pi - n \pi^2 \\
&amp;= n \pi (1 - \pi). \qquad \qquad \qquad \square
\end{align*}
\]</span></p>
</div>
</section></section><section id="sec-negative-binomial-distribution" class="level2" data-number="D.3"><h2 data-number="D.3" class="anchored" data-anchor-id="sec-negative-binomial-distribution">
<span class="header-section-number">D.3</span> Negative Binomial</h2>
<p>Suppose you execute a series of independent <strong>Bernoulli trials</strong>, each one with a probability of success <span class="math inline">\(\pi\)</span>. Let <span class="math inline">\(Y\)</span> be the number of failures in this series of Bernoulli trials you obtain before experiencing <span class="math inline">\(k\)</span> successes. Therefore, <span class="math inline">\(Y\)</span> is said to have a <strong>Negative Binomial distribution</strong> with parameters <span class="math inline">\(k\)</span> and <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[
Y \sim \text{NegBin}(k, \pi).
\]</span></p>
<section id="probability-mass-function-2" class="level3" data-number="D.3.1"><h3 data-number="D.3.1" class="anchored" data-anchor-id="probability-mass-function-2">
<span class="header-section-number">D.3.1</span> Probability Mass Function</h3>
<p>The PMF of <span class="math inline">\(Y\)</span> is the following:</p>
<p><span id="eq-app-neg-binomial-pmf"><span class="math display">\[
\begin{align*}
P_Y \left( Y = y \mid k, \pi \right) &amp;= {k + y - 1 \choose y} \pi^k (1 - \pi)^y \\
&amp; \qquad \qquad \qquad \quad \text{for $y \in \{ 0, 1, \dots \}$.}
\end{align*}
\tag{D.17}\]</span></span></p>
<p>Parameter <span class="math inline">\(\pi \in [0, 1]\)</span> refers to the probability of success of each Bernoulli trial, whereas <span class="math inline">\(k\)</span> refers to the number of successes.</p>
<div class="Tip">
<div class="Tip-header">
<p>Tip on an alternative Negative Binomial PMF!</p>
</div>
<div class="Tip-container">
<p>There is an alternative parametrization to define a Negative Binomial distribution in which we have a random variable <span class="math inline">\(Z\)</span> defined as the <strong>total number of Bernoulli trials</strong> (i.e., <span class="math inline">\(k\)</span> successes plus the <span class="math inline">\(Y\)</span> failures depicted in <a href="#eq-app-neg-binomial-pmf" class="quarto-xref">Equation&nbsp;<span>D.17</span></a>):</p>
<p><span class="math display">\[
Z = Y + k.
\]</span></p>
<p>This alternative parametrization of the Negative Binomial distribution yields the following PMF:</p>
<p><span class="math display">\[
\begin{align*}
P_Z \left( Z = z \mid k, \pi \right) &amp;= {z - 1 \choose k - 1} \pi^k (1 - \pi)^{z - k} \\
&amp; \qquad \qquad \qquad \text{for $z \in \{ k, k + 1, \dots \}$.}
\end{align*}
\]</span></p>
<p>Nevertheless, we will not dig into this version of the Negative Binomial distribution since <a href="11-negative-binomial.html" class="quarto-xref"><span>Chapter 11</span></a> delves into a modelling estimation via a joint PMF of the training set involving <a href="#eq-app-neg-binomial-pmf" class="quarto-xref">Equation&nbsp;<span>D.17</span></a>.</p>
</div>
</div>
<blockquote class="blockquote">
<p><strong>How can we verify that <a href="#eq-app-neg-binomial-pmf" class="quarto-xref">Equation&nbsp;<span>D.17</span></a> is a proper PMF (i.e., all the standalone probabilities over the support of <span class="math inline">\(Y\)</span> add up to one)?</strong></p>
</blockquote>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let us manipulate the factor involving the number of combinations corresponding to how many different possible subsets of size <span class="math inline">\(y\)</span> can be made from the larger set of size <span class="math inline">\(k + y - 1\)</span>:</p>
<p><span class="math display">\[
\begin{align*}
{k + y - 1 \choose y} &amp;= \frac{(k + y - 1)!}{(k + y - 1 - y)! y !} \\
&amp;= \frac{(k + y - 1)!}{(k - 1)! y!} \\
&amp;= \frac{(k + y - 1) (k + y - 2) \cdots (k + 1) (k) (k - 1)!}{(k - 1)! y!} \\
&amp;= \frac{(\overbrace{k + y - 1) (k + y - 2) \cdots (k + 1) k}^{\text{we have $y$ factors}}}{y!} \\
&amp;= (- 1)^y \frac{\overbrace{(-k - y + 1) (-k - y + 2) \cdots (-k - 1) (-k)}^{\text{multiplying each factor times $-1$}}}{y!} \\
&amp;= (- 1)^y \frac{\overbrace{(-k) (-k - 1) \cdots (-k - y + 2) (-k - y + 1)}^{\text{rearranging factors}}}{y!} \\
&amp;= (- 1)^y \frac{(-k) (-k - 1) \cdots (-k - y + 2) (-k - y + 1)}{y!} \times \\
&amp; \qquad \frac{(-k - y) (-k - y - 1) \cdots (1)}{(-k - y) (-k - y - 1) \cdots (1)} \\
&amp;= (- 1)^y \frac{(-k) (-k - 1) \cdots (-k - y + 2) (-k - y + 1)}{y!} \times \\
&amp; \qquad \frac{(-k - y) (-k - y - 1) \cdots (1)}{(-k - y)!}.
\end{align*}
\]</span></p>
<p>In the equation above, note that there are still several factors in the numerator, which can be summarized using a factorial as follows:</p>
<p><span class="math display">\[
\begin{align*}
(-k)! &amp;= (-k) (-k - 1) \cdots (-k - y + 2) (-k - y + 1) \times \\
&amp; \quad \qquad (-k - y) (-k - y - 1) \cdots (1).
\end{align*}
\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[
\begin{align*}
{k + y - 1 \choose y} &amp;= (- 1)^y \frac{(-k)!}{(-k - y)! y!}\\
&amp;= (- 1)^y {-k \choose y}.
\end{align*}
\]</span></p>
<p>Now, let us begin with the summation involving the Negative Binomial PMF depicted in <a href="#eq-app-neg-binomial-pmf" class="quarto-xref">Equation&nbsp;<span>D.17</span></a> from <span class="math inline">\(0\)</span> to <span class="math inline">\(\infty\)</span>:</p>
<p><span id="eq-proof-neg-binomial-PMF-adds-to-1-a"><span class="math display">\[
\begin{align*}
\sum_{y = 0}^{\infty} P_Y \left( Y = y \mid k, \pi \right) &amp;= \sum_{y = 0}^{\infty} {k + y - 1 \choose y} \pi^k (1 - \pi)^y \\
&amp;= \sum_{y = 0}^{\infty} (- 1)^y {-k \choose y} \pi^k (1 - \pi)^y \\
&amp;= \pi^k \sum_{y = 0}^{\infty} (- 1)^y {-k \choose y} (1 - \pi)^y \\
&amp;= \pi^k \sum_{y = 0}^{\infty} {-k \choose y} (-1 + \pi)^y.
\end{align*}
\tag{D.18}\]</span></span></p>
<p>On the right-hand side of <a href="#eq-proof-neg-binomial-PMF-adds-to-1-a" class="quarto-xref">Equation&nbsp;<span>D.18</span></a> we will add the following factor:</p>
<p><span class="math display">\[
(1)^{-k - y} = 1.
\]</span></p>
<p>Thus:</p>
<p><span id="eq-proof-neg-binomial-PMF-adds-to-1-b"><span class="math display">\[
\begin{align*}
\sum_{y = 0}^{\infty} P_Y \left( Y = y \mid k, \pi \right) &amp;= \pi^k \sum_{y = 0}^{\infty} {-k \choose y} (1)^{-k - y} (-1 + \pi)^y.
\end{align*}
\tag{D.19}\]</span></span></p>
<p>Now, by using the <strong>binomial theorem</strong> in <a href="#eq-app-binomial-theorem" class="quarto-xref">Equation&nbsp;<span>D.5</span></a>, let:</p>
<p><span class="math display">\[
\begin{gather*}
m  = -k\\
i = y \\
u = 1 \\
v = -1 + \pi.
\end{gather*}
\]</span></p>
<p>The above arrangement yields the following result in <a href="#eq-proof-neg-binomial-PMF-adds-to-1-b" class="quarto-xref">Equation&nbsp;<span>D.19</span></a>:</p>
<p><span id="eq-proof-neg-binomial-PMF-adds-to-1-c"><span class="math display">\[
\begin{align*}
\sum_{y = 0}^{\infty} P_Y \left( Y = y \mid k, \pi \right) &amp;=  \pi^k (1 - 1 + \pi)^{-k} \\
&amp;= \pi^k (\pi) ^{-k} \\
&amp;= \pi^0 \\
&amp;= 1. \qquad \qquad \qquad \square
\end{align*}
\tag{D.20}\]</span></span></p>
<blockquote class="blockquote">
<p><strong>Indeed, the Negative Binomial PMF is a proper probability distribution!</strong></p>
</blockquote>
</div>
</section><section id="expected-value-2" class="level3" data-number="D.3.2"><h3 data-number="D.3.2" class="anchored" data-anchor-id="expected-value-2">
<span class="header-section-number">D.3.2</span> Expected Value</h3>
<p>Via <a href="#eq-app-expected-value-discrete" class="quarto-xref">Equation&nbsp;<span>C.3</span></a>, the expected value or mean of a Negative Binomial-distributed random variable <span class="math inline">\(Y\)</span> can be found as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-proof-neg-binomial-mean-1"><span class="math display">\[
\begin{align*}
\mathbb{E}(Y) &amp;= \sum_{y = 0}^{\infty} y P_Y \left( Y = y \mid k, \pi \right) \\
&amp;= \sum_{y = 1}^{\infty} y P_Y \left( Y = y \mid k, \pi \right) \\
&amp; \quad \qquad \text{for $y = 0$, the addend is equal to zero} \\
&amp;= \sum_{y = 1}^{\infty} y \left[ {k + y - 1 \choose y} \pi^k (1 - \pi)^y \right] \\
&amp;= \sum_{y = 1}^{\infty} y \left[ \frac{(k + y - 1)!}{y! (k + y - 1 - y)!} \pi^k (1 - \pi)^y \right] \\
&amp;= \sum_{y = 1}^{\infty} y \left[ \frac{(k + y - 1)!}{y! (k - 1)!} \pi^k (1 - \pi)^y \right] \\
&amp;= \sum_{y = 1}^{\infty} y \Bigg[ \frac{(k + y - 1)!}{y (y - 1)! \underbrace{\left( \frac{k!}{k} \right)}_{(k - 1)!}} \pi^k (1 - \pi)^y \Bigg] \\
&amp;= \sum_{y = 1}^{\infty} k \left[ \frac{(k + y - 1)!}{k! (y - 1)!} \pi^k (1 - \pi)^y \right] \\
&amp;= k \sum_{y = 1}^{\infty} \left[ {k + y - 1 \choose y - 1} \pi^k (1 - \pi)^y \right] \\
&amp;= k \sum_{y = 1}^{\infty} \left[ {k + y - 1 \choose y - 1} \pi^{k + 1 - 1} (1 - \pi)^{y + 1 - 1} \right] \\
&amp; \quad \qquad \text{note $\pi^k = \pi^{k + 1 - 1}$ and $(1 - \pi)^y = (1 - \pi)^{y + 1 - 1}$} \\
&amp;= \frac{k (1 - \pi)}{\pi} \sum_{y = 1}^{\infty} \left[ {k + y - 1 \choose y - 1} \pi^{k + 1} (1 - \pi)^{y - 1} \right].
\end{align*}
\tag{D.21}\]</span></span></p>
<p>Now, let us make the following variable rearrangement:</p>
<p><span class="math display">\[
\begin{gather*}
m = k + 1 \\
z = y - 1 \\
m + z - 1  = k + y - 1.
\end{gather*}
\]</span></p>
<p>Going back to <a href="#eq-proof-neg-binomial-mean-1" class="quarto-xref">Equation&nbsp;<span>D.21</span></a>, <strong>and applying our above variable rearrangement within the summation</strong>, we have:</p>
<p><span id="eq-proof-neg-binomial-mean-2"><span class="math display">\[
\mathbb{E}(Y) = \frac{k (1 - \pi)}{\pi} \sum_{z = 0}^{\infty} \left[ {m + z - 1 \choose z} \pi^{m} (1 - \pi)^{z} \right].
\tag{D.22}\]</span></span></p>
<p>Note that, in the summation of <a href="#eq-proof-neg-binomial-mean-2" class="quarto-xref">Equation&nbsp;<span>D.22</span></a>, we encounter the PMF of a random variable <span class="math inline">\(Z\)</span> as follows:</p>
<p><span class="math display">\[
Z \sim \text{NegBin}(m, \pi).
\]</span></p>
<p>Since the summation, where this Negative Binomial PMF of <span class="math inline">\(Z\)</span> is depicted, goes from <span class="math inline">\(z = 0\)</span> to <span class="math inline">\(\infty\)</span>, we can apply our result from <a href="#eq-proof-neg-binomial-PMF-adds-to-1-c" class="quarto-xref">Equation&nbsp;<span>D.20</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}(Y) &amp;= \frac{k (1 - \pi)}{\pi} \underbrace{\sum_{z = 0}^m \left[ {m + z - 1 \choose z} \pi^{m} (1 - \pi)^{z} \right]}_{1} \\
&amp;= \frac{k (1 - \pi)}{\pi}. \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
\]</span></p>
</div>
</section><section id="variance-2" class="level3" data-number="D.3.3"><h3 data-number="D.3.3" class="anchored" data-anchor-id="variance-2">
<span class="header-section-number">D.3.3</span> Variance</h3>
<p>Via <a href="#eq-app-variance" class="quarto-xref">Equation&nbsp;<span>C.5</span></a> and the <a href="#eq-app-expected-value-discrete" class="quarto-xref">Equation&nbsp;<span>C.3</span></a> of a discrete expected value, the variance of a Negative Binomial-distributed random variable <span class="math inline">\(Y\)</span> can be found as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-proof-neg-binomial-variance-1"><span class="math display">\[
\begin{align*}
\text{Var} (Y) &amp;= \mathbb{E} \left( Y^2 \right) - \left[ \mathbb{E}(Y)\right]^2 \\
&amp;= \mathbb{E} \left( Y^2 \right) - \left[ \frac{k (1 - \pi)}{\pi} \right]^2 \quad \text{since $\mathbb{E}(Y) = \frac{k (1 - \pi)}{\pi}$.}
\end{align*}
\tag{D.23}\]</span></span></p>
<p>Now, we need to play around with the below expected value expression as follows:</p>
<p><span id="eq-proof-neg-binomial-variance-2"><span class="math display">\[
\begin{align*}
\mathbb{E} \left( Y^2 \right) &amp;= \mathbb{E} \left[ Y (Y - 1) \right] + \mathbb{E}(Y) \\
&amp;= \mathbb{E} \left[ Y (Y - 1) \right] + \frac{k (1 - \pi)}{\pi}.
\end{align*}
\tag{D.24}\]</span></span></p>
<p>To find <span class="math inline">\(\mathbb{E} \left[ Y (Y - 1) \right]\)</span>, we make the following derivation via the LOTUS from <a href="#eq-app-expected-value-discrete-function" class="quarto-xref">Equation&nbsp;<span>C.1</span></a> when <span class="math inline">\(g(Y) = y (y - 1)\)</span>:</p>
<p><span id="eq-proof-neg-binomial-variance-3"><span class="math display">\[
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &amp;= \sum_{y = 0}^{\infty} y (y - 1) P_Y \left( Y = y \mid k, \pi \right) \\
&amp;= \sum_{y = 2}^{\infty} y (y - 1) P_Y \left( Y = y \mid k, \pi \right) \\
&amp; \quad \qquad \text{for $y = \{0, 1\}$,} \\
&amp; \quad \qquad \text{the addends are equal to zero} \\
&amp;= \sum_{y = 2}^{\infty} y (y - 1) \left[ {k + y - 1 \choose y} \pi^k (1 - \pi)^y \right] \\
&amp;= \sum_{y = 2}^{\infty} y (y - 1) \left[ \frac{(k + y - 1)!}{y! (k + y - 1 - y)!} \pi^k (1 - \pi)^y \right] \\
&amp;= \sum_{y = 2}^{\infty} y (y - 1) \left[ \frac{(k + y - 1)!}{y! (k - 1)!} \pi^k (1 - \pi)^y \right] \\
&amp;= \sum_{y = 2}^{\infty} \frac{y (y - 1)}{y (y - 1)} \left[ \frac{(k + y - 1)!}{(y - 2)! (k - 1)!} \pi^k (1 - \pi)^y \right] \\
&amp; \quad \qquad \text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\
&amp;= \sum_{y = 2}^{\infty} \Bigg[ \frac{(k + y - 1)!}{(y - 2)! \underbrace{\frac{(k + 1)!}{k (k + 1)}}_{(k - 1)!}} \pi^k (1 - \pi)^y \Bigg] \\
&amp;= \sum_{y = 2}^{\infty} \left[ k (k + 1) \frac{(k + y - 1)!}{(k + 1)! (y - 2)!} \pi^k (1 - \pi)^y \right] \\
&amp;= k (k + 1) \sum_{y = 2}^{\infty} \left[ {k + y - 1 \choose y - 2} \pi^k (1 - \pi)^y \right] \\
&amp;= k (k + 1) \sum_{y = 2}^{\infty} \left[ {k + y - 1 \choose y - 2} \pi^{k + 2 - 2} (1 - \pi)^{y + 2 - 2} \right] \\
&amp; \quad \qquad \text{note $\pi^k = \pi^{k + 2 - 2}$ and} \\
&amp; \quad \qquad (1 - \pi)^y = (1 - \pi)^{y + 2 - 2} \\
&amp;= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} \times \\
&amp; \qquad \sum_{y = 2}^{\infty} \left[ {k + y - 1 \choose y - 2} \pi^{k + 2} (1 - \pi)^{y - 2} \right].
\end{align*}
\tag{D.25}\]</span></span></p>
<p>Then, we make the following variable rearrangement:</p>
<p><span class="math display">\[
\begin{gather*}
m = k + 2\\
z = y - 2 \\
m + z - 1  = k + y - 1.
\end{gather*}
\]</span></p>
<p>Going back to <a href="#eq-proof-neg-binomial-variance-3" class="quarto-xref">Equation&nbsp;<span>D.25</span></a>, <strong>and applying our above variable rearrangement within the summation</strong>, we have:</p>
<p><span id="eq-proof-neg-binomial-variance-4"><span class="math display">\[
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &amp;= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} \times \\
&amp; \qquad \sum_{y = 2}^{\infty} \left[ {m + z - 1 \choose z} \pi^m (1 - \pi)^z \right].
\end{align*}
\tag{D.26}\]</span></span></p>
<p>Note that, in the summation of <a href="#eq-proof-neg-binomial-variance-4" class="quarto-xref">Equation&nbsp;<span>D.26</span></a>, we encounter the PMF of a random variable <span class="math inline">\(Z\)</span> as follows:</p>
<p><span class="math display">\[
Z \sim \text{NegBin}(m, \pi).
\]</span></p>
<p>Since the summation, where this Binomial PMF of <span class="math inline">\(Z\)</span> is depicted, goes from <span class="math inline">\(z = 0\)</span> to <span class="math inline">\(\infty\)</span>, we can apply our result from <a href="#eq-proof-neg-binomial-PMF-adds-to-1-c" class="quarto-xref">Equation&nbsp;<span>D.20</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &amp;= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} \times \\
&amp; \qquad \underbrace{\sum_{y = 2}^{\infty} \left[ {m + z - 1 \choose z} \pi^m (1 - \pi)^z \right]}_{1} \\
&amp;= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2}.
\end{align*}
\]</span></p>
<p>Let us go back to <a href="#eq-proof-neg-binomial-variance-2" class="quarto-xref">Equation&nbsp;<span>D.24</span></a> and plug in the above result:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E} \left( Y^2 \right) &amp;= \mathbb{E} \left[ Y (Y - 1) \right] + \frac{k ( 1 - \pi)}{\pi} \\
&amp;= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} + \frac{k ( 1 - \pi)}{\pi}.
\end{align*}
\]</span></p>
<p>Finally, we plug in <span class="math inline">\(\mathbb{E} \left( Y^2 \right)\)</span> in <a href="#eq-proof-neg-binomial-variance-1" class="quarto-xref">Equation&nbsp;<span>D.23</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
\text{Var} (Y) &amp;= \mathbb{E} \left( Y^2 \right) - \left[ \frac{k (1 - \pi)}{\pi} \right]^2 \\
&amp;= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} + \frac{k ( 1 - \pi)}{\pi} - \left[ \frac{k (1 - \pi)}{\pi} \right]^2 \\
&amp;= \frac{k (1 - \pi)}{\pi} \left[ \frac{(k + 1) (1 - \pi)}{\pi} + 1 - \frac{k (1 - \pi)}{\pi} \right] \\
&amp;= \frac{k (1 - \pi)}{\pi} \left[ \frac{(k + 1) (1 - \pi) + \pi - k (1 - \pi)}{\pi} \right] \\
&amp;= \frac{k (1 - \pi)}{\pi} \left( \frac{k - k \pi + 1 - \pi + \pi - k + k \pi}{\pi} \right) \\
&amp;= \frac{k (1 - \pi)}{\pi} \left( \frac{1}{\pi} \right) \\
&amp;= \frac{k (1 - \pi)}{\pi^2}. \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
\]</span></p>
</div>
</section></section><section id="sec-classical-poisson-distribution" class="level2" data-number="D.4"><h2 data-number="D.4" class="anchored" data-anchor-id="sec-classical-poisson-distribution">
<span class="header-section-number">D.4</span> Classical Poisson</h2>
<p>Suppose you observe the count of events happening in a <strong>fixed interval of time or space</strong>. Let <span class="math inline">\(Y\)</span> be the number of counts considered of integer type. Then, <span class="math inline">\(Y\)</span> is said to have a <strong>classical Poisson distribution</strong> with a continuous parameter <span class="math inline">\(\lambda\)</span>:</p>
<p><span class="math display">\[
Y \sim \text{Pois}(\lambda).
\]</span></p>
<section id="probability-mass-function-3" class="level3" data-number="D.4.1"><h3 data-number="D.4.1" class="anchored" data-anchor-id="probability-mass-function-3">
<span class="header-section-number">D.4.1</span> Probability Mass Function</h3>
<p>The PMF of this count-type <span class="math inline">\(Y\)</span> is the following:</p>
<p><span id="eq-app-classical-poisson-pmf"><span class="math display">\[
P_Y \left( Y = y \mid \lambda \right) = \frac{\lambda^y \exp{(-\lambda)}}{y!} \quad \text{for $y \in \{ 0, 1, 2, \dots\}$,}
\tag{D.27}\]</span></span></p>
<p>where <span class="math inline">\(\exp{(\cdot)}\)</span> depicts the base <span class="math inline">\(e\)</span> (i.e., <strong>Euler’s number</strong>, <span class="math inline">\(e = 2.71828...\)</span>) and <span class="math inline">\(y!\)</span> is the factorial</p>
<p><span class="math display">\[
y! = y \times (y - 1) \times (y - 2) \times (y - 3) \times \cdots \times 3 \times 2 \times 1.  
\]</span></p>
<p>with</p>
<p><span class="math display">\[
0! = 1.
\]</span></p>
<p>The continuous parameter <span class="math inline">\(\lambda \in (0, \infty)\)</span> represents the average rate at which these events happen (i.e., events per area unit or events per time unit). Curiously, even though the random variable <span class="math inline">\(Y\)</span> is considered discrete in this case, <span class="math inline">\(\lambda\)</span> is modelled as continuous!</p>
<blockquote class="blockquote">
<p><strong>How can we verify that <a href="#eq-app-classical-poisson-pmf" class="quarto-xref">Equation&nbsp;<span>D.27</span></a> is a proper PMF (i.e., all the standalone probabilities over the support of <span class="math inline">\(Y\)</span> add up to one)?</strong></p>
</blockquote>
<p>To elaborate on this, we need to use some mathematical tools called the <strong>Taylor series expansions</strong> and a derived result called <strong>Maclaurin series expansions</strong>.</p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on the Taylor and Maclaurin series expansions!</p>
</div>
<div class="Heads-up-container">
<p>In mathematics, there are helpful tools known as <strong>Taylor series expansions</strong>, which were officially published by English mathematician Brook Taylor in <em>Methodus Incrementorum Directa &amp; Inversa</em> <span class="citation" data-cites="taylor1715">(<a href="references.html#ref-taylor1715" role="doc-biblioref">Taylor 1715</a>)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/brook-taylor.jpg" class="img-fluid figure-img" width="300"></p>
<figcaption>Portrait of mathematician Brook Taylor <span class="citation" data-cites="earlom1793">(<a href="references.html#ref-earlom1793" role="doc-biblioref">Earlom 1793</a>)</span>.</figcaption></figure>
</div>
<p>However, it is essential to note that Scottish mathematician James Gregory introduced the notion of these series expansions in his work <em>Vera Circuli et Hyperbolae Quadratura</em> <span class="citation" data-cites="gregory1668">(<a href="references.html#ref-gregory1668" role="doc-biblioref">Gregory 1668</a>)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/james-gregory.jpg" class="img-fluid figure-img" width="300"></p>
<figcaption>Portrait of mathematician James Gregory <span class="citation" data-cites="watson1886">(<a href="references.html#ref-watson1886" role="doc-biblioref">Scotland, n.d.</a>)</span>.</figcaption></figure>
</div>
<p>These series approximate complex mathematical functions through an infinite sum of polynomial terms. For example, in <strong>machine learning</strong>, the Taylor series expansions can be utilized in <strong>gradient-based optimization methods</strong>. Specifically, <strong>Newton’s method</strong> uses these expansions to find roots of equations that cannot be solved analytically, which is common in maximum likelihood-based parameter estimation for the varied regression models discussed throughout this book. Moreover, we can find these series in <strong>different engineering and scientific fields such as physics</strong>.</p>
<p>Suppose we have <strong>real function</strong> <span class="math inline">\(f(u)\)</span> around a point <span class="math inline">\(u = a\)</span>, then the one-dimensional <strong>infinite</strong> Taylor series expansion is given by the expression</p>
<p><span id="eq-taylor-series"><span class="math display">\[
\begin{align*}
f(u) &amp;= f(a) + f'(a) (u - a) + \frac{f''(a)}{2!} (u - a)^2 + \\
&amp; \qquad \frac{f^{(3)}(a)}{3!} (u - a)^3 + \frac{f^{(4)}(a)}{4!} (u - a)^4 + \\
&amp; \qquad \frac{f^{(5)}(a)}{5!} (u - a)^5 + \cdots \\
&amp;= \sum_{j = 0}^{\infty} \frac{f^{(j)}(a)}{j!} (u - a)^j.
\end{align*}
\tag{D.28}\]</span></span></p>
<p>A complete mathematical derivation of <a href="#eq-taylor-series" class="quarto-xref">Equation&nbsp;<span>D.28</span></a> can be found in <span class="citation" data-cites="weisstein">Weisstein (<a href="references.html#ref-weisstein" role="doc-biblioref">n.d.b</a>)</span>. Moving along, specifically in the last line of this equation which shows an infinite summation, note the following:</p>
<ul>
<li>
<span class="math inline">\(f^{(j)}(a)\)</span> indicates the <span class="math inline">\(j\)</span>th order derivative of <span class="math inline">\(f(u)\)</span> and evaluated at point <span class="math inline">\(a\)</span>.</li>
<li>
<span class="math inline">\(j!\)</span> implicates the factorial of <span class="math inline">\(j\)</span> such that</li>
</ul>
<p><span class="math display">\[
j! = j \times (j - 1) \times (j - 2) \times (j - 3) \times \cdots \times 3 \times 2 \times 1.
\]</span></p>
<p>with</p>
<p><span class="math display">\[
0! = 1.
\]</span></p>
<p>If we go even further with <a href="#eq-taylor-series" class="quarto-xref">Equation&nbsp;<span>D.28</span></a>, we have a specific case when <span class="math inline">\(a = 0\)</span> called the <strong>Maclaurin series expansions</strong>. This case was introduced by the Scottish mathematician Colin Maclaurin in his work <em>A Treatise of Fluxions</em> <span class="citation" data-cites="maclaurin1742">(<a href="references.html#ref-maclaurin1742" role="doc-biblioref">Maclaurin 1742</a>)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/colin-maclaurin.jpg" class="img-fluid figure-img" width="300"></p>
<figcaption>Portrait of mathematician Colin Maclaurin <span class="citation" data-cites="harding1798">(<a href="references.html#ref-harding1798" role="doc-biblioref">Harding 1798</a>)</span>.</figcaption></figure>
</div>
<p>Hence, in a Mclaurin series, <a href="#eq-taylor-series" class="quarto-xref">Equation&nbsp;<span>D.28</span></a> becomes:</p>
<p><span id="eq-maclaurin-series"><span class="math display">\[
\begin{align*}
f(u) &amp;= f(0) + f'(0) (u) + \frac{f''(0)}{2!} u^2 + \\
&amp; \qquad \frac{f^{(3)}(0)}{3!} u^3 + \frac{f^{(4)}(0)}{4!} u^4 + \\
&amp; \qquad \frac{f^{(5)}(0)}{5!} u^5 + \cdots \\
&amp;= \sum_{j = 0}^{\infty} \frac{f^{(j)}(0)}{j!} u^j.
\end{align*}
\tag{D.29}\]</span></span></p>
<p>Different statistical proofs make use of Taylor series expansions as well as the Mclaurin series, and the Poisson distribution is not an exception at all!</p>
</div>
</div>
<p>The above Mclaurin series in <a href="#eq-maclaurin-series" class="quarto-xref">Equation&nbsp;<span>D.29</span></a> will help us to show that our Poisson PMF in <a href="#eq-app-classical-poisson-pmf" class="quarto-xref">Equation&nbsp;<span>D.27</span></a> actually adds up to one all over the support of the random variable:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-proof-poisson-PMF-sum"><span class="math display">\[
\begin{align*}
\sum_{y = 0}^{\infty} P_Y \left( Y = y \mid \lambda \right) &amp;= \sum_{y = 0}^{\infty} \frac{\lambda^y \exp{(-\lambda)}}{y!} \\
&amp;= \exp{(-\lambda)} \sum_{y = 0}^{\infty} \frac{\lambda^y}{y!} \\
&amp; \quad \qquad \text{factoring out $\exp{(-\lambda)}$,} \\
&amp; \quad \qquad \text{since it does not depend on $y$.}
\end{align*}
\tag{D.30}\]</span></span></p>
<p>Now, we will focus on the above summation</p>
<p><span class="math display">\[
\sum_{y = 0}^{\infty} \frac{\lambda^y}{y!}
\]</span> and use the Mclaurin series from <a href="#eq-maclaurin-series" class="quarto-xref">Equation&nbsp;<span>D.29</span></a> by letting</p>
<p><span id="eq-base-e"><span class="math display">\[
f(u) = \exp(u).
\tag{D.31}\]</span></span></p>
<p>We know that all derivatives of the above function are equal</p>
<p><span class="math display">\[
f'(u) = f''(u) = f^{(3)}(u) = f^{(4)}(u) = f^{(5)}(u) = \cdots = \exp{(u)},
\]</span> which allows us to conclude that the <span class="math inline">\(j\)</span>th derivative is</p>
<p><span class="math display">\[
f^{(j)}(u) = \exp(u).
\]</span></p>
<p>This <span class="math inline">\(j\)</span>th derivative evaluated at <span class="math inline">\(u = 0\)</span> becomes</p>
<p><span class="math display">\[
f^{(j)}(0) = \exp(0) = 1.
\]</span></p>
<p>Therefore, the Mclaurin series for <a href="#eq-base-e" class="quarto-xref">Equation&nbsp;<span>D.31</span></a> is the following:</p>
<p><span id="eq-maclaurin-series-base-e"><span class="math display">\[
\begin{align*}
f(u) &amp;= \exp(u) \\
&amp;= \sum_{j = 0}^{\infty} \frac{\exp(0)}{j!} u^j \\
&amp;= \sum_{j = 0}^{\infty} \frac{u^j }{j!}.
\end{align*}
\tag{D.32}\]</span></span></p>
<p>That said, using <a href="#eq-maclaurin-series-base-e" class="quarto-xref">Equation&nbsp;<span>D.32</span></a>, let:</p>
<p><span class="math display">\[
\begin{gather*}
\lambda = u \\
y = j.
\end{gather*}
\]</span></p>
<p>Thus, we have the following:</p>
<p><span class="math display">\[
\sum_{y = 0}^{\infty} \frac{\lambda^y}{y!} = \exp{(\lambda)}.
\]</span></p>
<p>Finally, going back to <a href="#eq-proof-poisson-PMF-sum" class="quarto-xref">Equation&nbsp;<span>D.30</span></a>:</p>
<p><span id="eq-proof-poisson-PMF-adds-to-1"><span class="math display">\[
\begin{align*}
\sum_{y = 0}^{\infty} P_Y \left( Y = y \mid \lambda \right) &amp;= \exp{(-\lambda)} \overbrace{\sum_{y = 0}^{\infty} \frac{\lambda^y}{y!}}^{\exp{(\lambda)}} \\
&amp;= \exp{(-\lambda)} \times \exp{(\lambda)} \\
&amp;= \exp{(-\lambda + \lambda)} \\
&amp;= \exp{(0)} \\
&amp;= 1. \qquad \qquad \qquad \qquad \square
\end{align*}
\tag{D.33}\]</span></span></p>
<blockquote class="blockquote">
<p><strong>Indeed, the Poisson PMF is a proper probability distribution!</strong></p>
</blockquote>
</div>
</section><section id="expected-value-3" class="level3" data-number="D.4.2"><h3 data-number="D.4.2" class="anchored" data-anchor-id="expected-value-3">
<span class="header-section-number">D.4.2</span> Expected Value</h3>
<p>Via <a href="#eq-app-expected-value-discrete" class="quarto-xref">Equation&nbsp;<span>C.3</span></a>, the expected value or mean of a Poisson-distributed random variable <span class="math inline">\(Y\)</span> can be found as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-proof-poisson-mean-1"><span class="math display">\[
\begin{align*}
\mathbb{E}(Y) &amp;= \sum_{y = 0}^{\infty} y P_Y \left( Y = y \mid \lambda \right) \\
&amp;= \sum_{y = 1}^{\infty} y P_Y \left( Y = y \mid \lambda \right) \\
&amp; \quad \qquad \text{for $y = 0$, the addend is equal to zero} \\
&amp;= \sum_{y = 1}^{\infty} y \left[ \frac{\lambda^y \exp{(-\lambda)}}{y!} \right] \\
&amp;= \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{y \lambda^y}{y!} \\
&amp; \quad \qquad \text{factoring out $\exp{(-\lambda)}$,} \\
&amp; \quad \qquad \text{since it does not depend on $y$} \\
&amp;= \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{y \lambda^y}{y (y - 1)!} \\
&amp; \quad \qquad \text{in the denominator, $y! = y (y - 1)!$}\\
&amp;= \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{\lambda^y}{(y - 1)!} \\
&amp;= \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{\lambda^{y + 1 - 1}}{(y - 1)!} \\
&amp; \quad \qquad \text{note $\lambda^y = \lambda^{y + 1 - 1}$} \\
&amp;= \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{\lambda \lambda^{y - 1}}{(y - 1)!} \\
&amp; \quad \qquad \text{rearranging terms} \\
&amp;= \lambda \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{\lambda^{y - 1}}{(y - 1)!} \\
&amp; \quad \qquad \text{factoring out $\lambda$,} \\
&amp; \quad \qquad \text{since it does not depend on $y$.}
\end{align*}
\tag{D.34}\]</span></span></p>
<p>Then, let us make the following variable rearrangement:</p>
<p><span class="math display">\[
z = y - 1.
\]</span></p>
<p>Going back to <a href="#eq-proof-poisson-mean-1" class="quarto-xref">Equation&nbsp;<span>D.34</span></a>, <strong>and applying our above variable rearrangement within the summation</strong>, we have:</p>
<p><span id="eq-proof-poisson-mean-2"><span class="math display">\[
\mathbb{E}(Y) = \lambda \exp{(-\lambda)} \sum_{z = 0}^{\infty} \frac{\lambda^z}{z!}
\tag{D.35}\]</span></span></p>
<p>Using <a href="#eq-maclaurin-series-base-e" class="quarto-xref">Equation&nbsp;<span>D.32</span></a>, let:</p>
<p><span class="math display">\[
\begin{gather*}
\lambda = u \\
z = j.
\end{gather*}
\]</span></p>
<p>Hence, we have the following:</p>
<p><span class="math display">\[
\sum_{z = 0}^{\infty} \frac{\lambda^z}{z!} = \exp{(\lambda)}.
\]</span></p>
<p>Finally, going back to <a href="#eq-proof-poisson-mean-2" class="quarto-xref">Equation&nbsp;<span>D.35</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}(Y) &amp;= \lambda \exp{(-\lambda)} \overbrace{\sum_{z = 0}^{\infty} \frac{\lambda^z}{z!}}^{\exp{(\lambda)}} \\
&amp;= \lambda \exp{(-\lambda)} \times \exp{(\lambda)} \\
&amp;= \lambda \exp{(-\lambda + \lambda)} \\
&amp;= \lambda \exp{(0)} \\
&amp;= \lambda. \qquad \qquad \qquad \qquad \square
\end{align*}
\]</span></p>
</div>
</section><section id="variance-3" class="level3" data-number="D.4.3"><h3 data-number="D.4.3" class="anchored" data-anchor-id="variance-3">
<span class="header-section-number">D.4.3</span> Variance</h3>
<p>Via <a href="#eq-app-variance" class="quarto-xref">Equation&nbsp;<span>C.5</span></a> and the <a href="#eq-app-expected-value-discrete" class="quarto-xref">Equation&nbsp;<span>C.3</span></a> of a discrete expected value, the variance of a Poisson-distributed random variable <span class="math inline">\(Y\)</span> can be found as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-proof-poisson-variance-1"><span class="math display">\[
\begin{align*}
\text{Var} (Y) &amp;= \mathbb{E} \left( Y^2 \right) - \left[ \mathbb{E}(Y)\right]^2 \\
&amp;= \mathbb{E} \left( Y^2 \right) - \lambda^2 \qquad \text{since $\mathbb{E}(Y) = \lambda$.}
\end{align*}
\tag{D.36}\]</span></span></p>
<p>Now, we need to play around with the below expected value expression as follows:</p>
<p><span id="eq-proof-poisson-variance-2"><span class="math display">\[
\begin{align*}
\mathbb{E} \left( Y^2 \right) &amp;= \mathbb{E} \left[ Y (Y - 1) \right] + \mathbb{E}(Y) \\
&amp;= \mathbb{E} \left[ Y (Y - 1) \right] + \lambda \qquad \text{since $\mathbb{E}(Y) = \lambda$.}
\end{align*}
\tag{D.37}\]</span></span></p>
<p>Now, to find <span class="math inline">\(\mathbb{E} \left[ Y (Y - 1) \right]\)</span>, we make the following derivation via the LOTUS from <a href="#eq-app-expected-value-discrete-function" class="quarto-xref">Equation&nbsp;<span>C.1</span></a> when <span class="math inline">\(g(Y) = y (y - 1)\)</span>:</p>
<p><span id="eq-proof-poisson-variance-3"><span class="math display">\[
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &amp;= \sum_{y = 0}^{\infty} y (y - 1) P_Y \left( Y = y \mid \lambda \right) \\
&amp;= \sum_{y = 2}^{\infty} y (y - 1) P_Y \left( Y = y \mid \lambda \right) \\
&amp; \quad \qquad \text{for $y = \{0, 1\}$,} \\
&amp; \quad \qquad \text{the addends are equal to zero} \\
&amp;= \sum_{y = 2}^{\infty} y (y - 1) \left[ \frac{\lambda^y \exp{(-\lambda)}}{y!} \right] \\
&amp;= \exp{(-\lambda)} \sum_{y = 2}^{\infty} \left[ \frac{y (y - 1) \lambda^y}{y!} \right] \\
&amp; \quad \qquad \text{factoring out $\exp{(-\lambda)}$,} \\
&amp; \quad \qquad \text{since it does not depend on $y$} \\
&amp;= \exp{(-\lambda)} \sum_{y = 2}^{\infty} \left[ \frac{y (y - 1) \lambda^y}{y (y - 1) (y - 2)!} \right] \\
&amp; \quad \qquad \text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\
&amp;= \exp{(-\lambda)} \sum_{y = 2}^{\infty} \frac{\lambda^y}{(y - 2)!} \\
&amp;= \exp{(-\lambda)} \sum_{y = 2}^{\infty} \frac{\lambda^{y + 2 - 2}}{(y - 2)!} \\
&amp; \quad \qquad \text{note $\lambda^y = \lambda^{y + 2 - 2} $} \\
&amp;= \lambda^2 \exp{(-\lambda)} \sum_{y = 2}^{\infty} \frac{\lambda^{y - 2}}{(y - 2)!} \\
&amp; \quad \qquad \text{factoring out $\lambda^2$,} \\
&amp; \quad \qquad \text{since it does not depend on $y$.} \\
\end{align*}
\tag{D.38}\]</span></span></p>
<p>Then, we make the following variable rearrangement:</p>
<p><span class="math display">\[
z = y - 2.
\]</span></p>
<p>Going back to <a href="#eq-proof-poisson-variance-3" class="quarto-xref">Equation&nbsp;<span>D.38</span></a>, <strong>and applying our above variable rearrangement within the summation</strong>, we have:</p>
<p><span id="eq-proof-poisson-variance-4"><span class="math display">\[
\mathbb{E} \left[ Y (Y - 1) \right] = \lambda^2 \exp{(-\lambda)} \sum_{z = 0}^{\infty} \frac{\lambda^z}{z!}.
\tag{D.39}\]</span></span></p>
<p>Using <a href="#eq-maclaurin-series-base-e" class="quarto-xref">Equation&nbsp;<span>D.32</span></a>, let:</p>
<p><span class="math display">\[
\begin{gather*}
\lambda = u \\
z = j.
\end{gather*}
\]</span></p>
<p>Thus, we have the following:</p>
<p><span class="math display">\[
\sum_{z = 0}^{\infty} \frac{\lambda^z}{z!} = \exp{(\lambda)}.
\]</span></p>
<p>Going back to <a href="#eq-proof-poisson-variance-4" class="quarto-xref">Equation&nbsp;<span>D.39</span></a>:</p>
<p><span id="eq-proof-poisson-variance-5"><span class="math display">\[
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &amp;= \lambda^2 \exp{(-\lambda)} \overbrace{\sum_{z = 0}^{\infty} \frac{\lambda^z}{z!}}^{\exp{(\lambda)}} \\
&amp;= \lambda^2 \exp{(-\lambda)} \times \exp{\lambda} \\
&amp;= \lambda^2 \exp{(-\lambda + \lambda)} \\
&amp;= \lambda^2 \exp{(0)} \\
&amp;= \lambda^2.
\end{align*}
\tag{D.40}\]</span></span></p>
<p>Let us retake <a href="#eq-proof-poisson-variance-2" class="quarto-xref">Equation&nbsp;<span>D.37</span></a> and plug in the above result:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E} \left( Y^2 \right) &amp;= \mathbb{E} \left[ Y (Y - 1) \right] + \lambda \\
&amp;= \lambda^2 + \lambda. \\
\end{align*}
\]</span></p>
<p>Finally, we plug in <span class="math inline">\(\mathbb{E} \left( Y^2 \right)\)</span> in <a href="#eq-proof-poisson-variance-1" class="quarto-xref">Equation&nbsp;<span>D.36</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
\text{Var} (Y) &amp;= \mathbb{E} \left( Y^2 \right) - \lambda^2 \\
&amp;= \lambda^2 + \lambda - \lambda^2 \\
&amp;= \lambda. \qquad \qquad \square
\end{align*}
\]</span></p>
</div>
</section></section><section id="sec-generalized-poisson-distribution" class="level2" data-number="D.5"><h2 data-number="D.5" class="anchored" data-anchor-id="sec-generalized-poisson-distribution">
<span class="header-section-number">D.5</span> Generalized Poisson</h2>
<p>The generalized Poisson (GP) distribution is viewed as the general Poisson case. It was introduced by <span class="citation" data-cites="consul1973">Consul and Jain (<a href="references.html#ref-consul1973" role="doc-biblioref">1973</a>)</span>. Suppose you observe the count of events happening in a fixed interval of time or space. Let <span class="math inline">\(Y\)</span> be the number of counts considered of integer type. Then, <span class="math inline">\(Y\)</span> is said to have a GP distribution with continuous parameters <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
Y \sim \text{GP}(\lambda, \theta).
\]</span></p>
<section id="probability-mass-function-4" class="level3" data-number="D.5.1"><h3 data-number="D.5.1" class="anchored" data-anchor-id="probability-mass-function-4">
<span class="header-section-number">D.5.1</span> Probability Mass Function</h3>
<p>The PMF of this count-type <span class="math inline">\(Y\)</span> is the following:</p>
<p><span id="eq-app-generalized-poisson-pmf"><span class="math display">\[
\begin{align*}
P_Y \left( Y = y \mid \lambda, \theta \right) &amp;= \frac{\lambda (\lambda + y \theta)^{y - 1} \exp{\left[ -(\lambda + y \theta) \right]}}{y!} \\
&amp; \qquad \qquad \qquad \text{for $y \in \{ 0, 1, 2, \dots\}$,}
\end{align*}
\tag{D.41}\]</span></span></p>
<p>where <span class="math inline">\(\exp{(\cdot)}\)</span> depicts the base <span class="math inline">\(e\)</span> (i.e., <strong>Euler’s number</strong>, <span class="math inline">\(e = 2.71828...\)</span>) and <span class="math inline">\(y!\)</span> is the factorial</p>
<p><span class="math display">\[
y! = y \times (y - 1) \times (y - 2) \times (y - 3) \times \cdots \times 3 \times 2 \times 1.  
\]</span></p>
<p>with</p>
<p><span class="math display">\[
0! = 1.
\]</span></p>
<p>The continuous parameter <span class="math inline">\(\lambda \in (0, \infty)\)</span> represents the average rate at which these events happen (i.e., events per area unit or events per time unit). As in the case of the classical Poisson case, even though the GP random variable <span class="math inline">\(Y\)</span> is considered discrete, <span class="math inline">\(\lambda\)</span> is modelled as continuous!</p>
<p>On the other hand, the continuous and bounded parameter <span class="math inline">\(\theta \in (-1, 1)\)</span> controls for dispersion present in the GP random variable Y as follows:</p>
<ol type="1">
<li><p>When <span class="math inline">\(0 &lt; \theta &lt; 1\)</span>, the GP <span class="math inline">\(Y\)</span> shows overdispersion which implies that <span class="math display">\[\text{Var}(Y) &gt; \mathbb{E}(Y).\]</span></p></li>
<li><p>When <span class="math inline">\(-1 &lt; \theta &lt; 0\)</span>, the GP <span class="math inline">\(Y\)</span> shows underdispersion which implies that <span class="math display">\[\text{Var}(Y) &lt; \mathbb{E}(Y).\]</span></p></li>
<li><p>When <span class="math inline">\(\theta = 0\)</span>, the PMF of the GP <span class="math inline">\(Y\)</span> in <a href="#eq-app-generalized-poisson-pmf" class="quarto-xref">Equation&nbsp;<span>D.41</span></a> becomes the classical Poisson PMF from <a href="#eq-app-classical-poisson-pmf" class="quarto-xref">Equation&nbsp;<span>D.27</span></a>: <span class="math display">\[
\begin{align*}
P_Y \left( Y = y \mid \lambda, \theta = 0 \right) &amp;= \frac{\lambda (\lambda + y \theta)^{y - 1} \exp{\left[ -(\lambda + y \theta) \right]}}{y!} \\
&amp;= \frac{\lambda (\lambda)^{y - 1} \exp{\left( -\lambda \right)}}{y!} \qquad \text{setting $\theta = 0$} \\
&amp;= \frac{\lambda^y \exp{\left( -\lambda \right)}}{y!} \\
&amp; \qquad \qquad \qquad \text{for $y \in \{ 0, 1, 2, \dots\}$.}
\end{align*}
\]</span></p></li>
</ol>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on equidispersion in a generalized Poisson random variable!</p>
</div>
<div class="Heads-up-container">
<p>In a GP-distributed <span class="math inline">\(Y\)</span>, when <span class="math inline">\(\theta = 0\)</span> in its corresponding PMF, we have equidispersion which implies <span class="math display">\[
\mathbb{E}(Y \mid \theta = 0) = \frac{\lambda}{1 - \theta} = \lambda
\]</span> <span class="math display">\[
\text{Var}(Y \mid \theta = 0) = \frac{\lambda}{(1 - \theta)^2} = \lambda
\]</span> <span class="math display">\[
\mathbb{E}(Y \mid \theta = 0) = \text{Var}(Y).
\]</span></p>
</div>
</div>
<blockquote class="blockquote">
<p><strong>How can we verify that <a href="#eq-app-generalized-poisson-pmf" class="quarto-xref">Equation&nbsp;<span>D.41</span></a> is a proper PMF (i.e., all the standalone probabilities over the support of <span class="math inline">\(Y\)</span> add up to one)?</strong></p>
</blockquote>
</section><section id="expected-value-4" class="level3" data-number="D.5.2"><h3 data-number="D.5.2" class="anchored" data-anchor-id="expected-value-4">
<span class="header-section-number">D.5.2</span> Expected Value</h3>
</section><section id="variance-4" class="level3" data-number="D.5.3"><h3 data-number="D.5.3" class="anchored" data-anchor-id="variance-4">
<span class="header-section-number">D.5.3</span> Variance</h3>
</section></section><section id="sec-zero-inflated-poisson-distribution" class="level2" data-number="D.6"><h2 data-number="D.6" class="anchored" data-anchor-id="sec-zero-inflated-poisson-distribution">
<span class="header-section-number">D.6</span> Zero-inflated Poisson</h2>
</section><section id="sec-multinomial-distribution" class="level2" data-number="D.7"><h2 data-number="D.7" class="anchored" data-anchor-id="sec-multinomial-distribution">
<span class="header-section-number">D.7</span> Multinomial</h2>
</section></section><section id="continuous-random-variables" class="level1" data-number="E"><h1 data-number="E">
<span class="header-section-number">E</span> Continuous Random Variables</h1>
<p>Let us recall what a continuous random variable is. This type of variable is defined to take on a set of uncountable possible values. In other words, these values belong to an infinite set. <a href="#fig-app-distributions" class="quarto-xref">Figure&nbsp;<span>C.1</span></a> delves into the following specific probability distributions:</p>
<ul>
<li>
<strong>Weibull.</strong> A random variable <span class="math inline">\(Y\)</span> with a support of <span class="math inline">\(y \in [0, \infty)\)</span>. It is a generalization of the <strong>Exponential distribution</strong> and is used in waiting time modelling for some event of interest to happen (i.e., <strong>survival times</strong>). Note its distributional parameters are the <strong>scale</strong> continuous parameter <span class="math inline">\(\beta \in (0, \infty)\)</span> and <strong>shape</strong> continuous parameter <span class="math inline">\(\gamma \in (0, \infty)\)</span>.</li>
<li>
<strong>Lognormal.</strong> A random variable <span class="math inline">\(Y\)</span> with a support of <span class="math inline">\(y \in [0, \infty)\)</span>, whose transformation <span class="math inline">\(\log{(Y)}\)</span> yields a Normal distribution. It is used in waiting time modelling for some event of interest to happen (i.e., <strong>survival times</strong>). Its distributional parameters are the <strong>Normal location</strong> continuous parameter <span class="math inline">\(\mu \in (-\infty, \infty)\)</span> and <strong>Normal scale</strong> continuous parameter <span class="math inline">\(\sigma^2 \in (0, \infty)\)</span>.</li>
<li>
<strong>Exponential.</strong> A random variable <span class="math inline">\(Y\)</span> with a support of <span class="math inline">\(y \in [0, \infty)\)</span>, which is also often used to model waiting times for some event of interest to happen (i.e., <strong>survival times</strong>). Its single distributional parameter can come in either one of the following forms:
<ul>
<li>As a <strong>rate</strong> <span class="math inline">\(\lambda \in (0, \infty)\)</span>, which generally defines the mean number of events of interest per time interval or space unit.</li>
<li>As a <strong>scale</strong> <span class="math inline">\(\beta \in (0, \infty)\)</span>, which generally defines the mean time until the next event of interest occurs.</li>
</ul>
</li>
<li>
<strong>Gamma</strong> A random variable <span class="math inline">\(Y\)</span> with a support of <span class="math inline">\(y \in [0, \infty)\)</span>. It is used in waiting time modelling for some event of interest to happen (i.e., <strong>survival times</strong>). Its distributional parameters are the <strong>shape</strong> continuous parameter <span class="math inline">\(\eta \in (0, \infty)\)</span> and <strong>scale</strong> continuous parameter <span class="math inline">\(\theta \in (0, \infty)\)</span>.</li>
<li>
<strong>Normal.</strong> A random variable <span class="math inline">\(Y\)</span> with a support of <span class="math inline">\(y \in (-\infty, \infty)\)</span>. It is well-known for its <strong>bell shape</strong>. Note its distributional parameters are the <strong>location</strong> continuous parameter <span class="math inline">\(\mu \in (-\infty, \infty)\)</span> and <strong>scale</strong> continuous parameter <span class="math inline">\(\sigma^2 \in (0, \infty)\)</span>.</li>
</ul>
<div id="tbl-distributions-continuous" class="striped hover quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure"><figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-distributions-continuous-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;E.1: Univariate continuous probability distributions for a random variable <span class="math inline">\(Y\)</span>; including parameter(s), support, mean, and variance.
</figcaption><div aria-describedby="tbl-distributions-continuous-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table-striped table-hover caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 24%">
<col style="width: 24%">
<col style="width: 24%">
</colgroup>
<thead><tr class="header">
<th style="text-align: center;">Distribution and <br> Parametrization</th>
<th style="text-align: center;">Support</th>
<th style="text-align: center;">Mean</th>
<th style="text-align: center;">Variance</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">
<strong>Weibull</strong> as in <br><span class="math inline">\(Y \sim \text{Weibull}(\beta, \gamma)\)</span> with scale <br><span class="math inline">\(\beta \in (0, \infty)\)</span> <br> and shape <br><span class="math inline">\(\gamma \in (0, \infty)\)</span>
</td>
<td style="text-align: center;"><span class="math display">\[y \in [0, \infty)\]</span></td>
<td style="text-align: center;"><span class="math display">\[\beta \Gamma \left( \frac{1}{\gamma} + 1 \right)\]</span></td>
<td style="text-align: center;">
<span class="math display">\[\beta^2 \Bigg[ \Gamma \left( \frac{2}{\gamma} + 1 \right) - \]</span> <span class="math display">\[ \quad \Gamma^2 \left( \frac{1}{\gamma} + 1 \right) \Bigg]\]</span>
</td>
</tr>
<tr class="even">
<td style="text-align: center;">
<strong>Lognormal</strong> as in <br><span class="math inline">\(Y \sim \text{Lognormal}(\mu, \sigma^2)\)</span> with Normal location <br><span class="math inline">\(\mu \in (-\infty, \infty)\)</span> <br> and Normal scale <br><span class="math inline">\(\sigma^2 \in (0, \infty)\)</span>
</td>
<td style="text-align: center;"><span class="math display">\[y \in [0, \infty)\]</span></td>
<td style="text-align: center;"><span class="math display">\[\exp{\left[ \mu + \left( \frac{\sigma^2}{2} \right) \right]}\]</span></td>
<td style="text-align: center;">
<span class="math display">\[\exp{\left[ 2 \left( \mu + \sigma^2 \right) \right]} - \]</span> <span class="math display">\[ \quad \exp{\left( 2 \mu + \sigma^2 \right)}\]</span>
</td>
</tr>
<tr class="odd">
<td style="text-align: center;">
<strong>Exponential</strong> as in <br><span class="math inline">\(Y \sim \text{Exponential}(\lambda)\)</span> with rate <br><span class="math inline">\(\lambda \in (0, \infty)\)</span> <br> or <span class="math inline">\(Y \sim \text{Exponential}(\beta)\)</span> with scale <br><span class="math inline">\(\beta \in (0, \infty)\)</span>
</td>
<td style="text-align: center;"><span class="math display">\[y \in [0, \infty)\]</span></td>
<td style="text-align: center;">
<span class="math inline">\(\frac{1}{\lambda}\)</span> <br> for rate <br> parametrization <br> or <br><span class="math inline">\(\beta\)</span> <br> for scale <br> parametrization</td>
<td style="text-align: center;">
<span class="math inline">\(\frac{1}{\lambda^2}\)</span> <br> for rate <br> parametrization <br> or <br><span class="math inline">\(\beta^2\)</span> <br> for scale <br> parametrization</td>
</tr>
<tr class="even">
<td style="text-align: center;">
<strong>Gamma</strong> as in <br><span class="math inline">\(Y \sim \text{Gamma}(\eta, \theta)\)</span> with shape <br><span class="math inline">\(\eta \in (0, \infty)\)</span> <br> and scale <br><span class="math inline">\(\theta \in (0, \infty)\)</span>
</td>
<td style="text-align: center;"><span class="math display">\[y \in [0, \infty)\]</span></td>
<td style="text-align: center;"><span class="math display">\[\eta \theta\]</span></td>
<td style="text-align: center;"><span class="math display">\[\eta \theta^2\]</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">
<strong>Normal</strong> as in <br><span class="math inline">\(Y \sim \text{Normal}(\mu, \sigma^2)\)</span> with location <br><span class="math inline">\(\mu \in (-\infty, \infty)\)</span> <br> and scale <br><span class="math inline">\(\sigma^2 \in (0, \infty)\)</span>
</td>
<td style="text-align: center;"><span class="math display">\[y \in (-\infty, \infty)\]</span></td>
<td style="text-align: center;"><span class="math display">\[\mu\]</span></td>
<td style="text-align: center;"><span class="math display">\[\sigma^2\]</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<section id="sec-weibull-distribution" class="level2" data-number="E.1"><h2 data-number="E.1" class="anchored" data-anchor-id="sec-weibull-distribution">
<span class="header-section-number">E.1</span> Weibull</h2>
<p>Suppose you observe the waiting times for some event of interest to happen (i.e., <strong>survival times</strong>). Let random variable <span class="math inline">\(Y\)</span> be considered continuous and nonnegative. Then, <span class="math inline">\(Y\)</span> is said to have a <strong>Weibull distribution</strong> with the following <strong>scale</strong> continuous parameter <span class="math inline">\(\beta\)</span> and <strong>shape</strong> continuous parameter <span class="math inline">\(\gamma\)</span>:</p>
<p><span class="math display">\[
Y \sim \text{Weibull}(\beta, \gamma).
\]</span></p>
<section id="probability-density-function" class="level3" data-number="E.1.1"><h3 data-number="E.1.1" class="anchored" data-anchor-id="probability-density-function">
<span class="header-section-number">E.1.1</span> Probability Density Function</h3>
<p>The PDF of <span class="math inline">\(Y\)</span> is the following:</p>
<p><span id="eq-app-weibull-pdf"><span class="math display">\[
f_Y \left(y \mid \beta, \gamma \right) = \frac{\gamma}{\beta} \left( \frac{y}{\beta} \right)^{\gamma - 1} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \quad \text{for $y \in [0, \infty )$.}
\tag{E.1}\]</span></span></p>
<p>Parameters <span class="math inline">\(\beta \in (0, \infty)\)</span> and <span class="math inline">\(\gamma \in (0, \infty)\)</span> refer to the random process’ scale and shape, respectively. <a href="#fig-weibull-family" class="quarto-xref">Figure&nbsp;<span>E.1</span></a> shows nine members of the Weibull parametric family, i.e., nine different PDFs with all possible pairwise combinations for three different scale parameters <span class="math inline">\(\beta = 0.5, 1, 2\)</span> and shape parameters <span class="math inline">\(\gamma = 1.5, 3, 6\)</span>. We can highlight the following:</p>
<ul>
<li>Regardless of the shape parameter <span class="math inline">\(\gamma\)</span>, <strong>as we increase the scale parameter <span class="math inline">\(\beta\)</span></strong>, note that there is more spread in the corresponding distributions.</li>
<li>Regardless of the scale parameter <span class="math inline">\(\beta\)</span>, <strong>as we increase the shape parameter <span class="math inline">\(\gamma\)</span></strong>, note the peak of the distribution moves more to the right.</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div id="fig-weibull-family" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-weibull-family-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="C-distributional-mind-map_files/figure-html/fig-weibull-family-1.png" class="img-fluid figure-img" width="1344">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-weibull-family-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;E.1: Some members of the Weibull parametric family.
</figcaption></figure>
</div>
</div>
</div>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on the Weibull and Exponential distributions in survival analysis!</p>
</div>
<div class="Heads-up-container">
<p>The Weibull distribution extends its Exponential counterpart (as in <a href="#sec-exponential-distribution" class="quarto-xref"><span>Section E.3</span></a>) by allowing the event rate (or hazard) to change over time, rather than staying constant. This makes it especially useful in survival analysis and reliability studies, where capturing how the risk of an event evolves is critical.</p>
<p>As a side note, the Weibull and Exponential PDFs are mathematically related. When <span class="math inline">\(\gamma = 1\)</span> in <a href="#eq-app-weibull-pdf" class="quarto-xref">Equation&nbsp;<span>E.1</span></a>, the Weibull PDF is equal to the Exponential PDF under the <strong>scale parametrization</strong> as in <a href="#eq-app-exponential-pdf-scale" class="quarto-xref">Equation&nbsp;<span>E.11</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
f_Y \left(y \mid \beta, \gamma = 1 \right) &amp;= \frac{\gamma}{\beta} \left( \frac{y}{\beta} \right)^{\gamma - 1} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \\
&amp;= \frac{1}{\beta} \underbrace{\left( \frac{y}{\beta} \right)^0}_{1} \exp{\left( -\frac{y}{\beta} \right)} \\
&amp;= \frac{1}{\beta} \exp{\left( -\frac{y}{\beta} \right)} \quad \text{for $y \in [0, \infty )$}.
\end{align*}
\]</span></p>
</div>
</div>
<blockquote class="blockquote">
<p><strong>How can we verify that <a href="#eq-app-weibull-pdf" class="quarto-xref">Equation&nbsp;<span>E.1</span></a> is a proper PDF (i.e., <a href="#eq-app-weibull-pdf" class="quarto-xref">Equation&nbsp;<span>E.1</span></a> integrates to one over the support of <span class="math inline">\(Y\)</span>)?</strong></p>
</blockquote>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-proof-weibull-PMF-adds-to-1-1"><span class="math display">\[
\begin{align*}
\int_{y = 0}^{y = \infty} f_Y \left(y \mid \beta, \gamma \right) \mathrm{d}y &amp;= \int_{y = 0}^{y = \infty} \frac{\gamma}{\beta} \left( \frac{y}{\beta} \right)^{\gamma - 1} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y.
\end{align*}
\tag{E.2}\]</span></span></p>
<p>Now, let us make the variable substitution:</p>
<p><span class="math display">\[
\begin{align*}
u &amp;= \left( \frac{y}{\beta} \right)^{\gamma} \\
y &amp;= \beta u^{\frac{1}{\gamma}} \qquad \Rightarrow \qquad \mathrm{d}y = \frac{\beta}{\gamma} u^{\frac{1}{\gamma} - 1} \mathrm{d}u.
\end{align*}
\]</span></p>
<p>The above rearrangement yields the following in <a href="#eq-proof-weibull-PMF-adds-to-1-1" class="quarto-xref">Equation&nbsp;<span>E.2</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
\int_{y = 0}^{y = \infty} f_Y \left(y \mid \beta, \gamma \right) \mathrm{d}y &amp;= \int_{u = 0}^{u = \infty} \frac{\gamma}{\beta} \left( \frac{\beta u^{\frac{1}{\gamma}}}{\beta} \right)^{\gamma - 1} \exp{\left( -u \right)} \frac{\beta}{\gamma} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&amp;= \int_{u = 0}^{u = \infty} \left( u^{\frac{1}{\gamma}} \right)^{\gamma - 1} \exp{\left( -u \right)} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&amp;= \int_{u = 0}^{u = \infty} u^{\frac{\gamma - 1}{\gamma}} \exp{\left( -u \right)} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&amp;= \int_{u = 0}^{u = \infty} u^{\frac{\gamma - 1}{\gamma} + \frac{1}{\gamma} - 1} \exp{\left( -u \right)} \mathrm{d}u \\
&amp;= \int_{u = 0}^{u = \infty} u^{1 - \frac{1}{\gamma} + \frac{1}{\gamma} - 1} \exp{\left( -u \right)} \mathrm{d}u \\
&amp;= \int_{u = 0}^{u = \infty} u^0 \exp{\left( -u \right)} \mathrm{d}u \\
&amp;= \int_{u = 0}^{u = \infty} \exp{\left( -u \right)} \mathrm{d}u \\
&amp;= -\exp{\left( -u \right)} \Bigg|_{u = 0}^{u = \infty} \\
&amp;= - \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \\
&amp;= - \left( 0 - 1 \right) \\
&amp;= 1. \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \square
\end{align*}
\]</span></p>
<blockquote class="blockquote">
<p><strong>Indeed, the Weibull PDF is a proper probability distribution!</strong></p>
</blockquote>
</div>
</section><section id="expected-value-5" class="level3" data-number="E.1.2"><h3 data-number="E.1.2" class="anchored" data-anchor-id="expected-value-5">
<span class="header-section-number">E.1.2</span> Expected Value</h3>
<p>Via <a href="#eq-app-expected-value-continuous" class="quarto-xref">Equation&nbsp;<span>C.4</span></a>, the expected value or mean of a Weibull-distributed random variable <span class="math inline">\(Y\)</span> can be found as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-proof-weibull-mean-1"><span class="math display">\[
\begin{align*}
\mathbb{E}(Y) &amp;= \int_{y = 0}^{y = \infty} y f_Y \left(y \mid \beta, \gamma \right) \mathrm{d}y \\
&amp;= \int_{y = 0}^{y = \infty} y \frac{\gamma}{\beta} \left( \frac{y}{\beta} \right)^{\gamma - 1} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y \\
&amp;= \frac{\gamma}{\beta} \int_{y = 0}^{y = \infty} y \left( \frac{y}{\beta} \right)^{\gamma - 1} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y \\
&amp;= \frac{\gamma}{\beta} \int_{y = 0}^{y = \infty} y \frac{y^{\gamma - 1}}{\beta^{\gamma - 1}} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y \\
&amp;= \frac{\gamma}{\beta} \int_{y = 0}^{y = \infty} \frac{y^{\gamma}}{\beta^{\gamma - 1}} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y \\
&amp;= \frac{\gamma}{\beta^{\gamma}} \int_{y = 0}^{y = \infty} y^{\gamma} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y.
\end{align*}
\tag{E.3}\]</span></span></p>
<p>Then, we make the following variable substitution:</p>
<p><span class="math display">\[
\begin{align*}
u &amp;= \left( \frac{y}{\beta} \right)^{\gamma} \\
y &amp;= \beta u^{\frac{1}{\gamma}} \qquad \Rightarrow \qquad \mathrm{d}y = \frac{\beta}{\gamma} u^{\frac{1}{\gamma} - 1} \mathrm{d}u.
\end{align*}
\]</span></p>
<p>The above rearrangement yields the following in <a href="#eq-proof-weibull-mean-1" class="quarto-xref">Equation&nbsp;<span>E.3</span></a>:</p>
<p><span id="eq-proof-weibull-mean-2"><span class="math display">\[
\begin{align*}
\mathbb{E}(Y) &amp;= \frac{\gamma}{\beta^{\gamma}} \int_{u = 0}^{u = \infty} \left( \beta u^{\frac{1}{\gamma}} \right)^{\gamma} \exp{\left( -u \right)} \frac{\beta}{\gamma} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&amp;= \frac{\gamma}{\beta^{\gamma}} \int_{u = 0}^{u = \infty} \beta^{\gamma} u \exp{\left( -u \right)} \frac{\beta}{\gamma} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&amp;= \frac{\gamma \beta^{\gamma} \beta}{\beta^{\gamma} \gamma} \int_{u = 0}^{u = \infty} u \exp{\left( -u \right)} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&amp;= \beta \int_{u = 0}^{u = \infty} u^{\frac{1}{\gamma}} \exp{\left( -u \right)} \mathrm{d}u \\
&amp;= \beta \int_{u = 0}^{u = \infty} u^{\left( \frac{1}{\gamma} + 1 \right) - 1} \exp{\left( -u \right)} \mathrm{d}u \\
&amp; \quad \qquad \text{note $\frac{1}{\gamma} = \left( \frac{1}{\gamma} + 1 \right) - 1$.}
\end{align*}
\tag{E.4}\]</span></span></p>
<p>The integral on the right-hand side of <a href="#eq-proof-weibull-mean-2" class="quarto-xref">Equation&nbsp;<span>E.4</span></a> corresponds to the so-called <strong>Gamma function</strong> as described below.</p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on the Gamma function!</p>
</div>
<div class="Heads-up-container">
<p>The Gamma function is a mathematical generalization of the <strong>factorial function</strong>, but applied to non-integer numbers. In many different probability distributions, this function appears as a <strong>normalizing constant</strong>. Moreover, it also appears as part of the expressions of expected values and variances.</p>
<p>That said, for a variable <span class="math inline">\(z\)</span> in general, we can represent the Gamma function via the following integral:</p>
<p><span id="eq-app-gamma-function"><span class="math display">\[
\Gamma(z) = \int_{t = 0}^{t = \infty} t^{z - 1} \exp{\left( -t \right)} \mathrm{d}t.
\tag{E.5}\]</span></span></p>
<p><span class="citation" data-cites="weisstein2">Weisstein (<a href="references.html#ref-weisstein2" role="doc-biblioref">n.d.a</a>)</span> provides further insights on this Gamma function along with some useful properties.</p>
</div>
</div>
<p>Thus, via the Gamma function from <a href="#eq-app-gamma-function" class="quarto-xref">Equation&nbsp;<span>E.5</span></a>, we set the following:</p>
<p><span class="math display">\[
\begin{gather*}
t = u \\
z = \frac{1}{\gamma} + 1,
\end{gather*}
\]</span></p>
<p>which yields</p>
<p><span class="math display">\[
\Gamma \left( \frac{1}{\gamma} + 1 \right) = \int_{u = 0}^{u = \infty} u^{\left( \frac{1}{\gamma} + 1 \right) - 1} \exp{\left( -u \right)} \mathrm{d}u.
\]</span></p>
<p>Moving along with <a href="#eq-proof-weibull-mean-2" class="quarto-xref">Equation&nbsp;<span>E.4</span></a>, we have:</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}(Y) &amp;= \beta \int_{u = 0}^{u = \infty} u^{\left( \frac{1}{\gamma} + 1 \right) - 1} \exp{\left( -u \right)} \mathrm{d}u \\
&amp;= \beta \Gamma \left( \frac{1}{\gamma} + 1 \right). \qquad \qquad \qquad \quad \square
\end{align*}
\]</span></p>
</div>
</section><section id="variance-5" class="level3" data-number="E.1.3"><h3 data-number="E.1.3" class="anchored" data-anchor-id="variance-5">
<span class="header-section-number">E.1.3</span> Variance</h3>
<p>Via <a href="#eq-app-variance" class="quarto-xref">Equation&nbsp;<span>C.5</span></a> and the <a href="#eq-app-expected-value-continuous" class="quarto-xref">Equation&nbsp;<span>C.4</span></a> of a continuous expected value, the variance of a Weibull-distributed random variable <span class="math inline">\(Y\)</span> can be found as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-proof-weibull-variance-1"><span class="math display">\[
\begin{align*}
\text{Var} (Y) &amp;= \mathbb{E} \left( Y^2 \right) - \left[ \mathbb{E}(Y)\right]^2 \\
&amp;= \mathbb{E} \left( Y^2 \right) - \beta^2 \Gamma^2 \left( \frac{1}{\gamma} + 1 \right) \\
&amp; \quad \qquad \text{since $\mathbb{E}(Y) = \beta \Gamma \left( \frac{1}{\gamma} + 1 \right)$}.
\end{align*}
\tag{E.6}\]</span></span></p>
<p>Now, we need to find <span class="math inline">\(\mathbb{E} \left( Y^2 \right)\)</span> from <a href="#eq-proof-weibull-variance-1" class="quarto-xref">Equation&nbsp;<span>E.6</span></a>. Thus, we make the following derivation via the LOTUS from <a href="#eq-app-expected-value-continuous-function" class="quarto-xref">Equation&nbsp;<span>C.2</span></a> when <span class="math inline">\(g(Y) = y^2\)</span>:</p>
<p><span id="eq-proof-weibull-variance-2"><span class="math display">\[
\begin{align*}
\mathbb{E} \left( Y^2 \right) &amp;= \int_{y = 0}^{y = \infty} y^2 f_Y \left(y \mid \beta, \gamma \right) \mathrm{d}y \\
&amp;= \int_{y = 0}^{y = \infty} y^2 \frac{\gamma}{\beta} \left( \frac{y}{\beta} \right)^{\gamma - 1} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y \\
&amp;= \frac{\gamma}{\beta} \int_{y = 0}^{y = \infty} y^2 \left( \frac{y}{\beta} \right)^{\gamma - 1} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y \\
&amp;= \frac{\gamma}{\beta} \int_{y = 0}^{y = \infty} y^2 \frac{y^{\gamma - 1}}{\beta^{\gamma - 1}} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y \\
&amp;= \frac{\gamma}{\beta^{\gamma}} \int_{y = 0}^{y = \infty} y^{\gamma + 1} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y.
\end{align*}
\tag{E.7}\]</span></span></p>
<p>Then, we make the following variable substitution:</p>
<p><span class="math display">\[
\begin{align*}
u &amp;= \left( \frac{y}{\beta} \right)^{\gamma} \\
y &amp;= \beta u^{\frac{1}{\gamma}} \qquad \Rightarrow \qquad \mathrm{d}y = \frac{\beta}{\gamma} u^{\frac{1}{\gamma} - 1} \mathrm{d}u.
\end{align*}
\]</span></p>
<p>The above rearrangement yields the following in <a href="#eq-proof-weibull-variance-2" class="quarto-xref">Equation&nbsp;<span>E.7</span></a>:</p>
<p><span id="eq-proof-weibull-variance-3"><span class="math display">\[
\begin{align*}
\mathbb{E} \left( Y^2 \right) &amp;= \frac{\gamma}{\beta^{\gamma}} \int_{u = 0}^{u = \infty} \left( \beta u^{\frac{1}{\gamma}} \right)^{\gamma + 1} \exp{\left( -u \right)} \frac{\beta}{\gamma} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&amp;= \frac{\gamma}{\beta^{\gamma}} \int_{u = 0}^{u = \infty} \beta^{\gamma + 1} u^{1 + \frac{1}{\gamma}} \exp{\left( -u \right)} \frac{\beta}{\gamma} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&amp;= \frac{\gamma \beta^{\gamma + 1} \beta}{\beta^{\gamma} \gamma} \int_{u = 0}^{u = \infty} u^{1 + \frac{1}{\gamma}} \exp{\left( -u \right)} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&amp;= \beta^2 \int_{u = 0}^{u = \infty} u^{1 + \frac{1}{\gamma} + \frac{1}{\gamma} - 1} \exp{\left( -u \right)} \mathrm{d}u \\
&amp;= \beta^2 \int_{u = 0}^{u = \infty} u^{\left( \frac{2}{\gamma} + 1 \right) - 1} \exp{\left( -u \right)} \mathrm{d}u.
\end{align*}
\tag{E.8}\]</span></span></p>
<p>Hence, via the Gamma function from <a href="#eq-app-gamma-function" class="quarto-xref">Equation&nbsp;<span>E.5</span></a>, we set the following:</p>
<p><span class="math display">\[
\begin{gather*}
t = u \\
z = \frac{2}{\gamma} + 1,
\end{gather*}
\]</span></p>
<p>which yields</p>
<p><span class="math display">\[
\Gamma \left( \frac{2}{\gamma} + 1 \right) = \int_{u = 0}^{u = \infty} u^{\left( \frac{2}{\gamma} + 1 \right) - 1} \exp{\left( -u \right)} \mathrm{d}u.
\]</span></p>
<p>Moving along with <a href="#eq-proof-weibull-variance-3" class="quarto-xref">Equation&nbsp;<span>E.8</span></a>, we have:</p>
<p><span id="eq-proof-weibull-variance-4"><span class="math display">\[
\begin{align*}
\mathbb{E} \left( Y^2 \right) &amp;= \beta^2 \int_{u = 0}^{u = \infty} u^{\left( \frac{2}{\gamma} + 1 \right) - 1} \exp{\left( -u \right)} \mathrm{d}u \\
&amp;= \beta^2 \Gamma \left( \frac{2}{\gamma} + 1 \right).
\end{align*}
\tag{E.9}\]</span></span></p>
<p>Finally, we plug <a href="#eq-proof-weibull-variance-4" class="quarto-xref">Equation&nbsp;<span>E.9</span></a> into <a href="#eq-proof-weibull-variance-1" class="quarto-xref">Equation&nbsp;<span>E.6</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
\text{Var} (Y) &amp;= \mathbb{E}\left( Y^2 \right) - \beta^2 \Gamma^2 \left( \frac{1}{\gamma} + 1 \right) \\
&amp;= \beta^2 \Gamma \left( \frac{2}{\gamma} + 1 \right) - \beta^2 \Gamma^2 \left( \frac{1}{\gamma} + 1 \right) \\
&amp;= \beta^2 \left[  \Gamma \left( \frac{2}{\gamma} + 1 \right) - \Gamma^2 \left( \frac{1}{\gamma} + 1 \right) \right]. \qquad \qquad \square
\end{align*}
\]</span></p>
</div>
</section></section><section id="sec-lognormal-distribution" class="level2" data-number="E.2"><h2 data-number="E.2" class="anchored" data-anchor-id="sec-lognormal-distribution">
<span class="header-section-number">E.2</span> Lognormal</h2>
</section><section id="sec-exponential-distribution" class="level2" data-number="E.3"><h2 data-number="E.3" class="anchored" data-anchor-id="sec-exponential-distribution">
<span class="header-section-number">E.3</span> Exponential</h2>
<p>Suppose you observe the waiting times for some event of interest to happen (i.e., <strong>survival times</strong>). Let random variable <span class="math inline">\(Y\)</span> be considered continuous and nonnegative. Then, <span class="math inline">\(Y\)</span> is said to have an <strong>Exponential distribution</strong> with the following <strong>rate</strong> continuous parameter <span class="math inline">\(\lambda\)</span>:</p>
<p><span class="math display">\[
Y \sim \text{Exponential}(\lambda).
\]</span></p>
<p>We can also model <span class="math inline">\(Y\)</span> with the following <strong>scale</strong> continuous parameter <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
Y \sim \text{Exponential}(\beta).
\]</span></p>
<section id="probability-density-functions" class="level3" data-number="E.3.1"><h3 data-number="E.3.1" class="anchored" data-anchor-id="probability-density-functions">
<span class="header-section-number">E.3.1</span> Probability Density Functions</h3>
<p>Given the two above parametrizations of the Exponential distribution, there are two possible PDFs as discussed below.</p>
<section id="rate-parametrization" class="level4"><h4 class="anchored" data-anchor-id="rate-parametrization">Rate Parametrization</h4>
<p>The PDF of <span class="math inline">\(Y\)</span> is the following:</p>
<p><span id="eq-app-exponential-pdf-rate"><span class="math display">\[
f_Y \left(y \mid \lambda \right) = \lambda \exp \left( -\lambda y \right) \quad \text{for $y \in [0, \infty )$.}
\tag{E.10}\]</span></span></p>
<p>Parameter <span class="math inline">\(\lambda \in (0, \infty)\)</span> refers to the random process’ rate. <a href="#fig-exponential-family-rate" class="quarto-xref">Figure&nbsp;<span>E.2</span></a> shows three members of the Exponential parametric family, i.e., three different PDFs with different rate parameters <span class="math inline">\(\lambda = 0.25, 0.5, 1\)</span>. <strong>As we increase the rate parameter</strong>, note that smaller observed values <span class="math inline">\(y\)</span> get more probable.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-exponential-family-rate" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-exponential-family-rate-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="C-distributional-mind-map_files/figure-html/fig-exponential-family-rate-1.png" class="img-fluid figure-img" width="1344">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-exponential-family-rate-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;E.2: Some members of the Exponential parametric family with rate parametrization.
</figcaption></figure>
</div>
</div>
</div>
<blockquote class="blockquote">
<p><strong>How can we verify that <a href="#eq-app-exponential-pdf-rate" class="quarto-xref">Equation&nbsp;<span>E.10</span></a> is a proper PDF (i.e., <a href="#eq-app-exponential-pdf-rate" class="quarto-xref">Equation&nbsp;<span>E.10</span></a> integrates to one over the support of <span class="math inline">\(Y\)</span>)?</strong></p>
</blockquote>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{align*}
\int_{y = 0}^{y = \infty} f_Y \left(y \mid \lambda \right) \mathrm{d}y &amp;= \int_{y = 0}^{y = \infty} \lambda \exp \left( -\lambda y \right) \mathrm{d}y \\
&amp;= \lambda \int_{y = 0}^{y = \infty} \exp \left( -\lambda y \right) \mathrm{d}y \\
&amp;= - \frac{\lambda}{\lambda} \exp \left( -\lambda y \right) \Bigg|_{y = 0}^{y = \infty} \\
&amp;= - \exp \left( -\lambda y \right) \Bigg|_{y = 0}^{y = \infty} \\
&amp;= - \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \\
&amp;= - \left( 0 - 1 \right) \\
&amp;= 1. \qquad \qquad \qquad \qquad \quad \square
\end{align*}
\]</span></p>
<blockquote class="blockquote">
<p><strong>Indeed, the Exponential PDF, under a rate parametrization, is a proper probability distribution!</strong></p>
</blockquote>
</div>
</section><section id="scale-parametrization" class="level4"><h4 class="anchored" data-anchor-id="scale-parametrization">Scale Parametrization</h4>
<p>The PDF of <span class="math inline">\(Y\)</span> is the following:</p>
<p><span id="eq-app-exponential-pdf-scale"><span class="math display">\[
f_Y \left(y \mid \beta \right) = \frac{1}{\beta} \exp \left( -\frac{y}{\beta} \right) \quad \text{for $y \in [0, \infty )$.}
\tag{E.11}\]</span></span></p>
<p>Parameter <span class="math inline">\(\beta \in (0, \infty)\)</span> refers to the random process’ scale. <a href="#fig-exponential-family-scale" class="quarto-xref">Figure&nbsp;<span>E.3</span></a> shows three members of the Exponential parametric family, i.e., three different PDFs with different scale parameters <span class="math inline">\(\beta = 0.25, 0.5, 1\)</span>. <strong>As we increase the scale parameter</strong>, note that larger observed values <span class="math inline">\(y\)</span> get more probable.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-exponential-family-scale" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-exponential-family-scale-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="C-distributional-mind-map_files/figure-html/fig-exponential-family-scale-1.png" class="img-fluid figure-img" width="1344">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-exponential-family-scale-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;E.3: Some members of the Exponential parametric family with scale parametrization.
</figcaption></figure>
</div>
</div>
</div>
<blockquote class="blockquote">
<p><strong>How can we verify that <a href="#eq-app-exponential-pdf-scale" class="quarto-xref">Equation&nbsp;<span>E.11</span></a> is a proper PDF (i.e., <a href="#eq-app-exponential-pdf-scale" class="quarto-xref">Equation&nbsp;<span>E.11</span></a> integrates to one over the support of <span class="math inline">\(Y\)</span>)?</strong></p>
</blockquote>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{align*}
\int_{y = 0}^{y = \infty} f_Y \left(y \mid \beta \right) \mathrm{d}y &amp;= \int_{y = 0}^{y = \infty} \frac{1}{\beta} \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \\
&amp;= \frac{1}{\beta} \int_{y = 0}^{y = \infty} \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \\
&amp;= - \frac{\beta}{\beta} \exp \left( -\frac{y}{\beta} \right) \Bigg|_{y = 0}^{y = \infty} \\
&amp;= - \exp \left( -\frac{y}{\beta} \right) \Bigg|_{y = 0}^{y = \infty} \\
&amp;= - \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \\
&amp;= - \left( 0 - 1 \right) \\
&amp;= 1. \qquad \qquad \qquad \qquad \quad \square
\end{align*}
\]</span></p>
<blockquote class="blockquote">
<p><strong>Indeed, the Exponential PDF, under a scale parametrization, is a proper probability distribution!</strong></p>
</blockquote>
</div>
</section></section><section id="expected-value-6" class="level3" data-number="E.3.2"><h3 data-number="E.3.2" class="anchored" data-anchor-id="expected-value-6">
<span class="header-section-number">E.3.2</span> Expected Value</h3>
<p>Again, given the two above parametrizations of the Exponential distribution, there are two possible mathematical expressions for the expected value as discussed below.</p>
<section id="rate-parametrization-1" class="level4"><h4 class="anchored" data-anchor-id="rate-parametrization-1">Rate Parametrization</h4>
<p>Via <a href="#eq-app-expected-value-continuous" class="quarto-xref">Equation&nbsp;<span>C.4</span></a>, the expected value or mean of an Exponential-distributed random variable <span class="math inline">\(Y\)</span> can be found as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-app-exponential-rate-mean"><span class="math display">\[
\begin{align*}
\mathbb{E}(Y) &amp;= \int_{y = 0}^{y = \infty} y f_Y \left(y \mid \lambda \right) \mathrm{d}y \\
&amp;= \int_{y = 0}^{y = \infty} y \lambda \exp \left( -\lambda y \right) \mathrm{d}y \\
&amp;= \lambda \int_{y = 0}^{y = \infty} y \exp \left( -\lambda y \right) \mathrm{d}y. \\
\end{align*}
\tag{E.12}\]</span></span></p>
<p><a href="#eq-app-exponential-rate-mean" class="quarto-xref">Equation&nbsp;<span>E.12</span></a> cannot be solved straightforwardly, we need to use <strong>integration by parts</strong> as follows:</p>
<p><span class="math display">\[
\begin{align*}
u &amp;= y &amp; &amp;\Rightarrow &amp; \mathrm{d}u &amp;= \mathrm{d}y \\
\mathrm{d}v &amp;= \exp \left( -\lambda y \right) \mathrm{d}y &amp; &amp;\Rightarrow &amp; v &amp;= -\frac{1}{\lambda} \exp \left( -\lambda y \right),
\end{align*}
\]</span></p>
<p>which yields</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}(Y) &amp;= \lambda \left[ u v \Bigg|_{y = 0}^{y = \infty} - \int_{y = 0}^{y = \infty} v \mathrm{d}u \right] \\
&amp;= \lambda \left\{ \left[ -\frac{1}{\lambda} y \exp(-\lambda y) \right] \Bigg|_{y = 0}^{y = \infty} + \frac{1}{\lambda} \int_{y = 0}^{y = \infty} \exp{\left( -\lambda y \right)} \mathrm{d}y \right\} \\
&amp;= \lambda \Bigg\{ -\frac{1}{\lambda} \Bigg[ \underbrace{\infty \times \exp(-\infty)}_{0} - \underbrace{0 \times \exp(0)}_{0} \Bigg] - \\
&amp; \qquad \frac{1}{\lambda^2} \exp{\left( -\lambda y \right)} \Bigg|_{y = 0}^{y = \infty} \Bigg\} \\
&amp;= \lambda \left\{ -\frac{1}{\lambda} (0) - \frac{1}{\lambda^2} \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \right\} \\
&amp;= \lambda \left[ 0 - \frac{1}{\lambda^2} (0 - 1) \right] \\
&amp;= \frac{\lambda}{\lambda^2} \\
&amp;= \frac{1}{\lambda}. \qquad \qquad \qquad \qquad \qquad \qquad \quad \qquad \qquad \quad \qquad \qquad \square
\end{align*}
\]</span></p>
</div>
</section><section id="scale-parametrization-1" class="level4"><h4 class="anchored" data-anchor-id="scale-parametrization-1">Scale Parametrization</h4>
<p>Via <a href="#eq-app-expected-value-continuous" class="quarto-xref">Equation&nbsp;<span>C.4</span></a>, the expected value or mean of an Exponential-distributed random variable <span class="math inline">\(Y\)</span> can be found as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-app-exponential-scale-mean"><span class="math display">\[
\begin{align*}
\mathbb{E}(Y) &amp;= \int_{y = 0}^{y = \infty} y f_Y \left(y \mid \beta \right) \mathrm{d}y \\
&amp;= \int_{y = 0}^{y = \infty} \frac{y}{\beta} \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \\
&amp;= \frac{1}{\beta} \int_{y = 0}^{y = \infty} y \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y. \\
\end{align*}
\tag{E.13}\]</span></span></p>
<p><a href="#eq-app-exponential-scale-mean" class="quarto-xref">Equation&nbsp;<span>E.13</span></a> cannot be solved straightforwardly, we need to use <strong>integration by parts</strong> as follows:</p>
<p><span class="math display">\[
\begin{align*}
u &amp;= y &amp; &amp;\Rightarrow &amp; \mathrm{d}u &amp;= \mathrm{d}y \\
\mathrm{d}v &amp;= \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y &amp; &amp;\Rightarrow &amp; v &amp;= -\beta \exp \left( -\frac{y}{\beta} \right),
\end{align*}
\]</span></p>
<p>which yields</p>
<p><span class="math display">\[
\begin{align*}
\mathbb{E}(Y) &amp;= \frac{1}{\beta} \left[ u v \Bigg|_{y = 0}^{y = \infty} - \int_{y = 0}^{y = \infty} v \mathrm{d}u \right] \\
&amp;= \frac{1}{\beta} \left\{ \left[ -\beta y \exp \left( -\frac{y}{\beta} \right) \right] \Bigg|_{y = 0}^{y = \infty} + \beta \int_{y = 0}^{y = \infty} \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \right\} \\
&amp;= \frac{1}{\beta} \Bigg\{ -\beta \Bigg[ \underbrace{\infty \times \exp(-\infty)}_{0} - \underbrace{0 \times \exp(0)}_{0} \Bigg] - \\
&amp; \qquad \beta^2 \exp \left( -\frac{y}{\beta} \right) \Bigg|_{y = 0}^{y = \infty} \Bigg\} \\
&amp;= \frac{1}{\beta} \left\{ -\beta (0) - \beta^2 \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \right\} \\
&amp;= \frac{1}{\beta} \left[ 0 - \beta^2 (0 - 1) \right] \\
&amp;= \frac{\beta^2}{\beta} \\
&amp;= \beta. \qquad \qquad \qquad \qquad \qquad \qquad \quad \qquad \qquad \quad \qquad \qquad \square
\end{align*}
\]</span></p>
</div>
</section></section><section id="variance-6" class="level3" data-number="E.3.3"><h3 data-number="E.3.3" class="anchored" data-anchor-id="variance-6">
<span class="header-section-number">E.3.3</span> Variance</h3>
<p>Given the two above parametrizations of the Exponential distribution, there are two possible mathematical expressions for the variance as discussed below.</p>
<section id="rate-parametrization-2" class="level4"><h4 class="anchored" data-anchor-id="rate-parametrization-2">Rate Parametrization</h4>
<p>Via <a href="#eq-app-variance" class="quarto-xref">Equation&nbsp;<span>C.5</span></a> and the <a href="#eq-app-expected-value-continuous" class="quarto-xref">Equation&nbsp;<span>C.4</span></a> of a continuous expected value, the variance of an Exponential-distributed random variable <span class="math inline">\(Y\)</span> can be found as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-proof-exponential-rate-variance-1"><span class="math display">\[
\begin{align*}
\text{Var} (Y) &amp;= \mathbb{E} \left( Y^2 \right) - \left[ \mathbb{E}(Y)\right]^2 \\
&amp;= \mathbb{E} \left( Y^2 \right) - \frac{1}{\lambda^2} \qquad \text{since $\mathbb{E}(Y) = \frac{1}{\lambda}$}.
\end{align*}
\tag{E.14}\]</span></span></p>
<p>Now, we need to find <span class="math inline">\(\mathbb{E} \left( Y^2 \right)\)</span> from <a href="#eq-proof-exponential-rate-variance-1" class="quarto-xref">Equation&nbsp;<span>E.14</span></a>. Hence, we make the following derivation via the LOTUS from <a href="#eq-app-expected-value-continuous-function" class="quarto-xref">Equation&nbsp;<span>C.2</span></a> when <span class="math inline">\(g(Y) = y^2\)</span>:</p>
<p><span id="eq-proof-exponential-rate-variance-2"><span class="math display">\[
\begin{align*}
\mathbb{E} \left( Y^2 \right) &amp;= \int_{y = 0}^{y = \infty} y^2 f_Y \left(y \mid \lambda \right) \mathrm{d}y \\
&amp;= \int_{y = 0}^{y = \infty} y^2 \lambda \exp \left( -\lambda y \right) \mathrm{d}y \\
&amp;= \lambda \int_{y = 0}^{y = \infty} y^2 \exp \left( -\lambda y \right) \mathrm{d}y. \\
\end{align*}
\tag{E.15}\]</span></span></p>
<p><a href="#eq-proof-exponential-rate-variance-2" class="quarto-xref">Equation&nbsp;<span>E.15</span></a> cannot be solved straightforwardly, we need to use <strong>integration by parts</strong> as follows:</p>
<p><span class="math display">\[
\begin{align*}
u &amp;= y^2 &amp; &amp;\Rightarrow &amp; \mathrm{d}u &amp;= 2y \mathrm{d}y \\
\mathrm{d}v &amp;= \exp \left( -\lambda y \right) \mathrm{d}y &amp; &amp;\Rightarrow &amp; v &amp;= -\frac{1}{\lambda} \exp \left( -\lambda y \right),
\end{align*}
\]</span></p>
<p>which yields</p>
<p><span id="eq-proof-exponential-rate-variance-3"><span class="math display">\[
\begin{align*}
\mathbb{E} \left( Y^2 \right) &amp;= \lambda \left[ u v \Bigg|_{y = 0}^{y = \infty} - \int_{y = 0}^{y = \infty} v \mathrm{d}u \right] \\
&amp;= \lambda \bigg\{ \left[ -\frac{1}{\lambda} y^2 \exp(-\lambda y) \right] \Bigg|_{y = 0}^{y = \infty} + \\
&amp; \qquad \frac{2}{\lambda} \int_{y = 0}^{y = \infty} y \exp{\left( -\lambda y \right)} \mathrm{d}y \bigg\} \\
&amp;= \lambda \Bigg\{ -\frac{1}{\lambda} \Bigg[ \underbrace{\infty \times \exp(-\infty)}_{0} - \underbrace{0 \times \exp(0)}_{0} \Bigg] + \\
&amp; \qquad \frac{2}{\lambda} \int_{y = 0}^{y = \infty} y \exp{\left( -\lambda y \right)} \mathrm{d}y \Bigg\} \\
&amp;= \lambda \left\{ -\frac{1}{\lambda} (0) + \frac{2}{\lambda} \int_{y = 0}^{y = \infty} y \exp{\left( -\lambda y \right)} \mathrm{d}y \right\} \\
&amp;= \lambda \left\{ 0 + \frac{2}{\lambda} \int_{y = 0}^{y = \infty} y \exp{\left( -\lambda y \right)} \mathrm{d}y \right\} \\
&amp;= 2 \int_{y = 0}^{y = \infty} y \exp{\left( -\lambda y \right)} \mathrm{d}y. \\
\end{align*}
\tag{E.16}\]</span></span></p>
<p>Again, we need to apply <strong>integration by parts</strong> to solve <a href="#eq-proof-exponential-rate-variance-3" class="quarto-xref">Equation&nbsp;<span>E.16</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
u &amp;= y &amp; &amp;\Rightarrow &amp; \mathrm{d}u &amp;= \mathrm{d}y \\
\mathrm{d}v &amp;= \exp \left( -\lambda y \right) \mathrm{d}y &amp; &amp;\Rightarrow &amp; v &amp;= -\frac{1}{\lambda} \exp \left( -\lambda y \right),
\end{align*}
\]</span></p>
<p>which yields</p>
<p><span id="eq-proof-exponential-rate-variance-4"><span class="math display">\[
\begin{align*}
\mathbb{E} \left( Y^2 \right) &amp;= 2 \left[ u v \Bigg|_{y = 0}^{y = \infty} - \int_{y = 0}^{y = \infty} v \mathrm{d}u \right] \\
&amp;= 2 \left\{ \left[ -\frac{1}{\lambda} y \exp(-\lambda y) \right] \Bigg|_{y = 0}^{y = \infty} + \frac{1}{\lambda} \int_{y = 0}^{y = \infty} \exp{\left( -\lambda y \right)} \mathrm{d}y \right\} \\
&amp;= 2 \Bigg\{ -\frac{1}{\lambda} \Bigg[ \underbrace{\infty \times \exp(-\infty)}_{0} - \underbrace{0 \times \exp(0)}_{0} \Bigg] - \\
&amp; \qquad \frac{1}{\lambda^2} \exp{\left( -\lambda y \right)} \Bigg|_{y = 0}^{y = \infty} \Bigg\} \\
&amp;= 2 \left\{ -\frac{1}{\lambda} (0) - \frac{1}{\lambda^2} \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \right\} \\
&amp;= 2 \left[ 0 - \frac{1}{\lambda^2} (0 - 1) \right] \\
&amp;= \frac{2}{\lambda^2}.
\end{align*}
\tag{E.17}\]</span></span></p>
<p>Finally, we plug <a href="#eq-proof-exponential-rate-variance-4" class="quarto-xref">Equation&nbsp;<span>E.17</span></a> into <a href="#eq-proof-exponential-rate-variance-1" class="quarto-xref">Equation&nbsp;<span>E.14</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
\text{Var} (Y) &amp;= \mathbb{E} \left( Y^2 \right) - \frac{1}{\lambda^2} \\
&amp;= \frac{2}{\lambda^2} - \frac{1}{\lambda^2} \\
&amp;= \frac{1}{\lambda^2}. \qquad \qquad \square
\end{align*}
\]</span></p>
</div>
</section><section id="scale-parametrization-2" class="level4"><h4 class="anchored" data-anchor-id="scale-parametrization-2">Scale Parametrization</h4>
<p>Via <a href="#eq-app-variance" class="quarto-xref">Equation&nbsp;<span>C.5</span></a> and the <a href="#eq-app-expected-value-continuous" class="quarto-xref">Equation&nbsp;<span>C.4</span></a> of a continuous expected value, the variance of an Exponential-distributed random variable <span class="math inline">\(Y\)</span> can be found as follows:</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span id="eq-proof-exponential-scale-variance-1"><span class="math display">\[
\begin{align*}
\text{Var} (Y) &amp;= \mathbb{E} \left( Y^2 \right) - \left[ \mathbb{E}(Y)\right]^2 \\
&amp;= \mathbb{E} \left( Y^2 \right) - \beta^2 \qquad \text{since $\mathbb{E}(Y) = \beta$}.
\end{align*}
\tag{E.18}\]</span></span></p>
<p>Now, we need to find <span class="math inline">\(\mathbb{E} \left( Y^2 \right)\)</span> from <a href="#eq-proof-exponential-scale-variance-1" class="quarto-xref">Equation&nbsp;<span>E.18</span></a>. Hence, we make the following derivation via the LOTUS from <a href="#eq-app-expected-value-continuous-function" class="quarto-xref">Equation&nbsp;<span>C.2</span></a> when <span class="math inline">\(g(Y) = y^2\)</span>:</p>
<p><span id="eq-proof-exponential-scale-variance-2"><span class="math display">\[
\begin{align*}
\mathbb{E} \left( Y^2 \right) &amp;= \int_{y = 0}^{y = \infty} y^2 f_Y \left(y \mid \beta \right) \mathrm{d}y \\
&amp;= \int_{y = 0}^{y = \infty} y^2 \frac{1}{\beta} \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \\
&amp;= \frac{1}{\beta} \int_{y = 0}^{y = \infty} y^2 \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y. \\
\end{align*}
\tag{E.19}\]</span></span></p>
<p><a href="#eq-proof-exponential-scale-variance-2" class="quarto-xref">Equation&nbsp;<span>E.19</span></a> cannot be solved straightforwardly, we need to use <strong>integration by parts</strong> as follows:</p>
<p><span class="math display">\[
\begin{align*}
u &amp;= y^2 &amp; &amp;\Rightarrow &amp; \mathrm{d}u &amp;= 2y \mathrm{d}y \\
\mathrm{d}v &amp;= \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y &amp; &amp;\Rightarrow &amp; v &amp;= -\beta \exp \left( -\frac{y}{\beta} \right),
\end{align*}
\]</span></p>
<p>which yields</p>
<p><span id="eq-proof-exponential-scale-variance-3"><span class="math display">\[
\begin{align*}
\mathbb{E} \left( Y^2 \right) &amp;= \frac{1}{\beta} \left[ u v \Bigg|_{y = 0}^{y = \infty} - \int_{y = 0}^{y = \infty} v \mathrm{d}u \right] \\
&amp;= \frac{1}{\beta} \Bigg\{ \left[ -\beta y^2 \exp \left( -\frac{y}{\beta} \right) \right] \Bigg|_{y = 0}^{y = \infty} + \\
&amp; \qquad 2 \beta \int_{y = 0}^{y = \infty} y \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \Bigg\} \\
&amp;= \frac{1}{\beta} \Bigg\{ -\beta \Bigg[ \underbrace{\infty \times \exp(-\infty)}_{0} - \underbrace{0 \times \exp(0)}_{0} \Bigg] + \\
&amp; \qquad 2 \beta \int_{y = 0}^{y = \infty} y \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \Bigg\} \\
&amp;= \frac{1}{\beta} \left\{ -\beta (0) + 2 \beta \int_{y = 0}^{y = \infty} y \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \right\} \\
&amp;= \frac{1}{\beta} \left\{ 0 + 2 \beta \int_{y = 0}^{y = \infty} y \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \right\} \\
&amp;= 2 \int_{y = 0}^{y = \infty} y \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y. \\
\end{align*}
\tag{E.20}\]</span></span></p>
<p>Again, we need to apply <strong>integration by parts</strong> to solve <a href="#eq-proof-exponential-scale-variance-3" class="quarto-xref">Equation&nbsp;<span>E.20</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
u &amp;= y &amp; &amp;\Rightarrow &amp; \mathrm{d}u &amp;= \mathrm{d}y \\
\mathrm{d}v &amp;= \exp \left( -\frac{y}{\beta} \right)\mathrm{d}y &amp; &amp;\Rightarrow &amp; v &amp;= -\beta \exp \left( -\frac{y}{\beta} \right),
\end{align*}
\]</span></p>
<p>which yields</p>
<p><span id="eq-proof-exponential-scale-variance-4"><span class="math display">\[
\begin{align*}
\mathbb{E} \left( Y^2 \right) &amp;= 2 \left[ u v \Bigg|_{y = 0}^{y = \infty} - \int_{y = 0}^{y = \infty} v \mathrm{d}u \right] \\
&amp;= 2 \left\{ \left[ -\beta y \exp \left( -\frac{y}{\beta} \right) \right] \Bigg|_{y = 0}^{y = \infty} + \beta \int_{y = 0}^{y = \infty} \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \right\} \\
&amp;= 2 \Bigg\{ -\beta \Bigg[ \underbrace{\infty \times \exp(-\infty)}_{0} - \underbrace{0 \times \exp(0)}_{0} \Bigg] - \\
&amp; \qquad \beta^2 \exp \left( -\frac{y}{\beta} \right) \Bigg|_{y = 0}^{y = \infty} \Bigg\} \\
&amp;= 2 \left\{ -\beta (0) - \beta^2 \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \right\} \\
&amp;= 2 \left[ 0 - \beta^2 (0 - 1) \right] \\
&amp;= 2 \beta^2.
\end{align*}
\tag{E.21}\]</span></span></p>
<p>Finally, we plug <a href="#eq-proof-exponential-scale-variance-4" class="quarto-xref">Equation&nbsp;<span>E.21</span></a> into <a href="#eq-proof-exponential-scale-variance-1" class="quarto-xref">Equation&nbsp;<span>E.18</span></a>:</p>
<p><span class="math display">\[
\begin{align*}
\text{Var} (Y) &amp;= \mathbb{E} \left( Y^2 \right) - \beta^2 \\
&amp;= 2 \beta^2 - \beta^2 \\
&amp;= \beta^2. \qquad \qquad \square
\end{align*}
\]</span></p>
</div>
</section></section></section><section id="sec-gamma-distribution" class="level2" data-number="E.4"><h2 data-number="E.4" class="anchored" data-anchor-id="sec-gamma-distribution">
<span class="header-section-number">E.4</span> Gamma</h2>
</section><section id="sec-logistic-distribution" class="level2" data-number="E.5"><h2 data-number="E.5" class="anchored" data-anchor-id="sec-logistic-distribution">
<span class="header-section-number">E.5</span> Logistic</h2>
</section><section id="sec-normal-distribution" class="level2" data-number="E.6"><h2 data-number="E.6" class="anchored" data-anchor-id="sec-normal-distribution">
<span class="header-section-number">E.6</span> Normal</h2>
<p>This is possibly one the most famous probability distributions, and it is also known as <strong>Gaussian</strong>. It appears in many different statistical tools in the literature where certain regression models are included. Let random variable <span class="math inline">\(Y\)</span> be considered continuous and unbounded. Then, <span class="math inline">\(Y\)</span> is said to have a <strong>Normal distribution</strong> with the following <strong>location</strong> continuous parameter <span class="math inline">\(\mu\)</span> and <strong>scale</strong> continuous parameter <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[
Y \sim \text{Normal}(\mu, \sigma^2).
\]</span></p>
<section id="probability-density-function-1" class="level3" data-number="E.6.1"><h3 data-number="E.6.1" class="anchored" data-anchor-id="probability-density-function-1">
<span class="header-section-number">E.6.1</span> Probability Density Function</h3>
<p>The PDF of <span class="math inline">\(Y\)</span> is the following:</p>
<p><span id="eq-app-normal-pdf"><span class="math display">\[
f_Y \left(y \mid \mu, \sigma^2 \right) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{\left[ - \frac{(y - \mu)^2}{2 \sigma^2} \right]} \quad \text{for $y \in ( -\infty, \infty )$.}
\tag{E.22}\]</span></span></p>
<div class="Heads-up">
<div class="Heads-up-header">
<p>Heads-up on the use of <span class="math inline">\(\pi\)</span> in the Normal distribution!</p>
</div>
<div class="Heads-up-container">
<p>The term <span class="math inline">\(\pi\)</span> in the Normal PDF depicted in <a href="#eq-app-normal-pdf" class="quarto-xref">Equation&nbsp;<span>E.22</span></a> corresponds to the mathematical constant <span class="math inline">\(3.141592...\)</span> Hence, this term <strong>does not</strong> correspond to another population parameter in this probability distribution.</p>
</div>
</div>
<p>Parameters <span class="math inline">\(\mu \in (-\infty, \infty)\)</span> and <span class="math inline">\(\sigma \in (0, \infty)\)</span> refer to the random process’ location and scale, respectively. <a href="#fig-normal-family" class="quarto-xref">Figure&nbsp;<span>E.4</span></a> shows nine members of the Normal parametric family, i.e., nine different PDFs with all possible pairwise combinations for three different scale parameters <span class="math inline">\(\mu = -3, 0, 3\)</span> and shape parameters <span class="math inline">\(\sigma^2 = 0.25, 1, 4\)</span>. We can highlight the following:</p>
<ul>
<li>Regardless of the scale parameter <span class="math inline">\(\sigma^2\)</span>, <strong>as we increase the location parameter <span class="math inline">\(\mu\)</span></strong>, note the center of the corresponding symmetric distribution moves more to the right.</li>
<li>Regardless of the location parameter <span class="math inline">\(\mu\)</span>, <strong>as we increase the scale parameter <span class="math inline">\(\sigma^2\)</span></strong>, note that there is more spread in the corresponding symmetric distribution.</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div id="fig-normal-family" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-normal-family-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="C-distributional-mind-map_files/figure-html/fig-normal-family-1.png" class="img-fluid figure-img" width="1344">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normal-family-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;E.4: Some members of the Normal or Gaussian parametric family.
</figcaption></figure>
</div>
</div>
</div>
<blockquote class="blockquote">
<p><strong>How can we verify that <a href="#eq-app-normal-pdf" class="quarto-xref">Equation&nbsp;<span>E.22</span></a> is a proper PDF (i.e., <a href="#eq-app-normal-pdf" class="quarto-xref">Equation&nbsp;<span>E.22</span></a> integrates to one over the support of <span class="math inline">\(Y\)</span>)?</strong></p>
</blockquote>
</section></section><section id="sec-beta-distribution" class="level2" data-number="E.7"><h2 data-number="E.7" class="anchored" data-anchor-id="sec-beta-distribution">
<span class="header-section-number">E.7</span> Beta</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-consul1973" class="csl-entry" role="listitem">
Consul, P. C., and G. C. Jain. 1973. <span>“A Generalization of the Poisson Distribution.”</span> <em>Technometrics</em> 15 (4): 791–99. <a href="http://www.jstor.org/stable/1267389">http://www.jstor.org/stable/1267389</a>.
</div>
<div id="ref-earlom1793" class="csl-entry" role="listitem">
Earlom, Richard. 1793. <span>“Brook Taylor - National Portrait Gallery.”</span> <em>NPG D6930; Brook Taylor - Portrait - National Portrait Gallery</em>. National Portrait Gallery. <a href="https://www.npg.org.uk/collections/search/portrait/mw40921/Brook-Taylor">https://www.npg.org.uk/collections/search/portrait/mw40921/Brook-Taylor</a>.
</div>
<div id="ref-gregory1668" class="csl-entry" role="listitem">
Gregory, James. 1668. <em><span class="nocase">Vera circuli et hyperbolae quadratura cui accedit geometria pars vniuersalis inseruiens quantitatum curuarum transmutationi &amp; mensurae. Authore Iacobo Gregorio Abredonensi</span></em>. Padua, Italy: Patavii: typis heredum Pauli Frambotti bibliop. <a href="https://archive.org/details/ita-bnc-mag-00001357-001/page/n10/mode/2up">https://archive.org/details/ita-bnc-mag-00001357-001/page/n10/mode/2up</a>.
</div>
<div id="ref-harding1798" class="csl-entry" role="listitem">
Harding, Edward. 1798. <em>Portrait of Colin MacLaurin.</em> <em>Courtesy of the Smithsonian Libraries and Archives</em>. <a href="https://library.si.edu/image-gallery/72863">https://library.si.edu/image-gallery/72863</a>.
</div>
<div id="ref-maclaurin1742" class="csl-entry" role="listitem">
Maclaurin, Colin. 1742. <em>A Treatise of Fluxions</em>. Edinburgh, Scotland: Printed for the Author by T.W.; T. Ruddimans. <a href="https://archive.org/details/treatiseonfluxio02macl/page/n5/mode/2up">https://archive.org/details/treatiseonfluxio02macl/page/n5/mode/2up</a>.
</div>
<div id="ref-watson1886" class="csl-entry" role="listitem">
Scotland, National Galleries of. n.d. <em>Professor James Gregory, 1638 - 1675 (1). Mathematician</em>. <em>Professor James Gregory, 1638 - 1675 (1). Mathematician | National Galleries</em>. <a href="https://www.nationalgalleries.org/art-and-artists/31132/professor-james-gregory-1638-1675-mathematician">https://www.nationalgalleries.org/art-and-artists/31132/professor-james-gregory-1638-1675-mathematician</a>.
</div>
<div id="ref-taylor1715" class="csl-entry" role="listitem">
Taylor, Brook. 1715. <em><span class="nocase">Methodus incrementorum directa &amp; inversa. Auctore Brook Taylor, LL. D. &amp; Regiae Societatis Secretario</span></em>. London, England: Typis Pearsonianis Prostant apud Gul. Innys ad Insignia Principis in Coemeterio Paulino MDCCXV. <a href="https://archive.org/details/bim_eighteenth-century_methodus-incrementorum-d_taylor-brook_1717">https://archive.org/details/bim_eighteenth-century_methodus-incrementorum-d_taylor-brook_1717</a>.
</div>
<div id="ref-weisstein2" class="csl-entry" role="listitem">
Weisstein, Eric W. n.d.a. <span>“Gamma Function.”</span> <em>From MathWorld–A Wolfram Web Resource</em>. <a href="https://mathworld.wolfram.com/GammaFunction.html">https://mathworld.wolfram.com/GammaFunction.html</a>.
</div>
<div id="ref-weisstein" class="csl-entry" role="listitem">
———. n.d.b. <span>“Taylor Series.”</span> <em>From MathWorld–A Wolfram Web Resource</em>. <a href="https://mathworld.wolfram.com/TaylorSeries.html">https://mathworld.wolfram.com/TaylorSeries.html</a>.
</div>
</div>
</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/alexrod61\.github\.io\/regression-cookbook\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
          // default icon
          link.classList.add("external");
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><script src="https://giscus.app/client.js" data-repo="alexrod61/regression-cookbook" data-repo-id="R_kgDOMSJoUA" data-category="General" data-category-id="" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" async="">
</script><input type="hidden" id="giscus-base-theme" value="light"><input type="hidden" id="giscus-alt-theme" value="dark"><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../book/B-greek-alphabet.html" class="pagination-link" aria-label="Greek Alphabet">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Greek Alphabet</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../book/D-regression-mind-map.html" class="pagination-link" aria-label="Regression Mind Map">
        <span class="nav-page-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Regression Mind Map</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025; G. Alexi Rodríguez-Arelis, Andy Tai, and Ben Chen</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/alexrod61/regression-cookbook/edit/main/book/C-distributional-mind-map.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/alexrod61/regression-cookbook/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.com/alexrod61/regression-cookbook/blob/main/book/C-distributional-mind-map.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>


<script src="../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>