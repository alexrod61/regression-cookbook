[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Regression Cookbook (in development)",
    "section": "",
    "text": "Preface\nData science is a field in which we become aware of the fascinating overlap between machine learning and statistics. Many data science students usually come across everyday machine learning and statistics concepts or ideas that might only differ in names. For instance, simple terms such as weights in supervised learning (and their statistical counterpart as regression coefficients) might be misleading for students starting their data science formation. On the other hand, from an instructor’s perspective in a data science program that subsets its courses in machine learning in Python and statistics in R, regression courses in R also demand the inclusion of Python-related packages as alternative tools. Furthermore, in a graduate program such as the Master of Data Science (MDS) at the University of British Columbia, this is especially critical for students whose career plan leans towards the industry job market where Python is more heavily used.\nThat said, we can state that data science is a substantial synergy between machine learning and statistics. Nevertheless, many gaps between both disciplines still need to be addressed. Thus, closing these critical gaps is imperative in a domain with accelerated growth, such as data science. In this regard, the MDS Stat-ML dictionary has inspired us to write this textbook. It basically consists of common ground between foundational supervised learning models from machine learning and regression models commonly used in statistics. We strive to explore linear modelling approaches as a primary step while highlighting different terminology found in both fields. Furthermore, this discussion is more comprehensive than a simple conceptual exploration. Hence, the second step is hands-on practice via the corresponding Python packages for machine learning and R for statistics.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#g.-alexi-rodríguez-arelis",
    "href": "index.html#g.-alexi-rodríguez-arelis",
    "title": "The Regression Cookbook (in development)",
    "section": "G. Alexi Rodríguez-Arelis",
    "text": "G. Alexi Rodríguez-Arelis\n\n\n\n\n\n\n\n\n\n\n\nI'm an Assistant Professor of Teaching in the Department of Statistics and Master of Data Science at the University of British Columbia. Throughout my academic and professional journey, I've been involved in diverse fields, such as credit risk management, statistical consulting, and data science teaching. My doctoral research in statistics is primarily focused on computer experiments that emulate scientific and engineering systems via Gaussian stochastic processes (i.e., kriging regression). I'm incredibly passionate about teaching regression topics while combining statistical and machine learning contexts.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#andy-tai",
    "href": "index.html#andy-tai",
    "title": "The Regression Cookbook (in development)",
    "section": "Andy Tai",
    "text": "Andy Tai\n\n\n\n\n\n\n\n\n\n\n\nI'm a Postdoctoral Teaching and Learning Fellow in the Department of Statistics and Master of Data Science at the University of British Columbia. Throughout my academic and professional journey, I've been involved in diverse fields, such as addiction psychiatry, machine learning, and data science teaching. My doctoral research in neuroscience primarily focused on using machine learning to predict the risk of fatal overdose. I am interested in leveraging data science and machine learning to solve complex problems, and I strive to inspire others to explore the vast potential of these fields.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#ben-chen",
    "href": "index.html#ben-chen",
    "title": "The Regression Cookbook (in development)",
    "section": "Ben Chen",
    "text": "Ben Chen\n\n\n\n\n\n\n\n\n\n\n\nI hold a Master's degree in Data Science from the University of British Columbia, and I am passionate about educating others in the fields of statistics and data science. With experience teaching students how to use statistical methods and data science tools, I also enjoy sharing my knowledge through writing. My blog focuses on making complex statistical concepts accessible to everyone. Additionally, I've worked on a variety of data science projects, ranging from developing recommendation systems to building Generative Adversarial Network (GAN) models.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "The Regression Cookbook (in development)",
    "section": "",
    "text": "Special thanks to Jonathan Graves, who mentioned the cookbook term when this textbook was conceptualized during very early stages.↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "book/privacy-policy.html",
    "href": "book/privacy-policy.html",
    "title": "Website Privacy Policy",
    "section": "",
    "text": "Information Collection and Use\nYour privacy is important to us. This policy outlines how this online textbook created for courses at the University of British Columbia (UBC) (“we,” “us,” or “our”) collects, uses, and protects your information.\nWe use Google Analytics, a web analytics service provided by Google, LLC. (“Google”). Google Analytics uses cookies to help analyze how students interact with the textbook, including tracking which sections are accessed most frequently. Information generated by cookies about your use of our website (including IP address) will be transmitted to and stored by Google on servers in the United States.\nGoogle will use this information solely for evaluating textbook usage, compiling usage reports to enhance the educational effectiveness of the textbook, and providing related services.\nYou may refuse the use of cookies by selecting the appropriate settings in your browser; however, please note this may affect your textbook browsing experience.",
    "crumbs": [
      "Website Privacy Policy"
    ]
  },
  {
    "objectID": "book/privacy-policy.html#personal-information",
    "href": "book/privacy-policy.html#personal-information",
    "title": "Website Privacy Policy",
    "section": "Personal Information",
    "text": "Personal Information\nWe do not collect personally identifiable information through Google Analytics. Any personally identifiable information, such as your name and email address, would only be collected if voluntarily submitted for specific educational purposes (e.g., feedback or course-related inquiries). We will never sell or distribute your personal information to third parties.\nFor any questions or concerns, please contact us at alexrod@stat.ubc.ca.",
    "crumbs": [
      "Website Privacy Policy"
    ]
  },
  {
    "objectID": "book/audience-scope.html",
    "href": "book/audience-scope.html",
    "title": "Audience and Scope",
    "section": "",
    "text": "This book mainly focuses on regression analysis and its supervised learning counterpart. Thus, it is not introductory statistics and machine learning material. Also, some coding background on R (R Core Team 2024) and/or Python (Van Rossum and Drake 2009) is recommended. That said, the following topics are suggested as fundamental reviews:\n\nMutivariable differential calculus and linear algebra. Certain sections of each chapter pertain to modelling estimation. Therefore, topics such as partial derivatives and matrix algebra are a great asset. You can find helpful learning resources on the MDS webpage.\nBasic Python programming. When necessary, Python {pandas} (The Pandas Development Team 2024) library will be used to perform data wrangling. The MDS course DSCI 511 (Programming for Data Science) is an ideal example of a quick review.\n\n\n\n\nImage by Lubos Houska via Pixabay.\n\n\n\nBasic R programming. Knowledge of data wrangling and plotting through R {tidyverse} (Wickham et al. 2019) is recommended for hands-on practice via the cases provided in each one of the chapters of this book. The MDS courses DSCI 523 (Programming for Data Manipulation) and DSCI 531 (Data Visualization I) are ideal examples of a quick review.\nFoundations of probability and basic distributional knowledge. The reader should be familiar with elemental discrete and continuous distributions since they are a vital component of any given regression or supervised learning model. The MDS course DSCI 551 (Descriptive Statistics and Probability for Data Science) is an ideal example of a quick review.\nFoundations of frequentist statistical inference. One of the data science paradigms to be covered in this book is statistical inference, i.e., identifying relationships between different variables in a given population or system of interest via a sampled dataset. I only aim to cover a frequentist approach using inferential tools such as parameter estimation, hypothesis testing, and confidence intervals. The MDS course DSCI 552 (Statistical Inference and Computation I) is an ideal example of a quick review.\nFoundations of supervised learning. The second data science paradigm to be covered pertains to prediction, which is core in machine learning. The reader should be familiar with basic terminology, such as training and testing data, overfitting, underfitting, cross-validation, etc. The MDS course DSCI 571 (Machine Learning I) provides these foundations.\nFoundations of feature and model selection. This prerequisite also relates to machine learning and its corresponding prediction paradigm. Basic knowledge of prediction accuracy and variable selection tools is recommended. The MDS course DSCI 573 (Feature and Model Selection) is an ideal example of a quick review.\n\n\n\nA further remark on probability and statistical inference\n\n\nIn case the reader is not 100% familiar with probabilistic and inferential topics, as discussed above, we will provide a fundamental refresher in ?sec-stats-review with crucial points that are needed to follow along the statistical way each one of the chapters is delivered (more specifically for modelling estimation/training matters!).\n\nFurthermore, this refresher will be integrated into the three big pillars that will be fully expanded in this book, more concretely in ?sec-intro: a data science workflow, the right workflow flavour (inferential or predictive), and a regression toolbox.\n\n\n\n\n\n\nR Core Team. 2024. “R: A Language and Environment for Statistical Computing.” Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nThe Pandas Development Team. 2024. “Pandas-Dev/Pandas: Pandas.” Zenodo. https://doi.org/10.5281/zenodo.3509134.\n\n\nVan Rossum, Guido, and Fred L. Drake. 2009. Python 3 Reference Manual. Scotts Valley, CA: CreateSpace.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Audience and Scope"
    ]
  },
  {
    "objectID": "book/01-intro.html",
    "href": "book/01-intro.html",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "",
    "text": "1.1 The ML-Stats Dictionary\nFirst things first! Let us prepare for all the different regression techniques to be introduced in ?sec-ols.\nThat said, we want to highlight one guiding principle for all of our work:\nThe above guiding principle rests on foundational statistical ideas on how data is generated and how it can be modelled through various regression methods. We will explore these underlying concepts in Chapter 2. Before doing so, however, this chapter will build on the three core pillars introduced in Audience and Scope:\nLet us establish a convention for using admonitions throughout this textbook. These admonitions will help distinguish between key concepts, important insights, and supplementary material, ensuring clarity as we explore different regression techniques. We will start using these admonitions in Section 1.1.\nThe core idea of the above admonition arrangement is to allow the reader to discern between ideas or concepts that are key to grasp from those whose understanding might not be highly essential (but still interesting to check out in further literature!). With this structure in place, we can now introduce another foundational resource: a common ground between machine learning and statistics which will be elaborated on in the next section.\nMachine learning and statistics often overlap, especially in regression modelling. Topics covered in a regression-focused course, under a purely statistical framework, can also appear in machine learning-based courses on supervised learning, but the terminology can differ. Recognizing this overlap, the Master of Data Science (MDS) program at the University of British Columbia (UBC) provides the MDS Stat-ML dictionary (Gelbart 2017) under the following premises:\nBoth disciplines have a tremendous amount of jargon and terminology. As mentioned in the Preface, machine learning and statistics construct a substantial synergy reflected in data science. Despite this overlap, misunderstandings can still happen due to differences in terminology. To prevent this, we need clear bridges between these disciplines. Therefore, the above definition callout box will pave the way to a complimentary resource called the ML-Stats dictionary (ML stands for Machine Learning). This ML-Stats dictionary clarifies terminology that differs between statistics and machine learning, specifically in the context of supervised learning and regression analysis.\nNote that ?sec-dictionary will be the section in this book where the reader can find all those statistical and machine learning-related terms in alphabetical order. Notable terms (either statistical or machine learning-related) will include an admonition identifying which terms (again, either statistical or machine learning-related) are equivalent or somewhat equivalent (or even NOT equivalent if that is the case!).\nHence, in ?sec-dictionary, readers will find all those statistical and machine learning-related terms in alphabetical order as in a regular dictionary. Notable terms will include clear notes on their equivalence or non-equivalence. For instance, consider the statistical term dependent variable:\nThen, the above definition will be followed by this admonition:\nAbove, we have identified four equivalent terms for the term dependent variable. Furthermore, these terms can be statistical or machine learning-related. Finally, it is important to highlight we will start using this colour scheme in Chapter 2.\nNext, we will introduce the three main foundations of this textbook: a data science workflow, choosing the correct workflow flavour (inferential or predictive), and building your regression toolbox.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-ml-stats-dictionary",
    "href": "book/01-intro.html#sec-ml-stats-dictionary",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "",
    "text": "This document is intended to help students navigate the large amount of jargon, terminology, and acronyms encountered in the MDS program and beyond.\n\n\nThis section covers terms that have different meanings in different contexts, specifically statistics vs. machine learning (ML).\n\n\n\n\nHeads-up on terminology highlights!\n\n\nThroughout the book, following the ML-Stats dictionary, all statistical terms will be highlighted in blue whereas the machine learning terms will be highlighted in magenta. This color scheme helps readers move easily between disciplines. With practice, readers will comfortably use concepts from either field.\n\n\n\n\n\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, in a statistical inference framework, the variable we are trying explain.\n\n\n\n\nEquivalent to:\n\n\nResponse variable, outcome, output or target.\n\n\n\n\n\nHeads-up on the use of terminology!\n\n\nThroughout this book, we will use specific terms interchangeably while explaining different regression methods. If confusion arises, readers should always check definitions and equivalences (or non-equivalences) in ?sec-dictionary.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-ds-workflow",
    "href": "book/01-intro.html#sec-ds-workflow",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "\n1.2 The Data Science Workflow",
    "text": "1.2 The Data Science Workflow\nUnderstanding the data science workflow is essential for mastering regression analysis. This workflow serves as a blueprint that guides us through each stage of our analysis, ensuring that we apply a systematic approach to solving our inquiries in a reproducible way. Each of the three pillars of this textbook—data science workflow, the right workflow flavor (inferential or predictive), and a regression toolbox—are deeply interconnected. Regardless of the regression model we explore, this general workflow provides a consistent framework that helps us navigate our data analysis with clarity and purpose. As shown in Figure 1.1, the data science workflow is composed of the following stages (each of which will be discussed in more detail in subsequent subsections):\n\n\nStudy design: Define the research question, objectives, and variables of interest to ensure the analysis is purpose-driven and aligned with the problem at hand.\n\nData collection and wrangling: Gather and clean data, addressing issues such as missing values, outliers, and inconsistencies to transform it into a usable format.\n\nExploratory data analysis: Explore the data through statistical summaries and visualizations to identify patterns, trends, and potential anomalies.\n\nData modelling: Apply statistical or machine learning models to uncover relationships between variables or make predictions based on the data.\n\nEstimation: Calculate model parameters to quantify relationships between variables and assess the accuracy and reliability of the model.\n\nGoodness of fit: Evaluate the model’s performance using metrics and diagnostic checks to determine how well it explains the data.\n\nResults: Interpret the model’s outputs to derive meaningful insights and provide answers to the original research question.\n\nStorytelling Communicate the findings through a clear, engaging narrative that is accessible to a non-technical audience.\n\nBy adhering to this workflow, we ensure that our regression analysis are not only systematic and thorough but also capable of producing results that are meaningful within the context of the problem we aim to solve.\n\n\n\n\n\n\nThe Importance of a Formal Structure in Regression Analysis\n\n\n\nFrom the earliest stages of learning data analysis, understanding the importance of a structured workflow is crucial. If we do not adhere to a predefined workflow, we risk misinterpreting the data, leading to incorrect conclusions that fail to address the core questions of our analysis. Such missteps can result in outcomes that are not only meaningless but potentially misleading when taken out of the problem’s context. Therefore, it is essential for aspiring data scientists to internalize this workflow from the very beginning of their education. A systematic approach ensures that each stage of the analysis is conducted with precision, ultimately producing reliable and contextually relevant results.\n\n\n\n\n\n\n\nFigure 1.1: Data science workflow for inferential and predictive inquiries in regression analysis and supervised learning, respectively. The workflow is structured in eight stages: study design, data collection and wrangling, exploratory data analysis, data modelling, estimation, goodness of fit, results, and storytelling.\n\n\n\n1.2.1 Study Design\nThe first stage of this workflow is centered around defining the main statistical inquiries we aim to address throughout the data analysis process. As a data scientist, your primary task is to translate these inquiries from the stakeholders into one of two categories: inferential or predictive. This classification determines the direction of your analysis and the methods you will use.\n\nInferential: The objective here is to explore and quantify relationships of association or causation between explanatory variables (regressors) and the response variable within the context of the problem at hand. For example, you may seek to determine whether a specific marketing campaign (regressor) significantly impacts sales revenue (response) and, if so, by how much.\nPredictive: In this case, the focus is on making accurate predictions about the response variable based on future observations of the regressors. Unlike inferential inquiries, where understanding the relationship between variables is key, the primary goal here is to maximize prediction accuracy. This approach is fundamental in machine learning. For instance, you might build a model to predict future sales revenue based on past marketing expenditures, without necessarily needing to understand the underlying relationship between the two.\n\nExample: Predicting Housing Prices\nTo illustrate the study design stage, let’s consider a simple example: predicting housing prices in a specific city.\n\nIf our goal is inferential, we might be interested in understanding the relationship between various factors (like square footage, number of bedrooms, and proximity to schools) and housing prices. Specifically, we would ask questions like, “How much does the number of bedrooms affect the price of a house, after accounting for other factors?”\nIf our goal is predictive, we would focus on creating a model that can accurately predict the price of a house based on its features, regardless of whether we fully understand how each factor contributes to the price.\n\nIn both cases, the study design stage involves clearly defining these objectives and determining the appropriate methods to address them. This stage sets the foundation for all subsequent steps in the data science workflow, as illustrated in Figure 1.2. Once the study design is established, the next stage is data collection and wrangling.\n\n\n\n\n\nFigure 1.2: Study design stage from the data science workflow in Figure 1.1. This stage is directly followed by data collection and wrangling.\n\n\n\n1.2.2 Data Collection and Wrangling\nOnce we have clearly defined our statistical questions, the next crucial step is to collect the data that will form the basis of our analysis. The way we collect this data is vital because it directly affects the accuracy and reliability of our results:\n\n\nFor inferential inquiries, we focus on understanding large groups or systems (populations) that we cannot fully observe. These populations are governed by characteristics (parameters) that we want to estimate. Because we can’t study every individual in the population, we collect a smaller, representative subset called a sample. The method we use to collect this sample—known as sampling—is crucial. A proper sampling method ensures that our sample reflects the larger population, allowing us to make accurate generalizations (inferences) about the entire group. After collecting the sample, it’s common practice to randomly split the data into training and test sets. This split allows us to build and validate our models, ensuring that the findings are robust and not overly tailored to the specific data at hand.\n\n\n\n\n\n\n\nA Quick Debrief on Sampling!\n\n\n\nAlthough this book does not cover sampling methods in detail, it’s important to know that the way you collect your sample can greatly influence your results. Depending on the problem, you might use different techniques:\n\n\nSimple Random Sampling: Every individual in the population has an equal chance of being selected.\n\nSystematic Sampling: You select individuals at regular intervals from a list of the population.\n\nStratified Sampling: You divide the population into subgroups (strata) and take a proportional sample from each subgroup.\n\nClustered Sampling: You divide the population into clusters and randomly select whole clusters for your sample.\nEtc.\n\nAs in the case of Regression Analysis, statistical sampling is a vast field, and we could spend a whole course on it. If you’re interested in learning more about these methods, Sampling: design and analysis by Lohr is a great resource.\n\n\n\n\nFor predictive inquiries, our goal is often to use existing data to make predictions about future events or outcomes. In these cases, we usually work with large datasets (databases) that have already been collected. Instead of focusing on whether the data represents a population (as in inferential inquiries), we focus on cleaning and preparing the data so that it can be used to train models that make accurate predictions. After wrangling the data, it is typically split into training, validation, and test sets. The training set is used to build the model, the validation set is used to tune model parameters, and the test set evaluates the model’s final performance on unseen data.\n\nExample: Collecting Data for Housing Price Predictions\nLet’s continue with our housing price prediction example to illustrate these concepts:\n\nInferential Approach: Suppose we want to understand how the number of bedrooms affects housing prices in a city. To do this, we would collect a sample of house sales that accurately represents the city’s entire housing market. For instance, we might use stratified sampling to ensure that we include houses from different neighborhoods in proportion to how common they are. After collecting the data, we would split it into training and test sets. The training set helps us build our model and estimate the relationship between variables, while the test set allows us to evaluate how well our findings generalize to new data.\nPredictive Approach: If our goal is to predict the selling price of a house based on its features (like size, number of bedrooms, and location), we would gather a large dataset of recent house sales. This data might come from a real estate database that tracks the details of each sale. Before we can use this data to train a model, we would clean it by filling in any missing information, converting data to a consistent format, and making sure all variables are ready for analysis. After preprocessing, we would split the data into training, validation, and test sets. The training set would be used to fit the model, the validation set to fine-tune it, and the test set to assess how well the model can predict prices for houses it hasn’t seen before.\n\nAs shown in Figure 1.3, the data collection and wrangling stage is fundamental to the workflow. It directly follows the study design and sets the stage for exploratory data analysis.\n\n\n\n\n\nFigure 1.3: Data collection and wrangling stage from the data science workflow in Figure 1.1. This stage is directly followed by exploratory data analysis and preceded by study design.\n\n\n\n1.2.3 Exploratory Data Analysis\nBefore diving into data modelling, it’s crucial to develop a deep understanding of the relationships between the variables in your training data. This is where the third stage of the data science workflow—Exploratory Data Analysis (EDA)—comes into play. EDA serves as a vital process that allows you to visualize and summarize your data, uncover patterns, detect anomalies, and test key assumptions that will inform your modelling decisions.\nThe first step in EDA is to classify your variables according to their types. This classification is essential because it guides your choice of analysis techniques and models. Specifically, you need to determine whether each variable is discrete or continuous, and whether it has any specific characteristics such as being bounded or unbounded.\n\n\nResponse Variable:\n\nDetermine if your response variable is discrete (e.g., binary, count-based, categorical) or continuous.\nIf it is continuous, consider whether it is bounded (e.g., percentages that range between 0 and 100) or unbounded (e.g., a variable like temperature that can take on a wide range of values).\n\n\n\nRegressors:\n\nFor each regressor, identify whether it is discrete or continuous.\nIf a regressor is discrete, classify it further as binary, count-based, or categorical.\nIf a regressor is continuous, determine whether it is bounded or unbounded.\n\n\n\nThis classification step ensures that you are prepared to choose the correct visualization and statistical methods for your analysis, as different types of variables often require different approaches.\nThis classification step ensures that you are prepared to choose the correct visualization and statistical methods for your analysis, as different types of variables often require different approaches.\nAfter classifying your variables, the next step is to create visualizations and calculate descriptive statistics using your training data. This involves coding plots that can reveal the underlying distribution of each variable and the relationships between them. For instance, you might create histograms to visualize distributions, scatter plots to explore relationships between continuous variables, and box plots to compare categorical variables against the response variable.\nAlongside these visualizations, it is important to calculate key descriptive statistics such as the mean, median, and standard deviation. These statistics provide a numerical summary of your data, offering insights into central tendency and variability. You might also use a correlation matrix to assess the strength of relationships between continuous variables.\nOnce you have generated these plots and statistics, they should be displayed in a clear and logical manner. The goal here is to interpret the data and draw preliminary conclusions about the relationships between variables. Presenting these findings effectively helps to uncover key insights and prepares you for the modelling stage.\nFinally, the insights gained from this exploratory analysis must be clearly articulated. This involves summarizing the key findings and considering their implications for the next stage of the workflow—data modelling. Observing patterns, correlations, and potential outliers in this stage will inform your modelling approach and ensure that it is grounded in a thorough and informed analysis.\nThis structured approach to EDA is visually summarized in Figure 1.4, illustrating the sequential steps from variable classification to the delivery of exploratory insights.\nExample: EDA for Housing Price Predictions\nTo illustrate the EDA process, let’s apply it to the example of predicting housing prices.\nWe start with variable classification:\n\nThe response variable is the sale price of a house, a continuous and unbounded variable.\nThe regressors include:\n\n\nNumber of bedrooms: Discrete, count-based.\n\nSquare footage: Continuous and unbounded.\n\nNeighborhood type: Discrete, categorical (e.g., urban, suburban, rural).\n\nProximity to schools: Continuous, potentially bounded by distance.\n\n\n\nOnce the variables are classified, we move on to coding plots and calculating descriptive statistics. Here are a couple of visualizations that can be helpful in this context:\n\nA histogram of sale prices helps visualize the distribution and spot any outliers.\nA scatter plot of square footage versus sale price shows the relationship, typically revealing a positive correlation.\n\nBox plots compare sale prices across different neighborhood types, highlighting any variations in median prices.\n\nDescriptive statistics like the mean and standard deviation provide a numerical summary, while a correlation matrix helps assess relationships between continuous variables like square footage and sale price.\n\nFinally, in displaying and interpreting results, these plots and statistics guide us in understanding the data:\n\nThe histogram might show most houses fall within a mid-range price.\nThe scatter plot could confirm that larger houses generally sell for more.\nBox plots may reveal that urban homes tend to have higher prices.\n\nThese exploratory insights help identify key predictors like square footage and neighborhood type, and highlight any outliers that may require further attention during modelling.\nBy following these steps, the EDA process in the housing price prediction example lays a solid foundation for effective modelling, ensuring that the key variables and their relationships are well understood.\n\n\n\n\n\nFigure 1.4: Exploratory data analysis stage from the data science workflow in Figure 1.1. This stage is directly followed by data modelling and preceded by data collection and wrangling.\n\n\n\n1.2.4 Data Modelling\n\n\n\n\n\nFigure 1.5: Data modelling stage from the data science workflow in Figure 1.1. This stage is directly preceded by exploratory data analysis. On the other hand, it is directly followed by estimation but indirectly with goodness of fit. If necessary, the goodness of fit stage could retake the process to data modelling.\n\n\n\n1.2.5 Estimation\n\n\n\n\n\nFigure 1.6: Estimation stage from the data science workflow in Figure 1.1. This stage is directly preceded by data modelling and followed by goodness of fit. If necessary, the goodness of fit stage could retake the process to data modelling and then to estimation.\n\n\n\n1.2.6 Goodness of Fit\n\n\n\n\n\nFigure 1.7: Goodness of fit stage from the data science workflow in Figure 1.1. This stage is directly preceded by estimation and followed by results. If necessary, the goodness of fit stage could retake the process to data modelling and then to estimation.\n\n\n\n1.2.7 Results\n\n\n\n\n\nFigure 1.8: Results stage from the data science workflow in Figure 1.1. This stage is directly followed by storytelling and preceded by goodness of fit.\n\n\n\n1.2.8 Storytelling\n\n\n\n\n\nFigure 1.9: Storytelling stage from the data science workflow in Figure 1.1. This stage preceded by results.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-regression-mindmap",
    "href": "book/01-intro.html#sec-regression-mindmap",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "\n1.3 Mind Map of Regression Analysis",
    "text": "1.3 Mind Map of Regression Analysis\nHaving defined the necessary statistical aspects to execute a proper supervised learning analysis, either inferential or predictive across its seven sequential phases, we must dig into the different approaches we might encounter in practice as regression models. The nature of our outcome of interest will dictate any given modelling approach to apply, depicted as clouds in Figure 1.10. Note these regression models can be split into two sets depending on whether the outcome of interest is continuous or discrete. Therefore, under a probabilistic view, identifying the nature of a given random variable is crucial in regression analysis.\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n        {{Overdispersed or &lt;br/&gt;Underdispersed &lt;br/&gt;Data}}\n          )Chapter 13: &lt;br/&gt;Generalized &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Generalized &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n        {{Zero Inflated &lt;br/&gt;Data}}\n          )Chapter 12: &lt;br/&gt;Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n      {{Categorical &lt;br/&gt;Outcome Y}}\n        {{Nominal &lt;br/&gt;Outcome Y}}\n          )Chapter 14: &lt;br/&gt;Multinomial &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Multinomial &lt;br/&gt;Outcome Y)\n        {{Ordinal &lt;br/&gt;Outcome Y}}\n          )Chapter 15: &lt;br/&gt;Ordinal &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Logistic &lt;br/&gt;Distributed &lt;br/&gt;Cumulative Outcome &lt;br/&gt;Probability)\n\n\n\n\n\n\n\n\nFigure 1.10: Regression analysis mind map depicting all modelling techniques to be explored in this book. Depending on the type of outcome \\(Y\\), these techniques are split into two large zones: discrete and continuous.\n\n\nThat said, we will go beyond OLS regression and explore further regression techniques. In practice, these techniques have been developed in the statistical literature to address practical cases where the OLS modelling framework and assumptions are not suitable anymore. Thus, throughout this block, we will cover (at least) one new regression model per lecture.\nAs we can see in the clouds of Figure 1.10, there are 13 regression models: 8 belonging to discrete outcomes and 5 to continuous outcomes. Each of these models is contained in a chapter of this book, beginning with the most basic regression tool known as ordinary least-squares in ?sec-ols. We must clarify that the current statistical literature is not restricted to these 13 regression models. The field of regression analysis is vast, and one might encounter more complex models to target certain specific inquiries. Nonetheless, I consider these models the fundamental regression approaches that any data scientist must be familiar with in everyday practice.\nEven though this book comprises 13 chapters, each depicting a different regression model, we have split these chapters into two major subsets: those with continuous outcomes and those with discrete outcomes.\n\n\n\n\nGelbart, Michael. 2017. “Data Science Terminology.” UBC MDS. Master of Data Science at the University of British Columbia. https://ubc-mds.github.io/resources_pages/terminology/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/02-stats-review.html",
    "href": "book/02-stats-review.html",
    "title": "2  Basic Cuisine: A Review on Probability and Frequentist Statistical Inference",
    "section": "",
    "text": "2.1 Basics of Probability\nThis chapter will delve into probability and frequentist statistical inference. We can view these sections as a quick review of introductory probability and statistics concepts. Moreover, this review will be important to understanding the philosophy of modelling parameter estimation as outlined in Section 1.2.5. Then, we will pave the way to the rationale behind statistical inference in the Results stage (as in Section 1.2.7) in our workflow from Figure 1.1. Note that we aim to explain all these statistical and probabilistic concepts in the most possible practical way via a made-up case study throughout this chapter (while still presenting useful theoretical admonitions as explained in Chapter 1). Note we will use an appropriate level of jargon and will follow the colour convention found in ?sec-dictionary along with the definition callout box.\nImagine you are an undergraduate engineering student. Moreover, last term, you just took and passed your first course in probability and statistics (inference included!) in an industrial engineering context. Moreover, as it could happen while taking an introductory course in probability and statistics, you used to feel quite overwhelmed by the large amount of jargon and formulas one had to grasp and use regularly for primary engineering fields such as quality control in a manufacturing facility. Population parameters, hypothesis testing, tests statistics, significance level, \\(p\\)-values, and confidence intervals (do not worry, our statistical/machine learning scheme will come in later in this review) were appearing here and there! And to your frustration, you could never find a statistical connection between all these inferential tools! Instead, you relied on mechanistic procedures when solving assignments or exam problems.\nFor instance, when performing hypothesis testing for a two-sample \\(t\\)-test, you struggled to reflect what the hypotheses were trying to indicate for the corresponding population parameters or how the test statistic was related to these hypotheses. Moreover, your interpretation of the resulting \\(p\\)-value and/or confidence interval was purely mechanical with the inherent claim:\nTruthfully, this whole mechanical way of doing statistics is not ideal in a teaching, research or industry environment. Along the same lines, the above situation should also not happen when we learn key statistical topics for the very first time as undergraduate students. That is why we will investigate a more intuitive way of viewing probability and its crucial role in statistical inference. This matter will help us deliver more coherent storytelling (as in Section 1.2.8) when presenting our results in practice during any regression analysis to our peers or stakeholders. Note that the role of probability also extends to model training (as in Section 1.2.5) when it comes to supervised learning and not just regarding statistical inference.\nHaving said all this, it is time to introduce a statement that is key when teaching hypothesis testing in an introductory statistical inference course:\nThat is quite a bold statement! Nonetheless, once one starts teaching statistical topics to audiences not entirely familiar with the usual field jargon, the idea of randomness always persists across many different tools. And, of course, regression analysis is not an exception at all since it also involves inference on population parameters of interest! This is why we have allocated this section in the textbook to explain core probabilistic and inferential concepts to pave the way to its role in regression analysis.\nFinally, even though this book has suggested reviews related to the basics of probability via different distributions and the fundamentals of frequentist statistical inference as stated in Audience and Scope, we will retake essential concepts as follows:\nWithout further ado, let us start with reviewing core concepts in probability via quite a tasty example.\nIn terms of regression analysis and its supervised learning counterpart (either on an inferential or predictive framework), probability can be viewed as the solid foundation on which more complex tools, including estimation and hypothesis testing, are built upon. Having said that, let us scaffold across all the necessary probabilistic concepts that will allow us to move forward into these more complex tools.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span>"
    ]
  },
  {
    "objectID": "book/02-stats-review.html#sec-basics-prob",
    "href": "book/02-stats-review.html#sec-basics-prob",
    "title": "2  Basic Cuisine: A Review on Probability and Frequentist Statistical Inference",
    "section": "",
    "text": "2.1.1 First Insights\nTo start building up our solid probabilistic foundation, we assume our data is coming from a given population or system of interest. Moreover, the population or system is assumed to be governed by parameters which, as data scientists or researchers, they are of our best interest to study. That said, the terms population and parameter will pave the way to our first statistical definitions.\n\n\nDefinition of population\n\n\nIt is a whole collection of individuals or items that share distinctive attributes. As data scientists or researchers, we are interested in studying these attributes, which we assume are governed by parameters. In practice, we must be as specific as possible when defining our given population such that we would frame our entire data modelling process since its very early stages. Examples of a population could be the following:\n\nChildren between the ages of 5 and 10 years old in states of the American West Coast.\nCustomers of musical vinyl records in the Canadian provinces of British Columbia and Alberta.\nAvocado trees grown in the Mexican state of Michoacán.\nAdult giant pandas in the Southwestern Chinese province of Sichuan.\nMature açaí palm trees from the Brazilian Amazonian jungle.\n\n\n\nImage by Eak K. via Pixabay.\n\nNote that the term population could be exchanged for the term system, given that certain contexts do not particularly refer to individuals or items. Instead, these contexts could refer to processes whose attributes are also governed by parameters. Examples of a system could be the following:\n\nThe production of cellular phones from a given model in a set of manufacturing facilities.\nThe sale process in the Vancouver franchises of a well-known ice cream parlour.\nThe transit cycle during rush hours on weekdays in the twelve lines of Mexico City’s subway.\n\n\n\n\n\nDefinition of parameter\n\n\nIt is a characteristic (numerical or even non-numerical, such as a distinctive category) that summarizes the state of our population or system of interest. Examples of a population parameter can be described as follows:\n\nThe average weight of children between the ages of 5 and 10 years old in states of the American west coast (numerical).\nThe variability in the height of the mature açaí palm trees from the Brazilian Amazonian jungle (numerical).\nThe proportion of defective items in the production of cellular phones in a set of manufacturing facilities (numerical).\nThe average customer waiting time to get their order in the Vancouver franchises of a well-known ice cream parlour (numerical).\nThe most favourite pizza topping of vegetarian adults between the ages of 30 and 40 years old in Edmonton (non-numerical).\n\n\n\nImage by meineresterampe via Pixabay.\n\nNote the standard mathematical notation for population parameters are Greek letters (for more insights, you can check ?sec-greek-alphabet). Moreover, in practice, these population parameter(s) of interest will be unknown to the data scientist or researcher. Instead, they would use formal statistical inference to estimate them.\n\n\nThe parameter definition points out a crucial fact in investigating any given population or system:\n\nOur parameter(s) of interest are usually unknown!\n\nGiven this fact, it would be pretty unfortunate and inconvenient if we eventually wanted to discover any significant insights about the population or system. Therefore, let us proceed to our so-called tasty example so we can dive into the need for statistical inference and why probability is our perfect ally in this parameter quest.\nImagine you are the owner of a large fleet of ice cream carts, around 900 to be exact. These ice cream carts operate across different parks in the following Canadian cities: Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montreal. In the past, to optimize operational costs, you decided to limit ice cream cones to only two items: vanilla and chocolate flavours, as in Figure 2.1.\n\n\n\n\n\nFigure 2.1: The two flavours of the ice cream cone you sell across all your ice cream carts: vanilla and chocolate. Image by tomekwalecki via Pixabay.\n\n\nNow, let us direct this whole case onto a more statistical and probabilistic field; suppose you have a well-defined overall population of interest for those above eight Canadian cities: children between 4 and 11 years old attending these parks during the Summer weekends. Of course, Summer time is coming this year, and you would like to know which ice cream cone flavour is the favourite one for this population (and by how much!). As a business owner, investigating ice cream flavour preferences would allow you to plan Summer restocks more carefully with your corresponding suppliers. Therefore, it would be essential to start collecting consumer data so the company can tackle this demand query.\nAlso, suppose there is a second query. For the sake of our case, we will call it a time query. As a critical component of demand planning, besides estimating which cone flavour is the most preferred one (and by how much!) for the above population of interest, the operations area is currently requiring a realistic estimation of the average waiting time from one customer to the next one in any given cart during Summer weekends. This average waiting time would allow the operations team to plan carefully how much stock each cart should have so there will not be any waste or shortage.\n\n\nImage by Icons8 Team via Unsplash.\n\nNote that the nature of the aforementioned time query is more related to a larger population. Therefore, we can define it as all our ice cream customers during the Summer weekends. Furthermore, this second definition would expand this query to our corresponding general ice cream customers, given the requirements of our operations team, and not all the children between 4 and 11 years old attending the parks during Summer weekends. Consequently, it is crucial to note that the nature of our queries will dictate how we define our population and our subsequent data modelling and statistical inference.\nSummer time represents the most profitable season from a business perspective, thus solving these above two queries is a significant priority for your company. Hence, you decide to organize a meeting with your eight general managers (one per Canadian city). Finally, during the meeting with the general managers, it was decided to do the following:\n\nFor the demand query, a comprehensive market study will be run on the population of interest across the eight Canadian cities right before next Summer; suppose we are currently in Spring.\nFor the time query, since the operations team has not previously recorded any historical data (surprisingly!), ALL vendor staff from 900 carts will start collecting data on the waiting time in seconds between each customer this upcoming Summer.\n\nWhen discussing study requirements for the marketing firm who would be in charge of it for the demand query, Vancouver’s general manager dares to state the following:\n\nSince we’re already planning to collect consumer data on these cities, let’s mimic a census-type study to ensure we can have the MOST PRECISE results on their preferences.\n\nOn the other hand, when agreeing on the specific operations protocol to start recording waiting times for all the 900 vending carts this upcoming Summer, Ottawa’s general manager provides a comment for further statistical food for thought:\n\nThe operations protocol for recording waiting times in the 900 vending carts looks too cumbersome to implement straightforwardly this upcoming Summer. Why don’t we select A SMALLER SET of waiting times between two general customers across the 900 ice cream carts in the eight cities to have a more efficient process implementation that would allow us to optimize operational costs?\n\nBingo! Ottawa’s general manager just nailed the probabilistic way of making inference on our population parameter of interest for the time query. Indeed, their comment was primarily framed from a business perspective of optimizing operational costs. Still, this fact does not take away a crucial insight on which statistical inference is built: a random sample (as in its corresponding definition). As for Vancouver’s general manager, ironically, their statement is NOT PRECISE (from an inferential point of view)! Mimicking a census-type study might not be the most optimal decision for the demand query given the time constraint and the potential size of its target population.\n\n\nHeads-up on the use random sampling with probabilistic foundations!\n\n\nLet us clarify things from the start, especially from a statistical perspective:\n\nRealistically, there is no cheap and efficient way to conduct a census-type study for either of the two queries.\n\nWe must rely on probabilistic random sampling, selecting two small subsets of individuals from our two populations of interest. This approach allows us to save both financial and operational resources compared to conducting a complete census. However, random sampling requires us to use various probabilistic and inferential tools to manage and report the uncertainty associated with the estimation of the corresponding population parameters, which will help us answer our initial main queries.\n\n\nImage by manfredsteger via Pixabay.\n\nTherefore, having said all this, let us assume that in this ice cream case, the company decided to go ahead with random sampling to answer both queries.\n\n\nMoving on to one of the core topics in this chapter, we can state that probability is viewed as the language to decode random phenomena that occur in any given population or system of interest. In our example, we have two random phenomena:\n\nFor the demand query, a phenomenon can be represented by the preferred ice cream cone flavour of any randomly selected child between 4 and 11 years old attending the parks of the above eight Canadian cities during the Summer weekends.\nRegarding the time query, a phenomenon of this kind can be represented by any randomly recorded waiting time between two customers during a Summer weekend in any of the above eight Canadian cities across the 900 ice cream carts.\n\nNow, let us finally define what we mean by probability along with the inherent concept of sample space.\n\n\nDefinition of probability\n\n\nLet \\(A\\) be an event of interest in a random phenomenon of a population or system of interest, whose all possible outcomes belong to a given sample space \\(S\\). Generally, the probability for this event \\(A\\) happening can be mathematically depicted as \\(P(A)\\). Moreover, suppose we observe the random phenomenon \\(n\\) times such as we were running some class of experiment, then \\(P(A)\\) is defined as the following ratio:\n\\[\nP(A) = \\frac{\\text{Number of times event $A$ is observed}}{n},\n\\tag{2.1}\\]\nas the \\(n\\) times we observe the random phenomenon goes to infinity.\nEquation 2.1 will always put \\(P(A)\\) in the following numerical range:\n\\[\n0 \\leq P(A) \\leq 1.\n\\]\n\n\n\n\nDefinition of sample space\n\n\nLet \\(A\\) be an event of interest in a random phenomenon of a population or system of interest. The sample space \\(S\\) of event \\(A\\) denotes the set of all the possible random outcomes we might encounter every time we randomly observe \\(A\\) such as we were running some class of experiment.\nNote each of these outcomes has a determined probability associated with them. If we add up all these probabilities, the probability of the sample space \\(S\\) will be one, i.e.,\n\\[\nP(S) = 1.\n\\tag{2.2}\\]\n\n\n\n2.1.2 Schools of Statistical Thinking\nNote the above definition for the probability of an event \\(A\\) specifically highlights the following:\n\n… as the \\(n\\) times we observe the random phenomenon goes to infinity.\n\nThe “infinity” term is key when it comes to understanding the philosophy behind the frequentist school of statistical thinking in contrast to its Bayesian counterpart. In general, the frequentist way of practicing statistics in terms of probability and inference is the approach we usually learn in introductory courses, more specifically when it comes to hypothesis testing and confidence intervals which will be explored in Section 2.3. That said, the Bayesian approach is another way of practicing statistical inference. Its philosophy differs in what information is used to infer our population parameters of interest. Below, we briefly define both schools of thinking.\n\n\nDefinition of frequentist statistics\n\n\nThis statistical school of thinking heavily relies on the frequency of events to estimate specific parameters of interest in a population or system. This frequency of events is reflected in the repetition of \\(n\\) experiments involving a random phenomenon within this population or system.\nUnder the umbrella of this approach, we assume that our governing parameters are fixed. Note that, within the philosophy of this school of thinking, we can only make precise and accurate predictions as long as we repeat our \\(n\\) experiments as many times as possible, i.e.,\n\\[\nn \\rightarrow \\infty.\n\\]\n\n\n\n\nDefinition of Bayesian statistics\n\n\nThis statistical school of thinking also relies on the frequency of events to estimate specific parameters of interest in a population or system. Nevertheless, unlike frequentist statistics, Bayesian statisticians use prior knowledge on the population parameters to update their estimations on them along with the current evidence they can gather. This evidence is in the form of the repetition of \\(n\\) experiments involving a random phenomenon. All these ingredients allow Bayesian statisticians to make inference by conducting appropriate hypothesis testings, which are designed differently from their mainstream frequentist counterpart.\n\n\nThe unique known portrait of Reverend Thomas Bayes according to O’Donnell, T. (1936), even though Bellhouse (2004) argues it might not be a Bayes’ portrait.\n\nUnder the umbrella of this approach, we assume that our governing parameters are random; i.e., they have their own sample space and probabilities associated to their corresponding outcomes. The statistical process of inference is heavily backed up by probability theory mostly in the form of the Bayes theorem (named after Reverend Thomas Bayes, an English statistician from the 18th century). This theorem uses our current evidence along with our prior beliefs to deliver a posterior distribution of our random parameter(s) of interest.\n\n\nLet us put the definitions for these two schools of statistical thinking into a more concrete example. We can use the demand query from our ice cream case as a starting point. More concretely, we can dig more into a standalone population parameter such as the probability that a randomly selected child between 4 and 11 years old, attending the parks of the above eight Canadian cities during the Summer weekends, prefers the chocolate-flavoured ice cream cone over the vanilla one. Think about the following two hypothetical questions:\n\nFrom a frequentist point of view, what is the estimated probability of preferring chocolate over vanilla after randomly surveying \\(n = 100\\) children from our population of interest?\nUsing a Bayesian approach, suppose the marketing team has found ten prior market studies on similar children populations on their preferred ice cream flavour (between chocolate and vanilla). Therefore, along with our actual random survey of \\(n = 100\\) children from our population of interest, what is the posterior estimation of the probability of preferring chocolate over vanilla?\n\nBy comparing the above (a) and (b), we can see one characteristic in common when it comes to the estimation of the probability of preferring chocolate over vanilla: both frequentist and Bayesian approaches rely on the gathered evidence coming from the random survey of \\(n = 100\\) children from our population of interest. On the one hand, the frequentist approach solely relies on observed data to estimate this single probability of preferring chocolate over vanilla. On the other hand, the Bayesian approach uses the observed data in conjunction with the prior knowledge provided by the ten estimated probabilities to deliver a whole posterior distribution (i.e., the posterior estimation) of the probability of preferring chocolate over vanilla.\n\n\nHeads-up on the debate between frequentist and Bayesian statistics!\n\n\nEven though most of us began our statistical journey in a frequentist framework, we might be tempted to state that a Bayesian paradigm for parameter estimation and inference is better than a frequentist one since the former only takes into account the observed evidence without the prior knowledge on our parameters of interest.\n\n\nImage by Manfred Steger via Pixabay.\n\nIn the statistical community, there could be a fascinating debate between the pros and cons of each school of thinking. That said, it is crucial to state that no paradigm is considered wrong! Instead, using a pragmatic strategy of performing statistics according to our data science context is more convenient.\n\n\n\n\nTip on further Bayesian and frequentist insights!\n\n\nLet us check the following two examples (aside from our ice cream case) to illustrate the above pragmatic way of doing things:\n\nTake the production of cellular phones from a given model in a set of manufacturing facilities as the context. Hence, one might find a frequentist estimation of the proportion of defective items as a quicker and more efficient way to correct any given manufacturing process. That is, we will sample products from our finalized batches and check their status (defective or non-defective, our observed evidence) to deliver a proportion estimation of defective items.\nNow, take a physician’s context. It would not make a lot of sense to study the probability that a patient develops a certain disease by only using a frequentist approach, i.e., looking at the current symptoms which account for the observed evidence. In lieu, a Bayesian approach would be more suitable to study this probability which uses the observed evidence combined with the patient’s history (i.e., the prior knowledge) to deliver our posterior belief on the disease probability.\n\n\n\nHaving said all this, it is important to reiterate that the focus of this textbook is purely frequentist in regards to data modelling in regression analysis. If you would like to explore the fundamentals of the Bayesian paradigm; Johnson, Ott, and Dogucu (2022) have developed an amazing textbook on the basic probability theory behind this school of statistical thinking along with a whole variety regression techniques including the parameter estimation rationale.\n\n2.1.3 The Random Variables\nAs we continue our frequentist quest to review the probabilistic insights related to parameter estimation and statistical inference, we will focus on our ice cream case while providing a comprehensive array of definitions. Many of these definitions are inspired by the work of Casella and Berger (2024) and Soch et al. (2024).\nEach time we introduce a new probabilistic or statistical concept, we will apply it immediately to this ice cream case, allowing for hands-on practice that meets the learning objectives of this chapter. It is important to pay close attention to the definition and heads-up admonitions, as they are essential for fully understanding how these concepts apply to the ice cream case. On the other hand, the tip admonitions are designed to offer additional theoretical insights that may interest you, but they can be skipped if you prefer.\n\n\nTable 2.1: Table containing the general statements, populations, and parameters of interest for our demand and time queries.\n\n\n\n\n\n\n\n\n\nDemand Query\nTime Query\n\n\n\nStatement\nWe would like to know which ice cream flavour is the favourite one (either chocolate or vanilla) and by how much.\nWe would like to know the average waiting time from one customer to the next one in any given ice cream cart.\n\n\nPopulation of interest\n\nChildren between 4 and 11 years old attending different parks in Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montreal during Summer weekends.\n\nAll our general customer-to-customer waiting times in the different parks of Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montreal during Summer weekends across the 900 ice cream carts.\n\n\nParameter\n\nProportion of individuals from the population of interest who prefer the chocolate flavour versus the vanilla flavour.\n\nAverage waiting time from one customer to the next one.\n\n\n\n\n\n\nTable 2.1 presents the general statements and populations of interest derived from our two queries: demand and time. It is important to note that these general statements are based on the storytelling we initiated in Section 2.1.1. In practice, summarizing the overarching statistical problem is essential. This will enable us to translate the corresponding issue into a specific statement and population, from which we can define the parameters we aim to estimate later in our statistical process.\nNow, recall that in our initial meeting with the general managers, Ottawa’s general manager provided valuable statistical insights regarding the foundation of a random sample. For the time query, they suggested selecting a smaller set of waiting times between two general customers across the 900 ice cream carts. We already addressed this process as sampling, more specifically random sampling in technical language.\nSimilarly, we can apply this concept to the demand query by selecting a subgroup of children aged 4 to 11 who are visiting different parks in these eight cities. Then, we can ask them about their favorite ice cream flavour, specifically whether they prefer chocolate or vanilla. It is important to note that we are not conducting any census-type studies; instead, we are carrying out two studies that heavily rely on sampling to estimate population parameters.\n\n\nImage by Manfred Stege via Pixabay.\n\nFurthermore, we want to ensure that our two groups of observations—both children and waiting times—are representative of their respective populations. So, how can we achieve this? The baseline key is through what we call simple random sampling. This process involves the following per query:\n\nFor the demand query, let us assume there are \\(N_D\\) observations in our population of interest. In a simple random sampling scheme, our random sample will consist of \\(n_D\\) observations (noting that \\(n_D &lt;&lt; N_D\\)), each having the same probability of being selected for our estimation and inferential purposes, which is given by \\(\\frac{1}{N_D}\\).\nFor the time query, assume there are \\(N_T\\) observations in our population of interest. Again, in a simple random sampling scheme, our random sample will consist of \\(n_T\\) observations (noting that \\(n_T &lt;&lt; N_T\\)), each having the same probability of selection for estimation and inferential purposes, which is \\(\\frac{1}{N_T}\\).\n\nWe can observe the concept of randomness reflected throughout the sampling schemes mentioned above. This aligns with what we referred to as random phenomena in both queries back in Section 2.1.1. Consequently, there should be a way to mathematically represent these phenomena, and the random variable is the starting point in this process.\n\n\nDefinition of random variable\n\n\nA random variable is a function where the input values correspond to real numbers assigned to events belonging to the sample space \\(S\\), and whose outcome is one of these real numbers after executing a given random experiment. For instance, a random variable (and its support, i.e., real numbers) is depicted with an uppercase such that\n\\[Y \\in \\mathbb{R}.\\]\n\n\nTo begin experimenting with random variables in this ice cream case, we need to define them clearly. It is important to be as clear as possible when defining random variables, and we should also remember to use uppercase letters as follows:\n\\[\n\\begin{align*}\nD_i &= \\text{A favourite ice cream flavour of a randomly surveyed $i$th child} \\\\\n& \\qquad \\text{between 4 and 11 years old attending the parks of} \\\\\n& \\qquad \\text{Vancouver, Victoria, Edmonton, Calgary,} \\\\\n& \\qquad \\text{Winnipeg, Ottawa, Toronto, and Montreal} \\\\\n& \\qquad \\text{during the Summer weekends} \\\\\n& \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\text{for $i = 1, \\dots, n_D.$}  \\\\\n\\\\\nT_j &= \\text{A randomly recorded $j$th waiting time in minutes between two} \\\\\n& \\qquad \\text{customers during a Summer weekend in any of the above} \\\\\n& \\qquad \\text{eight Canadian cities across the 900 ice cream carts} \\\\\n& \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\text{for $j = 1, \\dots, n_T.$}  \\\\\n\\end{align*}\n\\]\nNote that the demand query corresponds to the \\(i\\)th random variable \\(D_i\\), where the subindex \\(i\\) ranges from \\(1\\) to \\(n_D\\). The term \\(n_D\\) represents the size of our sample for this query and theoretically indicates the number of random variables we intend to observe from our population of interest during our sampling. On the other hand, for the time query, we have the \\(j\\)th random variable \\(T_j\\), with the subindex \\(j\\) ranging from \\(1\\) to \\(n_T\\). In the context of this query, \\(n_T\\) denotes the size of our respective sample and indicates how many random variables we plan to observe from our population of interest as part of our sampling.\nNow, \\(D_i\\) will require real numbers that correspond to potential outcomes derived from the specific demand sample space of ice cream flavour, which we can denote as \\(S_D\\). It is crucial to note that a given child from our population may prefer a flavour other than chocolate or vanilla—for example, strawberry, salted caramel, or pistachio. However, we are limited by our available flavour menu as a company. Therefore, we will restrict our survey question regarding these potential \\(n_D\\) surveyed children as follows:\n\\[\nd_i =\n\\begin{cases}\n1 \\qquad \\text{The surveyed child prefers chocolate.}\\\\\n0 \\qquad \\text{Otherwise.}\n\\end{cases}\n\\tag{2.3}\\]\nIn the modelling associated with Equation 2.3, an observed random variable \\(d_i\\) (thus, the lowercase) can only yield values of \\(1\\) if the surveyed child prefers chocolate and \\(0\\) otherwise. The term “otherwise” refers to any flavour other than chocolate, which, in our limited menu context, is vanilla!\nTo define the real numbers from a given waiting time sample space \\(S_T\\), associated with an observed random variable \\(t_j\\) (thus, the lowercase) measured in minutes, we need to establish a possible range for these waiting times. It would not make sense to have observed negative waiting times in this ice cream scenario; therefore, our lower bound for this range of potential values should be \\(0\\) minutes. However, we cannot set an upper limit on these waiting times since any ice cream vendor might need to wait for \\(1, 2, 3, \\ldots, 10, \\ldots, 20, \\ldots, 60, \\ldots\\) minutes for the next customer to arrive. In fact, it is possible to wait for a very long time, especially on a low sales day! Thus, the range of this observed random variable can be expressed as:\n\\[\nt_j \\in [0, \\infty),\n\\]\nwhere the \\(\\infty\\) symbol indicates no upper bound.\nAfter defining the possible values for our two random variables \\(D_i\\) and \\(T_j\\), we will now classify them correctly using further probabilistic definitions as shown below.\n\n\nDefinition of discrete random variable\n\n\nLet \\(Y\\) be a random variable whose support is \\(\\mathcal{Y}\\). If this support \\(\\mathcal{Y}\\) corresponds to a finite set or a countably infinite set of possible values, then \\(Y\\) is considered a discrete random variable.\nFor instance, we can encounter discrete random variables which could be classified as\n\n\nbinary (i.e., a finite set of two possible values),\n\ncategorical (either nominal or ordinal, which have a finite set of three or more possible values), or\n\ncounts (which might have a finite set or a countably infinite set of possible values as integers).\n\n\n\nImage by Pexels via Pixabay.\n\n\n\n\n\nDefinition of continuous random variable\n\n\nLet \\(Y\\) be a random variable whose support is \\(\\mathcal{Y}\\). If this support \\(\\mathcal{Y}\\) corresponds to an uncountably infinite set of possible values, then \\(Y\\) is considered a continuous random variable.\nNote a continuous random variable could be\n\n\ncompletely unbounded (i.e., its set of possible values goes from \\(-\\infty\\) to \\(\\infty\\) as in \\(-\\infty &lt; y &lt; \\infty\\)),\n\npositively unbounded (i.e., its set of possible values goes from \\(0\\) to \\(\\infty\\) as in \\(0 \\leq y &lt; \\infty\\)),\n\nnegatively unbounded (i.e., its set of possible values goes from \\(-\\infty\\) to \\(0\\) as in \\(-\\infty &lt; y \\leq 0\\)), or\n\nbounded between two values \\(a\\) and \\(b\\) (i.e., its set of possible values goes from \\(a\\) to \\(b\\) as in \\(a \\leq y \\leq b\\)).\n\n\n\nImage by arielrobin via Pixabay.\n\n\n\nTherefore, we can classify our two random variables as follows:\n\nFor the demand query, the support of \\(D_i\\) (denoted as \\(\\mathcal{D}\\)) is a countable finite set with two possible values: \\(d_i \\in \\{0, 1\\}\\), as noted by Equation 2.3. Therefore, \\(D_i\\) is categorized as a binary discrete random variable.\nFor the time query, the support of \\(T_j\\) (denoted as \\(\\mathcal{T}\\)) is positively unbounded. This results in an uncountably infinite set of values that \\(T_j\\) can take, including (but not limited to!) \\(0, \\dots, 0.01, \\ldots, 0.02, \\ldots, 0.00234, \\ldots, 1, \\ldots, 1.5576, \\ldots\\) minutes. Therefore, \\(T_j\\) is classified as a positively unbounded continuous random variable.\n\nSo far, we have successfully translated our two statistical queries into proper random variables, along with clear definitions and classifications derived from our problem statements, as well as the populations of interest, as noted in Table 2.1. However, we still need to find a way to include our parameters. The upcoming section will allow us to do that.\n\n2.1.4 The Wonders of Generative Modelling and Probability Distributions\nBefore exploring the wonders of generative models, let us introduce Table 2.2, an extension of Table 2.1 that now includes the elements discussed in Section 2.1.3.\n\n\nTable 2.2: Table containing the general statements, populations, parameters of interest for our demand and time queries.\n\n\n\n\n\n\n\n\n\nDemand Query\nTime Query\n\n\n\nStatement\nWe would like to know which ice cream flavour is the favourite one (either chocolate or vanilla) and by how much.\nWe would like to know the average waiting time from one customer to the next one in any given ice cream cart.\n\n\nPopulation of interest\n\nChildren between 4 and 11 years old attending different parks in Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montreal during Summer weekends.\n\nAll our general customer-to-customer waiting times in the different parks of Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montreal during Summer weekends across the 900 ice cream carts.\n\n\nParameter\n\nProportion of individuals from the population of interest who prefer the chocolate flavour versus the vanilla flavour.\n\nAverage waiting time from one customer to the next one.\n\n\nRandom variable\n\n\\(D_i\\) for \\(i = 1, \\dots, n_D\\).\n\n\\(T_j\\) for \\(j = 1, \\dots, n_T\\).\n\n\nRandom variable definition\nA favourite ice cream flavour of a randomly surveyed \\(i\\)th child between 4 and 11 years old attending the parks of Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montreal during the Summer weekends.\nA randomly recorded \\(j\\)th waiting time in minutes between two customers during a Summer weekend across the 900 ice cream carts found in Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montreal.\n\n\nRandom variable type\nDiscrete and binary.\nContinuous and positively unbounded.\n\n\nRandom variable support\n\n\\(d_i \\in \\{ 0, 1\\}\\) as in Equation 2.3.\n\\(t_j \\in [0, \\infty).\\)\n\n\n\n\n\n\nHaving summarized all our probabilistic elements in Table 2.2, the parameters of interest must come into play for our data modelling game! Hence, the question is:\n\nIs there any feasible way to do so via the the foundations of random variables?\n\nThe answer lies in what we call a generative model, for which we have a whole toolbox corresponding to another important concept called probability distributions, as shown below.\n\n\nDefinition of generative model\n\n\nSuppose you observe some data \\(y\\) from a population or system of interest. Moreover, let us assume this population or system is governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nIf we state that the random variable \\(Y\\) follows certain probability distribution \\(\\mathcal{D}(\\cdot)\\), then we will have a generative model \\(m\\) such that\n\\[\nm: Y \\sim \\mathcal{D}(\\boldsymbol{\\theta}).\n\\]\n\n\nImage by Manfred Stege via Pixabay.\n\n\n\n\n\nDefinition of probability distribution\n\n\nWhen we set a random variable \\(Y\\), we also set a new set of \\(v\\) possible outcomes \\(\\mathcal{Y} = \\{ y_1, \\dots, y_v\\}\\) coming from the sample space \\(S\\). This new set of possible outcomes \\(\\mathcal{Y}\\) corresponds to the support of the random variable \\(Y\\) (i.e., all the possible values that could be taken on once we execute a given random experiment involving \\(Y\\)).\nThat said, let us suppose we have a sample space of \\(u\\) elements defined as\n\\[\nS = \\{ s_1, \\dots, s_u \\},\n\\]\nwhere each one of these elements has a probability assigned via a function \\(P_S(\\cdot)\\) such that\n\\[\nP(S) = \\sum_{i = 1}^u P_S(s_i) = 1.\n\\]\nwhich has to satisfy Equation 2.2.\nThen, the probability distribution of \\(Y\\), i.e., \\(P_Y(\\cdot)\\) assigns a probability to each observed value \\(Y = y_j\\) (with \\(j = 1, \\dots, v\\)) if and only if the outcome of the random experiment belongs to the sample space, i.e., \\(s_i \\in S\\) (for \\(i = 1, \\dots, u\\)) such that \\(Y(s_i) = y_j\\):\n\\[\nP_Y(Y = y_j) = P \\left( \\left\\{ s_i \\in S : Y(s_i) = y_j \\right\\} \\right).\n\\]\n\n\nSince we have two different queries, we will use two instances of generative models. It is worth noting that more complex modelling could refer to a single generative model. However, for the purposes of this review chapter, we will keep it simple with via two separate generative models.\nNow, let us introduce a specific notation for our discussion: the Greek alphabet. Greek letters are frequently used to statistically represent population parameters in modelling setups, estimation, and statistical inference. These letters will be quite useful for our parameters in this ice cream case!\n\n\nTip on the Greek alphabet in statistics!\n\n\nIn the early stages of learning statistical modelling, including concepts such as regression analysis, it is common to feel overwhelmed by unfamiliar letters and terminology. Whenever confusion arises in any of the main chapters of this book regarding these letters, we recommend referring to the Greek alphabet shared by ?sec-greek-alphabet. It is important to note that frequentist statistical inference primarily uses lowercase letters. With consistent practice over time, you will likely memorize most of this alphabet!\n\n\nImage by meineresterampe via Pixabay.\n\n\n\nLet us retake the row corresponding to parameters in Table 2.2 and assign their corresponding Greek letters:\n\nFor the demand query, we are interested in the parameter \\(\\pi\\), which represents the proportion of individuals from the children population who prefer the chocolate flavour over the vanilla flavour. It is crucial to note that a proportion is always bounded between \\(0\\) and \\(1\\), similar to how probabilities function! For instance, a proportion of \\(0.2\\) would mean that \\(20\\%\\) of the children in our population prefer chocolate flavour over vanilla. This definition establishes our demand query parameter as follows:\n\n\\[\n\\pi \\in [0, 1].\n\\]\n\n\nHeads-up on the use of \\(\\pi\\)!\n\n\nIn this textbook, unless stated otherwise, the letter \\(\\pi\\) will denote a population parameter and not the mathematical constant \\(3.141592...\\)\n\n\n\nFor the time query, we are interested in the parameter \\(\\beta\\), which represents the average waiting time in minutes from one customer to the next one in our population of interest. Unlike the above \\(\\pi\\) parameter, \\(\\beta\\) is only positively unbounded given the definition of our random variable \\(T_j\\). Therefore, this definition establishes our time query parameter as follows:\n\n\\[\n\\beta \\in (0, \\infty).\n\\]\nHaving defined our parameters of interest with proper lowercase Greek letters, it is time to declare our corresponding generative models on a general basis. For the demand query, there will be a single parameter called \\(\\pi\\), where the randomly surveyed child \\(D_i\\) will follow the model \\(m_D\\) such that\n\\[\n\\begin{gather*}\nm_D : D_i \\sim \\mathcal{D}_D(\\pi) \\\\\n\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\text{for $i = 1, \\dots, n_D.$}\n\\end{gather*}\n\\tag{2.4}\\]\nNow, for the time query, there will also be a single parameter called \\(\\beta\\). Thus, the randomly recorded waiting time \\(T_j\\) will follow the model \\(m_T\\) such that\n\\[\n\\begin{gather*}\nm_T : T_j \\sim \\mathcal{D}_T(\\beta) \\\\\n\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\text{for $j = 1, \\dots, n_T.$}\n\\end{gather*}\n\\tag{2.5}\\]\nNonetheless, we might wonder the following:\n\nHow can we determine the corresponding distributions \\(\\mathcal{D}_D(\\pi)\\) and \\(\\mathcal{D}_T(\\beta)\\)?\n\nOf course the above definition of a probability distribution will come in handy to resolve this question. That said, given that we have two types of random variables (discrete and continuous), it is necessary to introduce two specific types of probability functions: probability mass function (PMF) and probability density function (PDF).\n\n\nDefinition of probability mass function\n\n\nLet \\(Y\\) be a discrete random variable whose support is \\(\\mathcal{Y}\\). Moreover, suppose that \\(Y\\) has a probability distribution such that\n\\[\nP_Y(Y = y) : \\mathbb{R} \\rightarrow [0, 1]\n\\]\nwhere, for all \\(y \\notin \\mathcal{Y}\\), we have\n\\[\nP_Y(Y = y) = 0\n\\]\nand\n\\[\n\\sum_{y \\in \\mathcal{Y}} P_Y(Y = y) = 1.\n\\tag{2.6}\\] Then, \\(P_Y(Y = y)\\) is considered a PMF.\n\n\nAs we have discussed throughout this ice cream case, let us begin with the demand query. We have already defined the \\(i\\)th random variable \\(D_i\\) as discrete and binary. In statistical literature, certain random variables in common random processes can be modelled using what we call parametric families. We refer to these tools as parametric families because they are characterized by a specific set of parameters (in our case, each query has a single-element set, such as \\(\\pi\\) or \\(\\beta\\)).\nMoreover, we call them families since each member corresponds to a particular value of our parameter(s). For instance, in our demand query, a chosen member could be where \\(\\pi = 0.8\\) within the respective chosen parametric family to model our surveyed children. Other possible members could correspond to \\(\\pi = 0.2\\), \\(\\pi = 0.4\\) or \\(\\pi = 0.6\\). In fact, the number of members in our chosen parametric family is infinite in this demand query!\n\nTherefore, what parametric family can we choose for our demand query?\n\nThe question above introduces a new, valuable resource that is further elaborated upon in ?sec-distributional-mind-map. This resource outlines the various distributions that will be utilized in this textbook. In reality, the realm of parametric families—specifically, distributions—is quite extensive, and this material serves as only a brief overview of the many parametric families documented in statistical literature.\n\n\nTip on data modelling alternatives via different parametric families!\n\n\nAny data model is simply an abstraction of reality, and different parametric families can provide various alternatives for modelling. In practice, we often need to select a specific family based on our particular inquiries and the conditions of our data. This process requires time and experience to master. Furthermore, it is important to note that different families are often interconnected!\n\n\nImage by Manfred Stege via Pixabay.\n\nIf you wish to explore the world of univariate distribution families—which are used to model a single random variable—Leemis (n.d.) has created a comprehensive relational chart that covers 76 distinct probability distributions: 19 are discrete, and 57 are continuous. However, this chart does not encompass all the possible families that one might encounter in statistical literature (you can check another list at the end of this section).\n\n\nReferring back to our discussion about ?sec-distributional-mind-map, it is time to choose the most suitable parametric family for a discrete and a binary random variable, such as the \\(i\\)th random variable \\(D_i\\). A particular case we can examine is the Bernoulli distribution (also, commonly known as a Bernoulli trial). The Bernoulli distribution applies to a discrete random variable that can take one of two values: \\(0\\), which we refer to as a failure, and \\(1\\), identified as a success. This aligns with our previous definition from Equation 2.3:\n\\[\nd_i =\n\\begin{cases}\n1 \\qquad \\text{The surveyed child prefers chocolate.}\\\\\n0 \\qquad \\text{Otherwise.}\n\\end{cases}\n\\]\nThe equation above defines the chocolate preference of the \\(i\\)th surveyed child as a success, while another flavour—specifically vanilla in the context of our limited menu—is categorized as a failure. Thus, we can denote the support as \\(d_i \\in \\{0, 1\\}\\).\nWe need to define our population parameter for this demand query in the context of a Bernoulli trial, which is denoted by \\(\\pi \\in [0, 1]\\). This represents the proportion of children who prefer the chocolate flavour over the vanilla flavour. In a Bernoulli trial, this parameter refers to the probability of success. Lastly, we can specify our generative model accordingly:\n\\[\n\\begin{gather*}\nm_D : D_i \\sim \\text{Bern}(\\pi) \\\\\n\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\text{for $i = 1, \\dots, n_D.$}\n\\end{gather*}\n\\]\nIt is time to start with formal equations! We need to define the PMF corresponding to the above generative model. The statistical literature assigns the following PMF for the a Bernoulli trial \\(D_i\\):\n\\[\nP_{D_i} \\left( D_i = d_i \\mid \\pi \\right) = \\pi^{d_i } (1 - \\pi)^{1 - d_i } \\quad \\text{for $d_i \\in \\{ 0, 1 \\}$.}\n\\tag{2.7}\\]\nA further question arises regarding whether Equation 2.7 satisfies the condition of the total probability of the sample space defined in the Equation 2.6 under the definition of a PMF. This condition states that a valid PMF should result in a total probability equal to one when we sum all the probabilities produced by this function over every possible value that the random variable can take.\nHence, we can state Equation 2.7 is a proper probability distribution (i.e., all the standalone probabilities over the support of \\(D_i\\) add up to one) given that:\n\nProof. \\[\n\\begin{align*}\n\\sum_{d_i = 0}^1 P_{D_i} \\left( D_i = d_i \\mid \\pi \\right) &=  \\sum_{d_i = 0}^1 \\pi^{d_i} (1 - \\pi)^{1 - d_i}  \\\\\n&= \\underbrace{\\pi^0}_{1} (1 - \\pi) + \\pi \\underbrace{(1 - \\pi)^{0}}_{1} \\\\\n&= (1 - \\pi) + \\pi \\\\\n&= 1. \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\tag{2.8}\\]\n\nIndeed, this Bernoulli PMF is a proper probability distribution!\n\n\nThe probability distribution, obtained from Equation 2.8, is summarized in Table 2.3. Note that the chocolate preference has a probability equal to \\(\\pi\\), whereas the vanilla preference corresponds to the complement \\(1 - \\pi\\). This probability arrangement completely fulfils the corresponding probability condition of the sample space seen in Equation 2.6.\n\n\nTable 2.3: Probability distribution for the \\(i\\)th Bernoulli trial \\(D_i\\).\n\n\n\n\\(d_i\\)\n\\(P_{D_i} \\left( D_i = d_i \\mid \\pi \\right)\\)\n\n\n\n\\(0\\)\n\\(1 - \\pi\\)\n\n\n\\(1\\)\n\\(\\pi\\)\n\n\n\n\n\n\nTo proceed with the time query, we need to analyze the \\(j\\)th continuous random variable \\(T_j\\) and subsequently work with a PDF.\n\n\nDefinition of probability density function\n\n\nLet \\(Y\\) be a continuous random variable whose support is \\(\\mathcal{Y}\\). Furthermore, consider a function \\(f_Y(y)\\) such that\n\\[\nf_Y(y) : \\mathbb{R} \\rightarrow \\mathbb{R}\n\\]\nwith\n\\[\nf_Y(y) \\geq 0.\n\\]\nThen, \\(f_Y(y)\\) is considered a PDF if the probability of \\(Y\\) taking on a value within the range represented by the subset \\(A \\subset \\mathcal{Y}\\) is equal to\n\\[\nP_Y(Y \\in A) = \\int_A f_Y(y) \\mathrm{d}y\n\\]\nwith\n\\[\n\\int_{\\mathcal{Y}} f_Y(y) \\mathrm{d}y = 1.\n\\tag{2.9}\\]\n\n\nTo begin our second analysis, let us examine the nature of the variable \\(T_j\\) represented as a continuous random variable. This variable is nonnegative, meaning it is positively unbounded, as it models a waiting time. We can interpret \\(T_j\\) as the waiting time until a specific event of interest occurs, such as when the next customer arrives at the ice cream cart. In statistical literature, this is commonly referred to as a survival time. Hence, we might wonder:\n\nWhat is the most suitable parametric family to model a survival time?\n\n\n\nImage by Manfred Stege via Pixabay.\n\nWell, in this case within our textbook and in general in statistical literature, there is more than one alternative to model a continuous and nonnegative survival time. ?sec-distributional-mind-map offers four possible ways:\n\n\nExponential. A random variable with a single parameter that can come in either of the following forms:\n\nAs a rate \\(\\lambda \\in (0, \\infty)\\), which generally defines the mean number of events of interest per time interval or space unit.\nAs a scale \\(\\beta \\in (0, \\infty)\\), which generally defines the mean time until the next event of interest occurs.\n\n\n\nWeibull. A random variable that is a generalization of the Exponential distribution. Note its distributional parameters are the scale continuous parameter \\(\\beta \\in (0, \\infty)\\) and shape continuous parameter \\(\\gamma \\in (0, \\infty)\\).\n\nGamma A random variable whose distributional parameters are the shape continuous parameter \\(\\eta \\in (0, \\infty)\\) and scale continuous parameter \\(\\theta \\in (0, \\infty)\\).\n\nLognormal. A random variable whose logarithmic transformation yields a Normal distribution. Its distributional parameters are the Normal location continuous parameter \\(\\mu \\in (-\\infty, \\infty)\\) and Normal scale continuous parameter \\(\\sigma^2 \\in (0, \\infty)\\).\n\nIn our context, as summarized in the corresponding generative model, it is in our best interest to select a probability distribution characterized by a single parameter. Therefore, the Exponential distribution is the most suitable choice for our current time query, particularly under the scale parametrization, since we aim to estimate the waiting time between two customers.\n\n\nHeads-up on survival analysis!\n\n\nAlthough our ice cream case can be straightforwardly modelled using an Exponential distribution for our time query, by using a single population parameter which indicates a mean waiting time between two customers, it is important to stress that other distributions, such as the Weibull, Gamma, or Lognormal, are also entirely valid options. In fact, utilizing these distributions, that involve more than just a standalone parameter, can enhance the flexibility of our data modelling!\n\n\nImage by toushirou_px via Pixabay.\n\nAdditionally, there is a specialized statistical field focused on modelling waiting times—specifically, the time until an event of interest occurs. These types of times are formally referred to as survival times, and the associated field is known as survival analysis. It is worth noting that regression analysis can be extended to this area, and ?sec-parametric-survival will provide a more in-depth exploration of various parametric models that involve the Exponential, Weibull, and Lognormal distributions.\n\n\nSince we are using an Exponential distribution, we need to establish our population parameter for this time query. As mentioned in Table 2.2, this parameter refers to the average (or mean) waiting time from one customer to the next. This corresponds to a scale parametrization, where the parameter \\(\\beta \\in (0, \\infty)\\) defines the mean time until the next event of interest occurs (in this case, the next customer!). Therefore, we can specify our generative model as follows:\n\\[\n\\begin{gather*}\nm_T : T_j \\sim \\text{Exponential}(\\beta) \\\\\n\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\text{for $j = 1, \\dots, n_T.$}\n\\end{gather*}\n\\]\nSince \\(T_j\\) is a continuous random variable, we must define the PMF corresponding to the above generative model. The statistical literature assigns the following PDF for \\(T_j\\):\n\\[\nf_{T_j} \\left(t_j \\mid \\beta \\right) = \\frac{1}{\\beta} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\quad \\text{for $t_j \\in [0, \\infty )$.}\n\\tag{2.10}\\]\nNow, we might wonder whether Equation 2.10 satisfies the condition of the total probability of the sample space defined in the Equation 2.9 under the definition of a PDF. This condition states that a valid PDF should result in a total probability equal to one when we integrate this function over all the support of \\(T_j\\).\nThus, we can state that Equation 2.10 is a proper probability distribution (i.e., Equation 2.10 integrates to one over the support of \\(T_j\\)) given that:\n\nProof. \\[\n\\begin{align*}\n\\int_{t_j = 0}^{t_j = \\infty} f_{T_j} \\left(t_j \\mid \\beta \\right) \\mathrm{d}y &= \\int_{t_j = 0}^{t_j = \\infty} \\frac{1}{\\beta} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j \\\\\n&= \\frac{1}{\\beta} \\int_{t_j = 0}^{t_j = \\infty} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j \\\\\n&= - \\frac{\\beta}{\\beta} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\Bigg|_{t_j = 0}^{t_j = \\infty} \\\\\n&= - \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\Bigg|_{t_j = 0}^{t_j = \\infty} \\\\\n&= - \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\\\\n&= - \\left( 0 - 1 \\right) \\\\\n&= 1. \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\tag{2.11}\\]\n\nIndeed, the Exponential PDF, under a scale parametrization, is a proper probability distribution!\n\n\nUnlike our demand query, which features a table illustrating the PMF for \\(D_i \\in \\{ 0, 1 \\}\\) (see Table 2.3), it is not feasible to create a table for the PDF of \\(T_j \\in [0, \\infty)\\) because it represents an uncountably infinite set of possible values. However, we can plot the corresponding PDF using three specific members of the Exponential parametric family as examples. Figure 2.2 presents these three example members, with scale parameters values of \\(\\beta = 0.25, 0.5, 1\\) minutes, representing waiting times through their corresponding PDFs. Based on our findings in Equation 2.11, we know that the area under these three density plots equals one, indicating the total probability of the sample space. Additionally, it is important to note that as we increase the scale parameter, larger observed values \\(t_j\\) become more probable.\n\n\n\n\n\n\n\nFigure 2.2: Some members of the Exponential family with scale parametrization.\n\n\n\n\n\n2.1.5 Characterizing Probability Distributions\nSo far, we have been exploring the basics of random variables, as well as the importance of generative modelling and probability distributions in addressing different data inquiries. These concepts are fundamental to understanding the population parameter setup before we actually collect data and solve these inquiries to create effective storytelling. Therefore, before we delve into those stages, however, we need to identify and explain efficient ways to summarize probability distributions. This will help us make our storytelling compelling for a general audience, as we will discuss further.\n\n\nHeads-up on coding tabs!\n\n\nYou may be wondering:\n\nWhere do we begin with some R or Python code?\n\nIt is time to introduce our very first lines of code and provide some explanations about the coding approach in this book. As implied in the Preface, our goal is to make this book “bilingual,” meaning that all hands-on coding practices can be performed in either R or Python. Whenever we present a specific proof of concept or data modelling exercise, you will find two tabs: one for R and another for Python. We will first show the input code, followed by the output.\n\n\nImage by Manfred Stege via Pixabay.\n\nWith this format, you can choose your coding journey based on your language preferences and interests as you progress through the book.\n\n\nAlright! Moving forward with the code, we need to work with some simulated populations to create the corresponding proofs of concept in this section and the subsequent ones. Let us start with our demand query. We will consider a population size of \\(N_D = 2,000,000\\) children. The code (in either R or Python) below assigns this value as N_D, along with a simulation seed to ensure our results are reproducible. Additionally, for the simulation purposes related to our generative modelling, we will assume that 65% of these children prefer chocolate over vanilla (i.e., \\(\\pi = 0.65\\)).\n\n\nHeads-up on real and unknown parameters!\n\n\nAlthough we are assigning a value of \\(\\pi = 0.65\\) as our true population parameter in this query, we can never know the exact value in practice unless we conduct a full census. This is why we rely on probabilistic tools, via random sampling and statistical inference, to estimate this \\(\\pi\\).\n\n\nLet us recall that we are assuming each child is modelled as a Bernoulli trial, where a success (denoted as 1) indicates that the child “prefers chocolate.” This also reflects the flavour mapping in the code. Furthermore, instead of using a Bernoulli random number generator, we are utilizing a Binomial random number generator. This is because the Binomial case with parameters \\(n = 1\\) and \\(\\pi\\) is equivalent to a Bernoulli trial with parameter \\(pi\\). Hence, in general, consider the following general Binomial case:\n\\[\nY \\sim \\text{Bin}(n = 1, \\pi),\n\\]\nwhose PMF is simplified as a Bernoulli given that\n\\[\n\\begin{align*}\nP_Y \\left( Y = y \\mid n = 1, \\pi \\right) &= {1 \\choose y} \\pi^y (1 - \\pi)^{1 - y} \\\\\n&= \\underbrace{\\frac{1!}{y!(1 - y)!}}_{\\text{$1$ for $y \\in \\{ 0, 1 \\}$}} \\pi^y (1 - \\pi)^{1 - y} \\\\\n&= \\pi^y (1 - \\pi)^{1 - y} \\\\\n& \\qquad \\qquad \\qquad \\qquad \\qquad \\text{for $y \\in \\{ 0, 1 \\}$.}\n\\end{align*}\n\\]\nThe final output of this quick simulation, which models a population of children as Bernoulli trials with a probability of success \\(\\pi = 0.65\\), consists of a data frame containing \\(N_D = 2,000,000\\) rows, with each row representing a child and their preferred ice cream flavor: either chocolate or vanilla. It is worth noting that the outputs from both R and Python differ due to the fact that each language employs different pseudo-random number generators.\n\n\nR Code\nPython Code\n\n\n\nset.seed(123)  # Seed for reproducibility\n\n# Population size\nN_D &lt;- 2000000\n\n# Simulate binary outcomes: 1 = chocolate, 0 = vanilla\nflavour_bin &lt;- rbinom(N_D, size = 1, prob = 0.65)\n\n# Map binary to flavour names\nflavours &lt;- ifelse(flavour_bin == 1, \"chocolate\", \"vanilla\")\n\n# Create data frame\nchildren_pop &lt;- data.frame(\n  children_ID = 1:N_D,\n  fav_flavour = flavours\n)\n\n# Showing the first 100 children of the population\nhead(children_pop, n = 100) \n\n\n# Importing libraries\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(123)   # Seed for reproducibility\n\n# Population size\nN_D = 2000000\n\n# Simulate binary outcomes: 1 = chocolate, 0 = vanilla\nflavour_bin = np.random.binomial(n = 1, p = 0.65, size = N_D)\n\n# Map binary to flavour names\nflavours = np.where(flavour_bin == 1, \"chocolate\", \"vanilla\")\n\n# Create data frame\nchildren_pop = pd.DataFrame({\n    \"children_ID\": np.arange(1, N_D + 1),\n    \"fav_flavour\": flavours\n})\n\n# Showing the first 100 children of the population\nprint(children_pop.head(100))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Code\nPython Code\n\n\n\nset.seed(123)  # Seed for reproducibility\n\n# Population size\nn &lt;- 500000\n\n# In R, 'rate' is 1 / scale and rounding to two decimal places\nwaiting_times &lt;- round(rexp(n, rate = 1 / 10), 2)\n\n# Create data frame\nwaiting_pop &lt;- data.frame(\n  time_ID = 1:200,\n  waiting_time = waiting_times\n)\n\n# Showing the first 100 waiting times of the population\nhead(waiting_pop, n = 100)\n\n\nnp.random.seed(123)  # Seed for reproducibility\n\n# Population size\nn = 500000\n\n# Simulate waiting times\nwaiting_times = np.round(np.random.exponential(scale = 10, size = n), 2)\n\n# Create DataFrame\nwaiting_pop = pd.DataFrame({\n    \"time_ID\": np.arange(1, n + 1),\n    \"waiting_time\": waiting_times\n})\n\n# Showing the first 100 waiting times of the population\nprint(waiting_pop.head((100))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, imagine that the data collection and analysis for the ice cream case have progressed further into the future. You have a follow-up meeting with your eight general managers, one from each Canadian city, to discuss how to address the statements related to both the demand and time queries as depicted in Table 2.2, in relation to our populations of interest. Additionally, suppose you have collected data from \\(n_D = 500\\) randomly surveyed children across these eight Canadian cities.\n\n\nR Code\nPython Code\n\n\n\nset.seed(123)  # Seed for reproducibility\n\n# Simple random sample of 500 children\nchildren_sample &lt;- children_pop[sample(1:nrow(children_pop), 500), ]\n\n# Showing the first 100 sampled children\nhead(children_sample, n = 100)\n\n\nnp.random.seed(123)  # Seed for reproducibility\n\n# Simple random sample of 500 rows\nchildren_sample = children_pop.sample(n = 500)\n\n# Showing the first 100 sampled children\nprint(children_sample.head(100))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlso, you have sampled data on \\(n_T = 200\\) randomly recorded waiting times between customers across our 900 ice cream carts in the same cities.\n\n\nR Code\nPython Code\n\n\n\nset.seed(123)  # Seed for reproducibility\n\n# Simple random sample of 200 waiting times\nwaiting_sample &lt;- waiting_pop[sample(1:nrow(waiting_pop), 200), ]\n\n# Showing the first 100 sampled waiting times\nhead(waiting_pop, n = 100)\n\n\nnp.random.seed(123)  # Seed for reproducibility\n\n# Simple random sample of 200 waiting times\nwaiting_sample = waiting_pop.sample(n = 500)\n\n# Showing the first 100 sampled waiting times\nprint(waiting_sample.head(100))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn terms of the executive meeting with the eight general managers, it would not be an efficient use of time to go individually over these \\(n_D = 500\\) and \\(n_T = 1000\\) data points along with abstract mathematical concepts such as PMFs or PDFs, as well as probabilistic definitions of random variables and parameters represented by Greek letters. Instead, there should be a more straightforward and simple way to explain how these random variables behaved during our data collection process. The key to addressing this complexity lies in understanding measures of central tendency and uncertainty.\n\n\nImage by Manfred Stege via [Pixabay](https://pixabay.com/vectors/pixel-cells-lecture-lecturer-3976299/.\n\n\n\nDefinition of measure of central tendency\n\n\nProbabilistically, a measure of central tendency is defined as a metric that identifies a central or typical value of a given probability distribution. In other words, a measure of central tendency refers to a central or typical value that a given random variable might take when we observe various realizations of this variable over a long period.\n\n\nImage by Manfred Stege via Pixabay.\n\n\n\n\n\nDefinition of expected value\n\n\nLet \\(Y\\) be a random variable whose support is \\(\\mathcal{Y}\\). In general, the expected value or mean \\(\\mathbb{E}(Y)\\) of this random variable is defined as a weighted average according to its corresponding probability distribution. In other words, this measure of central tendency \\(\\mathbb{E}(Y)\\) aims to find the middle value of this random variable by weighting all its possible values in its support \\(\\mathcal{Y}\\) as dictated by its probability distribution.\nGiven the above definition, when \\(Y\\) is a discrete random variable whose PMF is \\(P_Y(Y = y)\\), then its expected value is mathematically defined as\n\\[\n\\mathbb{E}(Y) = \\sum_{y \\in \\mathcal{Y}} y \\cdot P_Y(Y = y).\n\\tag{2.12}\\]\nWhen \\(Y\\) is a continuous random variable whose PDF is \\(f_Y(y)\\), its expected value is mathematically defined as\n\\[\n\\mathbb{E}(Y) = \\int_{\\mathcal{Y}} y \\cdot f_Y(y) \\mathrm{d}y.\n\\tag{2.13}\\]\n\n\nImage by Manfred Stege via Pixabay.\n\n\n\n\n\nDefinition of measure of uncertainty\n\n\nProbabilistically, a measure of uncertainty refers to the spread of a given random variable when we observe its different realizations in the long term. Note a larger spread indicates more variability in these realizations. On the other hand, a smaller spread denotes less variability in these realizations.\n\n\nImage by Manfred Stege via Pixabay.\n\n\n\n\n\nTip on the Law of the Unconscious Statistician!\n\n\nThe law of the unconscious statistician (LOTUS) is a particular theorem in probability theory that allows us to compute a wide variety of expected values. Let us properly define it for both discrete and continuous random variables.\n\nTheorem 2.1 Let \\(Y\\) be a discrete random variable whose support is \\(\\mathcal{Y}\\). The LOTUS indicates that the expected value of a general function \\(g(Y)\\) of this random variable \\(Y\\) can be obtained via \\(g(Y)\\) along with the corresponding PMF \\(P_Y(Y = y)\\). Hence, the expected value of \\(g(Y)\\) can be obtained as\n\\[\n\\mathbb{E}\\left[ g(Y) \\right] = \\sum_{y \\in \\mathcal{Y}} g(y) \\cdot P_Y(Y = y).\n\\tag{2.14}\\]\n\nProof. Let us explore the rationale provided by Soch et al. (2024). Thus, we will rename the general function \\(g(Y)\\) as another random variable called \\(Z\\) such that:\n\\[\nZ = g(Y).\n\\tag{2.15}\\]\nNote this function \\(g(Y)\\) can take on equal values \\(g(y_1), g(y_2), \\dots\\) coming from different observed values \\(y_1, y_2, \\dots\\); for example, if\n\\[\ng(y) = y^2\n\\]\nboth\n\\[\ny_1 = 2 \\quad \\text{and} \\quad y_2 = -2\n\\]\nyield\n\\[\ng(y_1) = g(y_2) = 4.\n\\]\nThe above Equation 2.15 is formally called a random variable transformation from the general function of random variable \\(Y\\), \\(g(Y)\\), to a new random variable \\(Z\\). Having said that, when we set up a transformation of this class, there will be a support mapping from this general function \\(g(Y)\\) to \\(Z\\). This will also yield a proper PMF,\n\\[\nP_Z(Z = z) : \\mathbb{R} \\rightarrow [0, 1] \\quad \\forall z \\in \\mathcal{Z},\n\\]\ngiven that \\(g(Y)\\) is a random variable-based function.\nTherefore, using the expected value definition for a discrete random variable as in Equation 2.12, we have the following for \\(Z\\):\n\\[\n\\mathbb{E}(Z) = \\sum_{z \\in \\mathcal{Z}} z \\cdot P_Z(Z = z).\n\\tag{2.16}\\]\nWithin the support \\(\\mathcal{Z}\\), suppose that \\(z_1, z_2, \\dots\\) are the possible different values of \\(Z\\) corresponding to function \\(g(Y)\\). Then, for the \\(i\\)th value \\(z_i\\) in this correspondence, let \\(I_i\\) be the collection of all \\(y_j\\) such that\n\\[\ng(y_j) = z_i.\n\\tag{2.17}\\]\nNow, let us tweak a bit the above expression from Equation 2.16 to include this setting:\n\\[\n\\begin{align*}\n\\mathbb{E}(Z) &= \\sum_{z \\in \\mathcal{Z}} z \\cdot P_Z(Z = z) \\\\\n&= \\sum_{i} z_i \\cdot P_{g(Y)}(Z = z_i) \\\\\n& \\qquad \\text{we subset the summation to all $z_i$ with $Z = g(Y)$}\\\\\n&= \\sum_{i} z_i \\sum_{j \\in I_i} P_Y(Y = y_j). \\\\\n\\end{align*}\n\\tag{2.18}\\]\nThe last line of Equation 2.18 maps the probabilities associated to all \\(z_i\\) in the corresponding PMF of \\(Z\\), \\(P_Z(\\cdot)\\) via the function \\(g(Y)\\), to the original PMF of \\(Y\\), \\(P_Y(\\cdot)\\), for all those \\(y_j\\) contained in the collection \\(I_i\\). Given that certain values \\(z_i\\) can be obtained with more than one value \\(y_j\\), such as in the above example when \\(g(y) = y^2\\) for \\(y_1 = 2\\) and \\(y_2 = -2\\), note we have a second summation of probabilities applied to the PMF of \\(Y\\).\nMoving along with Equation 2.18 in conjunction with Equation 2.17, we have that:\n\\[\n\\begin{align*}\n\\mathbb{E}(Z) &= \\sum_{i} z_i \\sum_{j \\in I_i} P_Y(Y = y_j) \\\\\n&= \\sum_{i} \\sum_{j \\in I_i} z_i \\cdot P_Y(Y = y_j) \\\\\n&= \\sum_{i} \\sum_{j \\in I_i} g(y_j) \\cdot P_Y(Y = y_j).\n\\end{align*}\n\\tag{2.19}\\]\nThe double summation in Equation 2.19 can be summarized into a single one, given neither of the factors on the right-hand side is subindexed by \\(i\\). Furthermore, this standalone summation can be applied to all \\(y \\in \\mathcal{Y}\\) while getting rid of the subindex \\(j\\) in the factors on the right-hand side:\n\\[\n\\begin{align*}\n\\mathbb{E}(Z) &= \\sum_{i} \\sum_{j \\in I_i} g(y_j) \\cdot P_Y(Y = y_j) \\\\\n&= \\sum_{y \\in \\mathcal{Y}} g(y) \\cdot P_Y(Y = y) \\\\\n&= \\mathbb{E}\\left[ g(Y) \\right].\n\\end{align*}\n\\]\nTherefore, we have:\n\\[\n\\mathbb{E}\\left[ g(Y) \\right] = \\sum_{y \\in \\mathcal{Y}} g(y) \\cdot P_Y(Y = y). \\quad \\square\n\\]\n\n\n\nTheorem 2.2 Let \\(Y\\) be a continuous random variable whose support is \\(\\mathcal{Y}\\). The LOTUS indicates that the expected value of a general function \\(g(Y)\\) of this random variable \\(Y\\) can be obtained via \\(g(Y)\\) along with the corresponding PDF \\(f_Y(y)\\). Thus, the expected value of \\(g(Y)\\) can be obtained as\n\\[\n\\mathbb{E}\\left[ g(Y) \\right] = \\int_{\\mathcal{Y}} g(y) \\cdot f_Y(y).\n\\tag{2.20}\\]\n\nProof. Let us explore the rationale provided by Soch et al. (2024). Hence, we will rename the general function \\(g(Y)\\) as another random variable called \\(Z\\) such that:\n\\[\nZ = g(Y).\n\\tag{2.21}\\]\nAs in the discrete LOTUS proof, the above Equation 2.21 is formally called a random variable transformation from the general function of random variable \\(Y,\\) \\(g(Y)\\), to a new random variable \\(Z\\). Therefore, when we set up a transformation of this class, there will be a support mapping from this general function \\(g(Y)\\) to \\(Z\\). This will also yield a proper PDF:\n\\[\nf_Z(z) : \\mathbb{R} \\rightarrow [0, 1] \\quad \\forall z \\in \\mathcal{Z},\n\\]\ngiven that \\(g(Y)\\) is a random variable-based function.\nNow, we will use the concept of the cumulative distribution function (CDF) for a continuous random variable \\(Z\\):\n\\[\n\\begin{align*}\nF_Z(z) &= P(Z \\leq z) \\\\\n&= P\\left[g(Y) \\leq z \\right] \\\\\n&= P\\left[Y \\leq g^{-1}(z) \\right] \\\\\n&= F_Y\\left[ g^{-1}(z) \\right].\n\\end{align*}\n\\tag{2.22}\\]\nA well-known Calculus result is the inverse function theorem. Assuming that\n\\[\nz = g(y)\n\\]\nis an invertible and differentiable function, then the inverse\n\\[\ny = g^{-1}(z)\n\\tag{2.23}\\]\nmust be differentiable as in:\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d}z} \\left[ g^{-1}(z) \\right] = \\frac{1}{g' \\left[ g^{-1}(z) \\right]}.\n\\tag{2.24}\\]\nNote that we differentiate Equation 2.23 as follows:\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d}z} y = \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left[ g^{-1}(z) \\right].\n\\tag{2.25}\\]\nThen, plugging Equation 2.25 into Equation 2.24, we obtain:\n\\[\n\\begin{gather*}\n\\frac{\\mathrm{d}}{\\mathrm{d}z} y = \\frac{1}{g' \\left[ g^{-1}(z) \\right]} \\\\\n\\mathrm{d}y = \\frac{1}{g' \\left[ g^{-1}(z) \\right]} \\mathrm{d}z.\n\\end{gather*}\n\\tag{2.26}\\]\nThen, we use the property that relates the CDF \\(F_Z(z)\\) to the PDF \\(f_Z(z)\\):\n\\[\nf_Z(z) = \\frac{\\mathrm{d}}{\\mathrm{d}z} F_Z(z).\n\\]\nUsing Equation 2.22, we have:\n\\[\n\\begin{align*}\nf_Z(z) &= \\frac{\\mathrm{d}}{\\mathrm{d}z} F_Z(z) \\\\\n&= \\frac{\\mathrm{d}}{\\mathrm{d}z} F_Y\\left[ g^{-1}(z) \\right] \\\\\n&= f_Y\\left[ g^{-1}(z) \\right] \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left[ g^{-1}(z) \\right].\n\\end{align*}\n\\]\nThen, via Equation 2.24, it follows that:\n\\[\nf_Z(z) = f_Y\\left[ g^{-1}(z) \\right] \\frac{1}{g' \\left[ g^{-1}(z) \\right]}.\n\\tag{2.27}\\]\nTherefore, using the expected value definition for a continuous random variable as in Equation 2.13, we have for \\(Z\\) that\n\\[\n\\mathbb{E}(Z) = \\int_{\\mathcal{Z}} z \\cdot f_Z(z) \\mathrm{d}z,\n\\]\nwhich yields via Equation 2.27:\n\\[\n\\mathbb{E}(Z) = \\int_{\\mathcal{Z}} z \\cdot f_Y \\left[ g^{-1}(z) \\right] \\frac{1}{g' \\left[ g^{-1}(z) \\right]} \\mathrm{d}z.\n\\]\nUsing Equation 2.23 and Equation 2.26, it follows that:\n\\[\n\\begin{align*}\n\\mathbb{E}(Z) &= \\int_{\\mathcal{Z}} z \\cdot f_Y(y) \\frac{1}{g' \\left[ g^{-1}(z) \\right]} \\mathrm{d}z \\\\\n&= \\int_{\\mathcal{Y}} g(y) \\cdot f_Y(y) \\mathrm{d}y.\n\\end{align*}\n\\]\nNote the last line in the above equation changes the integration limits to the support of \\(Y\\), given all terms end up depending on \\(y\\) on the right-hand side.\nFinally, given the random variable transformation from Equation 2.21, we have:\n\\[\n\\mathbb{E}\\left[ g(X) \\right] = \\int_{\\mathcal{Y}} g(y) \\cdot f_Y(y) \\mathrm{d}y. \\quad \\square\n\\]\n\n\n\n\n\n\nDefinition of variance\n\n\nLet \\(Y\\) be a discrete or continuous random variable whose support is \\(\\mathcal{Y}\\) with a mean represented by \\(\\mathbb{E}(Y)\\). Then, the variance of \\(Y\\) is the mean of the squared deviation from the corresponding mean as follows:\n\\[\n\\text{Var}(Y) = \\mathbb{E}\\left\\{[ Y - \\mathbb{E}(Y)]^2 \\right\\}. \\\\\n\\tag{2.28}\\]\nNote the expression above is equivalent to:\n\\[\n\\text{Var}(Y) = \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y) \\right]^2.\n\\tag{2.29}\\]\n\n\nImage by Manfred Stege via Pixabay.\n\n\n\n\n\nHeads-up on the two mathematical expressions of the variance!\n\n\nProving the equivalence of Equation 2.28 and Equation 2.29, requires the introduction of some further properties of the expected value of a random variable while using the LOTUS. We will dig into the insights provided by Casella and Berger (2024).\n\nTheorem 2.3 Let \\(Y\\) be a discrete or continuous random variable. Furthermore, let \\(a\\), \\(b\\), and \\(c\\) be constants. Thus, for any functions \\(g_1(y)\\) and \\(g_2(x)\\) whose means exist, we have that:\n\\[\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] = a \\mathbb{E}\\left[ g_1(Y) \\right] + b \\mathbb{E}\\left[ g_2(Y) \\right] + c.\n\\tag{2.30}\\]\nFirstly, let us prove Equation 2.30 for the discrete case.\n\nProof. Let \\(Y\\) be a discrete random variable whose support is \\(\\mathcal{Y}\\) and PMF is \\(P_Y(Y = y)\\). Let us apply the LOTUS as in Equation 2.14:\n\\[\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] = \\sum_{y \\in \\mathcal{Y}} \\left[ a g_1(y) + b g_2(y) + c \\right] \\cdot P_Y(Y = y).\n\\] We can distribute the summation across each addend as follows:\n\\[\n\\begin{align*}\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] &= \\sum_{y \\in \\mathcal{Y}} \\left[ a g_1(y) \\right] \\cdot P_Y(Y = y) + \\\\\n& \\qquad \\sum_{y \\in \\mathcal{Y}} \\left[ b g_2(y) \\right] \\cdot P_Y(Y = y) + \\\\\n& \\qquad \\sum_{y \\in \\mathcal{Y}} c \\cdot P_Y(Y = y).\n\\end{align*}\n\\]\nLet us take the constants out of the corresponding summations:\n\\[\n\\begin{align*}\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] &= a \\sum_{y \\in \\mathcal{Y}} g_1(y) \\cdot P_Y(Y = y) + \\\\\n& \\qquad b \\sum_{y \\in \\mathcal{Y}} g_2(y) \\cdot P_Y(Y = y) + \\\\\n& \\qquad c \\underbrace{\\sum_{y \\in \\mathcal{Y}} P_Y(Y = y)}_1 \\\\\n&= a \\underbrace{\\sum_{y \\in \\mathcal{Y}} g_1(y) \\cdot P_Y(Y = y)}_{\\mathbb{E} \\left[ g_1(Y) \\right]} + \\\\\n& \\qquad b \\underbrace{\\sum_{y \\in \\mathcal{Y}} g_2(y) \\cdot P_Y(Y = y)}_{\\mathbb{E} \\left[ g_2(Y) \\right]} + c.\n\\end{align*}\n\\]\nFor the first and second addends on the right-hand side in the above equation, let us apply the LOTUS again:\n\\[\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] = a \\mathbb{E} \\left[ g_1(Y) \\right] + b \\mathbb{E} \\left[ g_2(Y) \\right] + c. \\quad \\square\n\\]\n\nSecondly, let us prove Equation 2.30 for the continuous case.\n\nProof. Let \\(Y\\) be a continuous random variable whose support is \\(\\mathcal{Y}\\) and PDF is \\(f_Y(y)\\). Let us apply the LOTUS as in Equation 2.20:\n\\[\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] = \\int_{\\mathcal{Y}} \\left[ a g_1 (y) + b g_2(y) + c \\right] \\cdot f_Y(y) \\mathrm{d}y.\n\\]\nWe distribute the integral on the right-hand side of the above equation:\n\\[\n\\begin{align*}\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] &= \\int_{\\mathcal{Y}} \\left[ a g_1 (y) \\right] \\cdot f_Y(y) \\mathrm{d}y + \\\\\n& \\qquad \\int_{\\mathcal{Y}} \\left[ b g_2(y) \\right] \\cdot f_Y(y) \\mathrm{d}y + \\\\\n& \\qquad \\int_{\\mathcal{Y}} c \\cdot f_Y(y) \\mathrm{d}y.\n\\end{align*}\n\\]\nLet us take the constants out of the corresponding integrals:\n\\[\n\\begin{align*}\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] &= a \\int_{\\mathcal{Y}} g_1 (y) \\cdot f_Y(y) \\mathrm{d}y + \\\\\n& \\qquad b \\int_{\\mathcal{Y}} g_2(y) \\cdot f_Y(y) \\mathrm{d}y + \\\\\n& \\qquad c \\underbrace{\\int_{\\mathcal{Y}} f_Y(y) \\mathrm{d}y}_{1} \\\\\n&= a \\underbrace{\\int_{\\mathcal{Y}} g_1 (y) \\cdot f_Y(y) \\mathrm{d}y}_{\\mathbb{E} \\left[ g_1(Y) \\right]} + \\\\\n& \\qquad b \\underbrace{\\int_{\\mathcal{Y}} g_2(y) \\cdot f_Y(y) \\mathrm{d}y}_{\\mathbb{E} \\left[ g_2(Y) \\right]} + c.\n\\end{align*}\n\\]\nFor the first and second addends on the right-hand side in the above equation, let us apply the LOTUS again:\n\\[\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] = a \\mathbb{E} \\left[ g_1(Y) \\right] + b \\mathbb{E} \\left[ g_2(Y) \\right] + c. \\quad \\square\n\\]\n\n\nFinally, after applying some algebraic rearrangements and the expected value properties shown in Equation 2.30, Equation 2.28 and Equation 2.29 are equivalent as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var}(Y) &= \\mathbb{E}\\left\\{[ Y - \\mathbb{E}(Y)]^2 \\right\\} \\\\\n&= \\mathbb{E} \\left\\{ Y^2 - 2Y \\mathbb{E}(Y) + \\left[ \\mathbb{E}(Y) \\right]^2 \\right\\} \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - \\mathbb{E} \\left[ 2Y \\mathbb{E}(Y) \\right] + \\mathbb{E} \\left[ \\mathbb{E}(Y) \\right]^2 \\\\\n& \\qquad \\text{distributing the expected value operator} \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - 2 \\mathbb{E} \\left[ Y \\mathbb{E}(Y) \\right] + \\mathbb{E} \\left[ \\mathbb{E}(Y) \\right]^2 \\\\\n& \\qquad \\text{since $2$ is a constant} \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - 2 \\mathbb{E}(Y) \\mathbb{E} \\left( Y \\right) + \\left[ \\mathbb{E}(Y) \\right]^2 \\\\\n& \\qquad \\text{since $\\mathbb{E}(Y)$ is a constant} \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - 2 \\left[ \\mathbb{E}(Y) \\right]^2 + \\left[ \\mathbb{E}(Y) \\right]^2 \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y) \\right]^2.  \\qquad \\qquad \\qquad \\qquad \\qquad \\square\n\\end{align*}\n\\]\n\n\n\n\n\nDefinition of estimator\n\n\nAn estimator is a mathematical rule involving the random variables \\(Y_1, \\dots, Y_n\\) from our random sample of size \\(n\\). As its name says, this rule allows us to estimate our population parameter of interest.\n\n\n\n\nDefinition of estimate\n\n\nSuppose we have an observed random sample of size \\(n\\) with values \\(y_1, \\dots , y_n\\). Then, we apply a given estimator mathematical rule to these \\(n\\) observed values. Hence, this numerical computation is called an estimate of our population parameter of interest.\n\n\n\n\nR Code\nPython Code\n\n\n\npi_hat &lt;- round(mean(children_sample$fav_flavour == \"chocolate\"), 2)\npi_hat\n\n\npi_hat = round((children_sample[\"fav_flavour\"] == \"chocolate\").mean(), 2)\nprint(pi_hat)\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n[1] 0.68\n\n\n\n\n\n\n0.67\n\n\n\n\n\n\n\nR Code\nPython Code\n\n\n\nbeta_hat &lt;- round(mean(waiting_sample$waiting_time), 2)\nbeta_hat\n\n\nbeta_hat = round(waiting_sample[\"waiting_time\"].mean(), 2)\nprint(beta_hat)\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n[1] 10.63\n\n\n\n\n\n\n10.13\n\n\n\n\n\n\n2.1.6 The Rationale in Random Sampling\n\n\nDefinition of conditional probability\n\n\nSuppose you have two events of interest, \\(A\\) and \\(B\\), in a random phenomenon of a population or system of interest. These two events belong to the sample space \\(S\\). Moreover, assume that the probability of event \\(B\\) is such that\n\\[\nP(B) &gt; 0,\n\\]\nwhich is considered the conditioning event.\nHence, the conditional probability of event \\(A\\) given event \\(B\\) is defined as\n\\[\nP(A | B) = \\frac{P(A \\cap B)}{P(B)},\n\\tag{2.31}\\]\nwhere \\(P(A \\cap B)\\) is read as the probability of the intersection of events \\(A\\) and \\(B\\).\n\n\nImage by Manfred Steger via Pixabay.\n\n\n\n\n\nTip on the rationale behind conditional probability!\n\n\nWe can delve into the rationale of Equation 2.31 by using a handy probabilistic concept called cardinality, which refers to the corresponding total number of possible outcomes in a random phenomenon belonging to any given event or sample space.\n\nProof. Let \\(|S|\\) be the cardinality corresponding to the sample space in a random phenomenon. Hence, as in Equation 2.2, we have that:\n\\[\nP(S) = \\frac{|S|}{|S|} = 1.\n\\]\nMoreover, suppose that \\(A\\) is the primary event of interest whose cardinality is represented by \\(|A|\\). Alternatively to Equation 2.1, the probability of \\(A\\) can be represented as\n\\[\nP(A) = \\frac{|A|}{|S|}.\n\\]\nOn the other hand, the cardinality of the conditioning event is\n\\[\nP(B) = \\frac{|B|}{|S|}.\n\\tag{2.32}\\]\nNow, let \\(|A \\cap B|\\) be the cardinality of the intersection between events \\(A\\) and \\(B\\). Its probability can be represented as:\n\\[\nP(A \\cap B) = \\frac{|A \\cap B|}{|B|}.\n\\tag{2.33}\\]\nAnalogous to Equation 2.32 and Equation 2.33, we can view the conditional probability \\(P(A | B)\\) as an updated probability of the primary event \\(A\\) restricted to the cardinality of the conditioning event \\(|B|\\). This places \\(|A \\cap B|\\) in the numerator and \\(|B|\\) in the denominator as follows:\n\\[\nP(A | B) = \\frac{|A \\cap B|}{|B|}.\n\\tag{2.34}\\]\nTherefore, we can play around with Equation 2.34 along with Equation 2.32 and Equation 2.33 as follows:\n\\[\n\\begin{align*}\nP(A \\cap B) &= \\frac{|A \\cap B|}{|B|} \\\\\n&= \\frac{\\frac{|A \\cap B}{|S|}}{\\frac{|B|}{|S|}} \\qquad \\text{dividing numerator and denominator over $|S|$} \\\\\n&= \\frac{P(A \\cap B)}{P(B)}. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\n\n\n\n\nDefinition of the Bayes’ rule\n\n\nSuppose you have two events of interest, \\(A\\) and \\(B\\), in a random phenomenon of a population or system of interest. From Equation 2.31, we can state the following expression for the conditional probability of \\(A\\) given \\(B\\):\n\\[\nP(A | B) = \\frac{P(A \\cap B)}{P(B)} \\quad \\text{if $P(B) &gt; 0$.}\n\\tag{2.35}\\]\nNote the conditional probability of \\(B\\) given \\(A\\) can be stated as:\n\\[\n\\begin{align*}\nP(B | A) &= \\frac{P(B \\cap A)}{P(A)} \\quad \\text{if $P(A) &gt; 0$} \\\\\n&= \\frac{P(A \\cap B)}{P(A)} \\quad \\text{since $P(B \\cap A) = P(A \\cap B)$.}\n\\end{align*}\n\\tag{2.36}\\]\nThen, we can manipulate Equation 2.36 as follows:\n\\[\nP(A \\cap B) = P(B | A) \\times P(A).\n\\]\nThe above result can be plugged into Equation 2.35:\n\\[\n\\begin{align*}\nP(A | B) &= \\frac{P(A \\cap B)}{P(B)} \\\\\n&= \\frac{P(B | A) \\times P(A)}{P(B)}.\n\\end{align*}\n\\tag{2.37}\\]\nEquation 2.37 is called the Bayes’ rule. We are basically flipping around conditional probabilities.\n\n\n\n\nDefinition of independence\n\n\nSuppose you have two events of interest, \\(A\\) and \\(B\\), in a random phenomenon of a population or system of interest. These two events are statistically independent if event \\(B\\) does not affect event \\(A\\) and vice versa. Therefore, the probability of their corresponding intersection is given by:\n\\[\nP(A \\cap B) = P(A) \\times P(B).\n\\tag{2.38}\\]\nLet us expand the above definition to a random variable framework:\n\nSuppose you have a set of \\(n\\) discrete random variables \\(Y_1, \\dots, Y_n\\) whose supports are \\(\\mathcal{Y_1}, \\dots, \\mathcal{Y_n}\\) with PMFs \\(P_{Y_1}(Y_1 = y_1), \\dots, P_{Y_n}(Y_n = y_n)\\) respectively. That said, the joint PMF of these \\(n\\) random variables is the multiplication of their corresponding standalone PMFs:\n\n\\[\n\\begin{align*}\nP_{Y_1, \\dots, Y_n}(Y_1 = y_1, \\dots, Y_n = y_n) &= \\prod_{i = 1}^n P_{Y_i}(Y_i = y_i) \\\\\n& \\qquad \\text{for all} \\\\\n& \\qquad \\quad y_i \\in \\mathcal{Y}_i, i = 1, \\dots, n.\n\\end{align*}\n\\tag{2.39}\\]\n\nSuppose you have a set of \\(n\\) continuous random variables \\(Y_1, \\dots, Y_n\\) whose supports are \\(\\mathcal{Y_1}, \\dots, \\mathcal{Y_n}\\) with PDFs \\(f_{Y_1}(y_1), \\dots, f_{Y_n}(y_n)\\) respectively. That said, the joint PDF of these \\(n\\) random variables is the multiplication of their corresponding standalone PDFs:\n\n\\[\n\\begin{align*}\nf_{Y_1, \\dots, Y_n}(y_1, \\dots, y_n) &= \\prod_{i = 1}^n f_{Y_i}(y_i) \\\\\n& \\qquad \\text{for all} \\\\\n& \\qquad \\quad y_i \\in \\mathcal{Y}_i, i = 1, \\dots, n.\n\\end{align*}\n\\tag{2.40}\\]\n\n\n\n\nTip on the rationale behind the rule of independent events!\n\n\nWe can delve into the rationale of Equation 2.38 by using the Bayes’ rule from Equation 2.37 along with the basic conditional probability formula from Equation 2.31.\n\nProof. Firstly, let us assume that a given event \\(B\\) does not affect event \\(A\\) which can be probabilistically represented as\n\\[\nP(A | B) = P(A).\n\\tag{2.41}\\]\nIf the statement in Equation 2.41 holds, by using the Bayes’ rule from Equation 2.37, we have the following manipulation for the below conditional probability formula:\n\\[\n\\begin{align*}\nP(B | A) &= \\frac{P(B \\cap A)}{P(A)} \\\\\n&= \\frac{P(A \\cap B)}{P(A)} \\qquad \\text{since $P(B \\cap A) = P(A \\cap B$)} \\\\\n&= \\frac{P(A | B) \\times P(B)}{P(A)} \\qquad \\text{by the Bayes' rule} \\\\\n&= \\frac{P(A) \\times P(B)}{P(A)} \\qquad \\text{since $P(A | B) = P(A)$} \\\\\n&= P(B).\n\\end{align*}\n\\]\nThen, again by using the Bayes’ rule, we obtain \\(P(B \\cap A)\\) as follows:\n\\[\n\\begin{align*}\nP(B \\cap A) &= P(B | A) \\times P(A) \\\\\n&= P(B) \\times P(A) \\qquad \\text{since $P(B | A) = P(B)$.}\n\\end{align*}\n\\]\nFinally, we have that:\n\\[\n\\begin{align*}\nP(A \\cap B) &= P(B \\cap A) \\\\\n&= P(B) \\times P(A) \\\\\n&= P(A) \\times P(B). \\qquad \\square\n\\end{align*}\n\\]\n\n\n\n\n\nDefinition of random sample\n\n\nA random sample is a collection of random variables \\(Y_1, \\dots, Y_n\\) of size \\(n\\) coming from a given population or system of interest. Note that the most elementary definition of a random sample assumes that these \\(n\\) random variables are mutually independent and identically distributed (which is abbreviated as iid).\nThe fact that these \\(n\\) random variables are identically distributed indicates that they have the same mathematical form for their corresponding PMFs or PDFs, depending on whether they are discrete or continuous respectively. Hence, under a generative modelling approach in a population or system of interest governed by \\(k\\) parameters contained in the vector\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T,\n\\]\nwe can apply the iid property in an elementary random sample to obtain the following joint probability distributions:\n\nIn the case of \\(n\\) iid discrete random variables \\(Y_1, \\dots, Y_n\\) whose common standalone PMF is \\(P_Y(Y = y | \\boldsymbol{\\theta})\\) with support \\(\\mathcal{Y}\\), the joint PMF is mathematically expressed as\n\n\\[\n\\begin{align*}\nP_{Y_1, \\dots, Y_n}(Y_1 = y_1, \\dots, Y_n = y_n | \\boldsymbol{\\theta}) &= \\prod_{i = 1}^n P_Y(Y = y_i | \\boldsymbol{\\theta}) \\\\\n& \\quad \\text{for all} \\\\\n& \\quad \\quad y_i \\in \\mathcal{Y}, i = 1, \\dots, n.\n\\end{align*}\n\\tag{2.42}\\]\n\nIn the case of \\(n\\) iid continuous random variables \\(Y_1, \\dots, Y_n\\) whose common standalone PDF is \\(f_Y(y | \\boldsymbol{\\theta})\\) with support \\(\\mathcal{Y}\\), the joint PDF is mathematically expressed as\n\n\\[\n\\begin{align*}\nf_{Y_1, \\dots, Y_n}(y_1, \\dots, y_n | \\boldsymbol{\\theta}) &= \\prod_{i = 1}^n f_Y(y_i | \\boldsymbol{\\theta}) \\\\\n& \\quad \\text{for all} \\\\\n& \\quad \\quad y_i \\in \\mathcal{Y}, i = 1, \\dots, n.\n\\end{align*}\n\\tag{2.43}\\]\nUnlike Equation 2.39 and Equation 2.40, note that Equation 2.42 and Equation 2.43 do not indicate a subscript for \\(Y\\) in the corresponding probability distributions since we have identically distributed random variables. Furthermore, the joint distributions are conditioned on the population parameter vector \\(\\boldsymbol{\\theta}\\) which reflects our generative modelling approach.\n\n\nImage by Manfred Steger via Pixabay.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span>"
    ]
  },
  {
    "objectID": "book/02-stats-review.html#sec-mle",
    "href": "book/02-stats-review.html#sec-mle",
    "title": "2  Basic Cuisine: A Review on Probability and Frequentist Statistical Inference",
    "section": "\n2.2 What is Maximum Likelihood Estimation?",
    "text": "2.2 What is Maximum Likelihood Estimation?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span>"
    ]
  },
  {
    "objectID": "book/02-stats-review.html#sec-basics-inf",
    "href": "book/02-stats-review.html#sec-basics-inf",
    "title": "2  Basic Cuisine: A Review on Probability and Frequentist Statistical Inference",
    "section": "\n2.3 Basics of Frequentist Statistical Inference",
    "text": "2.3 Basics of Frequentist Statistical Inference\n\n\n\n\n\nFigure 2.3: Results stage from the data science workflow in Figure 1.1. This stage is directly followed by storytelling and preceded by goodness of fit.\n\n\n\n\n\n\n\nFigure 2.4: A classical-based hypothesis testing workflow structured in four substages: general settings, hypotheses definitions, test flavour and components, and inferential conclusions.\n\n\n\n2.3.1 General Settings\n\n\n\n\n\nFigure 2.5: General settings substage from the classical-based hypothesis testing workflow in Figure 2.4. This substage is directly followed by the hypotheses definitions.\n\n\nBased on the work by Soch et al. (2024), let us check some key definitions.\n\n\nDefinition of hypothesis\n\n\nSuppose you observe some data \\(y\\) from some population(s) or system(s) of interest governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nMoreover, we assume this observed data \\(y\\) follows certain probability distribution \\(\\mathcal{D}(\\cdot)\\) in a generative model \\(m\\) as in\n\\[\nm: y \\sim \\mathcal{D}(\\boldsymbol{\\theta}).\n\\]\nBeginning from the fact that \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\) where \\(\\boldsymbol{\\Theta} \\in \\mathbb{R}^k\\), a statistical hypothesis is a general statement about some parameter vector \\(\\boldsymbol{\\theta}\\) in regards to specific values contained in vector \\(\\boldsymbol{\\Theta}^*\\) such that\n\\[\nH: \\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}^* \\quad \\text{where} \\quad \\boldsymbol{\\Theta}^* \\subset \\boldsymbol{\\Theta}.\n\\]\n\n\n\n\nDefinition of null hypothesis\n\n\nIn a hypothesis(s) testing, a null hypothesis is denoted by \\(H_0\\). The whole inferential process is designed to assess the strength of the evidence in favour or against this null hypothesis. In plain words, \\(H_0\\) is an inferential statement associated to the status quo in some population(s) or system(s) of interest, which might refer to no signal for the researcher in question.\nAgain, suppose you observe some data \\(y\\) from some population(s) or system(s) of interest governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nMoreover, we assume this observed data \\(y\\) follows certain probability distribution \\(\\mathcal{D}(\\cdot)\\) in a generative model \\(m\\) as in\n\\[\nm: y \\sim \\mathcal{D}(\\boldsymbol{\\theta}).\n\\]\nLet \\(\\boldsymbol{\\Theta}_0 \\subset \\boldsymbol{\\theta}\\) denote the status quo for the parameter(s) to be tested. Then, the null hypothesis is mathematically defined as\n\\[\nH_0: \\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}_0 \\quad \\text{where} \\quad \\boldsymbol{\\Theta}_0 \\subset \\boldsymbol{\\theta}.\n\\tag{2.44}\\]\n\n\n\n\nDefinition of alternative hypothesis\n\n\nIn a hypothesis testing, an alternative hypothesis is denoted by \\(H_1\\). This hypothesis corresponds to the complement (i.e., the opposite) of the null hypothesis \\(H_0\\). Since the whole inferential process is designed to assess the strength of the evidence in favour or against of \\(H_0\\), any inferential conclusion against \\(H_0\\) can be worded as “rejecting \\(H_0\\) in favour of \\(H_1\\).” In plain words, \\(H_1\\) is an inferential statement associated to a non-status quo in some population(s) or system(s) of interest, which might refer to actual signal for the researcher in question.\nLet us assume you observe some data \\(y\\) from some population(s) or system(s) of interest governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nMoreover, suppose this observed data \\(y\\) follows certain probability distribution \\(\\mathcal{D}(\\cdot)\\) in a generative model \\(m\\) as in\n\\[\nm: y \\sim \\mathcal{D}(\\boldsymbol{\\theta}).\n\\]\nLet \\(\\boldsymbol{\\Theta}_0^c \\subset \\boldsymbol{\\theta}\\) denote the non-status quo for the parameter(s) to be tested. Then, the alternative hypothesis is mathematically defined as\n\\[\nH_1: \\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}_0^c \\quad \\text{where} \\quad \\boldsymbol{\\Theta}_0^c \\subset \\boldsymbol{\\theta}.\n\\tag{2.45}\\]\n\n\n\n\nDefinition of hypothesis testing\n\n\nA hypothesis testing is the decision rule we have to apply between the null and alternative hypotheses, via our sample data, to fail to reject or reject the null hypothesis.\n\n\n\n\nDefinition of type I error (false positive)\n\n\nType I error is defined as incorrectly rejecting the null hypothesis \\(H_0\\) in favour of the alternative hypothesis \\(H_1\\) when, in fact, \\(H_0\\) is true. Analogously, this type of error is also called false positive .\n\n\n\n\nDefinition of type II error (false negative)\n\n\nType II error is defined as incorrectly failing to reject the null hypothesis \\(H_0\\) in favour of the alternative hypothesis \\(H_1\\) when, in fact, \\(H_0\\) is false. Analogously, this type of error is also called false negative . Table 2.4 summarizes the types of inferential conclusions in function on whether \\(H_0\\) is true or not.\n\n\nTable 2.4: Types of inferential conclusions in a frequentist hypothesis testing.\n\n\n\n\n\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\nReject \\(H_0\\)\nType I error (False positive)\nCorrect\n\n\nFail to reject \\(H_0\\)\nCorrect\nType II error (False negative)\n\n\n\n\n\n\n\n\n\n\nDefinition of significance level\n\n\nThe significance level \\(\\alpha\\) is defined as the conditional probability of rejecting the null hypothesis \\(H_0\\) given that \\(H_0\\) is true. This can be mathematically represented as\n\\[\nP \\left( \\text{Reject $H_0$} | \\text{$H_0$ is true} \\right) = \\alpha.\n\\]\nIn plain words, \\(\\alpha \\in [0, 1]\\) allows us to probabilistically control for type I error since we are dealing with random variables in our inferential process. The significance level can be thought as one of the main hypothesis testing and power analysis settings. The larger the significance level in our power analysis and hypothesis testing, the less prone we are to commit a type I error.\n\n\n\n\nDefinition of power\n\n\nThe statistical power of a test \\(1 -\\beta\\) is the complement of the conditional probability \\(\\beta\\) of failing to reject the null hypothesis \\(H_0\\) given that \\(H_0\\) is false, which is mathematically represented as\n\\[\nP \\left( \\text{Failing to reject $H_0$} | \\text{$H_0$ is false} \\right) = \\beta;\n\\]\nyielding\n\\[\n\\text{Power} = 1 - \\beta.\n\\]\nIn plain words, \\(1 - \\beta \\in [0, 1]\\) is the probabilistic ability of our hypothesis testing to detect any signal in our inferential process, if there is any. The larger the power in our power analysis, the less prone we are to commit a type II error.\n\n\n\n\nDefinition of power analysis\n\n\nPower analysis is a set of statistical tools used to compute the minimum required sample size \\(n\\) for any given inferential study. These tools require the significance level, power, and effect size (i.e., the magnitude of the signal) the researcher aims to detect via their inferential study. This analysis seeks to determine whether observed results are likely due to chance or represent a true and meaningful effect.\n\n\nImage by Manfred Steger via Pixabay.\n\n\n\n\n2.3.2 Hypotheses Definitions\n\n\n\n\n\nFigure 2.6: Hypotheses definitions substage from the classical-based hypothesis testing workflow in Figure 2.4. This substage is directly preceded by general settings and followed by test flavour and components.\n\n\n\n2.3.3 Test Flavour and Components\n\n\n\n\n\nFigure 2.7: Test flavour and components substage from the classical-based hypothesis testing workflow in Figure 2.4. This substage is directly preceded by hypotheses definitions and followed by inferential conclusions.\n\n\n\n\nDefinition of observed effect\n\n\nAn observed effect is the difference between the estimate provided the observed random sample (of size \\(n\\), as in \\(y_1, \\dots, y_n\\)) to the hypothesized value(s) of the population parameter(s) depicted in the statistical hypotheses.\n\n\n\n\nDefinition of standard error\n\n\nThe standard error allows us to quantify the extent to which an estimate coming from an observed random sample (of size \\(n\\), as in \\(y_1, \\dots, y_n\\)) may deviate from the expected value under the assumption that the null hypothesis is true.\nIt plays a critical role in determining whether an observed effect is likely attributable to random variation or represents a statistically significant finding. In the absence of the standard error, it would not be possible to rigorously assess the reliability or precision of an estimate.\n\n\nImage by Manfred Steger via Pixabay.\n\n\n\n\n\nDefinition of test statistic\n\n\nThe test statistic is a function of the random sample of size \\(n\\), i.e., it is in the function of the random variables \\(Y_1, \\dots, Y_n\\). Therefore, the test statistic will also be a random variable, whose observed value will describe how closely the probability distribution from which the random sample comes from matches the probability distribution of the null hypothesis \\(H_0\\).\nMore specifically, once we have obtained the observed effect and standard error from our observed random sample, we can compute the corresponding observed test statistic. This test statistic computation will be placed on the corresponding \\(x\\)-axis of the probability distribution of \\(H_0\\) so we can reject or fail to reject it accordingly.\n\n\nImage by Manfred Steger via Pixabay.\n\n\n\n\n2.3.4 Inferential Conclusions\n\n\n\n\n\nFigure 2.8: Inferential conclusions substage from the classical-based hypothesis testing workflow in Figure 2.4. This substage is directly preceded by rest flavour and components and followed by the corresponding delivery significance conclusion within the results stage of the data science workflow as shown in Figure 2.3.\n\n\n\n\nDefinition of critical value\n\n\nThe critical value of a hypothesis testing defines the region for which we might reject \\(H_0\\) in favour of \\(H_1\\). This critical value is in the function of the significance level \\(\\alpha\\) and test flavour. It is located on the corresponding \\(x\\)-axis of the probability distribution of \\(H_0\\). Hence, this value acts as a threshold to decide either of the following:\n\nIf the observed test statistic exceeds a given critical value, then we have enough statistical evidence to reject \\(H_0\\) in favour of \\(H_1\\).\nIf the observed test statistic does not exceed a given critical value, then we have enough statistical evidence to fail to reject \\(H_0\\).\n\n\n\n\n\nDefinition of \\(p\\)-value\n\n\nA \\(p\\)-value refers to the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic coming from our observed random sample of size \\(n\\). This \\(p\\)-value is obtained via the probability distribution of \\(H_0\\) and the observed test statistic.\nAlternatively to a critical value, we can reject or fail to reject the null hypothesis \\(H_0\\) using this \\(p\\)-value as follows:\n\nIf the \\(p\\)-value associated to the observed test statistic exceeds a given significance level \\(\\alpha\\), then we have enough statistical evidence to reject \\(H_0\\) in favour of \\(H_1\\).\nIf the \\(p\\)-value associated to the observed test statistic does not exceed a given significance level \\(\\alpha\\), then we have enough statistical evidence to fail to reject \\(H_0\\).\n\n\n\n\n\nDefinition of confidence interval\n\n\nA confidence interval provides an estimated range of values within which the true population parameter is likely to fall, based on the sample data. It reflects the degree of uncertainty associated with the obtained estimate. For instance, a 95% confidence interval means that if the study were repeated many times using different random samples from the same population or system of interest, approximately 95% of the resulting intervals would contain the true parameter.\n\n\nImage by Manfred Steger via Pixabay.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span>"
    ]
  },
  {
    "objectID": "book/02-stats-review.html#sec-sup-learning-regression",
    "href": "book/02-stats-review.html#sec-sup-learning-regression",
    "title": "2  Basic Cuisine: A Review on Probability and Frequentist Statistical Inference",
    "section": "\n2.4 Supervised Learning and Regression Analysis",
    "text": "2.4 Supervised Learning and Regression Analysis\n\n\n\n\nBellhouse, D. R. 2004. “The Reverend Thomas Bayes, FRS: A Biography to Celebrate the Tercentenary of His Birth.” Statistical Science 19 (1): 3–43. https://doi.org/10.1214/088342304000000189.\n\n\nCasella, G., and R. Berger. 2024. Statistical Inference. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://www.taylorfrancis.com/books/mono/10.1201/9781003456285/statistical-inference-roger-berger-george-casella.\n\n\nJohnson, A. A., M. Q. Ott, and M. Dogucu. 2022. Bayes Rules!: An Introduction to Applied Bayesian Modeling. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://www.bayesrulesbook.com/.\n\n\nLeemis, Larry. n.d. “Univariate Distribution Relationship Chart.” https://www.math.wm.edu/~leemis/chart/UDR/UDR.html.\n\n\nO’Donnell, T. 1936. History of Life Insurance in Its Formative Years. Compiled from Approved Sources by T. O’Donnell. Chicago.\n\n\nSoch, Joram, The Book of Statistical Proofs, Maja, Pietro Monticone, Thomas J. Faulkenberry, Alex Kipnis, Kenneth Petrykowski, et al. 2024. “StatProofBook/StatProofBook.github.io: StatProofBook 2023.” Zenodo. https://doi.org/10.5281/zenodo.10495684.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html",
    "href": "book/03-ols.html",
    "title": "3  Zestylicious Ordinary Least-squares Regression",
    "section": "",
    "text": "3.1 Introduction\nWhen looking at data, we often want to know how different factors affect each other. For instance, if you have data on student finances, you might ask:\nOnce you have this financial data, the next step is to analyze it to find answers. One straightforward method for doing this is through regression analysis, and the simplest form is called Ordinary Least Squares (OLS).",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Zestylicious Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#introduction",
    "href": "book/03-ols.html#introduction",
    "title": "3  Zestylicious Ordinary Least-squares Regression",
    "section": "",
    "text": "How does having a job affect a student’s leftover money at the end of the month?\nWhat impact does receiving a monthly allowance have on their net savings?",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Zestylicious Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#what-is-ordinary-least-squares-ols",
    "href": "book/03-ols.html#what-is-ordinary-least-squares-ols",
    "title": "3  Zestylicious Ordinary Least-squares Regression",
    "section": "\n3.2 What is Ordinary Least Squares (OLS)?",
    "text": "3.2 What is Ordinary Least Squares (OLS)?\nOrdinary Least Squares (OLS) is a fundamental method in regression analysis for estimating the relationship between a dependent variable and one or more independent variables. In simple terms, OLS is like drawing the best straight line through a scatterplot of data points. Imagine you plotted students’ net savings on a graph, and each point represents a student’s financial outcome. OLS finds the line that best follows the trend of these points by minimizing the overall distance (error) between what the line predicts and what the actual data shows.\nOLS is widely used because it is:\n\n\nSimple: Easy to understand and compute.\n\nClear: Provides straightforward numbers (coefficients) that tell you how much each factor influences the outcome.\n\nVersatile: Applicable in many fields, from economics to social sciences, to help make informed decisions.\n\nIn this chapter, we will break down how OLS works in plain language, explore its underlying assumptions, and discuss its practical applications and limitations. This will give you a solid foundation in regression analysis, paving the way for more advanced techniques later on.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Zestylicious Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#the-best-line",
    "href": "book/03-ols.html#the-best-line",
    "title": "3  Zestylicious Ordinary Least-squares Regression",
    "section": "\n3.3 The “Best Line”",
    "text": "3.3 The “Best Line”\nWhen using Ordinary Least Squares (OLS) to fit a regression line, our goal is to find the line that best represents the relationship between our dependent variable \\(Y\\) and independent variable \\(X\\). But what does “best” mean?\nImagine you have a scatter plot of data points. Now, consider drawing two different lines through this plot. Each one of these lines represent a set of predictions. They also represent a way to represent the relationship between the dependent variable \\(Y\\) and independent variable \\(X\\)\n\n\nLine A (Blue): A line that follows the general trend of the data very well.\n\nLine B (Red): A line that doesn’t capture the trend as accurately.\n\n\n\n\n\n\n\n\n\n\nprint(\"HelloWorld\")\n\nHelloWorld\n\n\n\n3.3.1 Understanding Residuals\nFor each data point, the residual is the vertical distance between the actual \\(Y\\) value and the predicted \\(Y\\) value (denoted \\(\\hat{Y}\\)) on the line. In simple terms, it tells us how far off our prediction is for each point given the same \\(X\\) value. If a line fits well, these residuals will be small, meaning our predictions of the \\(Y\\) variable are close to the actual value.\nOLS quantifies how well a line fits the data by calculating the Sum of Squared Errors (SSE). The SSE is obtained by:\n\nComputing the residual for each data point.\nSquaring each residual (this ensures that errors do not cancel each other out).\nSumming all these squared values.\n\n\\[\nSSE=\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\n\\]\nA lower SSE indicates a line that is closer to the actual data points. OLS chooses the best line by finding the one with the smallest SSE.\n\n3.3.2 Quantifying the Fit with SSE\nWe can compare the two lines by computing their SSE. The code below calculates and prints the SSE for each line:\n\n# Calculate the Sum of Squared Errors for the correct model (Blue)\nsse_correct &lt;- sum((df$Price - df$Predicted_Correct)^2)\n\n# Calculate the Sum of Squared Errors for the manually adjusted model (Red)\nsse_wrong &lt;- sum((df$Price - df$Predicted_Wrong)^2)\n\n# Print the SSEs for each line\ncat(\"SSE for Best-Fit Line (Blue line):\", sse_correct, \"\\n\")\n\nSSE for Best-Fit Line (Blue line): 83.23529 \n\ncat(\"SSE for Worse-Fit Line (Red line):\", sse_wrong, \"\\n\")\n\nSSE for Worse-Fit Line (Red line): 3972 \n\n\nWhen you run this code, you’ll observe that the blue line (Line A) has a much lower SSE compared to the red line (Line B). This tells us that the blue line is a better fit for the data because its predictions are, on average, closer to the actual values.\nIn summary, OLS selects the “best line” by minimizing the sum of squared errors, ensuring that the total error between predicted and actual values is as small as possible.\n\n3.3.3 Why Squared Errors?\nWhen measuring how far off our predictions are, errors can be positive (if our prediction is too low) or negative (if it’s too high). If we simply added these errors together, they could cancel each other out, hiding the true size of the mistakes. By squaring each error, we convert all numbers to positive values so that every mistake counts.\nIn addition, squaring makes big errors count a lot more than small ones. This means that a large mistake will have a much bigger impact on the overall error, encouraging the model to reduce those large errors and improve its overall accuracy.\n\n3.3.4 The Mathematical Formulation of the OLS Model\nNow that we understand how OLS finds the best-fitting line by minimizing the differences between the actual and predicted values, let’s look at the math behind it.\nIn a simple linear regression with one predictor, we express the relationship between the outcome \\(Y\\) and the predictor \\(X\\) using the following equation. Note that OLS fits a straight line to the data, which is why the equation takes the familiar form of a straight line:\n\\[\nY=\\beta_0+\\beta_1X+\\epsilon\n\\]\nHere’s what each part of the equation means:\n\n\n\\(Y\\) is the dependent variable or the outcome we want to predict.\n\n\\(X\\) is the independent variable or the predictor that we believe influences \\(Y\\).\n\n\\(\\beta_0\\) is the intercept. It represents the predicted value of \\(Y\\) when \\(X=0\\).\n\n\\(\\beta_1\\) is the slope. It tells us how much \\(Y\\) is expected to change for each one-unit increase in \\(X\\).\n\n\\(\\epsilon\\) is the error term. It captures the random variation in \\(Y\\) that cannot be explained by \\(X\\).\n\nThis equation provides a clear mathematical framework for understanding how changes in \\(X\\) are expected to affect \\(Y\\), while also accounting for random variation. In the upcoming section, we will explore our toy dataset to showcase this equation and OLS in action.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Zestylicious Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#case-study-understanding-financial-behaviors",
    "href": "book/03-ols.html#case-study-understanding-financial-behaviors",
    "title": "3  Zestylicious Ordinary Least-squares Regression",
    "section": "\n3.4 Case Study: Understanding Financial Behaviors",
    "text": "3.4 Case Study: Understanding Financial Behaviors\nTo demonstrate Ordinary Least Squares (OLS) in action, we will walk through a case study using a toy dataset. This case study will help us understand the financial behaviors of students and identify the factors that influence their Net_Money, the amount of money left over at the end of each month. We will approach this case study using the data science workflow described in a previous chapter, ensuring a structured approach to problem-solving and model building.\n\n3.4.1 The Dataset\nOur dataset captures various aspects of students’ financial lives. Each row represents a student, and the columns describe different characteristics. Below is a breakdown of the variables:\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\nHas_Job\nWhether the student has a job (0 = No, 1 = Yes).\n\n\nYear_of_Study\nThe student’s current year of study (e.g., 1st year, 2nd year, etc.).\n\n\nFinancially_Dependent\nWhether the student is financially dependent on someone else (0 = No, 1 = Yes).\n\n\nMonthly_Allowance\nThe amount of financial support the student receives each month.\n\n\nCooks_at_Home\nWhether the student prepares their own meals (0 = No, 1 = Yes).\n\n\nLiving_Situation\nThe student’s living arrangement (e.g., living with family, in a shared apartment, etc.).\n\n\nHousing_Type\nThe type of housing the student lives in (e.g., rented, owned, dormitory).\n\n\nGoes_Out_Spends_Money\nHow frequently the student goes out and spends money (1 = rarely, 5 = very often).\n\n\nDrinks_Alcohol\nWhether the student drinks alcohol (0 = No, 1 = Yes).\n\n\nNet_Money\nThe amount of money the student has left at the end of the month after income and expenses.\n\n\nMonthly_Earnings\nThe student’s earnings from any part-time jobs or other income sources.\n\n\n\nHere’s a sample of the dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHas_Job\nYear_of_Study\nFinancially_Dependent\nMonthly_Allowance\nCooks_at_Home\nLiving_Situation\nHousing_Type\nGoes_Out_Spends_Money\nDrinks_Alcohol\nNet_Money\nMonthly_Earnings\n\n\n\n0\n1\n0\n658.99\n0\n3\n1\n6\n0\n529.34\n0.00\n\n\n1\n3\n0\n592.55\n0\n3\n2\n3\n1\n992.72\n941.92\n\n\n1\n4\n1\n602.54\n0\n2\n2\n2\n1\n557.30\n876.57\n\n\n\n\nThis dataset provides a structured way to analyze the financial habits of students and determine which factors contribute most to their financial stability.\n\n3.4.2 The Problem We’re Trying to Solve\nOur goal in this case study is to understand which factors impact a student’s net money. Specifically, we aim to identify which characteristics, such as having a job, monthly earnings, or financial support, explain why some students have more money left over at the end of the month than others.\nThe key question we want to answer is:\n\nWhich factors have the biggest influence on a student’s net money?\n\nBy applying OLS to this dataset, we can:\n\nMeasure how much each factor contributes to variations in net money. For example, we can determine the increase in net money associated with a one-unit increase in monthly earnings.\nIdentify whether each factor has a positive or negative effect on net money.\nUnderstand the unique contribution of each variable while accounting for the influence of others. This helps us isolate the effect of, say, having a job from that of receiving financial support.\nPredict a student’s net money based on their characteristics. These insights could help institutions design targeted financial literacy programs or interventions to improve financial stability.\nEvaluate the overall performance of our model using statistical measures such as R-squared and p-values. This not only confirms the significance of our findings but also guides improvements in future analyses.\n\nIn summary, using OLS in this case study allows us to break down complex financial behaviors into understandable components. This powerful tool provides clear, actionable insights into which factors are most important, paving the way for more informed decisions and targeted interventions.\n\n3.4.3 Study Design\nNow that we’ve introduced our case study and dataset, it’s time to follow the data science workflow step by step. The first step is to define the main statistical inquiries we want to address. As mentioned earlier, our key question is:\n\nWhich factors have the biggest influence on a student’s net money?\n\nTo answer this question, we will adopt an inferential analysis approach rather than a predictive analysis approach. Let’s quickly review the difference between these two methods:\nInferential vs. Predictive Analysis\n\n\nInferential Analysis explores and quantifies the relationships between explanatory variables (e.g., student characteristics) and the response variable (Net Money). For example, we might ask: Does having a part-time job significantly affect a student’s net money, and by how much? The goal here is to understand these effects and assess their statistical significance.\n\nPredictive Analysis focuses on accurately forecasting the response variable using new data. In this case, the question could be: Can we predict a student’s net money based on factors like monthly earnings, living situation, and spending habits? The emphasis is on building a model that produces reliable predictions, even if it doesn’t fully explain the underlying relationships.\n\n3.4.4 Applying Study Design to Our Case Study\nFor our case study, we are interested in understanding how factors such as Has_Job, Monthly_Earnings, and Spending_Habits affect a student’s Net Money. This leads us to adopt an inferential approach. We aim to answer questions like:\n\nDoes having a part-time job lead to significantly higher net money?\nHow much do a student’s monthly earnings influence their financial situation?\nDo spending habits, like going out frequently, decrease a student’s net money?\n\nUsing OLS, we will estimate the impact of each factor and determine whether these effects are statistically significant. This inferential analysis will help us understand which variables have the greatest influence on students’ financial outcomes.\nIf our goal were instead to predict a student’s future Net Money based on their characteristics, we would adopt a predictive approach. Although our focus here is on inference, it’s important to recognize that OLS is versatile and can be applied in both contexts.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Zestylicious Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#data-collection-and-wrangling",
    "href": "book/03-ols.html#data-collection-and-wrangling",
    "title": "3  Zestylicious Ordinary Least-squares Regression",
    "section": "\n3.5 Data Collection and Wrangling",
    "text": "3.5 Data Collection and Wrangling\nWith the statistical questions clearly defined, the next step is to ensure that the data is appropriately prepared for analysis. Although we already have the dataset, it is valuable to consider how this data could have been collected to better understand its context and potential limitations.\n\n3.5.1 Data Collection\nFor a study like ours, data on students’ financial behaviors could have been collected through various methods:\n\n\nSurveys: Students might have been asked about their employment status, earnings, and spending habits through structured questionnaires. While surveys can capture self-reported financial behaviors, they may suffer from recall bias or social desirability bias.\n\nAdministrative Data: Universities or employers may maintain records on student income and employment, providing a more objective source of financial information. However, access to such data may be limited due to privacy regulations.\n\nFinancial Tracking Apps: Digital financial management tools can offer detailed, real-time data on student income and spending patterns. While these apps provide high granularity, they may introduce selection bias, as only students who use such apps would be represented in the dataset.\n\nRegardless of the data collection method, each approach presents challenges, such as missing data, reporting errors, or sample biases. Addressing these issues is a critical aspect of data wrangling.\n\n3.5.2 Data Wrangling\nNow that our dataset is ready, the next step is to clean and organize it so that it’s in the best possible shape for analysis using OLS. Data wrangling involves several steps that ensure our data is accurate, consistent, and ready for modeling. Here are some key tasks:\nHandling Missing Data\nThe first task is to ensure data integrity by checking for missing values. Missing data can occur for various reasons, such as unrecorded responses or errors in data entry. When we find missing values—for example, if some students don’t have recorded earnings or net money—we must decide how to handle these gaps. Common strategies include:\n\n\nRemoving incomplete records: If the amount of missing data is minimal or missingness is random.\n\nImputing missing values: Using logical estimates or averages if missingness follows a systematic pattern.\n\nIn our toy dataset, there are no missing values, as confirmed by:\n\ncolSums(is.na(data))\n\n              Has_Job         Year_of_Study Financially_Dependent \n                    0                     0                     0 \n    Monthly_Allowance         Cooks_at_Home      Living_Situation \n                    0                     0                     0 \n         Housing_Type Goes_Out_Spends_Money        Drinks_Alcohol \n                    0                     0                     0 \n            Net_Money      Monthly_Earnings \n                    0                     0 \n\n\nEncoding Categorical Variables\nFor regression analysis, we need to convert categorical variables into numerical representations. In R, binary variables like Has_Job and Drinks_Alcohol should be transformed into factors so that the model correctly interprets them as categorical data rather than continuous numbers. For example:\n\n# Convert binary categorical variables to factors\ndata &lt;- data |&gt;\n  mutate(Has_Job = as.factor(Has_Job),\n         Drinks_Alcohol = as.factor(Drinks_Alcohol),\n         Financially_Dependent = as.factor(Financially_Dependent),\n         Cooks_at_Home = as.factor(Cooks_at_Home))\n\nDetecting and Handling Outliers\nOutliers in continuous variables like Monthly_Earnings and Net_Money can distort the regression analysis by skewing results. We use the Interquartile Range (IQR) method to identify these extreme values. Specifically, any observation falling below 1.5 times the IQR below the first quartile (Q1) or above 1.5 times the IQR above the third quartile (Q3) is flagged as an outlier. These outliers are then treated as missing values and removed:\n\n# Using IQR method to filter out extreme values in continuous variables\nremove_outliers &lt;- function(x) {\n  Q1 &lt;- quantile(x, 0.25, na.rm = TRUE)\n  Q3 &lt;- quantile(x, 0.75, na.rm = TRUE)\n  IQR &lt;- Q3 - Q1\n  x[x &lt; (Q1 - 1.5 * IQR) | x &gt; (Q3 + 1.5 * IQR)] &lt;- NA\n  return(x)\n}\n\ndata &lt;- data |&gt;\n  mutate(across(c(Monthly_Earnings, Net_Money), remove_outliers))\n\n# Remove rows with newly introduced NAs due to outlier handling\ndata &lt;- na.omit(data)\n\nSplitting the Data for Model Training\nTo ensure that our OLS model generalizes well to unseen data, we split the dataset into training and testing subsets. The training set is used to estimate the model parameters, and the testing set is used to evaluate the model’s performance. This split is typically done in an 80/20 ratio, as shown below:\n\n# Splitting the dataset into training and testing sets\nset.seed(123)  # For reproducibility\ntrain_indices &lt;- sample(seq_len(nrow(data)), size = 0.8 * nrow(data))\ntrain_data &lt;- data[train_indices, ]\ntest_data &lt;- data[-train_indices, ]\n\nBy following these steps, checking for missing values, encoding categorical variables, handling outliers, and splitting the data, we ensure that our dataset is clean, well-organized, and ready for regression analysis using OLS.\nIt’s important to note, however, that these are just a few of the many techniques available during the data wrangling stage. Depending on the dataset and the specific goals of your analysis, you might also consider additional strategies such as feature scaling, normalization, advanced feature engineering, handling duplicate records, or addressing imbalanced data. Each of these techniques comes with its own set of solutions, and the optimal approach will depend on the unique challenges and objectives of your case.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Zestylicious Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#exploratory-data-analysis-eda",
    "href": "book/03-ols.html#exploratory-data-analysis-eda",
    "title": "3  Zestylicious Ordinary Least-squares Regression",
    "section": "\n3.6 Exploratory Data Analysis (EDA)",
    "text": "3.6 Exploratory Data Analysis (EDA)\nBefore diving into data modeling, it is crucial to develop a deep understanding of the relationships between variables in the dataset. This stage, known as Exploratory Data Analysis (EDA), helps us visualize and summarize the data, uncover patterns, detect anomalies, and test key assumptions that will inform our modeling decisions.\n\n3.6.1 Classifying Variables\nThe first step in EDA is to classify variables according to their types. This classification guides the selection of appropriate visualization techniques and modeling strategies. In our toy dataset, we categorize variables as follows:\n\n\nNet_Money serves as the response variable, representing a continuous outcome constrained by realistic income and expenses.\n\nThe regressors include a mix of binary, categorical, ordinal, and continuous variables.\n\nBinary variables, such as Has_Job and Drinks_Alcohol, take on only two values and need to be encoded for modeling.\nCategorical variables, like Living_Situation and Housing_Type, represent qualitative distinctions between different student groups.\nSome predictors, like Year_of_Study and Goes_Out_Spends_Money, follow an ordinal structure, meaning they have a meaningful ranking but no consistent numerical spacing.\nFinally, Monthly_Allowance and Monthly_Earnings are continuous variables, requiring attention to their distributions and potential outliers.\n\nBy classifying variables correctly at the outset, we ensure that they are analyzed and interpreted appropriately throughout the modeling process.\n\n3.6.2 Visualizing Variable Distributions\nOnce variables are classified, the next step is to explore their distributions. Understanding how variables are distributed is crucial for identifying potential issues such as skewness, outliers, or missing values. We employ different visualizations depending on the variable type:\nContinuous Variables\nWe begin by examining continuous variables, which are best visualized using histograms and boxplots.\nHistograms\nHistograms display the frequency distribution of a continuous variable. They allow us to assess the overall shape, central tendency, and spread of the data. For example, the histogram of Net_Money helps us determine if the variable follows a roughly normal distribution or if it is skewed. A normal distribution often appears bell-shaped, while skewness can indicate that the data might benefit from transformations (like logarithmic transformations) to meet the assumptions of regression analysis. In our case, the histogram below shows that Net_Money appears roughly normal.\n\n# Histogram of Net_Money\nhist(train_data$Net_Money, \n     main = \"Distribution of Net Money\", \n     xlab = \"Net Money\", \n     col = \"blue\", \n     border = \"white\")\n\n\n\n\n\n\n\nBoxplots\nBoxplots provide a concise summary of a variable’s distribution by displaying its quartiles and highlighting potential outliers. Outliers are typically defined as data points that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR. The boxplot below visualizes Net_Money and helps us quickly assess if there are any extreme values that might skew the analysis. In this case, the boxplot suggests that there are no significant outliers according to the IQR method (the method commonly used by ggplot to identify outliers).\n\n# Boxplot of Net Money\nboxplot(train_data$Net_Money, \n        main = \"Boxplot of Net Money\", \n        ylab = \"Net Money\", \n        col = \"lightblue\")\n\n\n\n\n\n\n\nBy visualizing the distribution of Net_Money with these plots, we gain valuable insights into its behavior. This understanding not only informs whether transformations are needed but also prepares us for deeper analysis as we move forward with regression modeling.\nCategorical and Ordinal Variables\nCategorical variables require a different approach from continuous ones because they represent distinct groups rather than numerical values. For these variables, bar charts are very effective. They display the frequency of each category, helping us understand the distribution of qualitative attributes.\nFor example, consider the variable Living_Situation. The bar chart below shows how many students fall into each category. From the chart, we can see that category 1 is more heavily represented, while categories 2 and 3 have roughly similar counts. This insight can be critical—if a category is underrepresented, you might need to consider grouping it with similar categories or applying techniques such as one-hot encoding to ensure that each category contributes appropriately to the model.\n\n# Bar plot of Living Situation\nbarplot(table(train_data$Living_Situation), \n        main = \"Living Situation Distribution\", \n        xlab = \"Living Situation\", \n        ylab = \"Frequency\", \n        col = \"purple\")\n\n\n\n\n\n\n\nFor ordinal variables (which have a natural order but not a fixed numerical interval), you might still use bar charts to show the ranking or frequency of each level. Additionally, understanding these distributions can help you decide whether to treat them as categorical variables or convert them into numeric scores for analysis.\nExploring Relationships Between Variables\nBeyond examining individual variables, it is crucial to explore how they interact with one another—especially the predictors and the response variable. Understanding these relationships helps identify which predictors might be influential in the model and whether any issues, like multicollinearity, could affect regression estimates.\nCorrelation Matrices\nFor continuous variables, correlation matrices provide a numerical summary of how strongly pairs of variables are related. High correlations between predictors might signal multicollinearity, which can distort model estimates. For demonstration, consider the correlation matrix computed for Net_Money, Monthly_Allowance, and Monthly_Earnings:\n\n# Correlation matrix\ncor_matrix &lt;- cor(train_data[, c(\"Net_Money\", \"Monthly_Allowance\", \"Monthly_Earnings\")], use = \"complete.obs\")\nprint(cor_matrix)\n\n                  Net_Money Monthly_Allowance Monthly_Earnings\nNet_Money         1.0000000        0.28835746       0.75743542\nMonthly_Allowance 0.2883575        1.00000000      -0.03669097\nMonthly_Earnings  0.7574354       -0.03669097       1.00000000\n\n\nIn the output, we observe a strong positive correlation (corr = 0.757) between Monthly_Earnings and Net_Money. This result is intuitive. Higher earnings typically lead to more money left at the end of the month, resulting in a higher Net_Money.\nScatterplots\nScatter plots visually depict the relationship between two continuous variables. For example, plotting Monthly_Allowance against Net_Money helps us assess whether students with higher allowances tend to have higher or lower net savings. In the scatter plot below, a slightly positive trend is visible. However, the points are quite scattered, indicating that while there may be a relationship, it is not overwhelmingly strong. Such visual insights might prompt further investigation, perhaps considering polynomial transformations or interaction terms if nonlinearity is suspected.\n\n# Scatter plot of Monthly Allowance vs. Net Money\nplot(train_data$Monthly_Allowance, train_data$Net_Money, \n     main = \"Net Money vs. Monthly Allowance\", \n     xlab = \"Monthly Allowance\", \n     ylab = \"Net Money\", \n     col = \"blue\", \n     pch = 19)\nabline(lm(Net_Money ~ Monthly_Allowance, data = train_data), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\nBoxplots for Categorical Variables\nFor categorical predictors, boxplots are an excellent tool to compare the distribution of the response variable across different groups. For instance, examining how Net_Money varies by Living_Situation can reveal whether students in different living arrangements experience different financial outcomes. In the boxplot below, the distributions of Net_Money across categories of Living_Situation appear quite similar. This similarity may suggest that Living_Situation has little impact on Net_Money in our dataset.\n\n# Boxplot of Net Money by Living Situation\nboxplot(Net_Money ~ Living_Situation, \n        data = train_data, \n        main = \"Net Money by Living Situation\", \n        xlab = \"Living Situation\", \n        ylab = \"Net Money\", \n        col = \"lightgreen\")\n\n\n\n\n\n\n\nSummary Statistics\nIn addition to visual exploration, descriptive statistics provide a numerical summary of the dataset that is especially useful for beginners. Summary statistics give you a snapshot of the central tendency and spread of your data, helping you quickly grasp its overall characteristics.\nFor instance, if you notice that the mean of Monthly_Earning is significantly higher than its median, it might suggest that a few high values (or outliers) are skewing the data.\n\n# Summary statistics for numerical variables\nsummary(train_data[, c(\"Net_Money\", \"Monthly_Allowance\", \"Monthly_Earnings\")])\n\n   Net_Money        Monthly_Allowance Monthly_Earnings\n Min.   :-1587.52   Min.   :  51.33   Min.   :   0    \n 1st Qu.: -399.87   1st Qu.: 402.50   1st Qu.:   0    \n Median :   78.36   Median : 500.55   Median : 291    \n Mean   :  120.74   Mean   : 501.85   Mean   : 502    \n 3rd Qu.:  618.59   3rd Qu.: 603.46   3rd Qu.:1014    \n Max.   : 1932.42   Max.   :1088.94   Max.   :1763    \n\n\n\n3.6.3 Key Takeaways from EDA\nConducting Exploratory Data Analysis (EDA) allows us to gain an initial understanding of the data and its underlying patterns before moving on to model building. Through EDA, we identify the types of variables present, examine their distributions, and uncover potential issues such as skewness, outliers, or multicollinearity. This process helps to highlight which variables might be strong predictors and which may require additional transformation or treatment. For instance, a strong correlation between two variables, like Monthly_Earnings and Net_Money, signals that earnings are likely a key driver of net savings. At the same time, observing differences in distributions or spotting similar patterns across groups in boxplots can inform us about the impact of categorical factors like Living_Situation.\nIt is important to remember that the insights gained from EDA are preliminary and primarily serve to inform further analysis. When we explore relationships between only two variables, we might overlook the influence of other factors, which could lead to misleading conclusions if taken in isolation. EDA is a crucial step for forming initial hypotheses and guiding decisions regarding data transformations, feature engineering, and the overall modeling strategy. With this foundation, we are better prepared to build a robust Ordinary Least Squares (OLS) regression model on data that has been carefully examined and understood.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Zestylicious Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#data-modelling",
    "href": "book/03-ols.html#data-modelling",
    "title": "3  Zestylicious Ordinary Least-squares Regression",
    "section": "\n3.7 Data Modelling",
    "text": "3.7 Data Modelling\nAfter conducting Exploratory Data Analysis (EDA), we transition to the modeling stage, where we apply a structured approach to uncover relationships between variables and predict outcomes. In this section, we focus on Ordinary Least Squares (OLS) regression, a widely used statistical technique for modeling linear relationships.\nOLS aims to estimate the effect of multiple predictors on an outcome variable by minimizing the sum of squared differences between observed and predicted values. This approach helps quantify financial behaviors, allowing us to interpret the impact of various factors on students’ net financial balance.\n\n3.7.1 Choosing a Suitable Regression Model\nThe choice of regression model depends on the patterns identified in EDA and the objectives of our analysis. Regression techniques vary in complexity, with some handling simple linear relationships and others accounting for more nuanced effects. Below are common approaches:\n\n\nSimple Linear Regression models the relationship between a single predictor and the response variable. This approach is suitable when we suspect a dominant factor driving financial balance.\n\nMultiple Linear Regression extends simple regression by incorporating multiple predictors, allowing us to account for various financial influences simultaneously.\n\nPolynomial Regression captures non-linear relationships by introducing polynomial terms of predictors, useful when relationships observed in scatter plots are curved rather than strictly linear.\n\nLog-Linear Models transform skewed distributions to improve interpretability and meet regression assumptions.\n\nRegularized Regression (Ridge and Lasso) applies penalties to regression coefficients to handle multicollinearity and enhance model generalization by reducing overfitting.\n\nGiven that our goal is to examine how multiple factors—such as income, expenses, and living arrangements—affect students’ financial balance, we select Multiple Linear Regression via OLS. This method allows us to quantify the influence of each predictor while controlling for confounding effects.\n\n3.7.2 Defining Modeling Parameters\nOnce we select OLS regression, we define the key modeling components: the response variable (dependent variable) and the predictor variables (independent variables).\nResponse Variable (Y):\nThe response variable, also known as the dependent variable, represents the financial outcome we aim to explain:\n\n\nNet_Money: The dependent variable representing financial balance.\nPredictor Variables (X):\nEach predictor variable is chosen based on its theoretical and statistical relevance in explaining financial behavior:\n\n\nHas_Job (Binary) – Indicates whether the student has a job (1 = Yes, 0 = No).\n\nFinancially_Dependent (Binary) – Identifies students who rely on external financial support.\n\nYear_of_Study (Ordinal) – Represents academic seniority (higher values indicate later years).\n\nGoes_Out_Spends_Money (Ordinal) – Measures spending behavior on a scale from 1 to 6.\n\nDrinks_Alcohol (Binary) – Identifies whether a student consumes alcohol, which may impact discretionary spending.\n\nMonthly_Allowance (Continuous) – Represents financial support received from family or scholarships.\n\nMonthly_Earnings (Continuous) – Reflects the student’s personal income from work.\n\nLiving_Situation (Categorical) – Encodes different living arrangements (e.g., dormitory, shared apartment, living with family).\n\nHousing_Type (Categorical) – Further distinguishes between different types of housing situations.\n\nCooks_at_Home (Binary) – Indicates whether the student regularly prepares meals at home.\n\nThese predictors capture a mix of economic, behavioral, and lifestyle factors, providing a comprehensive view of the drivers of student financial balance.\n\n3.7.3 Setting Up the Modeling Equation\nWith all predictors defined, the OLS regression equation models the relationship between Net_Money and the predictor variables:\n\\[\n\\begin{align}\n\\text{Net_Money} = \\beta_0 \\\\\n               & + \\beta_1 \\times \\text{Has_Job} \\\\\n               & + \\beta_2 \\times \\text{Financially_Dependent} \\\\\n               & + \\beta_3 \\times \\text{Year_of_Study} \\\\\n               & + \\beta_4 \\times \\text{Goes_Out_Spends_Money} \\\\\n               & + \\beta_5 \\times \\text{Drinks_Alcohol} \\\\\n               & + \\beta_6 \\times \\text{Monthly_Allowance} \\\\\n               & + \\beta_7 \\times \\text{Monthly_Earnings} \\\\\n               & + \\beta_8 \\times \\text{Living_Situation} \\\\\n               & + \\beta_9 \\times \\text{Housing_Type} \\\\\n               & + \\beta_{10} \\times \\text{Cooks_at_Home} \\\\\n               & + \\epsilon\n\\end{align}\n\\]\nwhere:\n\n\n\\(\\beta_0\\) represents the intercept, or the baseline Net Money when all predictors are set to zero.\n\n\\(\\beta_1, \\beta_2, ..., \\beta_{10}\\) are the regression coefficients, quantifying the impact of each predictor on financial balance.\n\n\\(\\epsilon\\) is the error term, accounting for unexplained variability and random noise.\n\nEach coefficient provides insight into how Net_Money changes when a specific predictor increases by one unit, holding all other factors constant. For example:\n\n\n\\(\\beta_5\\) (Drinks Alcohol) measures the financial impact of alcohol consumption, which may reflect higher discretionary spending.\n\n\\(\\beta_6\\) (Monthly Allowance) quantifies the increase in Net_Money per additional dollar of allowance.\n\n\\(\\beta_10\\) (Cooks at Home) indicates how much more (or less) financially stable students are when they cook at home instead of eating out.\n\nIf significant interaction effects exist—such as students who live independently having a different financial impact from increased earnings compared to those living with family—we can extend the model by adding interaction terms.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Zestylicious Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#estimation",
    "href": "book/03-ols.html#estimation",
    "title": "3  Zestylicious Ordinary Least-squares Regression",
    "section": "\n3.8 Estimation",
    "text": "3.8 Estimation\nWith the data modeling stage completed, we now move to estimation, where we fit the Ordinary Least Squares (OLS) regression model to the data and obtain numerical estimates for the regression coefficients. These estimates quantify how much each predictor contributes to the response variable, allowing us to measure their individual effects on Net Money.\nThe goal of estimation is to determine the best-fitting regression line by minimizing the sum of squared residuals—the differences between the observed and predicted values. This step provides a mathematical basis for analyzing financial behaviors in students.\n\n3.8.1 Fitting the Model\nTo estimate the regression coefficients, we fit the OLS model to the training data using Python (statsmodels) or R (lm). The model is trained using least squares estimation, which finds the coefficients that minimize the total squared error between observed values and predictions.\nIn R, we can fit the regression model using the lm() function:\n\n# Load necessary library\nlibrary(stats)\n\n# Fit the OLS model\nols_model &lt;- lm(Net_Money ~ Has_Job + Financially_Dependent + Year_of_Study + Goes_Out_Spends_Money + Drinks_Alcohol + Monthly_Allowance + Monthly_Earnings + Living_Situation + Housing_Type + Cooks_at_Home, data = train_data)\n\n# Display summary of model results\nsummary(ols_model)\n\n\nCall:\nlm(formula = Net_Money ~ Has_Job + Financially_Dependent + Year_of_Study + \n    Goes_Out_Spends_Money + Drinks_Alcohol + Monthly_Allowance + \n    Monthly_Earnings + Living_Situation + Housing_Type + Cooks_at_Home, \n    data = train_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-742.4 -144.9   14.7  157.9  675.1 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            -595.37433   46.68928 -12.752  &lt; 2e-16 ***\nHas_Job1                 73.93112   39.75871   1.859   0.0633 .  \nFinancially_Dependent1 -499.36309   15.42689 -32.370  &lt; 2e-16 ***\nYear_of_Study           -96.51877    6.85478 -14.081  &lt; 2e-16 ***\nGoes_Out_Spends_Money   -54.60494    3.93783 -13.867  &lt; 2e-16 ***\nDrinks_Alcohol1        -145.10637   15.74322  -9.217  &lt; 2e-16 ***\nMonthly_Allowance         1.47138    0.05200  28.295  &lt; 2e-16 ***\nMonthly_Earnings          0.94906    0.03704  25.625  &lt; 2e-16 ***\nLiving_Situation        102.95241    9.32478  11.041  &lt; 2e-16 ***\nHousing_Type             56.65388    9.72781   5.824 8.38e-09 ***\nCooks_at_Home1          -96.31098   15.83074  -6.084 1.83e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 216.4 on 786 degrees of freedom\nMultiple R-squared:  0.9018,    Adjusted R-squared:  0.9006 \nF-statistic: 722.1 on 10 and 786 DF,  p-value: &lt; 2.2e-16\n\n\n\n3.8.2 Interpreting the Coefficients\nAfter fitting the model, we examine the estimated coefficients to understand their impact. Each coefficient obtained from the OLS regression represents the expected change in Net_Money for a one-unit increase in the corresponding predictor, holding all other variables constant. The estimated regression equation can be expressed as:\n\\[\n\\begin{align}\n\\text{Net_Money} = -595.37 \\\\\n               & + 73.93 \\times \\text{Has_Job} \\\\\n               & - 499.36 \\times \\text{Financially_Dependent} \\\\\n               & - 96.52 \\times \\text{Year_of_Study} \\\\\n               & - 54.60 \\times \\text{Goes_Out_Spends_Money} \\\\\n               & - 145.11 \\times \\text{Drinks_Alcohol} \\\\\n               & + 1.47 \\times \\text{Monthly_Allowance} \\\\\n               & + 0.95 \\times \\text{Monthly_Earnings} \\\\\n               & + 102.95 \\times \\text{Living_Situation} \\\\\n               & + 56.65 \\times \\text{Housing_Type} \\\\\n               & - 96.31 \\times \\text{Cooks_at_Home} \\\\\n               & + \\epsilon\n\\end{align}\n\\]\nFor example:\n\nThe intercept (\\(\\beta_0=-595.37\\)) represents the expected financial balance for a student who has zero income, allowance, and falls at the baseline category for all categorical variables.\nA \\(1 increase in Monthly Allowance (\\)_6=1.47$) is associated with a $1.47 increase in Net Money, meaning students with higher allowances tend to have a higher financial balance.\n\nThese estimates provide an initial understanding of the direction and magnitude of relationships between predictors and financial balance. However, before drawing conclusions, we need to validate model assumptions and evaluate the statistical significance of each coefficient.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Zestylicious Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#goodness-of-fit",
    "href": "book/03-ols.html#goodness-of-fit",
    "title": "3  Zestylicious Ordinary Least-squares Regression",
    "section": "\n3.9 Goodness of Fit",
    "text": "3.9 Goodness of Fit\nAfter estimating the regression coefficients, the next step is to assess how well the model fits the data and whether it satisfies the assumptions of Ordinary Least Squares (OLS) regression. This evaluation ensures that the model is not only statistically valid but also generalizes well to unseen data. A well-fitting model should explain a substantial proportion of variation in the response variable while adhering to key statistical assumptions. If these assumptions are violated, model estimates may be biased, leading to misleading conclusions.\n\n3.9.1 Checking Model Assumptions\nOLS regression is built on several fundamental assumptions:\n\nlinearity\nindependence of errors\nhomoscedasticity\nnormality of residuals\n\nIf these assumptions hold, OLS provides unbiased, efficient, and consistent estimates. We assess each assumption through diagnostic plots and statistical tests.\nLinearity\nA core assumption of OLS is that the relationship between each predictor and the response variable is linear. If this assumption is violated, the model may systematically under- or overestimate Net_Money, leading to biased predictions. The Residuals vs. Fitted values plot is a common diagnostic tool for checking linearity. In a well-specified linear model, residuals should be randomly scattered around zero, without any discernible patterns. If the residuals exhibit a U-shaped or curved pattern, this suggests a non-linear relationship, indicating that transformations such as logarithmic, square root, or polynomial terms may be necessary.\nTo visualize linearity, we plot the residuals against the fitted values:\n\n# Residuals vs Fitted plot (R)\nplot(ols_model$fitted.values, residuals(ols_model), \n     main = \"Residuals vs Fitted\", xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n\n\n\n\n\n\nIf the residual plot displays a clear trend, polynomial regression or feature engineering may be required to better capture the underlying data structure.\nIndependence of Errors\nThe residuals, or errors, in an OLS model should be independent of one another. This assumption is particularly relevant in time-series or sequential data, where errors from one observation might influence subsequent observations, leading to autocorrelation. If the errors are correlated, the estimated standard errors will be biased, making hypothesis testing unreliable.\nThe Durbin-Watson test is commonly used to detect autocorrelation. This test produces a statistic that ranges between 0 and 4, where values close to 2 indicate no significant autocorrelation, while values near 0 or 4 suggest positive or negative correlation in the residuals.\n\ndwtest(ols_model)\n\n\n    Durbin-Watson test\n\ndata:  ols_model\nDW = 1.9581, p-value = 0.2777\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nIf the test suggests autocorrelation, a possible solution is to use time-series regression models such as Autoregressive Integrated Moving Average (ARIMA) or introduce lagged predictors to account for dependencies in the data.\nHomoscedasticity (Constant Variance of Errors)\nOLS regression assumes that the variance of residuals remains constant across all fitted values. If this assumption is violated, the model exhibits heteroscedasticity, where the spread of residuals increases or decreases systematically. This can result in inefficient coefficient estimates, making some predictors appear statistically significant when they are not.\nTo check for heteroscedasticity, we plot residuals against the fitted values and conduct a Breusch-Pagan test, which formally tests whether residual variance is constant.\n\nncvTest(ols_model)  # Test for homoscedasticity\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.4018794, Df = 1, p = 0.52612\n\n\nIf heteroscedasticity is detected, solutions include applying weighted least squares (WLS) regression, transforming the dependent variable (e.g., using a log transformation), or computing robust standard errors to correct for variance instability.\nNormality of Residuals\nFor valid hypothesis testing and confidence interval estimation, OLS assumes that residuals follow a normal distribution. If residuals deviate significantly from normality, statistical inference may be unreliable, particularly for small sample sizes.\nA Q-Q plot (Quantile-Quantile plot) is used to assess normality. If residuals are normally distributed, the points should lie along the reference line.\n\nqqnorm(residuals(ols_model))\nqqline(residuals(ols_model), col = \"red\")\n\n\n\n\n\n\n\nIf the plot reveals heavy tails or skewness, potential solutions include applying log or Box-Cox transformations to normalize the distribution. In cases where normality is severely violated, using a non-parametric model or bootstrapping confidence intervals may be appropriate.\n\n3.9.2 Evaluating Model Fit\nA good model should explain a large proportion of variance in the response variable.\nR-Squared\nBeyond checking assumptions, it is essential to assess how well the model explains variability in the response variable. One of the most commonly used metrics is R-Squared (\\(R^2\\)), which measures the proportion of variance in Net_Money that is explained by the predictors. An \\(R^2\\) value close to 1 indicates a strong model fit, whereas a low value suggests that important predictors may be missing or that the model is poorly specified.\nWe can retrieve the R-squared and Adjusted R-squared values from the model summary:\n\nsummary(ols_model)$r.squared  # R-squared value\n\n[1] 0.9018396\n\nsummary(ols_model)$adj.r.squared  # Adjusted R-squared\n\n[1] 0.9005907\n\n\nWhile \\(R^2\\) provides insight into model fit, it has limitations. Adding more predictors will always increase \\(R^2\\), even if those predictors have little explanatory power. Adjusted R-squared accounts for this by penalizing unnecessary variables, making it a better indicator of true model performance.\nA high \\(R^2\\) does not imply causation, nor does it confirm that the model is free from omitted variable bias or multicollinearity. Therefore, a strong goodness-of-fit measure should be complemented by careful assessment of residual behavior and coefficient significance.\nIdentifying Outliers and Influential Points\nOutliers and influential observations can distort regression estimates, making it crucial to detect and address them appropriately. One way to identify extreme residuals is through residual plots, where large deviations from zero may indicate problematic data points.\n\nplot(residuals(ols_model), main = \"Residual Plot\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n\n\n\n\n\n\nAnother important diagnostic tool is Cook’s Distance, which measures the influence of each observation on the regression results. Data points with Cook’s Distance values greater than 0.5 may significantly impact model estimates.\n\ncook_values &lt;- cooks.distance(ols_model)\nplot(cook_values, type = \"h\", main = \"Cook's Distance\")\n\n\n\n\n\n\n\nIf influential points are identified, the next steps involve investigating data quality, testing robust regression techniques, or applying Winsorization, which involves replacing extreme values with more moderate ones to reduce their impact.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Zestylicious Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#results",
    "href": "book/03-ols.html#results",
    "title": "3  Zestylicious Ordinary Least-squares Regression",
    "section": "\n3.10 Results",
    "text": "3.10 Results\nAfter validating the goodness of fit, we now assess how well the model performs in both predictive analysis and inferential analysis. This step involves using the trained model to generate predictions on unseen data and evaluating how well it generalizes beyond the training set. Additionally, we analyze the estimated regression coefficients to draw meaningful conclusions about student financial behaviors.\n\n3.10.1 Predictive Analysis\nA key objective of regression modeling is to generate reliable predictions. To assess how well our model generalizes, we apply it to the test dataset—a portion of the original data that was not used for model training. If the model’s predictions align closely with actual outcomes, we can conclude that it has strong predictive power.\nIn R, we use the predict() function to apply the trained OLS model to the test dataset:\n\n# Generate predictions on the test set\ny_pred &lt;- predict(ols_model, newdata=test_data)\n\nOnce predictions are generated, we evaluate their accuracy using common regression error metrics.\nPerformance Metrics\nModel accuracy is assessed using four standard error metrics:\n\n\nMean Absolute Error (MAE) measures the average absolute differences between predicted and actual values. A lower MAE indicates better model accuracy.\n\nMean Squared Error (MSE) calculates the average squared differences between predicted and actual values, penalizing larger errors more heavily.\n\nRoot Mean Squared Error (RMSE) is the square root of MSE, making it easier to interpret since it retains the same units as the dependent variable (Net_Money).\n\nR-squared (\\(R^2\\)) quantifies the proportion of variance in Net_Money explained by the model. A higher \\(R^2\\) value indicates better model performance.\n\nThese metrics can be computed as follows:\n\n# Extract response variable from test data\ny_test &lt;- test_data$Net_Money\n\n# Calculate metrics in R\nmae &lt;- mean(abs(y_test - y_pred))\nmse &lt;- mean((y_test - y_pred)^2)\nrmse &lt;- sqrt(mse)\nr2 &lt;- summary(ols_model)$r.squared\n\ncat(sprintf(\"MAE: %.2f, MSE: %.2f, RMSE: %.2f, R-squared: %.2f\", mae, mse, rmse, r2))\n\nMAE: 183.76, MSE: 54660.22, RMSE: 233.80, R-squared: 0.90\n\n\nIf the RMSE is significantly larger than MAE, it suggests that the model is highly sensitive to large prediction errors, meaning that certain extreme values are having a disproportionate impact on the model’s performance. If the R-squared value is low, it may indicate that important predictors are missing from the model or that the relationship between predictors and Net_Money is more complex than a linear relationship can capture.\n\n3.10.2 Inferential Analysis\nBeyond prediction, OLS regression allows us to interpret the estimated coefficients to uncover patterns in students’ financial behaviors. Each coefficient represents the expected change in Net_Money for a one-unit increase in the corresponding predictor, assuming all other variables remain constant.\nInsights from Regression Coefficients\nHere are a few insights that we can extract from the regression model result:\n\nThe intercept (-595.37) represents the estimated financial balance for a baseline student (someone with Has_Job = 0, Financially_Dependent = 0, Year_of_Study = 0, etc.). Since a zero value for Year_of_Study is not meaningful in our context, the intercept itself has limited interpretability but serves as the starting point for estimating Net_Money based on predictor values.\nFinancial Dependency (Financially_Dependent) has a strong negative effect (-499.36, p &lt; 2e-16), meaning that students who rely financially on others (e.g., parents, guardians) tend to have significantly lower Net_Money. This could be due to a lack of independent income sources or higher expenses associated with dependence on external financial support.\nSocial Spending Habits (Goes_Out_Spends_Money) also have a negative impact (-54.60, p &lt; 2e-16), meaning that students who frequently go out and spend money experience a decline in Net_Money. This result aligns with expectations, as higher discretionary spending directly reduces available financial balance.\nCooking at Home (Cooks_at_Home) has a negative effect (-96.31, p = 1.83e-09), which might seem counterintuitive at first. One possible explanation is that students who cook at home may already have lower budgets, leading them to adopt cost-saving habits, rather than the habit itself directly causing financial decline.\nOverall, the model explains 90.18% of the variance in Net_Money (\\(R^2\\) = 0.9018, Adjusted \\(R^2\\) = 0.9006), suggesting a strong predictive capability. The F-statistic (722.1, p &lt; 2.2e-16) confirms that the model as a whole is statistically significant.\n\nThese findings provide a comprehensive view of student financial behaviors, reinforcing expected relationships (such as the role of allowances and earnings) while also highlighting unexpected trends (such as the negative effect of cooking at home). The next step is to integrate these results into a cohesive narrative, translating statistical insights into actionable recommendations.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Zestylicious Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#storytelling",
    "href": "book/03-ols.html#storytelling",
    "title": "3  Zestylicious Ordinary Least-squares Regression",
    "section": "\n3.11 Storytelling",
    "text": "3.11 Storytelling\nThe final step in our data science workflow is storytelling, where we translate our analytical findings into actionable insights. This stage ensures that our results are clearly understood by both technical and non-technical audiences. Effective storytelling involves summarizing insights, using visuals for clarity, and making data-driven recommendations.\n\n\nFun fact!\n\n\nZestylicious! That mouth-puckering, lemon-squirted, totally tangy kick!\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 3.2",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Zestylicious Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html",
    "href": "book/04-gamma.html",
    "title": "4  Smoketastic Gamma Regression",
    "section": "",
    "text": "Fun fact!\n\n\nSmoketastic! For foods that seem to have been grilled by a campfire enthusiast.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 4.1",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Smoketastic Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/05-beta.html",
    "href": "book/05-beta.html",
    "title": "5  Soup-erb Beta Regression",
    "section": "",
    "text": "Fun fact!\n\n\nSoup-erb! Soup that’s so heartwarming it feels like a cozy hug.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 5.1",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Soup-erb Beta Regression</span>"
    ]
  },
  {
    "objectID": "book/06-parametric-survival.html",
    "href": "book/06-parametric-survival.html",
    "title": "6  Crunchified Parametric Survival Regression",
    "section": "",
    "text": "Fun fact!\n\n\nCrunchified! Extra crunchy, borderline noisy; could probably shatter glass.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 6.1\n\n\n\n\nDefinition of cumulative distribution function\n\n\nLet \\(Y\\) be a random variable either discrete or continuous. Its cumulative distribution function (CDF) \\(F_Y(y)  : \\mathbb{R} \\rightarrow [0, 1]\\) refers to the probability that \\(Y\\) is less or equal than an observed value \\(y\\):\n\\[\nF_Y(y) = P(Y \\leq y).\n\\tag{6.1}\\]\nThen, we have the following by type of random variable:\n\nWhen \\(Y\\) is discrete, whose support is \\(\\mathcal{Y}\\), suppose it has a PMF \\(P_Y(Y = y)\\). Then, the CDF is mathematically represented as:\n\n\\[\nF_Y(y) = \\sum_{\\substack{t \\in \\mathcal{Y} \\\\ t \\leq y}} P_Y(Y = t).\n\\tag{6.2}\\]\n\nWhen \\(Y\\) is continuous, whose support is \\(\\mathcal{Y}\\), suppose it has a PDF \\(f_Y(y)\\). Then, the CDF is mathematically represented as:\n\n\\[\nF_Y(y) = \\int_{-\\infty}^y f_Y(t) \\mathrm{d}t.\n\\tag{6.3}\\]\nNote that in Equation 6.2 and Equation 6.3, we use the auxiliary variable \\(t\\) since we do not compute the summation or integral over the observed \\(y\\) given its role on either the PMF or PDF. Therefore, we use this auxiliary variable \\(t\\).\n\n\n\n\nHeads-up on the properties of the cumulative distribution function!\n\n\nIt is important to clarify that a valid CDF \\(F_Y(y)\\) fulfils the following properties:\n\n\n\\(F_Y(y)\\) must never be a decreasing function.\nGiven that \\(F_Y(y)  : \\mathbb{R} \\rightarrow [0, 1]\\), it must never evaluate to be \\(&lt; 0\\) or \\(&gt; 1\\). The output of a CDF is a cumulative probability, hence the previous bounds.\nWhen \\(y \\rightarrow -\\infty\\), if follows that \\(F_Y(y) \\rightarrow 0\\).\nWhen \\(y \\rightarrow \\infty\\), if follows that \\(F_Y(y) \\rightarrow 1\\).\n\nNow, in the case of a CDF corresponding to a continuous random variable \\(Y\\), there is an additional handy property that relates the CDF \\(F_Y(y)\\) to the PDF \\(f_Y(y)\\):\n\\[\nf_Y(y) = \\frac{\\mathrm{d}}{\\mathrm{d}y} F_Y(y).\n\\tag{6.4}\\]\nEquation 6.4 indicates that the PDF of \\(Y\\) can be obtained by taking the first derivative of the CDF with respect to \\(y\\).",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Crunchified Parametric Survival Regression</span>"
    ]
  },
  {
    "objectID": "book/07-semiparametric-survival.html",
    "href": "book/07-semiparametric-survival.html",
    "title": "7  Butteryfied Semiparametric Survival Regression",
    "section": "",
    "text": "Fun fact!\n\n\nButteryfied! So rich and buttery it practically slides off the plate.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 7.1",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Butteryfied Semiparametric Survival Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html",
    "href": "book/08-binary-logistic.html",
    "title": "Binary Logistic Regression",
    "section": "",
    "text": "8.1 Introduction\nIn many real-world problems, the outcome we’re trying to predict is not a number — it’s a yes or no, success or failure, clicked or didn’t click. For example:\nThese outcomes are binary: they can take on only two possible values, typically coded as 1 (event occurs) and 0 (event does not occur).\nTo model such outcomes, we need a regression approach that produces predicted probabilities between 0 and 1 — not arbitrary numbers on the real line. This is where Binary Logistic Regression comes in.\nIn this chapter, we’ll see how logistic regression:",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/09-binomial-logistic.html",
    "href": "book/09-binomial-logistic.html",
    "title": "9  Cheesified Binomial Logistic Regression",
    "section": "",
    "text": "Fun fact!\n\n\nCheesified! Oozing with cheese in every crevice; a cheese lover’s paradise.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma &lt;br/&gt;Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 9.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cheesified Binomial Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/10-classical-poisson.html",
    "href": "book/10-classical-poisson.html",
    "title": "10  Bubblarious Classical Poisson Regression",
    "section": "",
    "text": "Fun fact!\n\n\nBubblarious! For all the boba, fizzy drinks, and seltzers that go pop!\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 10.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bubblarious Classical Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/11-negative-binomial.html",
    "href": "book/11-negative-binomial.html",
    "title": "11  Umami-zing Negative Binomial Regression",
    "section": "",
    "text": "Fun fact!\n\n\nUmami-zing! Savory to the point where you start craving a second plate… and a third.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 11.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Umami-zing Negative Binomial Regression</span>"
    ]
  },
  {
    "objectID": "book/12-zero-inflated-poisson.html",
    "href": "book/12-zero-inflated-poisson.html",
    "title": "12  Spicetacular Zero-Inflated Poisson Regression",
    "section": "",
    "text": "Fun fact!\n\n\nSpicetacular! The kind of spicy that requires a fire extinguisher at the ready.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n        {{Overdispersed or &lt;br/&gt;Underdispersed &lt;br/&gt;Data}}\n        {{Zero Inflated &lt;br/&gt;Data}}\n          )Chapter 12: &lt;br/&gt;Zero Inflated Poisson &lt;br/&gt;Regression(\n            (Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 12.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Spicetacular Zero-Inflated Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/13-generalized-poisson.html",
    "href": "book/13-generalized-poisson.html",
    "title": "13  Herbalicious Generalized Poisson Regression",
    "section": "",
    "text": "Fun fact!\n\n\nHerbalicious! Loaded with herbs and greens; like chewing through a botanical garden.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: &lt;br/&gt;Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n        {{Zero Inflated &lt;br/&gt;Data}}\n          )Chapter 12: &lt;br/&gt;Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed or &lt;br/&gt;Underdispersed &lt;br/&gt;Data}}\n          )Chapter 13: &lt;br/&gt;Generalized &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Generalized &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 13.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Herbalicious Generalized Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/14-multinomial-logistic.html",
    "href": "book/14-multinomial-logistic.html",
    "title": "14  Picklified Multinomial Logistic Regression",
    "section": "",
    "text": "Fun fact!\n\n\nPicklified! When everything, even dessert, tastes a bit pickled!\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n        {{Zero Inflated &lt;br/&gt;Data}}\n          )Chapter 12: &lt;br/&gt;Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed or &lt;br/&gt;Underdispersed &lt;br/&gt;Data}}\n          )Chapter 13: &lt;br/&gt;Generalized &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Generalized &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n      {{Categorical &lt;br/&gt;Outcome Y}}\n        {{Nominal &lt;br/&gt;Outcome Y}}\n          )Chapter 14: &lt;br/&gt;Multinomial &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Multinomial &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 14.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Picklified Multinomial Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/15-ordinal-logistic.html",
    "href": "book/15-ordinal-logistic.html",
    "title": "15  Tang-tastic Ordinal Logistic Regression",
    "section": "",
    "text": "Fun fact!\n\n\nTang-tastic! So tangy it could wake you up better than coffee.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n        {{Overdispersed or &lt;br/&gt;Underdispersed &lt;br/&gt;Data}}\n          )Chapter 13: &lt;br/&gt;Generalized &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Generalized &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n        {{Zero Inflated &lt;br/&gt;Data}}\n          )Chapter 12: &lt;br/&gt;Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n      {{Categorical &lt;br/&gt;Outcome Y}}\n        {{Nominal &lt;br/&gt;Outcome Y}}\n          )Chapter 14: &lt;br/&gt;Multinomial &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Multinomial &lt;br/&gt;Outcome Y)\n        {{Ordinal &lt;br/&gt;Outcome Y}}\n          )Chapter 15: &lt;br/&gt;Ordinal &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Logistic &lt;br/&gt;Distributed &lt;br/&gt;Cumulative Outcome &lt;br/&gt;Probability)\n\n\n\n\n\n\n\n\nFigure 15.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Tang-tastic Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/references.html",
    "href": "book/references.html",
    "title": "References",
    "section": "",
    "text": "Bellhouse, D. R. 2004. “The Reverend Thomas\nBayes, FRS: A Biography to Celebrate the Tercentenary of His\nBirth.” Statistical Science 19 (1): 3–43. https://doi.org/10.1214/088342304000000189.\n\n\nCasella, G., and R. Berger. 2024. Statistical Inference.\nChapman & Hall/CRC Texts in Statistical Science. CRC Press. https://www.taylorfrancis.com/books/mono/10.1201/9781003456285/statistical-inference-roger-berger-george-casella.\n\n\nConsul, P. C., and G. C. Jain. 1973. “A Generalization of the\nPoisson Distribution.” Technometrics 15 (4): 791–99. http://www.jstor.org/stable/1267389.\n\n\nEarlom, Richard. 1793. “Brook Taylor - National Portrait\nGallery.” NPG D6930; Brook Taylor - Portrait - National\nPortrait Gallery. National Portrait Gallery. https://www.npg.org.uk/collections/search/portrait/mw40921/Brook-Taylor.\n\n\nGelbart, Michael. 2017. “Data Science Terminology.” UBC\nMDS. Master of Data Science at the University of British Columbia.\nhttps://ubc-mds.github.io/resources_pages/terminology/.\n\n\nGregory, James. 1668. Vera circuli et\nhyperbolae quadratura cui accedit geometria pars vniuersalis inseruiens\nquantitatum curuarum transmutationi & mensurae. Authore Iacobo\nGregorio Abredonensi. Padua, Italy: Patavii: typis heredum\nPauli Frambotti bibliop. https://archive.org/details/ita-bnc-mag-00001357-001/page/n10/mode/2up.\n\n\nHarding, Edward. 1798. Portrait of Colin MacLaurin.\nCourtesy of the Smithsonian Libraries and Archives. https://library.si.edu/image-gallery/72863.\n\n\nJohnson, A. A., M. Q. Ott, and M. Dogucu. 2022. Bayes Rules!: An\nIntroduction to Applied Bayesian Modeling. Chapman & Hall/CRC\nTexts in Statistical Science. CRC Press. https://www.bayesrulesbook.com/.\n\n\nLeemis, Larry. n.d. “Univariate\nDistribution Relationship\nChart.” https://www.math.wm.edu/~leemis/chart/UDR/UDR.html.\n\n\nMaclaurin, Colin. 1742. A Treatise of Fluxions. Edinburgh,\nScotland: Printed for the Author by T.W.; T. Ruddimans. https://archive.org/details/treatiseonfluxio02macl/page/n5/mode/2up.\n\n\nO’Donnell, T. 1936. History of Life Insurance\nin Its Formative Years. Compiled from Approved Sources by T.\nO’Donnell. Chicago.\n\n\nR Core Team. 2024. “R: A Language and Environment for Statistical\nComputing.” Vienna, Austria: R Foundation for Statistical\nComputing. https://www.R-project.org/.\n\n\nScotland, National Galleries of. n.d. Professor James Gregory, 1638\n- 1675 (1). Mathematician. Professor James Gregory, 1638 - 1675\n(1). Mathematician | National Galleries. https://www.nationalgalleries.org/art-and-artists/31132/professor-james-gregory-1638-1675-mathematician.\n\n\nSoch, Joram, The Book of Statistical Proofs, Maja, Pietro Monticone,\nThomas J. Faulkenberry, Alex Kipnis, Kenneth Petrykowski, et al. 2024.\n“StatProofBook/StatProofBook.github.io:\nStatProofBook 2023.” Zenodo. https://doi.org/10.5281/zenodo.10495684.\n\n\nTaylor, Brook. 1715. Methodus incrementorum\ndirecta & inversa. Auctore Brook Taylor, LL. D. & Regiae\nSocietatis Secretario. London, England: Typis Pearsonianis\nProstant apud Gul. Innys ad Insignia Principis in Coemeterio Paulino\nMDCCXV. https://archive.org/details/bim_eighteenth-century_methodus-incrementorum-d_taylor-brook_1717.\n\n\nThe Pandas Development Team. 2024. “Pandas-Dev/Pandas:\nPandas.” Zenodo. https://doi.org/10.5281/zenodo.3509134.\n\n\nVan Rossum, Guido, and Fred L. Drake. 2009. Python 3 Reference\nManual. Scotts Valley, CA: CreateSpace.\n\n\nWeisstein, Eric W. n.d.a. “Gamma Function.” From\nMathWorld–A Wolfram Web Resource. https://mathworld.wolfram.com/GammaFunction.html.\n\n\n———. n.d.b. “Taylor Series.” From MathWorld–A Wolfram\nWeb Resource. https://mathworld.wolfram.com/TaylorSeries.html.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "book/A-dictionary.html",
    "href": "book/A-dictionary.html",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "",
    "text": "A",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#a",
    "href": "book/A-dictionary.html#a",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "",
    "text": "Alternative hypothesis\nIn a hypothesis testing, an alternative hypothesis is denoted by \\(H_1\\). This hypothesis corresponds to the complement (i.e., the opposite) of the null hypothesis \\(H_0\\). Since the whole inferential process is designed to assess the strength of the evidence in favour or against of \\(H_0\\), any inferential conclusion against \\(H_0\\) can be worded as “rejecting \\(H_0\\) in favour of \\(H_1\\).” In plain words, \\(H_1\\) is an inferential statement associated to a non-status quo in some population(s) or system(s) of interest, which might refer to actual signal for the researcher in question.\nLet us assume you observe some data \\(y\\) from some population(s) or system(s) of interest governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nMoreover, suppose this observed data \\(y\\) follows certain probability distribution \\(\\mathcal{D}(\\cdot)\\) in a generative model \\(m\\) as in\n\\[\nm: y \\sim \\mathcal{D}(\\boldsymbol{\\theta}).\n\\]\nLet \\(\\boldsymbol{\\Theta}_0^c \\subset \\boldsymbol{\\theta}\\) denote the non-status quo for the parameter(s) to be tested. Then, the alternative hypothesis is mathematically defined as\n\\[\nH_1: \\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}_0^c \\quad \\text{where} \\quad \\boldsymbol{\\Theta}_0^c \\subset \\boldsymbol{\\theta}.\n\\]\nAttribute\n\n\nEquivalent to:\n\n\nCovariate, exogeneous variable, explanatory variable, feature, independent variable, input, predictor or regressor.\n\n\nAverage\nLet \\(Y\\) be a random variable whose support is \\(\\mathcal{Y}\\). In general, the expected value or mean \\(\\mathbb{E}(Y)\\) of this random variable is defined as a weighted average according to its corresponding probability distribution. In other words, this measure of central tendency \\(\\mathbb{E}(Y)\\) aims to find the middle value of this random variable by weighting all its possible values in its support \\(\\mathcal{Y}\\) as dictated by its probability distribution.\nGiven the above definition, when \\(Y\\) is a discrete random variable whose probability mass function (PMF) is \\(P_Y(Y = y)\\), then its weighted average is mathematically defined as\n\\[\n\\mathbb{E}(Y) = \\sum_{y \\in \\mathcal{Y}} y \\cdot P_Y(Y = y).\n\\]\nWhen \\(Y\\) is a continuous random variable whose probability density function (PDF) is \\(f_Y(y)\\), its weighted average is mathematically defined as\n\\[\n\\mathbb{E}(Y) = \\int_{\\mathcal{Y}} y \\cdot f_Y(y) \\mathrm{d}y.\n\\]\n\n\nEquivalent to:\n\n\nExpected value or mean.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#b",
    "href": "book/A-dictionary.html#b",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "B",
    "text": "B\nBayesian statistics\nThis statistical school of thinking also relies on the frequency of events to estimate specific parameters of interest in a population or system. Nevertheless, unlike frequentist statistics, Bayesian statisticians use prior knowledge on the population parameters to update their estimations on them along with the current evidence they can gather. This evidence is in the form of the repetition of \\(n\\) experiments involving a random phenomenon. All these ingredients allow Bayesian statisticians to make inference by conducting appropriate hypothesis testings, which are designed differently from their mainstream frequentist counterpart.\nUnder the umbrella of this approach, we assume that our governing parameters are random; i.e., they have their own sample space and probabilities associated to their corresponding outcomes. The statistical process of inference is heavily backed up by probability theory mostly in the form of the Bayes theorem (named after Reverend Thomas Bayes, an English statistician from the 18th century). This theorem uses our current evidence along with our prior beliefs to deliver a posterior distribution of our random parameter(s) of interest.\nBayes’ rule\nSuppose you have two events of interest, \\(A\\) and \\(B\\), in a random phenomenon of a population or system of interest. From Equation A.4, we can state the following expression for the conditional probability of \\(A\\) given \\(B\\):\n\\[\nP(A | B) = \\frac{P(A \\cap B)}{P(B)} \\quad \\text{if $P(B) &gt; 0$.}\n\\tag{A.1}\\]\nNote the conditional probability of \\(B\\) given \\(A\\) can be stated as:\n\\[\n\\begin{align*}\nP(B | A) &= \\frac{P(B \\cap A)}{P(A)} \\quad \\text{if $P(A) &gt; 0$} \\\\\n&= \\frac{P(A \\cap B)}{P(A)} \\quad \\text{since $P(B \\cap A) = P(A \\cap B)$.}\n\\end{align*}\n\\tag{A.2}\\]\nThen, we can manipulate Equation A.2 as follows:\n\\[\nP(A \\cap B) = P(B | A) \\times P(A).\n\\]\nThe above result can be plugged into Equation A.1:\n\\[\n\\begin{align*}\nP(A | B) &= \\frac{P(A \\cap B)}{P(B)} \\\\\n&= \\frac{P(B | A) \\times P(A)}{P(B)}.\n\\end{align*}\n\\tag{A.3}\\]\nEquation A.3 is called the Bayes’ rule. We are basically flipping around conditional probabilities.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#c",
    "href": "book/A-dictionary.html#c",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "C",
    "text": "C\nCritical value\nThe critical value of a hypothesis testing defines the region for which we might reject \\(H_0\\) in favour of \\(H_1\\). This critical value is in the function of the significance level \\(\\alpha\\) and test flavour. It is located on the corresponding \\(x\\)-axis of the probability distribution of \\(H_0\\). Hence, this value acts as a threshold to decide either of the following:\n\nIf the observed test statistic exceeds a given critical value, then we have enough statistical evidence to reject \\(H_0\\) in favour of \\(H_1\\).\nIf the observed test statistic does not exceed a given critical value, then we have enough statistical evidence to fail to reject \\(H_0\\).\nConditional probability\nSuppose you have two events of interest, \\(A\\) and \\(B\\), in a random phenomenon, in a population or system of interest. These two events belong to the sample space \\(S\\). Moreover, assume that the probability of event \\(B\\) is such that\n\\[\nP(B) &gt; 0,\n\\]\nwhich is considered the conditioning event.\nHence, the conditional probability event \\(A\\) given event \\(B\\) is defined as\n\\[\nP(A | B) = \\frac{P(A \\cap B)}{P(B)},\n\\tag{A.4}\\]\nwhere \\(P(A \\cap B)\\) is read as the probability of the intersection of events \\(A\\) and \\(B\\).\nConfidence interval\nA confidence interval provides an estimated range of values within which the true population parameter is likely to fall, based on the sample data. It reflects the degree of uncertainty associated with the obtained estimate.\nFor instance, a 95% confidence interval means that if the study were repeated many times using different random samples from the same population or system of interest, approximately 95% of the resulting intervals would contain the true parameter.\nContinuous random variable\nLet \\(Y\\) be a random variable whose support is \\(\\mathcal{Y}\\). If this support \\(\\mathcal{Y}\\) corresponds to an uncountably infinite set of possible values, then \\(Y\\) is considered a continuous random variable.\nNote a continuous random variable could be\n\n\ncompletely unbounded (i.e., its set of possible values goes from \\(-\\infty\\) to \\(\\infty\\) as in \\(-\\infty &lt; y &lt; \\infty\\)),\n\npositively unbounded (i.e., its set of possible values goes from \\(0\\) to \\(\\infty\\) as in \\(0 \\leq y &lt; \\infty\\)),\n\nnegatively unbounded (i.e., its set of possible values goes from \\(-\\infty\\) to \\(0\\) as in \\(-\\infty &lt; y \\leq 0\\)), or\n\nbounded between two values \\(a\\) and \\(b\\) (i.e., its set of possible values goes from \\(a\\) to \\(b\\) as in \\(a \\leq y \\leq b\\)).\nCovariate\n\n\nEquivalent to:\n\n\nAttribute, exogeneous variable, explanatory variable, feature, independent variable, input, predictor or regressor.\n\n\nCumulative distribution function\nLet \\(Y\\) be a random variable either discrete or continuous. Its cumulative distribution function (CDF) \\(F_Y(y)  : \\mathbb{R} \\rightarrow [0, 1]\\) refers to the probability that \\(Y\\) is less or equal than an observed value \\(y\\):\n\\[\nF_Y(y) = P(Y \\leq y).\n\\]\nThen, we have the following by type of random variable:\n\nWhen \\(Y\\) is discrete, whose support is \\(\\mathcal{Y}\\), suppose it has a probability mass function (PMF) \\(P_Y(Y = y)\\). Then, the CDF is mathematically represented as:\n\n\\[\nF_Y(y) = \\sum_{\\substack{t \\in \\mathcal{Y} \\\\ t \\leq y}} P_Y(Y = t).\n\\tag{A.5}\\]\n\nWhen \\(Y\\) is continuous, whose support is \\(\\mathcal{Y}\\), suppose it has a probability density function (PDF) \\(f_Y(y)\\). Then, the CDF is mathematically represented as:\n\n\\[\nF_Y(y) = \\int_{-\\infty}^y f_Y(t) \\mathrm{d}t.\n\\tag{A.6}\\]\nNote that in Equation A.5 and Equation A.6, we use the auxiliary variable \\(t\\) since we do not compute the summation or integral over the observed \\(y\\) given its role on either the PMF or PDF. Therefore, we use this auxiliary variable \\(t\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#d",
    "href": "book/A-dictionary.html#d",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "D",
    "text": "D\nDependent variable\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\nEquivalent to:\n\n\nEndogeneous variable, response variable, outcome, output or target.\n\n\nDiscrete random variable\nLet \\(Y\\) be a random variable whose support is \\(\\mathcal{Y}\\). If this support \\(\\mathcal{Y}\\) corresponds to a finite set or a countably infinite set of possible values, then \\(Y\\) is considered a discrete random variable.\nFor instance, we can encounter discrete random variables which could be classified as\n\n\nbinary (i.e., a finite set of two possible values),\n\ncategorical (either nominal or ordinal, which have a finite set of three or more possible values), or\n\ncounts (which might have a finite set or a countably infinite set of possible values as integers).\nDispersion",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#e",
    "href": "book/A-dictionary.html#e",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "E",
    "text": "E\nEndogeneous variable\n\n\nEquivalent to:\n\n\nDependent variable, outcome, output, response variable or target.\n\n\nEquidispersion\nEstimate\nSuppose we have an observed random sample of size \\(n\\) with values \\(y_1, \\dots , y_n\\). Then, we apply a given estimator mathematical rule to these \\(n\\) observed values. Hence, this numerical computation is called an estimate of our population parameter of interest.\nEstimator\nAn estimator is a mathematical rule involving the random variables \\(Y_1, \\dots, Y_n\\) from our random sample of size \\(n\\). As its name says, this rule allows us to estimate our population parameter of interest.\nExpected value\nLet \\(Y\\) be a random variable whose support is \\(\\mathcal{Y}\\). In general, the expected value or mean \\(\\mathbb{E}(Y)\\) of this random variable is defined as a weighted average according to its corresponding probability distribution. In other words, this measure of central tendency \\(\\mathbb{E}(Y)\\) aims to find the middle value of this random variable by weighting all its possible values in its support \\(\\mathcal{Y}\\) as dictated by its probability distribution.\nGiven the above definition, when \\(Y\\) is a discrete random variable whose probability mass function (PMF) is \\(P_Y(Y = y)\\), then its expected value is mathematically defined as\n\\[\n\\mathbb{E}(Y) = \\sum_{y \\in \\mathcal{Y}} y \\cdot P_Y(Y = y).\n\\tag{A.7}\\]\nWhen \\(Y\\) is a continuous random variable whose probability density function (PDF) is \\(f_Y(y)\\), its expected value is mathematically defined as\n\\[\n\\mathbb{E}(Y) = \\int_{\\mathcal{Y}} y \\cdot f_Y(y) \\mathrm{d}y.\n\\tag{A.8}\\]\n\n\nEquivalent to:\n\n\nAverage or mean.\n\n\nExogeneous variable\n\n\nEquivalent to:\n\n\nAttribute, covariate, explanatory variable, feature, independent variable, input, predictor or regressor.\n\n\nExplanatory variable\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, feature, independent variable, input, predictor or regressor.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#f",
    "href": "book/A-dictionary.html#f",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "F",
    "text": "F\nFalse negative\nA false negative is defined as incorrectly failing to reject the null hypothesis \\(H_0\\) in favour of the alternative hypothesis \\(H_1\\) when, in fact, \\(H_0\\) is false.\n\n\nEquivalent to:\n\n\nType II error.\n\n\nFalse positive\nA false positive is defined as incorrectly rejecting the null hypothesis \\(H_0\\) in favour of the alternative hypothesis \\(H_1\\) when, in fact, \\(H_0\\) is true. Table A.1 summarizes the types of inferential conclusions in function on whether \\(H_0\\) is true or not.\n\n\nTable A.1: Types of inferential conclusions in a frequentist hypothesis testing.\n\n\n\n\n\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\nReject \\(H_0\\)\nType I error (False positive)\nCorrect\n\n\nFail to reject \\(H_0\\)\nCorrect\nType II error (False negative)\n\n\n\n\n\n\n\n\nEquivalent to:\n\n\nType I error.\n\n\nFeature\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, explanatory variable, independent variable, input, predictor or regressor.\n\n\nFrequentist statistics\nThis statistical school of thinking heavily relies on the frequency of events to estimate specific parameters of interest in a population or system. This frequency of events is reflected in the repetition of \\(n\\) experiments involving a random phenomenon within this population or system.\nUnder the umbrella of this approach, we assume that our governing parameters are fixed. Note that, within the philosophy of this school of thinking, we can only make precise and accurate predictions as long as we repeat our \\(n\\) experiments as many times as possible, i.e.,\n\\[\nn \\rightarrow \\infty.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#g",
    "href": "book/A-dictionary.html#g",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "G",
    "text": "G\nGeneralized linear model\nGenerative model\nSuppose you observe some data \\(y\\) from a population or system of interest. Moreover, let us assume this population or system is governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nIf we state that the random variable \\(Y\\) follows certain probability distribution \\(\\mathcal{D}(\\cdot)\\), then we will have a generative model \\(m\\) such that\n\\[\nm: Y \\sim \\mathcal{D}(\\boldsymbol{\\theta}).\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#h",
    "href": "book/A-dictionary.html#h",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "H",
    "text": "H\nHypothesis\nSuppose you observe some data \\(y\\) from some population(s) or system(s) of interest governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nMoreover, we assume this observed data \\(y\\) follows certain probability distribution \\(\\mathcal{D}(\\cdot)\\) in a generative model \\(m\\) as in\n\\[\nm: y \\sim \\mathcal{D}(\\boldsymbol{\\theta}).\n\\]\nBeginning from the fact that \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\) where \\(\\boldsymbol{\\Theta} \\in \\mathbb{R}^k\\), a statistical hypothesis is a general statement about some parameter vector \\(\\boldsymbol{\\theta}\\) in regards to specific values contained in vector \\(\\boldsymbol{\\Theta}^*\\) such that\n\\[\nH: \\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}^* \\quad \\text{where} \\quad \\boldsymbol{\\Theta}^* \\subset \\boldsymbol{\\Theta}.\n\\]\nHypothesis testing\nA hypothesis testing is the decision rule we have to apply between the null and alternative hypotheses, via our sample data, to fail to reject or reject the null hypothesis.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#i",
    "href": "book/A-dictionary.html#i",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "I",
    "text": "I\nIndependence\nSuppose you have two events of interest, \\(A\\) and \\(B\\), in a random phenomenon of a population or system of interest. These two events are statistically independent if event \\(B\\) does not affect event \\(A\\) and vice versa. Therefore, the probability of their corresponding intersection is given by:\n\\[\nP(A \\cap B) = P(A) \\times P(B).\n\\]\nLet us expand the above definition to a random variable framework:\n\nSuppose you have a set of \\(n\\) discrete random variables \\(Y_1, \\dots, Y_n\\) whose supports are \\(\\mathcal{Y_1}, \\dots, \\mathcal{Y_n}\\) with probability mass functions (PMFs) \\(P_{Y_1}(Y_1 = y_1), \\dots, P_{Y_n}(Y_n = y_n)\\) respectively. That said, the joint PMF of these \\(n\\) random variables is the multiplication of their corresponding standalone PMFs:\n\n\\[\n\\begin{align*}\nP_{Y_1, \\dots, Y_n}(Y_1 = y_1, \\dots, Y_n = y_n) &= \\prod_{i = 1}^n P_{Y_i}(Y_i = y_i) \\\\\n& \\qquad \\text{for all} \\\\\n& \\qquad \\quad y_i \\in \\mathcal{Y}_i, i = 1, \\dots, n.\n\\end{align*}\n\\tag{A.9}\\]\n\nSuppose you have a set of \\(n\\) continuous random variables \\(Y_1, \\dots, Y_n\\) whose supports are \\(\\mathcal{Y_1}, \\dots, \\mathcal{Y_n}\\) with probability density functions (PDFs) \\(f_{Y_1}(y_1), \\dots, f_{Y_n}(y_n)\\) respectively. That said, the joint PDF of these \\(n\\) random variables is the multiplication of their corresponding standalone PDFs:\n\n\\[\n\\begin{align*}\nf_{Y_1, \\dots, Y_n}(y_1, \\dots, y_n) &= \\prod_{i = 1}^n f_{Y_i}(y_i) \\\\\n& \\qquad \\text{for all} \\\\\n& \\qquad \\quad y_i \\in \\mathcal{Y}_i, i = 1, \\dots, n.\n\\end{align*}\n\\tag{A.10}\\]\nIndependent variable\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, explanatory variable, feature, input, predictor or regressor.\n\n\nInput\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, explanatory variable, feature, independent variable, predictor or regressor.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#m",
    "href": "book/A-dictionary.html#m",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "M",
    "text": "M\nMean\nLet \\(Y\\) be a random variable whose support is \\(\\mathcal{Y}\\). In general, the expected value or mean \\(\\mathbb{E}(Y)\\) of this random variable is defined as a weighted average according to its corresponding probability distribution. In other words, this measure of central tendency \\(\\mathbb{E}(Y)\\) aims to find the middle value of this random variable by weighting all its possible values in its support \\(\\mathcal{Y}\\) as dictated by its probability distribution.\nGiven the above definition, when \\(Y\\) is a discrete random variable whose probability mass function (PMF) is \\(P_Y(Y = y)\\), then its mean is mathematically defined as\n\\[\n\\mathbb{E}(Y) = \\sum_{y \\in \\mathcal{Y}} y \\cdot P_Y(Y = y).\n\\]\nWhen \\(Y\\) is a continuous random variable whose probability density function (PDF) is \\(f_Y(y)\\), its mean is mathematically defined as\n\\[\n\\mathbb{E}(Y) = \\int_{\\mathcal{Y}} y \\cdot f_Y(y) \\mathrm{d}y.\n\\]\n\n\nEquivalent to:\n\n\nAverage or expected value.\n\n\nMeasure of central tendency\nProbabilistically, a measure of central tendency is defined as a metric that identifies a central or typical value of a given probability distribution. In other words, a measure of central tendency refers to a central or typical value that a given random variable might take when we observe various realizations of this variable over a long period.\nMeasure of uncertainty\nProbabilistically, a measure of uncertainty refers to the spread of a given random variable when we observe its different realizations in the long term. Note a larger spread indicates more variability in these realizations. On the other hand, a smaller spread denotes less variability in these realizations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#n",
    "href": "book/A-dictionary.html#n",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "N",
    "text": "N\nNull hypothesis\nIn a hypothesis(s) testing, a null hypothesis is denoted by \\(H_0\\). The whole inferential process is designed to assess the strength of the evidence in favour or against this null hypothesis. In plain words, \\(H_0\\) is an inferential statement associated to the status quo in some population(s) or system(s) of interest, which might refer to no signal for the researcher in question.\nAgain, suppose you observe some data \\(y\\) from some population(s) or system(s) of interest governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nMoreover, we assume this observed data \\(y\\) follows certain probability distribution \\(\\mathcal{D}(\\cdot)\\) in a generative model \\(m\\) as in\n\\[\nm: y \\sim \\mathcal{D}(\\boldsymbol{\\theta}).\n\\]\nLet \\(\\boldsymbol{\\Theta}_0 \\subset \\boldsymbol{\\theta}\\) denote the status quo for the parameter(s) to be tested. Then, the null hypothesis is mathematically defined as\n\\[\nH_0: \\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}_0 \\quad \\text{where} \\quad \\boldsymbol{\\Theta}_0 \\subset \\boldsymbol{\\theta}.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#o",
    "href": "book/A-dictionary.html#o",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "O",
    "text": "O\nObserved effect\nAn observed effect is the difference between the estimate provided the observed random sample (of size \\(n\\), as in \\(y_1, \\dots, y_n\\)) to the hypothesized value(s) of the population parameter(s) depicted in the statistical hypotheses.\nOutcome\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\nEquivalent to:\n\n\nDependent variable, endogeneous variable, response variable, output or target.\n\n\nOutput\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\nEquivalent to:\n\n\nDependent variable, endogeneous variable, response variable, outcome or target.\n\n\nOverdispersion",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#p",
    "href": "book/A-dictionary.html#p",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "P",
    "text": "P\nParameter\nIt is a characteristic (numerical or even non-numerical, such as a distinctive category) that summarizes the state of our population or system of interest.\nNote the standard mathematical notation for population parameters are Greek letters (for more insights, you can check Appendix B). Moreover, in practice, these population parameter(s) of interest will be unknown to the data scientist or researcher. Instead, they would use formal statistical inference to estimate them.\nPopulation\nIt is a whole collection of individuals or items that share distinctive attributes. As data scientists or researchers, we are interested in studying these attributes, which we assume are governed by parameters. In practice, we must be as specific as possible when defining our given population such that we would frame our entire data modelling process since its very early stages.\nNote that the term population could be exchanged for the term system, given that certain contexts do not specifically refer to individuals or items. Instead, these contexts could refer to processes whose attributes are also governed by parameters.\nPower\nThe statistical power of a test \\(1 -\\beta\\) is the complement of the conditional probability \\(\\beta\\) of failing to reject the null hypothesis \\(H_0\\) given that \\(H_0\\) is false, which is mathematically represented as\n\\[\nP \\left( \\text{Failing to reject $H_0$} | \\text{$H_0$ is false} \\right) = \\beta;\n\\]\nyielding\n\\[\n\\text{Power} = 1 - \\beta.\n\\]\nIn plain words, \\(1 - \\beta \\in [0, 1]\\) is the probabilistic ability of our hypothesis testing to detect any signal in our inferential process, if there is any. The larger the power in our power analysis, the less prone we are to commit a type II error.\n\n\nEquivalent to:\n\n\nTrue positive rate.\n\n\nPower analysis\nThe power analysis is a set of statistical tools used to compute the minimum required sample size \\(n\\) for any given inferential study. These tools require the significance level, power, and effect size (i.e., the magnitude of the signal) the researcher aims to detect via their inferential study. This analysis seeks to determine whether observed results are likely due to chance or represent a true and meaningful effect.\nPredictor\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, explanatory variable, feature, independent variable, input or regressor.\n\n\nProbability\nLet \\(A\\) be an event of interest in a random phenomenon, in a population or system of interest, whose all possible outcomes belong to a given sample space \\(S\\). Generally, the probability for this event \\(A\\) happening can be mathematically depicted as \\(P(A)\\). Moreover, suppose we observe the random phenomenon \\(n\\) times such as we were running some class of experiment, then \\(P(A)\\) is defined as the following ratio:\n\\[\nP(A) = \\frac{\\text{Number of times event $A$ is observed}}{n},\n\\tag{A.11}\\]\nas the \\(n\\) times we observe the random phenomenon goes to infinity.\nEquation A.11 will always put \\(P(A)\\) in the following numerical range:\n\\[\n0 \\leq P(A) \\leq 1.\n\\]\nProbability distribution\nWhen we set a random variable \\(Y\\), we also set a new set of \\(v\\) possible outcomes \\(\\mathcal{Y} = \\{ y_1, \\dots, y_v\\}\\) coming from the sample space \\(S\\). This new set of possible outcomes \\(\\mathcal{Y}\\) corresponds to the support of the random variable \\(Y\\) (i.e., all the possible values that could be taken on once we execute a given random experiment involving \\(Y\\)).\nThat said, let us suppose we have a sample space of \\(u\\) elements defined as\n\\[\nS = \\{ s_1, \\dots, s_u \\},\n\\]\nwhere each one of these elements has a probability assigned via a function \\(P_S(\\cdot)\\) such that\n\\[\nP(S) = \\sum_{i = 1}^u P_S(s_i) = 1.\n\\]\nwhich has to satisfy Equation A.14.\nThen, the probability distribution of \\(Y\\), i.e., \\(P_Y(\\cdot)\\) assigns a probability to each observed value \\(Y = y_j\\) (with \\(j = 1, \\dots, v\\)) if and only if the outcome of the random experiment belongs to the sample space, i.e., \\(s_i \\in S\\) (for \\(i = 1, \\dots, u\\)) such that \\(Y(s_i) = y_j\\):\n\\[\nP_Y(Y = y_j) = P \\left( \\left\\{ s_i \\in S : Y(s_i) = y_j \\right\\} \\right).\n\\]\nProbability density function\nLet \\(Y\\) be a continuous random variable whose support is \\(\\mathcal{Y}\\). Furthermore, consider a function \\(f_Y(y)\\) such that\n\\[\nf_Y(y) : \\mathbb{R} \\rightarrow \\mathbb{R}\n\\]\nwith\n\\[\nf_Y(y) \\geq 0.\n\\]\nThen, \\(f_Y(y)\\) is considered a probability density function (PDF) if the probability of \\(Y\\) taking on a value within the range represented by the subset \\(A \\subset \\mathcal{Y}\\) is equal to\n\\[\nP_Y(Y \\in A) = \\int_A f_Y(y) \\mathrm{d}y\n\\]\nwith\n\\[\n\\int_{\\mathcal{Y}} f_Y(y) \\mathrm{d}y = 1.\n\\]\nProbability mass function\nLet \\(Y\\) be a discrete random variable whose support is \\(\\mathcal{Y}\\). Moreover, suppose that \\(Y\\) has a probability distribution such that\n\\[\nP_Y(Y = y) : \\mathbb{R} \\rightarrow [0, 1]\n\\]\nwhere, for all \\(y \\notin \\mathcal{Y}\\), we have\n\\[\nP_Y(Y = y) = 0\n\\]\nand\n\\[\n\\sum_{y \\in \\mathcal{Y}} P_Y(Y = y) = 1.\n\\] Then, \\(P_Y(Y = y)\\) is considered a probability mass function (PMF).\n\\(p\\)-value\nA \\(p\\)-value refers to the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic coming from our observed random sample of size \\(n\\). This \\(p\\)-value is obtained via the probability distribution of \\(H_0\\) and the observed test statistic.\nAlternatively to a critical value, we can reject or fail to reject the null hypothesis \\(H_0\\) using this \\(p\\)-value as follows:\n\nIf the \\(p\\)-value associated to the observed test statistic exceeds a given significance level \\(\\alpha\\), then we have enough statistical evidence to reject \\(H_0\\) in favour of \\(H_1\\).\nIf the \\(p\\)-value associated to the observed test statistic does not exceed a given significance level \\(\\alpha\\), then we have enough statistical evidence to fail to reject \\(H_0\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#r",
    "href": "book/A-dictionary.html#r",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "R",
    "text": "R\nRandom Sample\nA random sample is a collection of random variables \\(Y_1, \\dots, Y_n\\) of size \\(n\\) coming from a given population or system of interest. Note that the most elementary definition of a random sample assumes that these \\(n\\) random variables are mutually independent and identically distributed (which is abbreviated as iid).\nThe fact that these \\(n\\) random variables are identically distributed indicates that they have the same mathematical form for their corresponding probability mass functions (PMFs) or probability density function (PDFs), depending on whether they are discrete or continuous respectively. Hence, under a generative modelling approach in a population or system of interest governed by \\(k\\) parameters contained in the vector\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T,\n\\]\nwe can apply the iid property in an elementary random sample to obtain the following joint probability distributions:\n\nIn the case of \\(n\\) iid discrete random variables \\(Y_1, \\dots, Y_n\\) whose common standalone PMF is \\(P_Y(Y = y | \\boldsymbol{\\theta})\\) with support \\(\\mathcal{Y}\\), the joint PMF is mathematically expressed as\n\n\\[\n\\begin{align*}\nP_{Y_1, \\dots, Y_n}(Y_1 = y_1, \\dots, Y_n = y_n | \\boldsymbol{\\theta}) &= \\prod_{i = 1}^n P_Y(Y = y_i | \\boldsymbol{\\theta}) \\\\\n& \\qquad \\text{for all} \\\\\n& \\qquad \\quad y_i \\in \\mathcal{Y}, i = 1, \\dots, n.\n\\end{align*}\n\\tag{A.12}\\]\n\nIn the case of \\(n\\) iid continuous random variables \\(Y_1, \\dots, Y_n\\) whose common standalone PDF is \\(f_Y(y | \\boldsymbol{\\theta})\\) with support \\(\\mathcal{Y}\\), the joint PDF is mathematically expressed as\n\n\\[\n\\begin{align*}\nf_{Y_1, \\dots, Y_n}(y_1, \\dots, y_n | \\boldsymbol{\\theta}) &= \\prod_{i = 1}^n f_Y(y_i | \\boldsymbol{\\theta}) \\\\\n& \\qquad \\text{for all} \\\\\n& \\qquad \\quad y_i \\in \\mathcal{Y}, i = 1, \\dots, n.\n\\end{align*}\n\\tag{A.13}\\]\nUnlike Equation A.9 and Equation A.10, note that Equation A.12 and Equation A.13 do not indicate a subscript for \\(Y\\) in the corresponding probability distributions since we have identically distributed random variables. Furthermore, the joint distributions are conditioned on the population parameter vector \\(\\boldsymbol{\\theta}\\) which reflects our generative modelling approach.\n\n\nSomewhat equivalent to:\n\n\nTraining dataset.\n\n\nRandom variable\nA random variable is a function where the input values correspond to real numbers assigned to events belonging to the sample space \\(S\\), and whose outcome is one of these real numbers after executing a given random experiment. For instance, a random variable (and its support, i.e., real numbers) is depicted with an uppercase such that\n\\[Y \\in \\mathbb{R}.\\]\nRegression analysis\nRegressor\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, explanatory variable, feature, independent variable, input or predictor.\n\n\nResponse variable\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\nEquivalent to:\n\n\nDependent variable, endogeneous variable, outcome, output or target.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#s",
    "href": "book/A-dictionary.html#s",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "S",
    "text": "S\nSample space\nLet \\(A\\) be an event of interest in a random phenomenon in a population or system of interest. The sample space \\(S\\) of event \\(A\\) denotes the set of all the possible random outcomes we might encounter every time we randomly observe \\(A\\) such as we were running some class of experiment.\nNote each of these outcomes has a determined probability associated with them. If we add up all these probabilities, the probability of the sample \\(S\\) will be one, i.e.,\n\\[\nP(S) = 1.\n\\tag{A.14}\\]\nSignificance level\nThe significance level \\(\\alpha\\) is defined as the conditional probability of rejecting the null hypothesis \\(H_0\\) given that \\(H_0\\) is true. This can be mathematically represented as\n\\[\nP \\left( \\text{Reject $H_0$} | \\text{$H_0$ is true} \\right) = \\alpha.\n\\]\nIn plain words, \\(\\alpha \\in [0, 1]\\) allows us to probabilistically control for type I error since we are dealing with random variables in our inferential process. The significance level can be thought as one of the main hypothesis testing and power analysis settings.\nStandard error\nThe standard error allows us to quantify the extent to which an estimate coming from an observed random sample (of size \\(n\\), as in \\(y_1, \\dots, y_n\\)) may deviate from the expected value under the assumption that the null hypothesis is true.\nIt plays a critical role in determining whether an observed effect is likely attributable to random variation or represents a statistically significant finding. In the absence of the standard error, it would not be possible to rigorously assess the reliability or precision of an estimate.\nSupervised learning\nSurvival analysis",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#t",
    "href": "book/A-dictionary.html#t",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "T",
    "text": "T\nTarget\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\nEquivalent to:\n\n\nDependent variable, endogeneous variable, response variable, outcome or output.\n\n\nTest statistic\nThe test statistic is a function of the random sample of size \\(n\\), i.e., it is in the function of the random variables \\(Y_1, \\dots, Y_n\\). Therefore, the test statistic will also be a random variable, whose observed value will describe how closely the probability distribution from which the random sample comes from matches the probability distribution of the null hypothesis \\(H_0\\).\nMore specifically, once we have obtained the observed effect and standard error from our observed random sample, we can compute the corresponding observed test statistic. This test statistic computation will be placed on the corresponding \\(x\\)-axis of the probability distribution of \\(H_0\\) so we can reject or fail to reject it accordingly.\nTraining dataset\n\n\nSomewhat equivalent to:\n\n\nRandom sample.\n\n\nTrue positive rate\nThe statistical true positive rate of a test \\(1 -\\beta\\) is the complement of the conditional probability \\(\\beta\\) of failing to reject the null hypothesis \\(H_0\\) given that \\(H_0\\) is false, which is mathematically represented as\n\\[\nP \\left( \\text{Failing to reject $H_0$} | \\text{$H_0$ is false} \\right) = \\beta;\n\\]\nyielding\n\\[\n\\text{Power} = 1 - \\beta.\n\\]\nIn plain words, \\(1 - \\beta \\in [0, 1]\\) is the probabilistic ability of our hypothesis testing to detect any signal in our inferential process, if there is any. The larger the true positive rate in our power analysis, the less prone we are to commit a type II error.\n\n\nEquivalent to:\n\n\nPower.\n\n\nType I error\nType I error is defined as incorrectly rejecting the null hypothesis \\(H_0\\) in favour of the alternative hypothesis \\(H_1\\) when, in fact, \\(H_0\\) is true.\n\n\nEquivalent to:\n\n\nFalse positive.\n\n\nType II error\nType II error is defined as incorrectly failing to reject the null hypothesis \\(H_0\\) in favour of the alternative hypothesis \\(H_1\\) when, in fact, \\(H_0\\) is false. Table A.2 summarizes the types of inferential conclusions in function on whether \\(H_0\\) is true or not.\n\n\nTable A.2: Types of inferential conclusions in a frequentist hypothesis testing.\n\n\n\n\n\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\nReject \\(H_0\\)\nType I error (False positive)\nCorrect\n\n\nFail to reject \\(H_0\\)\nCorrect\nType II error (False negative)\n\n\n\n\n\n\n\n\nEquivalent to:\n\n\nFalse negative.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#u",
    "href": "book/A-dictionary.html#u",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "U",
    "text": "U\nUnderdispersion",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#v",
    "href": "book/A-dictionary.html#v",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "V",
    "text": "V\nVariance\nLet \\(Y\\) be a discrete or continuous random variable whose support is \\(\\mathcal{Y}\\) with a mean represented by \\(\\mathbb{E}(Y)\\). Then, the variance of \\(Y\\) is the mean of the squared deviation from the corresponding mean as follows:\n\\[\n\\text{Var}(Y) = \\mathbb{E}\\left\\{[ Y - \\mathbb{E}(Y)]^2 \\right\\}. \\\\\n\\]\nNote the expression above is equivalent to:\n\\[\n\\text{Var}(Y) = \\mathbb{E}(Y^2) - \\left[ \\mathbb{E}(Y) \\right]^2.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/B-greek-alphabet.html",
    "href": "book/B-greek-alphabet.html",
    "title": "Appendix B — The Snazzalicious Greek Alphabet",
    "section": "",
    "text": "Fun fact!\n\n\nSnazzalicious! Food that’s dressed up, fancy, and begging for a photo.\n\n\nStatistical notation can be pretty particular and different from usual mathematical notation. One of these particularities is the constant use of Greek letters to denote unknown population parameters in modelling setup, estimation, and statistical inference. In that spirit, throughout this book, we use diverse Greek letters to denote our regression parameters across each of the outlined models in every chapter.\n\n\nImage by meineresterampe via Pixabay.\n\nDuring early learning stages of regression modelling, we may feel overwhelmed by these new letters, which could be unfamiliar. Therefore, whenever confusion arises in any of the main chapters in this book regarding the names of these letters, we recommend checking out the Greek alphabet from Table B.1. Note that frequentist statistical inference mostly uses lowercase letters. With practice over time, you would likely end up memorizing most of this alphabet.\n\n\nTable B.1: Greek alphabet composed of 24 letters, from left to right you can find the name of letter along with its corresponding uppercase and lowercase forms.\n\n\n\nName\nUppercase\nLowercase\n\n\n\nAlpha\n\\(\\text{A}\\)\n\\(\\alpha\\)\n\n\nBeta\n\\(\\text{B}\\)\n\\(\\beta\\)\n\n\nGamma\n\\(\\Gamma\\)\n\\(\\gamma\\)\n\n\nDelta\n\\(\\Delta\\)\n\\(\\delta\\)\n\n\nEpsilon\n\\(\\text{E}\\)\n\\(\\epsilon\\)\n\n\nZeta\n\\(\\text{Z}\\)\n\\(\\zeta\\)\n\n\nEta\n\\(\\text{H}\\)\n\\(\\eta\\)\n\n\nTheta\n\\(\\Theta\\)\n\\(\\theta\\)\n\n\nIota\n\\(\\text{I}\\)\n\\(\\iota\\)\n\n\nKappa\n\\(\\text{K}\\)\n\\(\\kappa\\)\n\n\nLambda\n\\(\\Lambda\\)\n\\(\\lambda\\)\n\n\nMu\n\\(\\text{M}\\)\n\\(\\mu\\)\n\n\nNu\n\\(\\text{N}\\)\n\\(\\nu\\)\n\n\nXi\n\\(\\Xi\\)\n\\(\\xi\\)\n\n\nO\n\\(\\text{O}\\)\n\\(\\text{o}\\)\n\n\nPi\n\\(\\Pi\\)\n\\(\\pi\\)\n\n\nRho\n\\(\\text{P}\\)\n\\(\\rho\\)\n\n\nSigma\n\\(\\Sigma\\)\n\\(\\sigma\\)\n\n\nTau\n\\(\\text{T}\\)\n\\(\\tau\\)\n\n\nUpsilon\n\\(\\Upsilon\\)\n\\(\\upsilon\\)\n\n\nPhi\n\\(\\Phi\\)\n\\(\\phi\\)\n\n\nChi\n\\(\\text{X}\\)\n\\(\\chi\\)\n\n\nPsi\n\\(\\Psi\\)\n\\(\\psi\\)\n\n\nOmega\n\\(\\Omega\\)\n\\(\\omega\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>The Snazzalicious Greek Alphabet</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html",
    "href": "book/C-distributional-mind-map.html",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "",
    "text": "D Discrete Random Variables\nLet us recall what a discrete random variable is. This type of variable is defined to take on a set of countable possible values. In other words, these values belong to a finite set. Figure C.1 delves into the following specific probability distributions:\nTable D.1 outlines the parameter(s), support, mean, and variance for each discrete probability distribution utilized to model the target \\(Y\\) in a specific regression tool explained in this book.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-bernoulli-distribution",
    "href": "book/C-distributional-mind-map.html#sec-bernoulli-distribution",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nD.1 Bernoulli",
    "text": "D.1 Bernoulli\nLet \\(Y\\) be a discrete random variable that is part of a random process or system. \\(Y\\) can only take on the following values:\n\\[\ny =\n\\begin{cases}\n1 \\; \\; \\; \\; \\text{if there is a success},\\\\\n0 \\; \\; \\; \\; \\mbox{otherwise}.\n\\end{cases}\n\\tag{D.1}\\]\nNote that the support of \\(Y\\) in Equation D.1 makes it binary with these outcomes: \\(1\\) for success and \\(0\\) for failure. Then, \\(Y\\) is said to have a Bernoulli distribution with parameter \\(\\pi\\):\n\\[\nY \\sim \\text{Bern}(\\pi).\n\\]\n\nD.1.1 Probability Mass Function\nThe PMF of \\(Y\\) is the following:\n\\[\nP_Y \\left( Y = y \\mid \\pi \\right) = \\pi^y (1 - \\pi)^{1 - y} \\quad \\text{for $y \\in \\{ 0, 1 \\}$.}\n\\tag{D.2}\\]\nParameter \\(\\pi \\in [0, 1]\\) refers to the probability of success. We can verify Equation D.2 is a proper probability distribution (i.e., all the standalone probabilities over the support of \\(Y\\) add up to one) given that:\n\nProof. \\[\n\\begin{align*}\n\\sum_{y = 0}^1 P_Y \\left( Y = y \\mid \\pi \\right) &=  \\sum_{y = 0}^1 \\pi^y (1 - \\pi)^{1 - y}  \\\\\n&= \\underbrace{\\pi^0}_{1} (1 - \\pi) + \\pi \\underbrace{(1 - \\pi)^{0}}_{1} \\\\\n&= (1 - \\pi) + \\pi \\\\\n&= 1. \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\nIndeed, the Bernoulli PMF is a proper probability distribution!\n\n\n\nD.1.2 Expected Value\nVia Equation C.3, the expected value or mean of a Bernoulli-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\sum_{y = 0}^1 y P_Y \\left( Y = y \\mid \\pi \\right) \\\\\n&= \\sum_{y = 0}^1 y \\left[ \\pi^y (1 - \\pi)^{1 - y} \\right] \\\\\n&= \\underbrace{(0) \\left[ \\pi^0 (1 - \\pi) \\right]}_{0} + (1) \\left[ \\pi (1 - \\pi)^{0} \\right] \\\\\n&= 0 + \\pi \\\\\n&= \\pi. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\n\nD.1.3 Variance\nVia Equation C.5 and the Equation C.3 of a discrete expected value, the variance of a Bernoulli-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y)\\right]^2 \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - \\pi^2 \\qquad \\text{since $\\mathbb{E}(Y) = \\pi$} \\\\\n&= \\sum_{y = 0}^1 y^2 P_Y \\left( Y = y \\mid \\pi \\right) - \\pi^2 \\\\\n&= \\left\\{ \\underbrace{(0^2) \\left[ \\pi^0 (1 - \\pi) \\right]}_{0} + \\underbrace{(1^2) \\left[ \\pi (1 - \\pi)^{0} \\right]}_{\\pi} \\right\\} - \\pi^2 \\\\\n&= (0 + \\pi) - \\pi^2 \\\\\n&= \\pi - \\pi^2 \\\\\n&= \\pi (1 - \\pi). \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-binomial-distribution",
    "href": "book/C-distributional-mind-map.html#sec-binomial-distribution",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nD.2 Binomial",
    "text": "D.2 Binomial\nSuppose you execute \\(n\\) independent Bernoulli trials, each one with a probability of success \\(\\pi\\). Let \\(Y\\) be the number of successes obtained within these \\(n\\) Bernoulli trials. Then, \\(Y\\) is said to have a Binomial distribution with parameters \\(n\\) and \\(\\pi\\):\n\\[\nY \\sim \\text{Bin}(n, \\pi).\n\\]\n\nD.2.1 Probability Mass Function\nThe PMF of \\(Y\\) is the following:\n\\[\n\\begin{align*}\nP_Y \\left( Y = y \\mid n, \\pi \\right) &= {n \\choose y} \\pi^y (1 - \\pi)^{n - y} \\\\\n& \\qquad \\qquad \\qquad \\text{for $y \\in \\{ 0, 1, \\dots, n \\}$.}\n\\end{align*}\n\\tag{D.3}\\]\nParameter \\(\\pi \\in [0, 1]\\) refers to the probability of success of each Bernoulli trial and \\(n \\in \\mathbb{N}\\) to the number of trials. On the other hand, the term \\({n \\choose y}\\) indicates the total number of possible combinations for \\(y\\) successes out of our \\(n\\) trials:\n\\[\n{n \\choose y} = \\frac{n!}{y!(n - y)!}.\n\\tag{D.4}\\]\n\nHow can we verify that Equation D.3 is a proper PMF (i.e., all the standalone probabilities over the support of \\(Y\\) add up to one)?\n\nTo elaborate on this, we need to use a handy mathematical result called the binomial theorem.\n\nTheorem D.1 (Binomial Theorem) This theorem is associated to the Pascal’s identity, and it defines the pattern of coefficients in the expansion of a polynomial in the form \\((u + v)^m\\). More specifically, the binomial theorem indicates that if \\(m\\) is a non-negative integer, then the polynomial \\((u + v)^m\\) can be expanded via the following series:\n\\[\n\\begin{align*}\n(u + v)^m &= u^m + {m \\choose 1} u^{m - 1} v + {m \\choose 2} u^{m - 2} v^2 + \\dots + \\\\\n& \\qquad {m \\choose r} u^{m - r} v^r + \\dots + \\\\\n& \\qquad {m \\choose m - 1} u v^{m - 1} + v^m \\\\\n&= \\underbrace{{m \\choose 0}}_1 u^m + {m \\choose 1} u^{m - 1} v + {m \\choose 2} u^{m - 2} v^2 + \\dots + \\\\\n& \\qquad {m \\choose r} u^{m - r} v^r + \\dots + \\\\\n& \\qquad {m \\choose m - 1} u v^{m - 1} + \\underbrace{{m \\choose m}}_1 v^m \\\\\n&= \\sum_{i = 0}^m {m \\choose i} u^{m - i} v^i.\n\\end{align*}\n\\tag{D.5}\\]\n\n\n\nTip on the binomial theorem and Pascal’s identity\n\n\nLet us dig into the proof of the binomial theorem from Equation D.5. This proof will require another important result called the Pascal’s identity. This identity states that for any integers \\(m\\) and \\(k\\), with \\(k \\in \\{ 1, \\dots, m \\}\\), it follows that:\n\nProof. \\[\n\\begin{align*}\n{m \\choose k - 1} + {m \\choose k} &= \\left[ \\frac{m!}{(k - 1)! (m - k + 1)!} \\right] \\\\\n& \\qquad + \\left[ \\frac{m!}{k! (m - k)!} \\right] \\\\\n&= m! \\biggl\\{ \\left[ \\frac{1}{(k - 1)! (m - k + 1)!} \\right] + \\\\\n& \\qquad \\left[ \\frac{1}{k! (m - k)!} \\right] \\biggl\\} \\\\\n&= m! \\Biggl\\{ \\Biggr[ \\frac{k}{\\underbrace{k (k - 1)!}_{k!} (m - k + 1)!} \\Biggr] + \\\\\n& \\qquad \\Biggr[ \\frac{m - k + 1}{k! \\underbrace{(m - k + 1)(m - k)!}_{(m - k + 1)!}} \\Biggr] \\Biggl\\}  \\\\\n&= m! \\left[ \\frac{k + m - k + 1}{k! (m - k + 1)!} \\right] \\\\\n&= m! \\left[ \\frac{m + 1}{k! (m - k + 1)!} \\right] \\\\\n&= \\frac{(m + 1)!}{k! (m + 1 - k)!} \\\\\n&= {m + 1 \\choose k }. \\qquad \\qquad \\qquad \\qquad \\square\n\\end{align*}\n\\tag{D.6}\\]\n\n\nProof. Now, we will use mathematical induction to prove the binomial theorem from Equation D.5. Firstly, on the left-hand side of the theorem, note that when \\(m = 0\\) we have:\n\\[\n(u + v)^0 = 1.\n\\]\nNow, when \\(m = 0\\), for the right-hand side of this equation, we have that\n\\[\n\\sum_{i = 0}^m {m \\choose i} u^{m - i} v^i  = \\sum_{i = 0}^0 {0 \\choose i} u^i v^{i} = {0 \\choose 0} u^0 v^0 = 1.\n\\]\nHence, the binomial theorem holds when \\(m = 0\\). This is what we call the base case in mathematical induction.\nThat said, let us proceed with the inductive hypothesis. We aim to prove that the binomial theorem\n\\[\n\\begin{align*}\n(u + v)^j &= u^j + {j \\choose 1} u^{j - 1} v + {j \\choose 2} u^{j - 2} v^2 + \\dots + \\\\\n& \\qquad {j \\choose r} u^{j - r} v^r + \\dots + \\\\\n& \\qquad {j \\choose j - 1} u v^{j - 1} + v^j \\\\\n&= \\underbrace{{j \\choose 0}}_1 u^j + {j \\choose 1} u^{j - 1} v + {j \\choose 2} u^{j - 2} v^2 + \\dots + \\\\\n& \\qquad {j \\choose r} u^{j - r} v^r + \\dots + \\\\\n& \\qquad {j \\choose j - 1} u v^{j - 1} + \\underbrace{{j \\choose j}}_1 v^j \\\\\n&= \\sum_{i = 0}^j {j \\choose i} u^{j - i} v^i\n\\end{align*}\n\\tag{D.7}\\]\nholds when integer \\(j \\geq 1\\). This is our inductive hypothesis.\nThen, we pave the way to the inductive step. Let us consider the following expansion:\n\\[\n\\begin{align*}\n(u + v)^{j + 1} &= (u + v) (u + v)^j \\\\\n&= (u + v) \\times \\\\\n& \\qquad \\bigg[ u^j + {j \\choose 1} u^{j - 1} v + {j \\choose 2} u^{j - 2} v^2 + \\dots + \\\\\n& \\qquad {j \\choose r} u^{j - r} v^r + \\dots + {j \\choose j - 1} u v^{j - 1} + v^j \\bigg] \\\\\n&= \\bigg[u^{j + 1} + {j \\choose 1} u^j v + {j \\choose 2} u^{j - 1} v^2 + \\dots + \\\\\n& \\qquad {j \\choose r} u^{j - r + 1} v^r + \\dots + \\\\\n& \\qquad {j \\choose j - 1} u^2 v^{j - 1} + u v^j \\bigg] + \\\\\n& \\qquad \\bigg[ u^j v + {j \\choose 1} u^{j - 1} v^2 + {j \\choose 2} u^{j - 2} v^3 + \\dots + \\\\\n& \\qquad {j \\choose r} u^{j - r} v^{r + 1} + \\dots + \\\\\n& \\qquad {j \\choose j - 1} u v^j + {j \\choose j} v^{j + 1} \\bigg] \\\\\n&= u^{j + 1} + \\left[ {j \\choose 0} + {j \\choose 1} \\right] u^j v + \\\\\n& \\qquad \\left[ {j \\choose 1} + {j \\choose 2} \\right] u^{j - 1} v^2 + \\dots + \\\\\n& \\qquad \\left[ {j \\choose r - 1} + {j \\choose r} \\right] u^{j - r + 1} v^r + \\dots + \\\\\n& \\qquad \\left[ {j \\choose j - 1} + {j \\choose j} \\right] u v^j + v^{j + 1}.\n\\end{align*}\n\\tag{D.8}\\]\nLet us plug in the Pascal’s identity from Equation D.6 into Equation D.8:\n\\[\n\\begin{align*}\n(u + v)^{j + 1} &= u^{j + 1} + {j + 1 \\choose 1} u^j v + \\\\\n& \\qquad {j + 1 \\choose 2} u^{j - 1} v^2 + \\dots + \\\\\n& \\qquad {j + 1 \\choose r} u^{j - r + 1} v^r + \\dots + \\\\\n& \\qquad {j + 1 \\choose j} u v^j + v^{j + 1} \\\\\n&= \\underbrace{{j + 1 \\choose 0}}_1 u^{j + 1} + {j + 1 \\choose 1} u^j v + \\\\\n& \\qquad {j + 1 \\choose 2} u^{j - 1} v^2 + \\dots + \\\\\n& \\qquad {j + 1 \\choose r} u^{j - r + 1} v^r + \\dots + \\\\\n& \\qquad {j + 1 \\choose j} u v^j + \\underbrace{{j + 1 \\choose j + 1}}_1 v^{j + 1} \\\\\n&= \\sum_{i = 0}^{j + 1} {j + 1 \\choose i} u^{j + 1 - i} v^i. \\qquad \\quad \\square\n\\end{align*}\n\\tag{D.9}\\]\nNote that the result for \\(j\\) in Equation D.7 also holds for \\(j + 1\\) in Equation D.9. Therefore, by induction, the binomial theorem from Equation D.5 is true for all positive integers \\(m\\).\n\n\n\nAfter the above fruitful digression on the binomial theorem, let us use it to show that our Binomial PMF in Equation D.3 actually adds up to one all over the support of the random variable:\n\nProof. \\[\n\\begin{align*}\n\\sum_{y = 0}^n P_Y \\left( Y = y \\mid n, \\pi \\right) &= \\sum_{y = 0}^n {n \\choose y} \\pi^y (1 - \\pi)^{n - y} \\\\\n&= \\sum_{y = 0}^n {n \\choose y} (1 - \\pi)^{n - y} \\pi^y \\\\\n& \\quad \\qquad \\text{rearranging factors.}\n\\end{align*}\n\\]\nNow, by using the binomial theorem in Equation D.5, let:\n\\[\n\\begin{gather*}\nm  = n\\\\\ni = y \\\\\nu = 1 - \\pi \\\\\nv = \\pi.\n\\end{gather*}\n\\]\nThe above arrangement yields the following result:\n\\[\n\\begin{align*}\n\\sum_{y = 0}^n P_Y \\left( Y = y \\mid n, \\pi \\right) &= (1 - \\pi + \\pi)^n \\\\\n&= 1^n = 1. \\qquad \\square\n\\end{align*}\n\\tag{D.10}\\]\n\nIndeed, the Binomial PMF is a proper probability distribution!\n\n\n\nD.2.2 Expected Value\nVia Equation C.3, the expected value or mean of a Binomial-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\sum_{y = 0}^n y P_Y \\left( Y = y \\mid n, \\pi \\right) \\\\\n&= \\sum_{y = 1}^n y P_Y \\left( Y = y \\mid n, \\pi \\right) \\\\\n& \\quad \\qquad \\text{for $y = 0$, the addend is equal to zero} \\\\\n&= \\sum_{y = 1}^n y \\left[ {n \\choose y} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n&= \\sum_{y = 1}^n y \\left[ \\frac{n!}{y! (n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n&= \\sum_{y = 1}^n \\left[ \\frac{y n!}{y (y - 1)!(n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{in the denominator, $y! = y (y - 1)!$}\\\\\n&= \\sum_{y = 1}^n \\left[ \\frac{n (n - 1)!}{(y - 1)!(n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{in the numerator, $n! = n (n - 1)!$} \\\\\n&= \\sum_{y = 1}^n \\left[ \\frac{n (n - 1)!}{(y - 1)!(n - y)!} \\pi^{y + 1 - 1} (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{note $\\pi^y = \\pi^{y + 1 - 1}$} \\\\\n&= n \\sum_{y = 1}^n \\left[ \\frac{(n - 1)!}{(y - 1)!(n - y)!} \\pi \\pi^{y - 1} (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{rearranging terms} \\\\\n&= n \\pi \\sum_{y = 1}^n \\left[ \\frac{(n - 1)!}{(y - 1)!(n - y)!} \\pi^{y - 1} (1 - \\pi)^{n - y} \\right].\n\\end{align*}\n\\tag{D.11}\\]\nNow, let us make the following variable rearrangement:\n\\[\n\\begin{gather*}\nm = n - 1 \\\\\nz = y - 1 \\\\\nm - z = n - y.\n\\end{gather*}\n\\]\nGoing back to Equation D.11, and applying our above variable rearrangement within the summation, we have:\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= n \\pi \\sum_{z = 0}^m \\left[ \\frac{m!}{z!(m - z)!} \\pi^{z} (1 - \\pi)^{m - z} \\right] \\\\\n&= n \\pi \\sum_{z = 0}^m \\left[ {m \\choose z}\\pi^{z} (1 - \\pi)^{m - z} \\right].\n\\end{align*}\n\\tag{D.12}\\]\nNote that, in the summation of Equation D.12, we encounter the PMF of a random variable \\(Z\\) as follows:\n\\[\nZ \\sim \\text{Bin}(m, \\pi).\n\\]\nSince the summation, where this Binomial PMF of \\(Z\\) is depicted, goes from \\(z = 0\\) to \\(m\\), we can apply our result from Equation D.10:\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= n \\pi \\underbrace{\\sum_{z = 0}^m \\left[ {m \\choose z}\\pi^{z} (1 - \\pi)^{m - z} \\right]}_{1} \\\\\n&= n \\pi. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\n\nD.2.3 Variance\nVia Equation C.5 and the Equation C.3 of a discrete expected value, the variance of a Binomial-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y)\\right]^2 \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - (n \\pi)^2 \\qquad \\text{since $\\mathbb{E}(Y) = n \\pi$.}\n\\end{align*}\n\\tag{D.13}\\]\nUnlike the Bernoulli random variable, finding \\(\\mathbb{E} \\left( Y^2 \\right)\\) is not quite straightforward. We need to play around with the below expected value expression as follows:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\mathbb{E} \\left[ Y (Y - 1) \\right] + \\mathbb{E}(Y) \\\\\n&= \\mathbb{E} \\left[ Y (Y - 1) \\right] + n \\pi \\qquad \\text{since $\\mathbb{E}(Y) = n \\pi$.}\n\\end{align*}\n\\tag{D.14}\\]\nNow, to find \\(\\mathbb{E} \\left[ Y (Y - 1) \\right]\\), we make the following derivation via the LOTUS from Equation C.1 when \\(g(Y) = y (y - 1)\\):\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= \\sum_{y = 0}^n y (y - 1) P_Y \\left( Y = y \\mid n, \\pi \\right) \\\\\n&= \\sum_{y = 2}^n y (y - 1) P_Y \\left( Y = y \\mid n, \\pi \\right) \\\\\n& \\quad \\qquad \\text{for $y = \\{0, 1\\}$,} \\\\\n& \\quad \\qquad \\text{the addends are equal to zero} \\\\\n&= \\sum_{y = 2}^n y (y - 1) \\left[ {n \\choose y} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n&= \\sum_{y = 2}^n y (y - 1) \\left[ \\frac{n!}{y! (n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n&= \\sum_{y = 2}^n \\left[ \\frac{y (y - 1) n!}{y (y - 1) (y - 2)! (n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\\\\n&= \\sum_{y = 2}^n \\left[ \\frac{n (n - 1) (n - 2)!}{(y - 2)! (n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{in the numerator, $n! = n (n - 1) (n - 2)!$} \\\\\n&= \\sum_{y = 2}^n \\left[ \\frac{n (n - 1) (n - 2)!}{(y - 2)! (n - y)!} \\pi^{y + 2 - 2} (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{note $\\pi^y = \\pi^{y + 2 - 2}$} \\\\\n&= n (n - 1) \\times \\\\\n& \\qquad \\sum_{y = 2}^n \\left[ \\frac{(n - 2)!}{(y - 2)! (n - y)!} \\pi^2 \\pi^{y - 2} (1 - \\pi)^{n - y} \\right] \\\\\n& \\qquad \\qquad \\text{rearranging terms} \\\\\n&= n (n - 1) \\pi^2 \\times \\\\\n& \\qquad \\sum_{y = 2}^n \\left[ \\frac{(n - 2)!}{(y - 2)! (n - y)!} \\pi^{y - 2} (1 - \\pi)^{n - y} \\right] \\\\\n& \\qquad \\qquad \\text{rearranging terms.}\n\\end{align*}\n\\tag{D.15}\\]\nThen, we make the following variable rearrangement:\n\\[\n\\begin{gather*}\nm = n - 2 \\\\\nz = y - 2 \\\\\nm - z = n - y.\n\\end{gather*}\n\\]\nGoing back to Equation D.15, and applying our above variable rearrangement within the summation, we have:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= n (n - 1) \\pi^2 \\sum_{z = 0}^m \\left[ \\frac{m!}{z! (m - z)!} \\pi^{z} (1 - \\pi)^{m - z} \\right] \\\\\n&= n (n - 1) \\pi^2 \\sum_{z = 0}^m \\left[ {m \\choose z} \\pi^{z} (1 - \\pi)^{m - z} \\right].\n\\end{align*}\n\\tag{D.16}\\]\nNote that, in the summation of Equation D.16, we encounter the PMF of a random variable \\(Z\\) as follows:\n\\[\nZ \\sim \\text{Bin}(m, \\pi).\n\\]\nSince the summation, where this Binomial PMF of \\(Z\\) is depicted, goes from \\(z = 0\\) to \\(m,\\) we can apply our result from Equation D.10:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= n (n - 1) \\pi^2 \\underbrace{\\sum_{z = 0}^m \\left[ {m \\choose z} \\pi^{z} (1 - \\pi)^{m - z} \\right]}_{1} \\\\\n&= n (n - 1) \\pi^2.\n\\end{align*}\n\\]\nLet us go back to Equation D.14 and plug in the above result:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\mathbb{E} \\left[ Y (Y - 1) \\right] + n \\pi \\\\\n&= n (n - 1) \\pi^2 + n \\pi. \\\\\n\\end{align*}\n\\]\nFinally, we plug in \\(\\mathbb{E} \\left( Y^2 \\right)\\) in Equation D.13:\n\\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - (n \\pi)^2 \\\\\n&= n (n - 1) \\pi^2 + n \\pi - n^2 \\pi^2 \\\\\n&= n^2 \\pi^2 - n \\pi^2 + n \\pi - n^2 \\pi^2 \\\\\n&= n \\pi - n \\pi^2 \\\\\n&= n \\pi (1 - \\pi). \\qquad \\qquad \\qquad \\square\n\\end{align*}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-negative-binomial-distribution",
    "href": "book/C-distributional-mind-map.html#sec-negative-binomial-distribution",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nD.3 Negative Binomial",
    "text": "D.3 Negative Binomial\nSuppose you execute a series of independent Bernoulli trials, each one with a probability of success \\(\\pi\\). Let \\(Y\\) be the number of failures in this series of Bernoulli trials you obtain before experiencing \\(k\\) successes. Therefore, \\(Y\\) is said to have a Negative Binomial distribution with parameters \\(k\\) and \\(\\pi\\):\n\\[\nY \\sim \\text{NegBin}(k, \\pi).\n\\]\n\nD.3.1 Probability Mass Function\nThe PMF of \\(Y\\) is the following:\n\\[\n\\begin{align*}\nP_Y \\left( Y = y \\mid k, \\pi \\right) &= {k + y - 1 \\choose y} \\pi^k (1 - \\pi)^y \\\\\n& \\qquad \\qquad \\qquad \\quad \\text{for $y \\in \\{ 0, 1, \\dots \\}$.}\n\\end{align*}\n\\tag{D.17}\\]\nParameter \\(\\pi \\in [0, 1]\\) refers to the probability of success of each Bernoulli trial, whereas \\(k\\) refers to the number of successes.\n\n\nTip on an alternative Negative Binomial PMF!\n\n\nThere is an alternative parametrization to define a Negative Binomial distribution in which we have a random variable \\(Z\\) defined as the total number of Bernoulli trials (i.e., \\(k\\) successes plus the \\(Y\\) failures depicted in Equation D.17):\n\\[\nZ = Y + k.\n\\]\nThis alternative parametrization of the Negative Binomial distribution yields the following PMF:\n\\[\n\\begin{align*}\nP_Z \\left( Z = z \\mid k, \\pi \\right) &= {z - 1 \\choose k - 1} \\pi^k (1 - \\pi)^{z - k} \\\\\n& \\qquad \\qquad \\qquad \\text{for $z \\in \\{ k, k + 1, \\dots \\}$.}\n\\end{align*}\n\\]\nNevertheless, we will not dig into this version of the Negative Binomial distribution since Chapter 11 delves into a modelling estimation via a joint PMF of the training set involving Equation D.17.\n\n\n\nHow can we verify that Equation D.17 is a proper PMF (i.e., all the standalone probabilities over the support of \\(Y\\) add up to one)?\n\n\nProof. Let us manipulate the factor involving the number of combinations corresponding to how many different possible subsets of size \\(y\\) can be made from the larger set of size \\(k + y - 1\\):\n\\[\n\\begin{align*}\n{k + y - 1 \\choose y} &= \\frac{(k + y - 1)!}{(k + y - 1 - y)! y !} \\\\\n&= \\frac{(k + y - 1)!}{(k - 1)! y!} \\\\\n&= \\frac{(k + y - 1) (k + y - 2) \\cdots (k + 1) (k) (k - 1)!}{(k - 1)! y!} \\\\\n&= \\frac{(\\overbrace{k + y - 1) (k + y - 2) \\cdots (k + 1) k}^{\\text{we have $y$ factors}}}{y!} \\\\\n&= (- 1)^y \\frac{\\overbrace{(-k - y + 1) (-k - y + 2) \\cdots (-k - 1) (-k)}^{\\text{multiplying each factor times $-1$}}}{y!} \\\\\n&= (- 1)^y \\frac{\\overbrace{(-k) (-k - 1) \\cdots (-k - y + 2) (-k - y + 1)}^{\\text{rearranging factors}}}{y!} \\\\\n&= (- 1)^y \\frac{(-k) (-k - 1) \\cdots (-k - y + 2) (-k - y + 1)}{y!} \\times \\\\\n& \\qquad \\frac{(-k - y) (-k - y - 1) \\cdots (1)}{(-k - y) (-k - y - 1) \\cdots (1)} \\\\\n&= (- 1)^y \\frac{(-k) (-k - 1) \\cdots (-k - y + 2) (-k - y + 1)}{y!} \\times \\\\\n& \\qquad \\frac{(-k - y) (-k - y - 1) \\cdots (1)}{(-k - y)!}.\n\\end{align*}\n\\]\nIn the equation above, note that there are still several factors in the numerator, which can be summarized using a factorial as follows:\n\\[\n\\begin{align*}\n(-k)! &= (-k) (-k - 1) \\cdots (-k - y + 2) (-k - y + 1) \\times \\\\\n& \\quad \\qquad (-k - y) (-k - y - 1) \\cdots (1).\n\\end{align*}\n\\]\nTherefore:\n\\[\n\\begin{align*}\n{k + y - 1 \\choose y} &= (- 1)^y \\frac{(-k)!}{(-k - y)! y!}\\\\\n&= (- 1)^y {-k \\choose y}.\n\\end{align*}\n\\]\nNow, let us begin with the summation involving the Negative Binomial PMF depicted in Equation D.17 from \\(0\\) to \\(\\infty\\):\n\\[\n\\begin{align*}\n\\sum_{y = 0}^{\\infty} P_Y \\left( Y = y \\mid k, \\pi \\right) &= \\sum_{y = 0}^{\\infty} {k + y - 1 \\choose y} \\pi^k (1 - \\pi)^y \\\\\n&= \\sum_{y = 0}^{\\infty} (- 1)^y {-k \\choose y} \\pi^k (1 - \\pi)^y \\\\\n&= \\pi^k \\sum_{y = 0}^{\\infty} (- 1)^y {-k \\choose y} (1 - \\pi)^y \\\\\n&= \\pi^k \\sum_{y = 0}^{\\infty} {-k \\choose y} (-1 + \\pi)^y.\n\\end{align*}\n\\tag{D.18}\\]\nOn the right-hand side of Equation D.18 we will add the following factor:\n\\[\n(1)^{-k - y} = 1.\n\\]\nThus:\n\\[\n\\begin{align*}\n\\sum_{y = 0}^{\\infty} P_Y \\left( Y = y \\mid k, \\pi \\right) &= \\pi^k \\sum_{y = 0}^{\\infty} {-k \\choose y} (1)^{-k - y} (-1 + \\pi)^y.\n\\end{align*}\n\\tag{D.19}\\]\nNow, by using the binomial theorem in Equation D.5, let:\n\\[\n\\begin{gather*}\nm  = -k\\\\\ni = y \\\\\nu = 1 \\\\\nv = -1 + \\pi.\n\\end{gather*}\n\\]\nThe above arrangement yields the following result in Equation D.19:\n\\[\n\\begin{align*}\n\\sum_{y = 0}^{\\infty} P_Y \\left( Y = y \\mid k, \\pi \\right) &=  \\pi^k (1 - 1 + \\pi)^{-k} \\\\\n&= \\pi^k (\\pi) ^{-k} \\\\\n&= \\pi^0 \\\\\n&= 1. \\qquad \\qquad \\qquad \\square\n\\end{align*}\n\\tag{D.20}\\]\n\nIndeed, the Negative Binomial PMF is a proper probability distribution!\n\n\n\nD.3.2 Expected Value\nVia Equation C.3, the expected value or mean of a Negative Binomial-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\sum_{y = 0}^{\\infty} y P_Y \\left( Y = y \\mid k, \\pi \\right) \\\\\n&= \\sum_{y = 1}^{\\infty} y P_Y \\left( Y = y \\mid k, \\pi \\right) \\\\\n& \\quad \\qquad \\text{for $y = 0$, the addend is equal to zero} \\\\\n&= \\sum_{y = 1}^{\\infty} y \\left[ {k + y - 1 \\choose y} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= \\sum_{y = 1}^{\\infty} y \\left[ \\frac{(k + y - 1)!}{y! (k + y - 1 - y)!} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= \\sum_{y = 1}^{\\infty} y \\left[ \\frac{(k + y - 1)!}{y! (k - 1)!} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= \\sum_{y = 1}^{\\infty} y \\Bigg[ \\frac{(k + y - 1)!}{y (y - 1)! \\underbrace{\\left( \\frac{k!}{k} \\right)}_{(k - 1)!}} \\pi^k (1 - \\pi)^y \\Bigg] \\\\\n&= \\sum_{y = 1}^{\\infty} k \\left[ \\frac{(k + y - 1)!}{k! (y - 1)!} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= k \\sum_{y = 1}^{\\infty} \\left[ {k + y - 1 \\choose y - 1} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= k \\sum_{y = 1}^{\\infty} \\left[ {k + y - 1 \\choose y - 1} \\pi^{k + 1 - 1} (1 - \\pi)^{y + 1 - 1} \\right] \\\\\n& \\quad \\qquad \\text{note $\\pi^k = \\pi^{k + 1 - 1}$ and $(1 - \\pi)^y = (1 - \\pi)^{y + 1 - 1}$} \\\\\n&= \\frac{k (1 - \\pi)}{\\pi} \\sum_{y = 1}^{\\infty} \\left[ {k + y - 1 \\choose y - 1} \\pi^{k + 1} (1 - \\pi)^{y - 1} \\right].\n\\end{align*}\n\\tag{D.21}\\]\nNow, let us make the following variable rearrangement:\n\\[\n\\begin{gather*}\nm = k + 1 \\\\\nz = y - 1 \\\\\nm + z - 1  = k + y - 1.\n\\end{gather*}\n\\]\nGoing back to Equation D.21, and applying our above variable rearrangement within the summation, we have:\n\\[\n\\mathbb{E}(Y) = \\frac{k (1 - \\pi)}{\\pi} \\sum_{z = 0}^{\\infty} \\left[ {m + z - 1 \\choose z} \\pi^{m} (1 - \\pi)^{z} \\right].\n\\tag{D.22}\\]\nNote that, in the summation of Equation D.22, we encounter the PMF of a random variable \\(Z\\) as follows:\n\\[\nZ \\sim \\text{NegBin}(m, \\pi).\n\\]\nSince the summation, where this Negative Binomial PMF of \\(Z\\) is depicted, goes from \\(z = 0\\) to \\(\\infty\\), we can apply our result from Equation D.20:\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\frac{k (1 - \\pi)}{\\pi} \\underbrace{\\sum_{z = 0}^m \\left[ {m + z - 1 \\choose z} \\pi^{m} (1 - \\pi)^{z} \\right]}_{1} \\\\\n&= \\frac{k (1 - \\pi)}{\\pi}. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\n\nD.3.3 Variance\nVia Equation C.5 and the Equation C.3 of a discrete expected value, the variance of a Negative Binomial-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y)\\right]^2 \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\frac{k (1 - \\pi)}{\\pi} \\right]^2 \\quad \\text{since $\\mathbb{E}(Y) = \\frac{k (1 - \\pi)}{\\pi}$.}\n\\end{align*}\n\\tag{D.23}\\]\nNow, we need to play around with the below expected value expression as follows:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\mathbb{E} \\left[ Y (Y - 1) \\right] + \\mathbb{E}(Y) \\\\\n&= \\mathbb{E} \\left[ Y (Y - 1) \\right] + \\frac{k (1 - \\pi)}{\\pi}.\n\\end{align*}\n\\tag{D.24}\\]\nTo find \\(\\mathbb{E} \\left[ Y (Y - 1) \\right]\\), we make the following derivation via the LOTUS from Equation C.1 when \\(g(Y) = y (y - 1)\\):\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= \\sum_{y = 0}^{\\infty} y (y - 1) P_Y \\left( Y = y \\mid k, \\pi \\right) \\\\\n&= \\sum_{y = 2}^{\\infty} y (y - 1) P_Y \\left( Y = y \\mid k, \\pi \\right) \\\\\n& \\quad \\qquad \\text{for $y = \\{0, 1\\}$,} \\\\\n& \\quad \\qquad \\text{the addends are equal to zero} \\\\\n&= \\sum_{y = 2}^{\\infty} y (y - 1) \\left[ {k + y - 1 \\choose y} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= \\sum_{y = 2}^{\\infty} y (y - 1) \\left[ \\frac{(k + y - 1)!}{y! (k + y - 1 - y)!} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= \\sum_{y = 2}^{\\infty} y (y - 1) \\left[ \\frac{(k + y - 1)!}{y! (k - 1)!} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= \\sum_{y = 2}^{\\infty} \\frac{y (y - 1)}{y (y - 1)} \\left[ \\frac{(k + y - 1)!}{(y - 2)! (k - 1)!} \\pi^k (1 - \\pi)^y \\right] \\\\\n& \\quad \\qquad \\text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\\\\n&= \\sum_{y = 2}^{\\infty} \\Bigg[ \\frac{(k + y - 1)!}{(y - 2)! \\underbrace{\\frac{(k + 1)!}{k (k + 1)}}_{(k - 1)!}} \\pi^k (1 - \\pi)^y \\Bigg] \\\\\n&= \\sum_{y = 2}^{\\infty} \\left[ k (k + 1) \\frac{(k + y - 1)!}{(k + 1)! (y - 2)!} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= k (k + 1) \\sum_{y = 2}^{\\infty} \\left[ {k + y - 1 \\choose y - 2} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= k (k + 1) \\sum_{y = 2}^{\\infty} \\left[ {k + y - 1 \\choose y - 2} \\pi^{k + 2 - 2} (1 - \\pi)^{y + 2 - 2} \\right] \\\\\n& \\quad \\qquad \\text{note $\\pi^k = \\pi^{k + 2 - 2}$ and} \\\\\n& \\quad \\qquad (1 - \\pi)^y = (1 - \\pi)^{y + 2 - 2} \\\\\n&= \\frac{k (k + 1) ( 1 - \\pi)^2}{\\pi^2} \\times \\\\\n& \\qquad \\sum_{y = 2}^{\\infty} \\left[ {k + y - 1 \\choose y - 2} \\pi^{k + 2} (1 - \\pi)^{y - 2} \\right].\n\\end{align*}\n\\tag{D.25}\\]\nThen, we make the following variable rearrangement:\n\\[\n\\begin{gather*}\nm = k + 2\\\\\nz = y - 2 \\\\\nm + z - 1  = k + y - 1.\n\\end{gather*}\n\\]\nGoing back to Equation D.25, and applying our above variable rearrangement within the summation, we have:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= \\frac{k (k + 1) ( 1 - \\pi)^2}{\\pi^2} \\times \\\\\n& \\qquad \\sum_{y = 2}^{\\infty} \\left[ {m + z - 1 \\choose z} \\pi^m (1 - \\pi)^z \\right].\n\\end{align*}\n\\tag{D.26}\\]\nNote that, in the summation of Equation D.26, we encounter the PMF of a random variable \\(Z\\) as follows:\n\\[\nZ \\sim \\text{NegBin}(m, \\pi).\n\\]\nSince the summation, where this Binomial PMF of \\(Z\\) is depicted, goes from \\(z = 0\\) to \\(\\infty\\), we can apply our result from Equation D.20:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= \\frac{k (k + 1) ( 1 - \\pi)^2}{\\pi^2} \\times \\\\\n& \\qquad \\underbrace{\\sum_{y = 2}^{\\infty} \\left[ {m + z - 1 \\choose z} \\pi^m (1 - \\pi)^z \\right]}_{1} \\\\\n&= \\frac{k (k + 1) ( 1 - \\pi)^2}{\\pi^2}.\n\\end{align*}\n\\]\nLet us go back to Equation D.24 and plug in the above result:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\mathbb{E} \\left[ Y (Y - 1) \\right] + \\frac{k ( 1 - \\pi)}{\\pi} \\\\\n&= \\frac{k (k + 1) ( 1 - \\pi)^2}{\\pi^2} + \\frac{k ( 1 - \\pi)}{\\pi}.\n\\end{align*}\n\\]\nFinally, we plug in \\(\\mathbb{E} \\left( Y^2 \\right)\\) in Equation D.23:\n\\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\frac{k (1 - \\pi)}{\\pi} \\right]^2 \\\\\n&= \\frac{k (k + 1) ( 1 - \\pi)^2}{\\pi^2} + \\frac{k ( 1 - \\pi)}{\\pi} - \\left[ \\frac{k (1 - \\pi)}{\\pi} \\right]^2 \\\\\n&= \\frac{k (1 - \\pi)}{\\pi} \\left[ \\frac{(k + 1) (1 - \\pi)}{\\pi} + 1 - \\frac{k (1 - \\pi)}{\\pi} \\right] \\\\\n&= \\frac{k (1 - \\pi)}{\\pi} \\left[ \\frac{(k + 1) (1 - \\pi) + \\pi - k (1 - \\pi)}{\\pi} \\right] \\\\\n&= \\frac{k (1 - \\pi)}{\\pi} \\left( \\frac{k - k \\pi + 1 - \\pi + \\pi - k + k \\pi}{\\pi} \\right) \\\\\n&= \\frac{k (1 - \\pi)}{\\pi} \\left( \\frac{1}{\\pi} \\right) \\\\\n&= \\frac{k (1 - \\pi)}{\\pi^2}. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-classical-poisson-distribution",
    "href": "book/C-distributional-mind-map.html#sec-classical-poisson-distribution",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nD.4 Classical Poisson",
    "text": "D.4 Classical Poisson\nSuppose you observe the count of events happening in a fixed interval of time or space. Let \\(Y\\) be the number of counts considered of integer type. Then, \\(Y\\) is said to have a classical Poisson distribution with a continuous parameter \\(\\lambda\\):\n\\[\nY \\sim \\text{Pois}(\\lambda).\n\\]\n\nD.4.1 Probability Mass Function\nThe PMF of this count-type \\(Y\\) is the following:\n\\[\nP_Y \\left( Y = y \\mid \\lambda \\right) = \\frac{\\lambda^y \\exp{(-\\lambda)}}{y!} \\quad \\text{for $y \\in \\{ 0, 1, 2, \\dots\\}$,}\n\\tag{D.27}\\]\nwhere \\(\\exp{(\\cdot)}\\) depicts the base \\(e\\) (i.e., Euler’s number, \\(e = 2.71828...\\)) and \\(y!\\) is the factorial\n\\[\ny! = y \\times (y - 1) \\times (y - 2) \\times (y - 3) \\times \\cdots \\times 3 \\times 2 \\times 1.  \n\\]\nwith\n\\[\n0! = 1.\n\\]\nThe continuous parameter \\(\\lambda \\in (0, \\infty)\\) represents the average rate at which these events happen (i.e., events per area unit or events per time unit). Curiously, even though the random variable \\(Y\\) is considered discrete in this case, \\(\\lambda\\) is modelled as continuous!\n\nHow can we verify that Equation D.27 is a proper PMF (i.e., all the standalone probabilities over the support of \\(Y\\) add up to one)?\n\nTo elaborate on this, we need to use some mathematical tools called the Taylor series expansions and a derived result called Maclaurin series expansions.\n\n\nHeads-up on the Taylor and Maclaurin series expansions!\n\n\nIn mathematics, there are helpful tools known as Taylor series expansions, which were officially published by English mathematician Brook Taylor in Methodus Incrementorum Directa & Inversa (Taylor 1715).\n\n\nPortrait of mathematician Brook Taylor (Earlom 1793).\n\nHowever, it is essential to note that Scottish mathematician James Gregory introduced the notion of these series expansions in his work Vera Circuli et Hyperbolae Quadratura (Gregory 1668).\n\n\nPortrait of mathematician James Gregory (Scotland, n.d.).\n\nThese series approximate complex mathematical functions through an infinite sum of polynomial terms. For example, in machine learning, the Taylor series expansions can be utilized in gradient-based optimization methods. Specifically, Newton’s method uses these expansions to find roots of equations that cannot be solved analytically, which is common in maximum likelihood-based parameter estimation for the varied regression models discussed throughout this book. Moreover, we can find these series in different engineering and scientific fields such as physics.\nSuppose we have real function \\(f(u)\\) around a point \\(u = a\\), then the one-dimensional infinite Taylor series expansion is given by the expression\n\\[\n\\begin{align*}\nf(u) &= f(a) + f'(a) (u - a) + \\frac{f''(a)}{2!} (u - a)^2 + \\\\\n& \\qquad \\frac{f^{(3)}(a)}{3!} (u - a)^3 + \\frac{f^{(4)}(a)}{4!} (u - a)^4 + \\\\\n& \\qquad \\frac{f^{(5)}(a)}{5!} (u - a)^5 + \\cdots \\\\\n&= \\sum_{j = 0}^{\\infty} \\frac{f^{(j)}(a)}{j!} (u - a)^j.\n\\end{align*}\n\\tag{D.28}\\]\nA complete mathematical derivation of Equation D.28 can be found in Weisstein (n.d.b). Moving along, specifically in the last line of this equation which shows an infinite summation, note the following:\n\n\n\\(f^{(j)}(a)\\) indicates the \\(j\\)th order derivative of \\(f(u)\\) and evaluated at point \\(a\\).\n\n\\(j!\\) implicates the factorial of \\(j\\) such that\n\n\\[\nj! = j \\times (j - 1) \\times (j - 2) \\times (j - 3) \\times \\cdots \\times 3 \\times 2 \\times 1.\n\\]\nwith\n\\[\n0! = 1.\n\\]\nIf we go even further with Equation D.28, we have a specific case when \\(a = 0\\) called the Maclaurin series expansions. This case was introduced by the Scottish mathematician Colin Maclaurin in his work A Treatise of Fluxions (Maclaurin 1742).\n\n\nPortrait of mathematician Colin Maclaurin (Harding 1798).\n\nHence, in a Mclaurin series, Equation D.28 becomes:\n\\[\n\\begin{align*}\nf(u) &= f(0) + f'(0) (u) + \\frac{f''(0)}{2!} u^2 + \\\\\n& \\qquad \\frac{f^{(3)}(0)}{3!} u^3 + \\frac{f^{(4)}(0)}{4!} u^4 + \\\\\n& \\qquad \\frac{f^{(5)}(0)}{5!} u^5 + \\cdots \\\\\n&= \\sum_{j = 0}^{\\infty} \\frac{f^{(j)}(0)}{j!} u^j.\n\\end{align*}\n\\tag{D.29}\\]\nDifferent statistical proofs make use of Taylor series expansions as well as the Mclaurin series, and the Poisson distribution is not an exception at all!\n\n\nThe above Mclaurin series in Equation D.29 will help us to show that our Poisson PMF in Equation D.27 actually adds up to one all over the support of the random variable:\n\nProof. \\[\n\\begin{align*}\n\\sum_{y = 0}^{\\infty} P_Y \\left( Y = y \\mid \\lambda \\right) &= \\sum_{y = 0}^{\\infty} \\frac{\\lambda^y \\exp{(-\\lambda)}}{y!} \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 0}^{\\infty} \\frac{\\lambda^y}{y!} \\\\\n& \\quad \\qquad \\text{factoring out $\\exp{(-\\lambda)}$,} \\\\\n& \\quad \\qquad \\text{since it does not depend on $y$.}\n\\end{align*}\n\\tag{D.30}\\]\nNow, we will focus on the above summation\n\\[\n\\sum_{y = 0}^{\\infty} \\frac{\\lambda^y}{y!}\n\\] and use the Mclaurin series from Equation D.29 by letting\n\\[\nf(u) = \\exp(u).\n\\tag{D.31}\\]\nWe know that all derivatives of the above function are equal\n\\[\nf'(u) = f''(u) = f^{(3)}(u) = f^{(4)}(u) = f^{(5)}(u) = \\cdots = \\exp{(u)},\n\\] which allows us to conclude that the \\(j\\)th derivative is\n\\[\nf^{(j)}(u) = \\exp(u).\n\\]\nThis \\(j\\)th derivative evaluated at \\(u = 0\\) becomes\n\\[\nf^{(j)}(0) = \\exp(0) = 1.\n\\]\nTherefore, the Mclaurin series for Equation D.31 is the following:\n\\[\n\\begin{align*}\nf(u) &= \\exp(u) \\\\\n&= \\sum_{j = 0}^{\\infty} \\frac{\\exp(0)}{j!} u^j \\\\\n&= \\sum_{j = 0}^{\\infty} \\frac{u^j }{j!}.\n\\end{align*}\n\\tag{D.32}\\]\nThat said, using Equation D.32, let:\n\\[\n\\begin{gather*}\n\\lambda = u \\\\\ny = j.\n\\end{gather*}\n\\]\nThus, we have the following:\n\\[\n\\sum_{y = 0}^{\\infty} \\frac{\\lambda^y}{y!} = \\exp{(\\lambda)}.\n\\]\nFinally, going back to Equation D.30:\n\\[\n\\begin{align*}\n\\sum_{y = 0}^{\\infty} P_Y \\left( Y = y \\mid \\lambda \\right) &= \\exp{(-\\lambda)} \\overbrace{\\sum_{y = 0}^{\\infty} \\frac{\\lambda^y}{y!}}^{\\exp{(\\lambda)}} \\\\\n&= \\exp{(-\\lambda)} \\times \\exp{(\\lambda)} \\\\\n&= \\exp{(-\\lambda + \\lambda)} \\\\\n&= \\exp{(0)} \\\\\n&= 1. \\qquad \\qquad \\qquad \\qquad \\square\n\\end{align*}\n\\tag{D.33}\\]\n\nIndeed, the Poisson PMF is a proper probability distribution!\n\n\n\nD.4.2 Expected Value\nVia Equation C.3, the expected value or mean of a Poisson-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\sum_{y = 0}^{\\infty} y P_Y \\left( Y = y \\mid \\lambda \\right) \\\\\n&= \\sum_{y = 1}^{\\infty} y P_Y \\left( Y = y \\mid \\lambda \\right) \\\\\n& \\quad \\qquad \\text{for $y = 0$, the addend is equal to zero} \\\\\n&= \\sum_{y = 1}^{\\infty} y \\left[ \\frac{\\lambda^y \\exp{(-\\lambda)}}{y!} \\right] \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 1}^{\\infty} \\frac{y \\lambda^y}{y!} \\\\\n& \\quad \\qquad \\text{factoring out $\\exp{(-\\lambda)}$,} \\\\\n& \\quad \\qquad \\text{since it does not depend on $y$} \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 1}^{\\infty} \\frac{y \\lambda^y}{y (y - 1)!} \\\\\n& \\quad \\qquad \\text{in the denominator, $y! = y (y - 1)!$}\\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 1}^{\\infty} \\frac{\\lambda^y}{(y - 1)!} \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 1}^{\\infty} \\frac{\\lambda^{y + 1 - 1}}{(y - 1)!} \\\\\n& \\quad \\qquad \\text{note $\\lambda^y = \\lambda^{y + 1 - 1}$} \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 1}^{\\infty} \\frac{\\lambda \\lambda^{y - 1}}{(y - 1)!} \\\\\n& \\quad \\qquad \\text{rearranging terms} \\\\\n&= \\lambda \\exp{(-\\lambda)} \\sum_{y = 1}^{\\infty} \\frac{\\lambda^{y - 1}}{(y - 1)!} \\\\\n& \\quad \\qquad \\text{factoring out $\\lambda$,} \\\\\n& \\quad \\qquad \\text{since it does not depend on $y$.}\n\\end{align*}\n\\tag{D.34}\\]\nThen, let us make the following variable rearrangement:\n\\[\nz = y - 1.\n\\]\nGoing back to Equation D.34, and applying our above variable rearrangement within the summation, we have:\n\\[\n\\mathbb{E}(Y) = \\lambda \\exp{(-\\lambda)} \\sum_{z = 0}^{\\infty} \\frac{\\lambda^z}{z!}\n\\tag{D.35}\\]\nUsing Equation D.32, let:\n\\[\n\\begin{gather*}\n\\lambda = u \\\\\nz = j.\n\\end{gather*}\n\\]\nHence, we have the following:\n\\[\n\\sum_{z = 0}^{\\infty} \\frac{\\lambda^z}{z!} = \\exp{(\\lambda)}.\n\\]\nFinally, going back to Equation D.35:\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\lambda \\exp{(-\\lambda)} \\overbrace{\\sum_{z = 0}^{\\infty} \\frac{\\lambda^z}{z!}}^{\\exp{(\\lambda)}} \\\\\n&= \\lambda \\exp{(-\\lambda)} \\times \\exp{(\\lambda)} \\\\\n&= \\lambda \\exp{(-\\lambda + \\lambda)} \\\\\n&= \\lambda \\exp{(0)} \\\\\n&= \\lambda. \\qquad \\qquad \\qquad \\qquad \\square\n\\end{align*}\n\\]\n\n\nD.4.3 Variance\nVia Equation C.5 and the Equation C.3 of a discrete expected value, the variance of a Poisson-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y)\\right]^2 \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - \\lambda^2 \\qquad \\text{since $\\mathbb{E}(Y) = \\lambda$.}\n\\end{align*}\n\\tag{D.36}\\]\nNow, we need to play around with the below expected value expression as follows:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\mathbb{E} \\left[ Y (Y - 1) \\right] + \\mathbb{E}(Y) \\\\\n&= \\mathbb{E} \\left[ Y (Y - 1) \\right] + \\lambda \\qquad \\text{since $\\mathbb{E}(Y) = \\lambda$.}\n\\end{align*}\n\\tag{D.37}\\]\nNow, to find \\(\\mathbb{E} \\left[ Y (Y - 1) \\right]\\), we make the following derivation via the LOTUS from Equation C.1 when \\(g(Y) = y (y - 1)\\):\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= \\sum_{y = 0}^{\\infty} y (y - 1) P_Y \\left( Y = y \\mid \\lambda \\right) \\\\\n&= \\sum_{y = 2}^{\\infty} y (y - 1) P_Y \\left( Y = y \\mid \\lambda \\right) \\\\\n& \\quad \\qquad \\text{for $y = \\{0, 1\\}$,} \\\\\n& \\quad \\qquad \\text{the addends are equal to zero} \\\\\n&= \\sum_{y = 2}^{\\infty} y (y - 1) \\left[ \\frac{\\lambda^y \\exp{(-\\lambda)}}{y!} \\right] \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 2}^{\\infty} \\left[ \\frac{y (y - 1) \\lambda^y}{y!} \\right] \\\\\n& \\quad \\qquad \\text{factoring out $\\exp{(-\\lambda)}$,} \\\\\n& \\quad \\qquad \\text{since it does not depend on $y$} \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 2}^{\\infty} \\left[ \\frac{y (y - 1) \\lambda^y}{y (y - 1) (y - 2)!} \\right] \\\\\n& \\quad \\qquad \\text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 2}^{\\infty} \\frac{\\lambda^y}{(y - 2)!} \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 2}^{\\infty} \\frac{\\lambda^{y + 2 - 2}}{(y - 2)!} \\\\\n& \\quad \\qquad \\text{note $\\lambda^y = \\lambda^{y + 2 - 2} $} \\\\\n&= \\lambda^2 \\exp{(-\\lambda)} \\sum_{y = 2}^{\\infty} \\frac{\\lambda^{y - 2}}{(y - 2)!} \\\\\n& \\quad \\qquad \\text{factoring out $\\lambda^2$,} \\\\\n& \\quad \\qquad \\text{since it does not depend on $y$.} \\\\\n\\end{align*}\n\\tag{D.38}\\]\nThen, we make the following variable rearrangement:\n\\[\nz = y - 2.\n\\]\nGoing back to Equation D.38, and applying our above variable rearrangement within the summation, we have:\n\\[\n\\mathbb{E} \\left[ Y (Y - 1) \\right] = \\lambda^2 \\exp{(-\\lambda)} \\sum_{z = 0}^{\\infty} \\frac{\\lambda^z}{z!}.\n\\tag{D.39}\\]\nUsing Equation D.32, let:\n\\[\n\\begin{gather*}\n\\lambda = u \\\\\nz = j.\n\\end{gather*}\n\\]\nThus, we have the following:\n\\[\n\\sum_{z = 0}^{\\infty} \\frac{\\lambda^z}{z!} = \\exp{(\\lambda)}.\n\\]\nGoing back to Equation D.39:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= \\lambda^2 \\exp{(-\\lambda)} \\overbrace{\\sum_{z = 0}^{\\infty} \\frac{\\lambda^z}{z!}}^{\\exp{(\\lambda)}} \\\\\n&= \\lambda^2 \\exp{(-\\lambda)} \\times \\exp{\\lambda} \\\\\n&= \\lambda^2 \\exp{(-\\lambda + \\lambda)} \\\\\n&= \\lambda^2 \\exp{(0)} \\\\\n&= \\lambda^2.\n\\end{align*}\n\\tag{D.40}\\]\nLet us retake Equation D.37 and plug in the above result:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\mathbb{E} \\left[ Y (Y - 1) \\right] + \\lambda \\\\\n&= \\lambda^2 + \\lambda. \\\\\n\\end{align*}\n\\]\nFinally, we plug in \\(\\mathbb{E} \\left( Y^2 \\right)\\) in Equation D.36:\n\\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\lambda^2 \\\\\n&= \\lambda^2 + \\lambda - \\lambda^2 \\\\\n&= \\lambda. \\qquad \\qquad \\square\n\\end{align*}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-generalized-poisson-distribution",
    "href": "book/C-distributional-mind-map.html#sec-generalized-poisson-distribution",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nD.5 Generalized Poisson",
    "text": "D.5 Generalized Poisson\nThe generalized Poisson (GP) distribution is viewed as the general Poisson case. It was introduced by Consul and Jain (1973). Suppose you observe the count of events happening in a fixed interval of time or space. Let \\(Y\\) be the number of counts considered of integer type. Then, \\(Y\\) is said to have a GP distribution with continuous parameters \\(\\lambda\\) and \\(\\theta\\):\n\\[\nY \\sim \\text{GP}(\\lambda, \\theta).\n\\]\n\nD.5.1 Probability Mass Function\nThe PMF of this count-type \\(Y\\) is the following:\n\\[\n\\begin{align*}\nP_Y \\left( Y = y \\mid \\lambda, \\theta \\right) &= \\frac{\\lambda (\\lambda + y \\theta)^{y - 1} \\exp{\\left[ -(\\lambda + y \\theta) \\right]}}{y!} \\\\\n& \\qquad \\qquad \\qquad \\text{for $y \\in \\{ 0, 1, 2, \\dots\\}$,}\n\\end{align*}\n\\tag{D.41}\\]\nwhere \\(\\exp{(\\cdot)}\\) depicts the base \\(e\\) (i.e., Euler’s number, \\(e = 2.71828...\\)) and \\(y!\\) is the factorial\n\\[\ny! = y \\times (y - 1) \\times (y - 2) \\times (y - 3) \\times \\cdots \\times 3 \\times 2 \\times 1.  \n\\]\nwith\n\\[\n0! = 1.\n\\]\nThe continuous parameter \\(\\lambda \\in (0, \\infty)\\) represents the average rate at which these events happen (i.e., events per area unit or events per time unit). As in the case of the classical Poisson case, even though the GP random variable \\(Y\\) is considered discrete, \\(\\lambda\\) is modelled as continuous!\nOn the other hand, the continuous and bounded parameter \\(\\theta \\in (-1, 1)\\) controls for dispersion present in the GP random variable Y as follows:\n\nWhen \\(0 &lt; \\theta &lt; 1\\), the GP \\(Y\\) shows overdispersion which implies that \\[\\text{Var}(Y) &gt; \\mathbb{E}(Y).\\]\nWhen \\(-1 &lt; \\theta &lt; 0\\), the GP \\(Y\\) shows underdispersion which implies that \\[\\text{Var}(Y) &lt; \\mathbb{E}(Y).\\]\nWhen \\(\\theta = 0\\), the PMF of the GP \\(Y\\) in Equation D.41 becomes the classical Poisson PMF from Equation D.27: \\[\n\\begin{align*}\nP_Y \\left( Y = y \\mid \\lambda, \\theta = 0 \\right) &= \\frac{\\lambda (\\lambda + y \\theta)^{y - 1} \\exp{\\left[ -(\\lambda + y \\theta) \\right]}}{y!} \\\\\n&= \\frac{\\lambda (\\lambda)^{y - 1} \\exp{\\left( -\\lambda \\right)}}{y!} \\qquad \\text{setting $\\theta = 0$} \\\\\n&= \\frac{\\lambda^y \\exp{\\left( -\\lambda \\right)}}{y!} \\\\\n& \\qquad \\qquad \\qquad \\text{for $y \\in \\{ 0, 1, 2, \\dots\\}$.}\n\\end{align*}\n\\]\n\n\n\nHeads-up on equidispersion in a generalized Poisson random variable!\n\n\nIn a GP-distributed \\(Y\\), when \\(\\theta = 0\\) in its corresponding PMF, we have equidispersion which implies \\[\n\\mathbb{E}(Y \\mid \\theta = 0) = \\frac{\\lambda}{1 - \\theta} = \\lambda\n\\] \\[\n\\text{Var}(Y \\mid \\theta = 0) = \\frac{\\lambda}{(1 - \\theta)^2} = \\lambda\n\\] \\[\n\\mathbb{E}(Y \\mid \\theta = 0) = \\text{Var}(Y).\n\\]\n\n\n\nHow can we verify that Equation D.41 is a proper PMF (i.e., all the standalone probabilities over the support of \\(Y\\) add up to one)?\n\n\nD.5.2 Expected Value\n\nD.5.3 Variance",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-zero-inflated-poisson-distribution",
    "href": "book/C-distributional-mind-map.html#sec-zero-inflated-poisson-distribution",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nD.6 Zero-inflated Poisson",
    "text": "D.6 Zero-inflated Poisson",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-multinomial-distribution",
    "href": "book/C-distributional-mind-map.html#sec-multinomial-distribution",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nD.7 Multinomial",
    "text": "D.7 Multinomial",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-weibull-distribution",
    "href": "book/C-distributional-mind-map.html#sec-weibull-distribution",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nE.1 Weibull",
    "text": "E.1 Weibull\nSuppose you observe the waiting times for some event of interest to happen (i.e., survival times). Let random variable \\(Y\\) be considered continuous and nonnegative. Then, \\(Y\\) is said to have a Weibull distribution with the following scale continuous parameter \\(\\beta\\) and shape continuous parameter \\(\\gamma\\):\n\\[\nY \\sim \\text{Weibull}(\\beta, \\gamma).\n\\]\n\nE.1.1 Probability Density Function\nThe PDF of \\(Y\\) is the following:\n\\[\nf_Y \\left(y \\mid \\beta, \\gamma \\right) = \\frac{\\gamma}{\\beta} \\left( \\frac{y}{\\beta} \\right)^{\\gamma - 1} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\quad \\text{for $y \\in [0, \\infty )$.}\n\\tag{E.1}\\]\nParameters \\(\\beta \\in (0, \\infty)\\) and \\(\\gamma \\in (0, \\infty)\\) refer to the random process’ scale and shape, respectively. Figure E.1 shows nine members of the Weibull parametric family, i.e., nine different PDFs with all possible pairwise combinations for three different scale parameters \\(\\beta = 0.5, 1, 2\\) and shape parameters \\(\\gamma = 1.5, 3, 6\\). We can highlight the following:\n\nRegardless of the shape parameter \\(\\gamma\\), as we increase the scale parameter \\(\\beta\\), note that there is more spread in the corresponding distributions.\nRegardless of the scale parameter \\(\\beta\\), as we increase the shape parameter \\(\\gamma\\), note the peak of the distribution moves more to the right.\n\n\n\n\n\n\n\n\nFigure E.1: Some members of the Weibull parametric family.\n\n\n\n\n\n\nHeads-up on the Weibull and Exponential distributions in survival analysis!\n\n\nThe Weibull distribution extends its Exponential counterpart (as in Section E.3) by allowing the event rate (or hazard) to change over time, rather than staying constant. This makes it especially useful in survival analysis and reliability studies, where capturing how the risk of an event evolves is critical.\nAs a side note, the Weibull and Exponential PDFs are mathematically related. When \\(\\gamma = 1\\) in Equation E.1, the Weibull PDF is equal to the Exponential PDF under the scale parametrization as in Equation E.11:\n\\[\n\\begin{align*}\nf_Y \\left(y \\mid \\beta, \\gamma = 1 \\right) &= \\frac{\\gamma}{\\beta} \\left( \\frac{y}{\\beta} \\right)^{\\gamma - 1} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\\\\n&= \\frac{1}{\\beta} \\underbrace{\\left( \\frac{y}{\\beta} \\right)^0}_{1} \\exp{\\left( -\\frac{y}{\\beta} \\right)} \\\\\n&= \\frac{1}{\\beta} \\exp{\\left( -\\frac{y}{\\beta} \\right)} \\quad \\text{for $y \\in [0, \\infty )$}.\n\\end{align*}\n\\]\n\n\n\nHow can we verify that Equation E.1 is a proper PDF (i.e., Equation E.1 integrates to one over the support of \\(Y\\))?\n\n\nProof. \\[\n\\begin{align*}\n\\int_{y = 0}^{y = \\infty} f_Y \\left(y \\mid \\beta, \\gamma \\right) \\mathrm{d}y &= \\int_{y = 0}^{y = \\infty} \\frac{\\gamma}{\\beta} \\left( \\frac{y}{\\beta} \\right)^{\\gamma - 1} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y.\n\\end{align*}\n\\tag{E.2}\\]\nNow, let us make the variable substitution:\n\\[\n\\begin{gather*}\nu = \\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\\\\ny = \\beta u^{\\frac{1}{\\gamma}} \\\\\n\\mathrm{d}y = \\frac{\\beta}{\\gamma} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u.\n\\end{gather*}\n\\]\nThe above rearrangemet yields the following in Equation E.2:\n\\[\n\\begin{align*}\n\\int_{y = 0}^{y = \\infty} f_Y \\left(y \\mid \\beta, \\gamma \\right) \\mathrm{d}y &= \\int_{u = 0}^{u = \\infty} \\frac{\\gamma}{\\beta} \\left( \\frac{\\beta u^{\\frac{1}{\\gamma}}}{\\beta} \\right)^{\\gamma - 1} \\exp{\\left( -u \\right)} \\frac{\\beta}{\\gamma} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\int_{u = 0}^{u = \\infty} \\left( u^{\\frac{1}{\\gamma}} \\right)^{\\gamma - 1} \\exp{\\left( -u \\right)} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\int_{u = 0}^{u = \\infty} u^{\\frac{\\gamma - 1}{\\gamma}} \\exp{\\left( -u \\right)} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\int_{u = 0}^{u = \\infty} u^{\\frac{\\gamma - 1}{\\gamma} + \\frac{1}{\\gamma} - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n&= \\int_{u = 0}^{u = \\infty} u^{1 - \\frac{1}{\\gamma} + \\frac{1}{\\gamma} - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n&= \\int_{u = 0}^{u = \\infty} u^0 \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n&= \\int_{u = 0}^{u = \\infty} \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n&= -\\exp{\\left( -u \\right)} \\Bigg|_{u = 0}^{u = \\infty} \\\\\n&= - \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\\\\n&= - \\left( 0 - 1 \\right) \\\\\n&= 1. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\square\n\\end{align*}\n\\]\n\nIndeed, the Weibull PDF is a proper probability distribution!\n\n\n\nE.1.2 Expected Value\nVia Equation C.4, the expected value or mean of a Weibull-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\int_{y = 0}^{y = \\infty} y f_Y \\left(y \\mid \\beta, \\gamma \\right) \\mathrm{d}y \\\\\n&= \\int_{y = 0}^{y = \\infty} y \\frac{\\gamma}{\\beta} \\left( \\frac{y}{\\beta} \\right)^{\\gamma - 1} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y \\\\\n&= \\frac{\\gamma}{\\beta} \\int_{y = 0}^{y = \\infty} y \\left( \\frac{y}{\\beta} \\right)^{\\gamma - 1} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y \\\\\n&= \\frac{\\gamma}{\\beta} \\int_{y = 0}^{y = \\infty} y \\frac{y^{\\gamma - 1}}{\\beta^{\\gamma - 1}} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y \\\\\n&= \\frac{\\gamma}{\\beta} \\int_{y = 0}^{y = \\infty} \\frac{y^{\\gamma}}{\\beta^{\\gamma - 1}} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y \\\\\n&= \\frac{\\gamma}{\\beta^{\\gamma}} \\int_{y = 0}^{y = \\infty} y^{\\gamma} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y.\n\\end{align*}\n\\tag{E.3}\\]\nThen, we make the following variable substitution:\n\\[\n\\begin{gather*}\nu = \\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\\\\ny = \\beta u^{\\frac{1}{\\gamma}} \\\\\n\\mathrm{d}y = \\frac{\\beta}{\\gamma} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u.\n\\end{gather*}\n\\]\nThe above rearrangemet yields the following in Equation E.3:\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\frac{\\gamma}{\\beta^{\\gamma}} \\int_{u = 0}^{u = \\infty} \\left( \\beta u^{\\frac{1}{\\gamma}} \\right)^{\\gamma} \\exp{\\left( -u \\right)} \\frac{\\beta}{\\gamma} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\frac{\\gamma}{\\beta^{\\gamma}} \\int_{u = 0}^{u = \\infty} \\beta^{\\gamma} u \\exp{\\left( -u \\right)} \\frac{\\beta}{\\gamma} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\frac{\\gamma \\beta^{\\gamma} \\beta}{\\beta^{\\gamma} \\gamma} \\int_{u = 0}^{u = \\infty} u \\exp{\\left( -u \\right)} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\beta \\int_{u = 0}^{u = \\infty} u^{\\frac{1}{\\gamma}} \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n&= \\beta \\int_{u = 0}^{u = \\infty} u^{\\left( \\frac{1}{\\gamma} + 1 \\right) - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n& \\quad \\qquad \\text{note $\\frac{1}{\\gamma} = \\left( \\frac{1}{\\gamma} + 1 \\right) - 1$.}\n\\end{align*}\n\\tag{E.4}\\]\nThe integral on the right-hand side of Equation E.4 corresponds to the so-called Gamma function as described below.\n\n\nHeads-up on the Gamma function!\n\n\nThe Gamma function is a mathematical generalization of the factorial function, but applied to non-integer numbers. In many different probability distributions, this function appears as a normalizing constant. Moreover, it also appears as part of the expressions of expected values and variances.\nThat said, for a variable \\(z\\) in general, we can represent the Gamma function via the following integral:\n\\[\n\\Gamma(z) = \\int_{t = 0}^{t = \\infty} t^{z - 1} \\exp{\\left( -t \\right)} \\mathrm{d}t.\n\\tag{E.5}\\]\nWeisstein (n.d.a) provides further insights on this Gamma function along with some useful properties.\n\n\nThus, via the Gamma function from Equation E.5, we set the following:\n\\[\n\\begin{gather*}\nt = u \\\\\nz = \\frac{1}{\\gamma} + 1,\n\\end{gather*}\n\\]\nwhich yields\n\\[\n\\Gamma \\left( \\frac{1}{\\gamma} + 1 \\right) = \\int_{u = 0}^{u = \\infty} u^{\\left( \\frac{1}{\\gamma} + 1 \\right) - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u.\n\\]\nMoving along with Equation E.4, we have:\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\beta \\int_{u = 0}^{u = \\infty} u^{\\left( \\frac{1}{\\gamma} + 1 \\right) - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n&= \\beta \\Gamma \\left( \\frac{1}{\\gamma} + 1 \\right). \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\n\nE.1.3 Variance\nVia Equation C.5 and the Equation C.4 of a continuous expected value, the variance of a Weibull-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y)\\right]^2 \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - \\beta^2 \\Gamma^2 \\left( \\frac{1}{\\gamma} + 1 \\right) \\\\\n& \\quad \\qquad \\text{since $\\mathbb{E}(Y) = \\beta \\Gamma \\left( \\frac{1}{\\gamma} + 1 \\right)$}.\n\\end{align*}\n\\tag{E.6}\\]\nNow, we need to find \\(\\mathbb{E} \\left( Y^2 \\right)\\) from Equation E.6. Thus, we make the following derivation via the LOTUS from Equation C.2 when \\(g(Y) = y^2\\):\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\int_{y = 0}^{y = \\infty} y^2 f_Y \\left(y \\mid \\beta, \\gamma \\right) \\mathrm{d}y \\\\\n&= \\int_{y = 0}^{y = \\infty} y^2 \\frac{\\gamma}{\\beta} \\left( \\frac{y}{\\beta} \\right)^{\\gamma - 1} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y \\\\\n&= \\frac{\\gamma}{\\beta} \\int_{y = 0}^{y = \\infty} y^2 \\left( \\frac{y}{\\beta} \\right)^{\\gamma - 1} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y \\\\\n&= \\frac{\\gamma}{\\beta} \\int_{y = 0}^{y = \\infty} y^2 \\frac{y^{\\gamma - 1}}{\\beta^{\\gamma - 1}} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y \\\\\n&= \\frac{\\gamma}{\\beta^{\\gamma}} \\int_{y = 0}^{y = \\infty} y^{\\gamma + 1} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y.\n\\end{align*}\n\\tag{E.7}\\]\nThen, we make the following variable substitution:\n\\[\n\\begin{gather*}\nu = \\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\\\\ny = \\beta u^{\\frac{1}{\\gamma}} \\\\\n\\mathrm{d}y = \\frac{\\beta}{\\gamma} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u.\n\\end{gather*}\n\\]\nThe above rearrangemet yields the following in Equation E.7:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\frac{\\gamma}{\\beta^{\\gamma}} \\int_{u = 0}^{u = \\infty} \\left( \\beta u^{\\frac{1}{\\gamma}} \\right)^{\\gamma + 1} \\exp{\\left( -u \\right)} \\frac{\\beta}{\\gamma} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\frac{\\gamma}{\\beta^{\\gamma}} \\int_{u = 0}^{u = \\infty} \\beta^{\\gamma + 1} u^{1 + \\frac{1}{\\gamma}} \\exp{\\left( -u \\right)} \\frac{\\beta}{\\gamma} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\frac{\\gamma \\beta^{\\gamma + 1} \\beta}{\\beta^{\\gamma} \\gamma} \\int_{u = 0}^{u = \\infty} u^{1 + \\frac{1}{\\gamma}} \\exp{\\left( -u \\right)} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\beta^2 \\int_{u = 0}^{u = \\infty} u^{1 + \\frac{1}{\\gamma} + \\frac{1}{\\gamma} - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n&= \\beta^2 \\int_{u = 0}^{u = \\infty} u^{\\left( \\frac{2}{\\gamma} + 1 \\right) - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u.\n\\end{align*}\n\\tag{E.8}\\]\nHence, via the Gamma function from Equation E.5, we set the following:\n\\[\n\\begin{gather*}\nt = u \\\\\nz = \\frac{2}{\\gamma} + 1,\n\\end{gather*}\n\\]\nwhich yields\n\\[\n\\Gamma \\left( \\frac{2}{\\gamma} + 1 \\right) = \\int_{u = 0}^{u = \\infty} u^{\\left( \\frac{2}{\\gamma} + 1 \\right) - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u.\n\\]\nMoving along with Equation E.8, we have:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\beta^2 \\int_{u = 0}^{u = \\infty} u^{\\left( \\frac{2}{\\gamma} + 1 \\right) - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n&= \\beta^2 \\Gamma \\left( \\frac{2}{\\gamma} + 1 \\right).\n\\end{align*}\n\\tag{E.9}\\]\nFinally, we plug Equation E.9 into Equation E.6:\n\\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E}\\left( Y^2 \\right) - \\beta^2 \\Gamma^2 \\left( \\frac{1}{\\gamma} + 1 \\right) \\\\\n&= \\beta^2 \\Gamma \\left( \\frac{2}{\\gamma} + 1 \\right) - \\beta^2 \\Gamma^2 \\left( \\frac{1}{\\gamma} + 1 \\right) \\\\\n&= \\beta^2 \\left[  \\Gamma \\left( \\frac{2}{\\gamma} + 1 \\right) - \\Gamma^2 \\left( \\frac{1}{\\gamma} + 1 \\right) \\right]. \\qquad \\qquad \\square\n\\end{align*}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-lognormal-distribution",
    "href": "book/C-distributional-mind-map.html#sec-lognormal-distribution",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nE.2 Lognormal",
    "text": "E.2 Lognormal",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-exponential-distribution",
    "href": "book/C-distributional-mind-map.html#sec-exponential-distribution",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nE.3 Exponential",
    "text": "E.3 Exponential\nSuppose you observe the waiting times for some event of interest to happen (i.e., survival times). Let random variable \\(Y\\) be considered continuous and nonnegative. Then, \\(Y\\) is said to have an Exponential distribution with the following rate continuous parameter \\(\\lambda\\):\n\\[\nY \\sim \\text{Exponential}(\\lambda).\n\\]\nWe can also model \\(Y\\) with the following scale continuous parameter \\(\\beta\\):\n\\[\nY \\sim \\text{Exponential}(\\beta).\n\\]\n\nE.3.1 Probability Density Functions\nGiven the two above parametrizations of the Exponential distribution, there are two possible PDFs as discussed below.\nRate Parametrization\nThe PDF of \\(Y\\) is the following:\n\\[\nf_Y \\left(y \\mid \\lambda \\right) = \\lambda \\exp \\left( -\\lambda y \\right) \\quad \\text{for $y \\in [0, \\infty )$.}\n\\tag{E.10}\\]\nParameter \\(\\lambda \\in (0, \\infty)\\) refers to the random process’ rate. Figure E.2 shows three members of the Exponential parametric family, i.e., three different PDFs with different rate parameters \\(\\lambda = 0.25, 0.5, 1\\). As we increase the rate parameter, note that smaller observed values \\(y\\) get more probable.\n\n\n\n\n\n\n\nFigure E.2: Some members of the Exponential parametric family with rate parametrization.\n\n\n\n\n\nHow can we verify that Equation E.10 is a proper PDF (i.e., Equation E.10 integrates to one over the support of \\(Y\\))?\n\n\nProof. \\[\n\\begin{align*}\n\\int_{y = 0}^{y = \\infty} f_Y \\left(y \\mid \\lambda \\right) \\mathrm{d}y &= \\int_{y = 0}^{y = \\infty} \\lambda \\exp \\left( -\\lambda y \\right) \\mathrm{d}y \\\\\n&= \\lambda \\int_{y = 0}^{y = \\infty} \\exp \\left( -\\lambda y \\right) \\mathrm{d}y \\\\\n&= - \\frac{\\lambda}{\\lambda} \\exp \\left( -\\lambda y \\right) \\Bigg|_{y = 0}^{y = \\infty} \\\\\n&= - \\exp \\left( -\\lambda y \\right) \\Bigg|_{y = 0}^{y = \\infty} \\\\\n&= - \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\\\\n&= - \\left( 0 - 1 \\right) \\\\\n&= 1. \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\nIndeed, the Exponential PDF, under a rate parametrization, is a proper probability distribution!\n\n\nScale Parametrization\nThe PDF of \\(Y\\) is the following:\n\\[\nf_Y \\left(y \\mid \\beta \\right) = \\frac{1}{\\beta} \\exp \\left( -\\frac{y}{\\beta} \\right) \\quad \\text{for $y \\in [0, \\infty )$.}\n\\tag{E.11}\\]\nParameter \\(\\beta \\in (0, \\infty)\\) refers to the random process’ scale. Figure E.3 shows three members of the Exponential parametric family, i.e., three different PDFs with different scale parameters \\(\\beta = 0.25, 0.5, 1\\). As we increase the scale parameter, note that larger observed values \\(y\\) get more probable.\n\n\n\n\n\n\n\nFigure E.3: Some members of the Exponential parametric family with scale parametrization.\n\n\n\n\n\nHow can we verify that Equation E.11 is a proper PDF (i.e., Equation E.11 integrates to one over the support of \\(Y\\))?\n\n\nProof. \\[\n\\begin{align*}\n\\int_{y = 0}^{y = \\infty} f_Y \\left(y \\mid \\beta \\right) \\mathrm{d}y &= \\int_{y = 0}^{y = \\infty} \\frac{1}{\\beta} \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\\\\n&= \\frac{1}{\\beta} \\int_{y = 0}^{y = \\infty} \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\\\\n&= - \\frac{\\beta}{\\beta} \\exp \\left( -\\frac{y}{\\beta} \\right) \\Bigg|_{y = 0}^{y = \\infty} \\\\\n&= - \\exp \\left( -\\frac{y}{\\beta} \\right) \\Bigg|_{y = 0}^{y = \\infty} \\\\\n&= - \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\\\\n&= - \\left( 0 - 1 \\right) \\\\\n&= 1. \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\nIndeed, the Exponential PDF, under a scale parametrization, is a proper probability distribution!\n\n\n\nE.3.2 Expected Value\nAgain, given the two above parametrizations of the Exponential distribution, there are two possible mathematical expressions for the expected value as discussed below.\nRate Parametrization\nVia Equation C.4, the expected value or mean of an Exponential-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\int_{y = 0}^{y = \\infty} y f_Y \\left(y \\mid \\lambda \\right) \\mathrm{d}y \\\\\n&= \\int_{y = 0}^{y = \\infty} y \\lambda \\exp \\left( -\\lambda y \\right) \\mathrm{d}y \\\\\n&= \\lambda \\int_{y = 0}^{y = \\infty} y \\exp \\left( -\\lambda y \\right) \\mathrm{d}y. \\\\\n\\end{align*}\n\\tag{E.12}\\]\nEquation E.12 cannot be solved straightforwardly, we need to use integration by parts as follows:\n\\[\n\\begin{equation}\n  \\begin{split}\n    u &= y \\\\\n    \\mathrm{d}u &= \\mathrm{d}y\n  \\end{split}\n\\qquad \\qquad\n  \\begin{split}\n    \\mathrm{d}v &= \\exp \\left( -\\lambda y \\right) \\mathrm{d}y \\\\\n    v &= -\\frac{1}{\\lambda} \\exp \\left( -\\lambda y \\right),\n  \\end{split}\n\\end{equation}\n\\]\nwhich yields\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\lambda \\left[ u v \\Bigg|_{y = 0}^{y = \\infty} - \\int_{y = 0}^{y = \\infty} v \\mathrm{d}u \\right] \\\\\n&= \\lambda \\left\\{ \\left[ -\\frac{1}{\\lambda} y \\exp(-\\lambda y) \\right] \\Bigg|_{y = 0}^{y = \\infty} + \\frac{1}{\\lambda} \\int_{y = 0}^{y = \\infty} \\exp{\\left( -\\lambda y \\right)} \\mathrm{d}y \\right\\} \\\\\n&= \\lambda \\Bigg\\{ -\\frac{1}{\\lambda} \\Bigg[ \\underbrace{\\infty \\times \\exp(-\\infty)}_{0} - \\underbrace{0 \\times \\exp(0)}_{0} \\Bigg] - \\\\\n& \\qquad \\frac{1}{\\lambda^2} \\exp{\\left( -\\lambda y \\right)} \\Bigg|_{y = 0}^{y = \\infty} \\Bigg\\} \\\\\n&= \\lambda \\left\\{ -\\frac{1}{\\lambda} (0) - \\frac{1}{\\lambda^2} \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\right\\} \\\\\n&= \\lambda \\left[ 0 - \\frac{1}{\\lambda^2} (0 - 1) \\right] \\\\\n&= \\frac{\\lambda}{\\lambda^2} \\\\\n&= \\frac{1}{\\lambda}. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\qquad \\qquad \\quad \\qquad \\qquad \\square\n\\end{align*}\n\\]\n\nScale Parametrization\nVia Equation C.4, the expected value or mean of an Exponential-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\int_{y = 0}^{y = \\infty} y f_Y \\left(y \\mid \\beta \\right) \\mathrm{d}y \\\\\n&= \\int_{y = 0}^{y = \\infty} \\frac{y}{\\beta} \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\\\\n&= \\frac{1}{\\beta} \\int_{y = 0}^{y = \\infty} y \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y. \\\\\n\\end{align*}\n\\tag{E.13}\\]\nEquation E.13 cannot be solved straightforwardly, we need to use integration by parts as follows:\n\\[\n\\begin{equation}\n  \\begin{split}\n    u &= y \\\\\n    \\mathrm{d}u &= \\mathrm{d}y\n  \\end{split}\n\\qquad \\qquad\n  \\begin{split}\n    \\mathrm{d}v &= \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\\\\n    v &= -\\beta \\exp \\left( -\\frac{y}{\\beta} \\right),\n  \\end{split}\n\\end{equation}\n\\]\nwhich yields\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\frac{1}{\\beta} \\left[ u v \\Bigg|_{y = 0}^{y = \\infty} - \\int_{y = 0}^{y = \\infty} v \\mathrm{d}u \\right] \\\\\n&= \\frac{1}{\\beta} \\left\\{ \\left[ -\\beta y \\exp \\left( -\\frac{y}{\\beta} \\right) \\right] \\Bigg|_{y = 0}^{y = \\infty} + \\beta \\int_{y = 0}^{y = \\infty} \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\right\\} \\\\\n&= \\frac{1}{\\beta} \\Bigg\\{ -\\beta \\Bigg[ \\underbrace{\\infty \\times \\exp(-\\infty)}_{0} - \\underbrace{0 \\times \\exp(0)}_{0} \\Bigg] - \\\\\n& \\qquad \\beta^2 \\exp \\left( -\\frac{y}{\\beta} \\right) \\Bigg|_{y = 0}^{y = \\infty} \\Bigg\\} \\\\\n&= \\frac{1}{\\beta} \\left\\{ -\\beta (0) - \\beta^2 \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\right\\} \\\\\n&= \\frac{1}{\\beta} \\left[ 0 - \\beta^2 (0 - 1) \\right] \\\\\n&= \\frac{\\beta^2}{\\beta} \\\\\n&= \\beta. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\qquad \\qquad \\quad \\qquad \\qquad \\square\n\\end{align*}\n\\]\n\n\nE.3.3 Variance\nGiven the two above parametrizations of the Exponential distribution, there are two possible mathematical expressions for the variance as discussed below.\nRate Parametrization\nVia Equation C.5 and the Equation C.4 of a continuous expected value, the variance of an Exponential-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y)\\right]^2 \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - \\frac{1}{\\lambda^2} \\qquad \\text{since $\\mathbb{E}(Y) = \\frac{1}{\\lambda}$}.\n\\end{align*}\n\\tag{E.14}\\]\nNow, we need to find \\(\\mathbb{E} \\left( Y^2 \\right)\\) from Equation E.14. Hence, we make the following derivation via the LOTUS from Equation C.2 when \\(g(Y) = y^2\\):\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\int_{y = 0}^{y = \\infty} y^2 f_Y \\left(y \\mid \\lambda \\right) \\mathrm{d}y \\\\\n&= \\int_{y = 0}^{y = \\infty} y^2 \\lambda \\exp \\left( -\\lambda y \\right) \\mathrm{d}y \\\\\n&= \\lambda \\int_{y = 0}^{y = \\infty} y^2 \\exp \\left( -\\lambda y \\right) \\mathrm{d}y. \\\\\n\\end{align*}\n\\tag{E.15}\\]\nEquation E.15 cannot be solved straightforwardly, we need to use integration by parts as follows:\n\\[\n\\begin{equation}\n  \\begin{split}\n    u &= y^2 \\\\\n    \\mathrm{d}u &= 2y \\mathrm{d}y\n  \\end{split}\n\\qquad \\qquad\n  \\begin{split}\n    \\mathrm{d}v &= \\exp \\left( -\\lambda y \\right) \\mathrm{d}y \\\\\n    v &= -\\frac{1}{\\lambda} \\exp \\left( -\\lambda y \\right),\n  \\end{split}\n\\end{equation}\n\\]\nwhich yields\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\lambda \\left[ u v \\Bigg|_{y = 0}^{y = \\infty} - \\int_{y = 0}^{y = \\infty} v \\mathrm{d}u \\right] \\\\\n&= \\lambda \\bigg\\{ \\left[ -\\frac{1}{\\lambda} y^2 \\exp(-\\lambda y) \\right] \\Bigg|_{y = 0}^{y = \\infty} + \\\\\n& \\qquad \\frac{2}{\\lambda} \\int_{y = 0}^{y = \\infty} y \\exp{\\left( -\\lambda y \\right)} \\mathrm{d}y \\bigg\\} \\\\\n&= \\lambda \\Bigg\\{ -\\frac{1}{\\lambda} \\Bigg[ \\underbrace{\\infty \\times \\exp(-\\infty)}_{0} - \\underbrace{0 \\times \\exp(0)}_{0} \\Bigg] + \\\\\n& \\qquad \\frac{2}{\\lambda} \\int_{y = 0}^{y = \\infty} y \\exp{\\left( -\\lambda y \\right)} \\mathrm{d}y \\Bigg\\} \\\\\n&= \\lambda \\left\\{ -\\frac{1}{\\lambda} (0) + \\frac{2}{\\lambda} \\int_{y = 0}^{y = \\infty} y \\exp{\\left( -\\lambda y \\right)} \\mathrm{d}y \\right\\} \\\\\n&= \\lambda \\left\\{ 0 + \\frac{2}{\\lambda} \\int_{y = 0}^{y = \\infty} y \\exp{\\left( -\\lambda y \\right)} \\mathrm{d}y \\right\\} \\\\\n&= 2 \\int_{y = 0}^{y = \\infty} y \\exp{\\left( -\\lambda y \\right)} \\mathrm{d}y. \\\\\n\\end{align*}\n\\tag{E.16}\\]\nAgain, we need to apply integration by parts to solve Equation E.16:\n\\[\n\\begin{equation}\n  \\begin{split}\n    u &= y \\\\\n    \\mathrm{d}u &= \\mathrm{d}y\n  \\end{split}\n\\qquad \\qquad\n  \\begin{split}\n    \\mathrm{d}v &= \\exp \\left( -\\lambda y \\right) \\mathrm{d}y \\\\\n    v &= -\\frac{1}{\\lambda} \\exp \\left( -\\lambda y \\right),\n  \\end{split}\n\\end{equation}\n\\]\nwhich yields\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= 2 \\left[ u v \\Bigg|_{y = 0}^{y = \\infty} - \\int_{y = 0}^{y = \\infty} v \\mathrm{d}u \\right] \\\\\n&= 2 \\left\\{ \\left[ -\\frac{1}{\\lambda} y \\exp(-\\lambda y) \\right] \\Bigg|_{y = 0}^{y = \\infty} + \\frac{1}{\\lambda} \\int_{y = 0}^{y = \\infty} \\exp{\\left( -\\lambda y \\right)} \\mathrm{d}y \\right\\} \\\\\n&= 2 \\Bigg\\{ -\\frac{1}{\\lambda} \\Bigg[ \\underbrace{\\infty \\times \\exp(-\\infty)}_{0} - \\underbrace{0 \\times \\exp(0)}_{0} \\Bigg] - \\\\\n& \\qquad \\frac{1}{\\lambda^2} \\exp{\\left( -\\lambda y \\right)} \\Bigg|_{y = 0}^{y = \\infty} \\Bigg\\} \\\\\n&= 2 \\left\\{ -\\frac{1}{\\lambda} (0) - \\frac{1}{\\lambda^2} \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\right\\} \\\\\n&= 2 \\left[ 0 - \\frac{1}{\\lambda^2} (0 - 1) \\right] \\\\\n&= \\frac{2}{\\lambda^2}.\n\\end{align*}\n\\tag{E.17}\\]\nFinally, we plug Equation E.17 into Equation E.14:\n\\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\frac{1}{\\lambda^2} \\\\\n&= \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} \\\\\n&= \\frac{1}{\\lambda^2}. \\qquad \\qquad \\square\n\\end{align*}\n\\]\n\nScale Parametrization\nVia Equation C.5 and the Equation C.4 of a continuous expected value, the variance of an Exponential-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y)\\right]^2 \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - \\beta^2 \\qquad \\text{since $\\mathbb{E}(Y) = \\beta$}.\n\\end{align*}\n\\tag{E.18}\\]\nNow, we need to find \\(\\mathbb{E} \\left( Y^2 \\right)\\) from Equation E.18. Hence, we make the following derivation via the LOTUS from Equation C.2 when \\(g(Y) = y^2\\):\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\int_{y = 0}^{y = \\infty} y^2 f_Y \\left(y \\mid \\beta \\right) \\mathrm{d}y \\\\\n&= \\int_{y = 0}^{y = \\infty} y^2 \\frac{1}{\\beta} \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\\\\n&= \\frac{1}{\\beta} \\int_{y = 0}^{y = \\infty} y^2 \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y. \\\\\n\\end{align*}\n\\tag{E.19}\\]\nEquation E.19 cannot be solved straightforwardly, we need to use integration by parts as follows:\n\\[\n\\begin{equation}\n  \\begin{split}\n    u &= y^2 \\\\\n    \\mathrm{d}u &= 2y \\mathrm{d}y\n  \\end{split}\n\\qquad \\qquad\n  \\begin{split}\n    \\mathrm{d}v &= \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\\\\n    v &= -\\beta \\exp \\left( -\\frac{y}{\\beta} \\right),\n  \\end{split}\n\\end{equation}\n\\]\nwhich yields\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\frac{1}{\\beta} \\left[ u v \\Bigg|_{y = 0}^{y = \\infty} - \\int_{y = 0}^{y = \\infty} v \\mathrm{d}u \\right] \\\\\n&= \\frac{1}{\\beta} \\Bigg\\{ \\left[ -\\beta y^2 \\exp \\left( -\\frac{y}{\\beta} \\right) \\right] \\Bigg|_{y = 0}^{y = \\infty} + \\\\\n& \\qquad 2 \\beta \\int_{y = 0}^{y = \\infty} y \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\Bigg\\} \\\\\n&= \\frac{1}{\\beta} \\Bigg\\{ -\\beta \\Bigg[ \\underbrace{\\infty \\times \\exp(-\\infty)}_{0} - \\underbrace{0 \\times \\exp(0)}_{0} \\Bigg] + \\\\\n& \\qquad 2 \\beta \\int_{y = 0}^{y = \\infty} y \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\Bigg\\} \\\\\n&= \\frac{1}{\\beta} \\left\\{ -\\beta (0) + 2 \\beta \\int_{y = 0}^{y = \\infty} y \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\right\\} \\\\\n&= \\frac{1}{\\beta} \\left\\{ 0 + 2 \\beta \\int_{y = 0}^{y = \\infty} y \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\right\\} \\\\\n&= 2 \\int_{y = 0}^{y = \\infty} y \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y. \\\\\n\\end{align*}\n\\tag{E.20}\\]\nAgain, we need to apply integration by parts to solve Equation E.20:\n\\[\n\\begin{equation}\n  \\begin{split}\n    u &= y \\\\\n    \\mathrm{d}u &= \\mathrm{d}y\n  \\end{split}\n\\qquad \\qquad\n  \\begin{split}\n    \\mathrm{d}v &= \\exp \\left( -\\frac{y}{\\beta} \\right)\\mathrm{d}y \\\\\n    v &= -\\beta \\exp \\left( -\\frac{y}{\\beta} \\right),\n  \\end{split}\n\\end{equation}\n\\]\nwhich yields\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= 2 \\left[ u v \\Bigg|_{y = 0}^{y = \\infty} - \\int_{y = 0}^{y = \\infty} v \\mathrm{d}u \\right] \\\\\n&= 2 \\left\\{ \\left[ -\\beta y \\exp \\left( -\\frac{y}{\\beta} \\right) \\right] \\Bigg|_{y = 0}^{y = \\infty} + \\beta \\int_{y = 0}^{y = \\infty} \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\right\\} \\\\\n&= 2 \\Bigg\\{ -\\beta \\Bigg[ \\underbrace{\\infty \\times \\exp(-\\infty)}_{0} - \\underbrace{0 \\times \\exp(0)}_{0} \\Bigg] - \\\\\n& \\qquad \\beta^2 \\exp \\left( -\\frac{y}{\\beta} \\right) \\Bigg|_{y = 0}^{y = \\infty} \\Bigg\\} \\\\\n&= 2 \\left\\{ -\\beta (0) - \\beta^2 \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\right\\} \\\\\n&= 2 \\left[ 0 - \\beta^2 (0 - 1) \\right] \\\\\n&= 2 \\beta^2.\n\\end{align*}\n\\tag{E.21}\\]\nFinally, we plug Equation E.21 into Equation E.18:\n\\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\beta^2 \\\\\n&= 2 \\beta^2 - \\beta^2 \\\\\n&= \\beta^2. \\qquad \\qquad \\square\n\\end{align*}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-gamma-distribution",
    "href": "book/C-distributional-mind-map.html#sec-gamma-distribution",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nE.4 Gamma",
    "text": "E.4 Gamma",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-logistic-distribution",
    "href": "book/C-distributional-mind-map.html#sec-logistic-distribution",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nE.5 Logistic",
    "text": "E.5 Logistic",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-normal-distribution",
    "href": "book/C-distributional-mind-map.html#sec-normal-distribution",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nE.6 Normal",
    "text": "E.6 Normal\nThis is possibly one the most famous probability distributions, and it is also known as Gaussian. It appears in many different statistical tools in the literature where certain regression models are included. Let random variable \\(Y\\) be considered continuous and unbounded. Then, \\(Y\\) is said to have a Normal distribution with the following location continuous parameter \\(\\mu\\) and scale continuous parameter \\(\\sigma^2\\):\n\\[\nY \\sim \\text{Normal}(\\mu, \\sigma^2).\n\\]\n\nE.6.1 Probability Density Function\nThe PDF of \\(Y\\) is the following:\n\\[\nf_Y \\left(y \\mid \\mu, \\sigma^2 \\right) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{\\left[ - \\frac{(y - \\mu)^2}{2 \\sigma^2} \\right]} \\quad \\text{for $y \\in ( -\\infty, \\infty )$.}\n\\tag{E.22}\\]\n\n\nHeads-up on the use of \\(\\pi\\) in the Normal distribution!\n\n\nThe term \\(\\pi\\) in the Normal PDF depicted in Equation E.22 corresponds to the mathematical constant \\(3.141592...\\) Hence, this term does not correspond to another population parameter in this probability distribution.\n\n\nParameters \\(\\mu \\in (-\\infty, \\infty)\\) and \\(\\sigma \\in (0, \\infty)\\) refer to the random process’ location and scale, respectively. Figure E.4 shows nine members of the Normal parametric family, i.e., nine different PDFs with all possible pairwise combinations for three different scale parameters \\(\\mu = -3, 0, 3\\) and shape parameters \\(\\sigma^2 = 0.25, 1, 4\\). We can highlight the following:\n\nRegardless of the scale parameter \\(\\sigma^2\\), as we increase the location parameter \\(\\mu\\), note the center of the corresponding symmetric distribution moves more to the right.\nRegardless of the location parameter \\(\\mu\\), as we increase the scale parameter \\(\\sigma^2\\), note that there is more spread in the corresponding symmetric distribution.\n\n\n\n\n\n\n\n\nFigure E.4: Some members of the Normal or Gaussian parametric family.\n\n\n\n\n\nHow can we verify that Equation E.22 is a proper PDF (i.e., Equation E.22 integrates to one over the support of \\(Y\\))?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-beta-distribution",
    "href": "book/C-distributional-mind-map.html#sec-beta-distribution",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nE.7 Beta",
    "text": "E.7 Beta\n\n\n\n\nConsul, P. C., and G. C. Jain. 1973. “A Generalization of the Poisson Distribution.” Technometrics 15 (4): 791–99. http://www.jstor.org/stable/1267389.\n\n\nEarlom, Richard. 1793. “Brook Taylor - National Portrait Gallery.” NPG D6930; Brook Taylor - Portrait - National Portrait Gallery. National Portrait Gallery. https://www.npg.org.uk/collections/search/portrait/mw40921/Brook-Taylor.\n\n\nGregory, James. 1668. Vera circuli et hyperbolae quadratura cui accedit geometria pars vniuersalis inseruiens quantitatum curuarum transmutationi & mensurae. Authore Iacobo Gregorio Abredonensi. Padua, Italy: Patavii: typis heredum Pauli Frambotti bibliop. https://archive.org/details/ita-bnc-mag-00001357-001/page/n10/mode/2up.\n\n\nHarding, Edward. 1798. Portrait of Colin MacLaurin. Courtesy of the Smithsonian Libraries and Archives. https://library.si.edu/image-gallery/72863.\n\n\nMaclaurin, Colin. 1742. A Treatise of Fluxions. Edinburgh, Scotland: Printed for the Author by T.W.; T. Ruddimans. https://archive.org/details/treatiseonfluxio02macl/page/n5/mode/2up.\n\n\nScotland, National Galleries of. n.d. Professor James Gregory, 1638 - 1675 (1). Mathematician. Professor James Gregory, 1638 - 1675 (1). Mathematician | National Galleries. https://www.nationalgalleries.org/art-and-artists/31132/professor-james-gregory-1638-1675-mathematician.\n\n\nTaylor, Brook. 1715. Methodus incrementorum directa & inversa. Auctore Brook Taylor, LL. D. & Regiae Societatis Secretario. London, England: Typis Pearsonianis Prostant apud Gul. Innys ad Insignia Principis in Coemeterio Paulino MDCCXV. https://archive.org/details/bim_eighteenth-century_methodus-incrementorum-d_taylor-brook_1717.\n\n\nWeisstein, Eric W. n.d.a. “Gamma Function.” From MathWorld–A Wolfram Web Resource. https://mathworld.wolfram.com/GammaFunction.html.\n\n\n———. n.d.b. “Taylor Series.” From MathWorld–A Wolfram Web Resource. https://mathworld.wolfram.com/TaylorSeries.html.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/D-regression-mind-map.html",
    "href": "book/D-regression-mind-map.html",
    "title": "Appendix D — The Sugartastic Regression Mind Map",
    "section": "",
    "text": "Fun fact!\n\n\nSugartastic! So sweet, it could power a carnival’s worth of cotton candy machines.\n\n\n\n\n\nImage by Manfred Steger via Pixabay.\n\n\nThe regression mind map is a key component of the philosophy behind this book, besides the data science workflow from Section 1.2 and the ML-Stats dictionary found in Appendix A. Figure D.1 shows this regression mind map split in two zones by colour: discrete and continuous. This regression mind map is handy when executing the data modelling stage from the data science workflow, as explained in Section 1.2.4. Recall the first step in this stage is to choose a suitable regression model, and we made this decision in the function of the type of outcome \\(Y\\) we are dealing with. That said, the distributional mind map from Figure C.1 complements the regression mind map when identifying the correct type of outcome \\(Y\\).\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n        {{Overdispersed or &lt;br/&gt;Underdispersed &lt;br/&gt;Data}}\n          )Chapter 13: &lt;br/&gt;Generalized &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Generalized &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n        {{Zero Inflated &lt;br/&gt;Data}}\n          )Chapter 12: &lt;br/&gt;Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n      {{Categorical &lt;br/&gt;Outcome Y}}\n        {{Nominal &lt;br/&gt;Outcome Y}}\n          )Chapter 14: &lt;br/&gt;Multinomial &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Multinomial &lt;br/&gt;Outcome Y)\n        {{Ordinal &lt;br/&gt;Outcome Y}}\n          )Chapter 15: &lt;br/&gt;Ordinal &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Logistic &lt;br/&gt;Distributed &lt;br/&gt;Cumulative Outcome &lt;br/&gt;Probability)\n\n\n\n\n\n\n\n\nFigure D.1: Regression analysis mind map depicting all modelling techniques to be explored in this book. Depending on the type of outcome \\(Y\\), these techniques are split into two large zones: discrete and continuous.\n\n\nSuppose we start reading this regression map clockwise in the continuous zone. In that case, note this zone starts the cloud corresponding to Chapter 3 on the classical regression model called Ordinary Least-squares (OLS). Moreover, we can see that OLS is meant for an unbounded outcome \\(Y \\in (-\\infty, \\infty)\\). Then, we can proceed to the distributional assumption on \\(Y\\) in OLS, which would be Normal. Following up with the cloud corresponding to Chapter 4 on Gamma regression, we can see this model is meant for a nonnegative outcome \\(Y \\in [0, \\infty)\\) where we assume a Gamma distribution for \\(Y\\). This way of reading the continuous zone in the mind map will persist until the survival analysis models: Chapter 6 and Chapter 7.\nThen, we can proceed to the discrete zone with the cloud corresponding to Chapter 8 on the generalized linear model (GLM) called Binary Logistic regression which aims to model a binary outcome \\(Y \\in \\{0, 1 \\}\\). Note that the Binary Logistic regression model is meant for ungrouped data where each row in the training dataset contains unique feature values. Hence, in this modelling case, we assume the outcome \\(Y\\) as a Bernoulli trial where \\(1\\) indicates a success and \\(0\\) indicates a failure. Then, suppose we take another clockwise case such as Chapter 10 on the GLM Classical Poisson regression, this model is suitable for count-type outcomes where equidispersion is present (i.e., the mean of the counts is equal to its corresponding variance). Finally, this model assumes that the outcome is Poisson-distributed. This way of reading the discrete zone in the mind map will persist until Chapter 15 on Ordinal Logistic Regression.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>The Sugartastic Regression Mind Map</span>"
    ]
  },
  {
    "objectID": "book/continuous-zone.html",
    "href": "book/continuous-zone.html",
    "title": "Continuous Cuisine",
    "section": "",
    "text": "mindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 1: Initial regression analysis mind map where, depending on the type of outcome \\(Y\\), we will split out modelling techniques into two large zones: discrete and continuous.",
    "crumbs": [
      "Continuous Cuisine"
    ]
  },
  {
    "objectID": "book/discrete-zone.html",
    "href": "book/discrete-zone.html",
    "title": "Discrete Cuisine",
    "section": "",
    "text": "mindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 1",
    "crumbs": [
      "Discrete Cuisine"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#introduction",
    "href": "book/08-binary-logistic.html#introduction",
    "title": "Binary Logistic Regression",
    "section": "",
    "text": "Will a student default on a loan given their credit score and income?\nWill a student pass a course based on whether they have a part-time job and how many hours they studied?\nWill a student likely graduate within 4 years, based on their academic record and declared major?\n\n\n\n\n\nLinks input variables to the log-odds of the outcome,\nProduces interpretable coefficients (like odds ratios), and\nHelps us make informed predictions in binary classification problems.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#what-is-ordinary-least-squares-ols",
    "href": "book/08-binary-logistic.html#what-is-ordinary-least-squares-ols",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.9 What is Ordinary Least Squares (OLS)?",
    "text": "8.9 What is Ordinary Least Squares (OLS)?\nOrdinary Least Squares (OLS) is a fundamental method in regression analysis for estimating the relationship between a dependent variable and one or more independent variables. In simple terms, OLS is like drawing the best straight line through a scatterplot of data points. Imagine you plotted students’ net savings on a graph, and each point represents a student’s financial outcome. OLS finds the line that best follows the trend of these points by minimizing the overall distance (error) between what the line predicts and what the actual data shows.\nOLS is widely used because it is:\n\n\nSimple: Easy to understand and compute.\n\nClear: Provides straightforward numbers (coefficients) that tell you how much each factor influences the outcome.\n\nVersatile: Applicable in many fields, from economics to social sciences, to help make informed decisions.\n\nIn this chapter, we will break down how OLS works in plain language, explore its underlying assumptions, and discuss its practical applications and limitations. This will give you a solid foundation in regression analysis, paving the way for more advanced techniques later on.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#the-best-line",
    "href": "book/08-binary-logistic.html#the-best-line",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n9.1 The “Best Line”",
    "text": "9.1 The “Best Line”\nWhen using Ordinary Least Squares (OLS) to fit a regression line, our goal is to find the line that best represents the relationship between our dependent variable \\(Y\\) and independent variable \\(X\\). But what does “best” mean?\nImagine you have a scatter plot of data points. Now, consider drawing two different lines through this plot. Each one of these lines represent a set of predictions. They also represent a way to represent the relationship between the dependent variable \\(Y\\) and independent variable \\(X\\)\n\n\nLine A (Blue): A line that follows the general trend of the data very well.\n\nLine B (Red): A line that doesn’t capture the trend as accurately.\n\n\n\nR Code\nPython Code\n\n\n\n# Sample data\nset.seed(42)\nX &lt;- c(1000, 1200, 1500, 1800, 2000)\nY &lt;- c(200, 230, 250, 290, 310)\n\n# Create a data frame\ndf &lt;- data.frame(Size = X, Price = Y)\n\n# Fit the correct OLS model\ncorrect_model &lt;- lm(Price ~ Size, data = df)\n\n# Create predictions for the two lines\ndf$Predicted_Correct &lt;- predict(correct_model, newdata = df)\ndf$Predicted_Wrong &lt;- 110 + 0.08 * df$Size  # Adjusted manually\n\n# Reshape data for ggplot (to add legend)\ndf_long &lt;- data.frame(\n  Size = rep(df$Size, 2),\n  Price = c(df$Predicted_Correct, df$Predicted_Wrong),\n  Line = rep(c(\"Line A (Best Fit)\", \"Line B (Worse Fit)\"), each = nrow(df))\n)\n\n# Store the plot with a legend\nlibrary(ggplot2)\nplot &lt;- ggplot() +\n  geom_point(data = df, aes(x = Size, y = Price), size = 3, color = \"black\") +\n  geom_line(data = df_long, aes(x = Size, y = Price, color = Line), linewidth = 1.2) +\n  scale_color_manual(values = c(\"Line A (Best Fit)\" = \"blue\", \"Line B (Worse Fit)\" = \"red\")) +\n  labs(title = \"Comparing Regression Line Fits\",\n       x = \"House Size (sq ft)\",\n       y = \"House Price (in $1000s)\",\n       color = \"Regression Line\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        legend.position = \"bottom\")\n\nplot\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Sample data\nnp.random.seed(42)\nX = np.array([1000, 1200, 1500, 1800, 2000])\nY = np.array([200, 230, 250, 290, 310])\ndf = pd.DataFrame({'Size': X, 'Price': Y})\n\n# Fit the correct OLS model\nX_sm = sm.add_constant(df['Size'])\nmodel = sm.OLS(df['Price'], X_sm).fit()\ndf['Predicted_Correct'] = model.predict(X_sm)\n\n# Manually add the incorrect line\ndf['Predicted_Wrong'] = 110 + 0.08 * df['Size']\n\n# Reshape for plotting\ndf_long = pd.concat([\n    df[['Size', 'Predicted_Correct']].rename(columns={'Predicted_Correct': 'Price'}).assign(Line='Line A (Best Fit)'),\n    df[['Size', 'Predicted_Wrong']].rename(columns={'Predicted_Wrong': 'Price'}).assign(Line='Line B (Worse Fit)')\n])\n\n# Plot\nfig, ax = plt.subplots(figsize=(6, 4))\nax.scatter(df['Size'], df['Price'], color='black', label='Actual Data')\nfor label, group in df_long.groupby('Line'):\n    ax.plot(group['Size'], group['Price'], label=label)\nax.set_title(\"Comparing Regression Line Fits\", fontsize=14, fontweight='bold')\nax.set_xlabel(\"House Size (sq ft)\")\nax.set_ylabel(\"House Price (in $1000s)\")\nax.legend(title=\"Regression Line\", loc='lower right')\nplt.grid(True)\nplt.show()\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n9.1.1 Understanding Residuals\nFor each data point, the residual is the vertical distance between the actual \\(Y\\) value and the predicted \\(Y\\) value (denoted \\(\\hat{Y}\\)) on the line. In simple terms, it tells us how far off our prediction is for each point given the same \\(X\\) value. If a line fits well, these residuals will be small, meaning our predictions of the \\(Y\\) variable are close to the actual value.\nOLS quantifies how well a line fits the data by calculating the Sum of Squared Errors (SSE). The SSE is obtained by:\n\nComputing the residual for each data point.\nSquaring each residual (this ensures that errors do not cancel each other out).\nSumming all these squared values.\n\n\\[\nSSE=\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\n\\]\nA lower SSE indicates a line that is closer to the actual data points. OLS chooses the best line by finding the one with the smallest SSE.\n\n9.1.2 Quantifying the Fit with SSE\nWe can compare the two lines by computing their SSE. The code below calculates and prints the SSE for each line:\n\n\nR Code\nPython Code\n\n\n\n# Calculate the Sum of Squared Errors for the correct model (Blue)\nsse_correct &lt;- sum((df$Price - df$Predicted_Correct)^2)\n\n# Calculate the Sum of Squared Errors for the manually adjusted model (Red)\nsse_wrong &lt;- sum((df$Price - df$Predicted_Wrong)^2)\n\n# Print the SSEs for each line\ncat(\"SSE for Best-Fit Line (Blue line):\", sse_correct, \"\\n\")\ncat(\"SSE for Worse-Fit Line (Red line):\", sse_wrong, \"\\n\")\n\n\n# Calculate the Sum of Squared Errors for the correct model (Blue)\nsse_correct = np.sum((df['Price'] - df['Predicted_Correct']) ** 2)\n\n# Calculate the Sum of Squared Errors for the manually adjusted model (Red)\nsse_wrong = np.sum((df['Price'] - df['Predicted_Wrong']) ** 2)\n\n# Print the SSEs for each line\nprint(f\"SSE for Best-Fit Line (Blue line): {sse_correct}\")\nprint(f\"SSE for Worse-Fit Line (Red line): {sse_wrong}\")\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\nWhen you run this code, you’ll observe that the blue line (Line A) has a much lower SSE compared to the red line (Line B). This tells us that the blue line is a better fit for the data because its predictions are, on average, closer to the actual values.\nIn summary, OLS selects the “best line” by minimizing the sum of squared errors, ensuring that the total error between predicted and actual values is as small as possible.\n\n9.1.3 Why Squared Errors?\nWhen measuring how far off our predictions are, errors can be positive (if our prediction is too low) or negative (if it’s too high). If we simply added these errors together, they could cancel each other out, hiding the true size of the mistakes. By squaring each error, we convert all numbers to positive values so that every mistake counts.\nIn addition, squaring makes big errors count a lot more than small ones. This means that a large mistake will have a much bigger impact on the overall error, encouraging the model to reduce those large errors and improve its overall accuracy.\n\n9.1.4 The Mathematical Formulation of the OLS Model\nNow that we understand how OLS finds the best-fitting line by minimizing the differences between the actual and predicted values, let’s look at the math behind it.\nIn a simple linear regression with one predictor, we express the relationship between the outcome \\(Y\\) and the predictor \\(X\\) using the following equation. Note that OLS fits a straight line to the data, which is why the equation takes the familiar form of a straight line:\n\\[\nY=\\beta_0+\\beta_1X+\\epsilon\n\\]\nHere’s what each part of the equation means:\n\n\n\\(Y\\) is the dependent variable or the outcome we want to predict.\n\n\\(X\\) is the independent variable or the predictor that we believe influences \\(Y\\).\n\n\\(\\beta_0\\) is the intercept. It represents the predicted value of \\(Y\\) when \\(X=0\\).\n\n\\(\\beta_1\\) is the slope. It tells us how much \\(Y\\) is expected to change for each one-unit increase in \\(X\\).\n\n\\(\\epsilon\\) is the error term. It captures the random variation in \\(Y\\) that cannot be explained by \\(X\\).\n\nThis equation provides a clear mathematical framework for understanding how changes in \\(X\\) are expected to affect \\(Y\\), while also accounting for random variation. In the upcoming section, we will explore our toy dataset to showcase this equation and OLS in action.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#case-study-understanding-financial-behaviors",
    "href": "book/08-binary-logistic.html#case-study-understanding-financial-behaviors",
    "title": "Binary Logistic Regression",
    "section": "\n8.4 Case Study: Understanding Financial Behaviors",
    "text": "8.4 Case Study: Understanding Financial Behaviors\nStatistical models are most useful when they connect to a real-world problem. Here, we’ll use binary logistic regression to study financial behavior — specifically, the likelihood that a student defaults on a loan.\n\n\n8.4.1 The Dataset\nWe’ll use the Logistic_Regression dataset (packaged in cookbook).\n\n\nPredictors:\n\n\ncredit_score (numeric, higher = lower risk)\n\nincome (numeric, annual income in dollars)\n\nage (numeric)\n\nloan_amount (numeric, loan size)\n\nstudent (categorical: yes/no)\n\n\n\nOutcome:\n\n\ndefaulted (binary: 1 = default, 0 = no default)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is a binary classification problem: given a mix of continuous and categorical predictors, we want to predict whether a student defaults.\n\n\n\n8.4.2 The Problem We’re Trying to Solve\nBanks and lenders routinely face a critical decision:\n\nGiven a customer’s financial history, should we approve their loan application?\n\nWhile this sounds like a simple “Yes” or “No” question, the reality is a complex game of probability and risk management.\n1. The Balancing Act of Lending\nLending is a trade-off between two competing goals:\n\nMaximizing Profit: The bank wants to lend to as many people as possible to earn interest.\nMinimizing Loss: The bank wants to avoid lending to people who will not pay the money back (default).\n\nIf a bank is too strict, they reject good customers and lose profit. If they are too lenient, they lose their principal investment. The goal of Binary Logistic Regression here is not just to guess who defaults, but to quantify the risk (\\(\\hat{\\pi}_i\\)) so the bank can find the profitable “sweet spot.”\n2. The Asymmetry of Errors\nIn this context, being wrong in one direction is much more expensive than being wrong in the other.\n\n\nFalse Positive (Type I Error): The model predicts a customer will default, but they would have paid.\n\n\nCost: The bank loses the potential interest (opportunity cost).\n\n\n\nFalse Negative (Type II Error): The model predicts a customer is safe, but they default.\n\n\nCost: The bank loses the entire loan amount.\n\n\n\nBecause a False Negative is usually much costlier than a False Positive, lenders need a model that outputs a precise probability (e.g., “There is an 18% chance this person defaults”). This allows them to set a custom decision threshold—perhaps they only approve loans if the risk of default is below 10%, rather than the standard 50%.\n\n8.4.3 Study Design\n\nPredictor types: both continuous (income, age, credit_score, loan_amount) and categorical (student status).\nOutcome: binary (defaulted).\nSample size: dataset includes a few hundred observations (enough to illustrate the method).\n\nAssumptions:\n\nObservations are independent.\nPredictors have a linear relationship with the log-odds of default.\nNo perfect collinearity among predictors.\n\n\n\n\n8.4.4 Applying Study Design to Our Case Study\nBefore we fit the model, we must prepare the data. In predictive modeling, this involves three critical steps: splitting, checking for imbalance, and handling outliers.\nWhy We Split the Data (Training vs. Testing)\nA common mistake is to fit a model on all available data and then evaluate its performance on that same data. This leads to overfitting: the model memorizes the specific noise in the dataset rather than learning the underlying patterns.\nTo ensure our model generalizes to new, unseen borrowers, we split the data into two sets:\n\n\nTraining Set (e.g., 70%): Used to estimate the coefficients (\\(\\hat{\\beta}\\)). The model “sees” this data.\n\nTesting Set (e.g., 30%): Used only to evaluate performance. The model never sees this data during fitting.\nThe Challenge of Class Imbalance\nIn loan default data, “success” (paying back the loan) is much more common than “failure” (defaulting). This is known as class imbalance.\nIf 95% of borrowers pay back their loans, a naive model could simply predict “No Default” for everyone and achieve 95% accuracy. However, this model is useless to the bank because it identifies zero risks.\nTo handle this during the split, we use stratified sampling. This ensures that the proportion of defaulters is exactly the same in both the training and testing sets, preventing the “rare” event from disappearing entirely in the test set.\nInfluential Points and Outliers\nLogistic regression is sensitive to outliers (extreme values). A single borrower with an income of $10,000,000 or an age of 110 can pull the logistic curve disproportionately, biasing the results.\nBefore modeling, we should check for:\n\n\nData Entry Errors: (e.g., a credit score of 9000, which is impossible).\n\nExtreme Leverage Points: Legitimate but extreme values that might need to be capped or investigated.\n\n\n8.4.5 Data Preparation in Action\nBelow, we split the data while preserving the class balance (stratification) and encode the categorical variables.\n\n\nR Code\nPython Code\n\n\n\nlibrary(rsample) # Part of tidymodels for splitting\nlibrary(dplyr)\n\n# 1. Check Class Imbalance\n# Notice if defaults are rare (e.g., &lt; 10%)\nprop.table(table(BLR$defaulted))\n\n# 2. Stratified Split\n# We use 'strata = defaulted' to ensure both sets have the same % of defaults\nset.seed(123)\nsplit &lt;- initial_split(BLR, prop = 0.70, strata = defaulted)\n\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n# 3. Verify the split\nnrow(train_data)\nnrow(test_data)\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# 1. Check Class Imbalance\nprint(df['defaulted'].value_counts(normalize=True))\n\n# 2. Stratified Split\n# We use 'stratify=y' to ensure both sets have the same % of defaults\nX = df.drop(columns=['defaulted'])\ny = df['defaulted']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.3, \n    random_state=123, \n    stratify=y  # Crucial for imbalanced data\n)\n\n# 3. Verify the split\nprint(f\"Training set shape: {X_train.shape}\")\nprint(f\"Testing set shape: {X_test.shape}\")",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#data-collection-and-wrangling",
    "href": "book/08-binary-logistic.html#data-collection-and-wrangling",
    "title": "Binary Logistic Regression",
    "section": "\n8.8 Data Collection and Wrangling",
    "text": "8.8 Data Collection and Wrangling\nBefore building a logistic regression model, we need to ensure our data is reliable, clean, and structured for analysis. Poor data preparation can undermine even the best statistical methods.\n\n8.8.1 Data Collection\nIn our case, we’re working with a loan default dataset. Each row represents a borrower, and the key outcome is whether they defaulted on their loan (defaulted = 1) or not (defaulted = 0). Predictors include:\n\n\ncredit_score (continuous)\n\nincome (continuous)\n\neducation_years (continuous)\n\nmarried (binary categorical)\n\nowns_home (binary categorical)\n\nage (continuous)\n\nThe dataset also contains successes and trials (grouped-binomial form), but in this chapter we’ll treat outcomes as individual-level Bernoulli trials.\n\n8.8.2 Data Wrangling\nTo prepare the dataset:\n\nHandling Missing Data: If borrowers have missing credit scores or income values, we must decide whether to impute them (mean, median, regression-based) or drop those cases. Missingness should be checked systematically.\nEncoding Categorical Variables: Variables like married and owns_home need to be encoded as 0/1 indicators. In R and Python, logistic regression functions handle factors/dummies automatically, but it’s important to confirm coding.\n\nDealing with Class Imbalance: Loan defaults are usually much rarer than non-defaults. A dataset with 90% non-defaults and 10% defaults might lead a model to always predict “no default” and still achieve 90% accuracy. Strategies include:\n\nAdjusting the decision threshold (not always 0.5),\nUsing resampling methods (oversampling defaults, undersampling non-defaults), or\nApplying class weights in the model.\n\n\nSplitting Data for Training/Testing: To fairly assess predictive performance, we split the dataset into training (used to fit the model) and testing (used to evaluate it). A common split is 70/30. This prevents overfitting and ensures the model generalizes to new borrowers.\n\n\nTODOs for this section:\n\nCheck for missing values and document how they were handled.\nEnsure categorical variables (married, owns_home) are encoded properly.\nAssess class balance in defaulted and consider strategies if imbalance is severe.\nSplit dataset into training and test sets, keeping class balance in mind.\n\n\n\n\nR Code\nPython Code\n\n\n\nlibrary(dplyr)\nlibrary(caret)\n\n# Check missing data\ncolSums(is.na(BLR))\n\n# Encode categorical variables (ensure they are factors)\nBLR &lt;- BLR %&gt;%\n  mutate(\n    married = as.factor(married),\n    owns_home = as.factor(owns_home)\n  )\n\n# Check class balance\ntable(BLR$defaulted)\nprop.table(table(BLR$defaulted))\n\n# Split into training/testing sets\nset.seed(123)\ntrainIndex &lt;- createDataPartition(BLR$defaulted, p = .7, list = FALSE)\ntrain &lt;- BLR[ trainIndex, ]\ntest  &lt;- BLR[-trainIndex, ]\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Check missing data\nprint(df.isnull().sum())\n\n# Encode categorical variables\ndf['married'] = df['married'].astype('category')\ndf['owns_home'] = df['owns_home'].astype('category')\n\n# Check class balance\nprint(df['defaulted'].value_counts(normalize=True))\n\n# Split into train/test sets (stratify to preserve class balance)\ntrain, test = train_test_split(df, test_size=0.3, random_state=123,\n                               stratify=df['defaulted'])",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#exploratory-data-analysis-eda",
    "href": "book/08-binary-logistic.html#exploratory-data-analysis-eda",
    "title": "Binary Logistic Regression",
    "section": "\n8.9 Exploratory Data Analysis (EDA)",
    "text": "8.9 Exploratory Data Analysis (EDA)\nBefore fitting a logistic regression, we need to understand the structure of our data. Exploratory analysis helps us spot patterns, distributions, and potential issues.\n\n8.9.1 Classifying Variables\nTo select the appropriate modeling technique, we must first classify our variables. In this analysis, defaulted is our binary outcome, and the remaining variables serve as regressors (explanatory variables).\n\n\n\n\n\n\n\n\nVariable\nType\nRole\nDescription\n\n\n\ndefaulted\nCategorical (Binary)\nOutcome (\\(Y\\))\n\n\\(1=\\) Default, \\(0=\\) No Default\n\n\ncredit_score\nContinuous (Integer)\nRegressor (\\(X\\))\nCredit rating (300-850)\n\n\nincome\nContinuous\nRegressor (\\(X\\))\nAnnual income in CAD\n\n\nage\nContinuous (Integer)\nRegressor (\\(X\\))\nBorrower age in years\n\n\nloan_amount\nContinuous\nRegressor (\\(X\\))\nSize of loan in CAD\n\n\neducation_years\nCount / Discrete\nRegressor (\\(X\\))\nYears of schooling\n\n\nmarried\nCategorical (Nominal)\nRegressor (\\(X\\))\nMarital status\n\n\nowns_home\nCategorical (Nominal)\nRegressor (\\(X\\))\nHome ownership status\n\n\n\n8.9.2 Visualizing Distributions\n\n\nContinuous regressors (e.g., credit_score, income) can be visualized with histograms or density plots, split by default status.\n\nCategorical regressors (e.g., married, owns_home) can be compared with barplots of default rates.\n\n8.9.3 Exploring Relationships with Binary Outcome\nBoxplots or violin plots of credit_score by default status, or barplots of default rates by home ownership, help reveal potential relationships. We also check correlations among continuous regressors to watch for multicollinearity.\n\n\nR Code\nPython Code\n\n\n\nlibrary(ggplot2)\nlibrary(ggcorrplot)\n\n# Boxplot of credit score by default\nggplot(BLR, aes(x = as.factor(defaulted), y = credit_score)) +\n  geom_boxplot() +\n  labs(x = \"Defaulted\", y = \"Credit Score\", \n       title = \"Credit Score Distribution by Default Status\")\n\n# Barplot of default by home ownership\nggplot(BLR, aes(x = as.factor(owns_home), fill = as.factor(defaulted))) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Owns Home\", y = \"Proportion\", \n       fill = \"Defaulted\",\n       title = \"Default Rates by Home Ownership\")\n\n# Correlation heatmap (Numeric regressors only)\ncorr &lt;- cor(BLR[,c(\"credit_score\",\"income\",\"education_years\",\"age\")])\nggcorrplot(corr, lab = TRUE)\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Boxplot credit score by default\nsns.boxplot(data=df, x=\"defaulted\", y=\"credit_score\")\nplt.title(\"Credit Score Distribution by Default Status\")\nplt.show()\n\n# Barplot default by home ownership\nsns.barplot(data=df, x=\"owns_home\", y=\"defaulted\", errorbar=None)\nplt.title(\"Default Rates by Home Ownership\")\nplt.show()\n\n# Correlation heatmap\ncorr = df[['credit_score','income','education_years','age']].corr()\nsns.heatmap(corr, annot=True, cmap=\"coolwarm\")\nplt.title(\"Correlation Heatmap of Continuous Regressors\")\nplt.show()",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#data-modelling",
    "href": "book/08-binary-logistic.html#data-modelling",
    "title": "Binary Logistic Regression",
    "section": "\n8.10 Data Modelling",
    "text": "8.10 Data Modelling\nWith EDA complete, we’re ready to formally specify our logistic regression model.\n\n8.10.1 Choosing a Logistic Regression Model\nThe response variable is binary: defaulted \\(\\in \\{0,1\\}\\). Logistic regression is appropriate because it models the probability of the event occurring (\\(\\pi_i\\)) rather than the raw outcome.\n\n8.10.2 Defining the Probability Distribution\nUnlike linear regression, we do not model \\(Y_i\\) directly. Instead, we assume that the outcome follows a Bernoulli distribution:\n\\[\nY_i \\sim \\text{Bernoulli}(\\pi_i)\n\\] where \\(\\pi_i\\) is the probability that the \\(i\\)-th customer defaults.\n\n8.10.3 Setting Up the Logistic Regression Equation\nWe link the regressors to the probability \\(\\pi_i\\) using the logit function:\n\\[\\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\dots + \\beta_k X_{ki}\n\\]\n\n8.10.4 The Missing Error Term (\\(\\epsilon\\))\nYou may notice a key difference here compared to OLS equations (\\(Y = \\beta X + \\epsilon\\)): there is no error term \\(\\epsilon\\) at the end of the equation.\n\nIn OLS, \\(\\epsilon\\) captures random noise around the mean.\nIn Logistic Regression, the randomness is inherent in the Bernoulli distribution of \\(Y\\) itself. Once we know \\(\\pi_i\\) (the probability), the randomness comes from the “coin flip” of whether the event actually happens or not. We do not add an extra error term to the log-odds equation.\n\n8.10.5 Counting the Parameters\nIf we fit a model using credit_score and income to predict default, we are estimating 3 parameters:\n\n\n\\(\\beta_0\\): The intercept.\n\n\\(\\beta_1\\): The coefficient for credit_score.\n\n\\(\\beta_2\\): The coefficient for income.\n\nGenerally, if we have \\(k\\) regressors, we estimate \\(k+1\\) parameters.\n\n\n\nR Code\nPython Code\n\n\n\n# Fit the full model\n# We use family = binomial to indicate Y ~ Bernoulli(pi)\nlogit_model &lt;- glm(defaulted ~ credit_score + income + married + owns_home,\n                   data = BLR, family = binomial)\nsummary(logit_model)\n\n\nimport statsmodels.api as sm\n\n# Define Regressors (X) and Outcome (y)\n# Ensure categorical variables are encoded (if not done in wrangling step)\nX = df[['credit_score','income','married_Yes','owns_home_Yes']] \nX = sm.add_constant(X)\ny = df['defaulted']\n\nlogit_model = sm.Logit(y, X).fit()\nprint(logit_model.summary())",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#estimation",
    "href": "book/08-binary-logistic.html#estimation",
    "title": "Binary Logistic Regression",
    "section": "\n8.11 Estimation",
    "text": "8.11 Estimation\nUnder a model with \\(k\\) regressors, the coefficients \\(\\beta_0,\\beta_1,\\ldots,\\beta_k\\) are unknown and must be estimated from the data.\n\n8.11.1 Maximum Likelihood Estimation (MLE)\nBecause our outcome \\(Y_i\\) follows a Bernoulli distribution with probability \\(\\pi_i\\), we cannot use Least Squares. Instead, we use Maximum Likelihood Estimation.\nWe seek the values of \\(\\beta\\) that maximize the likelihood of observing the data we actually collected. The likelihood function is:\n\\[\nL(\\boldsymbol\\beta)=\\prod_{i=1}^n \\pi_i^{y_i}(1-\\pi_i)^{1-y_i}\n\\] Where \\(\\pi_i\\) is connected to our regressors via the inverse-logit:\n\\[\\pi_i=\\frac{e^{\\beta_0 + \\dots + \\beta_k X_{ki}}}{1+e^{\\beta_0 + \\dots + \\beta_k X_{ki}}}\n\\] The computer solves this using an iterative algorithm (Newton-Raphson) to find the \\(\\hat{\\beta}\\) values that make the observed defaults most probable.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#goodness-of-fit",
    "href": "book/08-binary-logistic.html#goodness-of-fit",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.22 Goodness of Fit",
    "text": "8.22 Goodness of Fit\nAfter estimating the regression coefficients, the next step is to assess how well the model fits the data and whether it satisfies the assumptions of Ordinary Least Squares (OLS) regression. This evaluation ensures that the model is not only statistically valid but also generalizes well to unseen data. A well-fitting model should explain a substantial proportion of variation in the response variable while adhering to key statistical assumptions. If these assumptions are violated, model estimates may be biased, leading to misleading conclusions.\n\n8.22.1 Checking Model Assumptions\nOLS regression is built on several fundamental assumptions:\n\nlinearity\nindependence of errors\nhomoscedasticity\nnormality of residuals\n\nIf these assumptions hold, OLS provides unbiased, efficient, and consistent estimates. We assess each assumption through diagnostic plots and statistical tests.\nLinearity\nA core assumption of OLS is that the relationship between each predictor and the response variable is linear. If this assumption is violated, the model may systematically under- or overestimate Net_Money, leading to biased predictions. The Residuals vs. Fitted values plot is a common diagnostic tool for checking linearity. In a well-specified linear model, residuals should be randomly scattered around zero, without any discernible patterns. If the residuals exhibit a U-shaped or curved pattern, this suggests a non-linear relationship, indicating that transformations such as logarithmic, square root, or polynomial terms may be necessary.\nTo visualize linearity, we plot the residuals against the fitted values:\n\n\nR Code\nPython Code\n\n\n\n\n# Residuals vs Fitted plot (R)\nplot(ols_model$fitted.values, residuals(ols_model), \n     main = \"Residuals vs Fitted\", xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Extract fitted values and residuals\nfitted_vals = ols_model.fittedvalues\nresiduals = ols_model.resid\n\n# Plot Residuals vs Fitted\nplt.figure(figsize=(6, 4))\nplt.scatter(fitted_vals, residuals, alpha=0.7)\nplt.axhline(y=0, color='red', linestyle='--')\nplt.title(\"Residuals vs Fitted\")\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nIf the residual plot displays a clear trend, polynomial regression or feature engineering may be required to better capture the underlying data structure. However, in this case, the residuals appear to be randomly scattered around the horizontal axis, with no obvious patterns such as curves or systematic structure. This suggests that the linearity assumption of the OLS model holds reasonably well in this case. Therefore, no immediate transformation or addition of nonlinear terms is necessary to capture the relationship between predictors and the response variable.\nIndependence of Errors\nThe residuals, or errors, in an OLS model should be independent of one another. This assumption is particularly relevant in time-series or sequential data, where errors from one observation might influence subsequent observations, leading to autocorrelation. If the errors are correlated, the estimated standard errors will be biased, making hypothesis testing unreliable.\nThe Durbin-Watson test is commonly used to detect autocorrelation. This test produces a statistic that ranges between 0 and 4, where values close to 2 indicate no significant autocorrelation, while values near 0 or 4 suggest positive or negative correlation in the residuals.\n\n\nR Code\nPython Code\n\n\n\n\ndwtest(ols_model)\n\n\n\n\nfrom statsmodels.stats.stattools import durbin_watson\n\n# Durbin-Watson test for autocorrelation of residuals\ndw_stat = durbin_watson(ols_model.resid)\n\nprint(f\"Durbin-Watson statistic: {dw_stat:.3f}\")\n\n\n\n\nBased on the Durbin-Watson test, the test statistic is 1.9437 with a p-value of 0.2133. Since the statistic is close to 2 and the p-value is not statistically significant, we do not find evidence of autocorrelation in the residuals. This suggests that the assumption of independence of errors holds for this model.\nIf the test suggests autocorrelation, a possible solution is to use time-series regression models such as Autoregressive Integrated Moving Average (ARIMA) or introduce lagged predictors to account for dependencies in the data.\nHomoscedasticity (Constant Variance of Errors)\nOLS regression assumes that the variance of residuals remains constant across all fitted values. If this assumption is violated, the model exhibits heteroscedasticity, where the spread of residuals increases or decreases systematically. This can result in inefficient coefficient estimates, making some predictors appear statistically significant when they are not.\nTo check for heteroscedasticity, we plot residuals against the fitted values and conduct a Breusch-Pagan test, which formally tests whether residual variance is constant.\n\n\nR Code\nPython Code\n\n\n\n\nbptest(ols_model)  # Uses all regressors, like Python\n\n\n\n\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n\n# Extract residuals and design matrix from the fitted model\nresiduals = ols_model.resid\nexog = ols_model.model.exog  # independent variables (with intercept)\n\n# Perform Breusch-Pagan test\nbp_test = het_breuschpagan(residuals, exog)\n\n# Unpack results\nbp_stat, bp_pvalue, _, _ = bp_test\n\nprint(f\"Breusch-Pagan test statistic: {bp_stat:.3f}\")\nprint(f\"P-value: {bp_pvalue:.4f}\")\n\n\n\n\nSince the p-value (0.1563) is greater than the conventional threshold of 0.05, we fail to reject the null hypothesis of constant variance. This indicates no significant evidence of heteroscedasticity, and thus the assumption of homoscedasticity appears to hold for this model.\nIf heteroscedasticity is detected, solutions include applying weighted least squares (WLS) regression, transforming the dependent variable (e.g., using a log transformation), or computing robust standard errors to correct for variance instability.\nNormality of Residuals\nFor valid hypothesis testing and confidence interval estimation, OLS assumes that residuals follow a normal distribution. If residuals deviate significantly from normality, statistical inference may be unreliable, particularly for small sample sizes.\nA Q-Q plot (Quantile-Quantile plot) is used to assess normality. If residuals are normally distributed, the points should lie along the reference line.\n\n\nR Code\nPython Code\n\n\n\n\nqqnorm(residuals(ols_model))\nqqline(residuals(ols_model), col = \"red\")\n\n\n\n\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# Extract residuals from the model\nresiduals = ols_model.resid\n\n# Q-Q plot of residuals\nplt.figure(figsize=(6, 4))\n_ = stats.probplot(residuals, dist=\"norm\", plot=plt)  # Suppress output by assigning to _\nplt.title(\"Q-Q Plot of Residuals\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe Q-Q plot shows that the residuals closely follow the 45-degree reference line, indicating that the residuals are approximately normally distributed. While there are some mild deviations at the tails, these are minimal and do not suggest serious violations of the normality assumption.\nIf the plot reveals heavy tails or skewness, potential solutions include applying log or Box-Cox transformations to normalize the distribution. In cases where normality is severely violated, using a non-parametric model or bootstrapping confidence intervals may be appropriate.\n\n8.22.2 Evaluating Model Fit\nA good model should explain a large proportion of variance in the response variable.\nR-Squared\nBeyond checking assumptions, it is essential to assess how well the model explains variability in the response variable. One of the most commonly used metrics is R-Squared (\\(R^2\\)), which measures the proportion of variance in Net_Money that is explained by the predictors. An \\(R^2\\) value close to 1 indicates a strong model fit, whereas a low value suggests that important predictors may be missing or that the model is poorly specified.\nWe can retrieve the R-squared and Adjusted R-squared values from the model summary:\n\n\nR Code\nPython Code\n\n\n\n\nsummary(ols_model)$r.squared  # R-squared value\nsummary(ols_model)$adj.r.squared  # Adjusted R-squared\n\n\n\n\n# R-squared\nr_squared = ols_model.rsquared\n\n# Adjusted R-squared\nadj_r_squared = ols_model.rsquared_adj\n\n# Print values\nprint(f\"R-squared: {r_squared:.4f}\")\nprint(f\"Adjusted R-squared: {adj_r_squared:.4f}\")\n\n\n\n\nIn this model, the R-squared is 0.894 and the adjusted R-squared is 0.892, indicating that around 89% of the variance in Net_Money is explained by the model.\nHowever, while \\(R^2\\) provides insight into model fit, it has limitations. Adding more predictors will always increase \\(R^2\\), even if those predictors have little explanatory power. That’s why Adjusted R-squared is often preferred, as it adjusts for the number of predictors and only increases when a new variable meaningfully improves the model.\nFinally, a high \\(R^2\\) should not be interpreted as evidence of causation, nor does it guarantee the model is free from issues like omitted variable bias or multicollinearity. Always complement goodness-of-fit metrics with residual diagnostics and statistical inference to ensure model reliability.\nIdentifying Outliers and Influential Points\nOutliers and influential observations can distort regression estimates, making it crucial to detect and address them appropriately. One way to identify extreme residuals is through residual plots, where large deviations from zero may indicate problematic data points.\n\n\nR Code\nPython Code\n\n\n\n\nplot(residuals(ols_model), main = \"Residual Plot\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Extract residuals\nresiduals = ols_model.resid\n\n# Residual plot (residuals vs observation index)\nplt.figure(figsize=(6, 4))\nplt.plot(residuals, marker='o', linestyle='none', alpha=0.7)\nplt.axhline(y=0, color='red', linestyle='--')\nplt.title(\"Residual Plot\")\nplt.xlabel(\"Observation Index\")\nplt.ylabel(\"Residuals\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nIn the residual plot above, the residuals appear to be evenly and randomly scattered around zero, with no clear pattern or extreme values. This suggests that there are no obvious outliers or highly influential observations in the dataset. The model residuals behave as expected, reinforcing the assumption that the data points do not exert undue influence on the regression fit.\nAnother important diagnostic tool is Cook’s Distance, which measures the influence of each observation on the regression results. Data points with Cook’s Distance values greater than 0.5 may significantly impact model estimates.\n\n\nR Code\nPython Code\n\n\n\n\ncook_values &lt;- cooks.distance(ols_model)\nplot(cook_values, type = \"h\", main = \"Cook's Distance\")\n\n\n\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Get Cook's distance values\ninfluence = ols_model.get_influence()\ncooks_d = influence.cooks_distance[0]  # Values\n\n# Plot Cook's distance\nfig, ax = plt.subplots(figsize=(6, 4))\nmarkerline, stemlines, baseline = ax.stem(cooks_d, markerfmt=\",\")\nax.set_title(\"Cook's Distance\")\nax.set_xlabel(\"Observation Index\")\nax.set_ylabel(\"Cook's Distance\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nIn the plot above, all observations have Cook’s Distance values well below the 0.5 threshold. This indicates that no individual data point has an undue influence on the model’s estimates.\nIf influential points are identified, the next steps involve investigating data quality, testing robust regression techniques, or applying Winsorization, which involves replacing extreme values with more moderate ones to reduce their impact.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#results",
    "href": "book/08-binary-logistic.html#results",
    "title": "Binary Logistic Regression",
    "section": "\n8.17 Results",
    "text": "8.17 Results\nOnce we’ve fit our logistic regression model, the next step is to present the results. Results can be organized into two complementary perspectives:\n\n\nPredictive Analysis — How well does our model classify customers into “default” vs. “non-default”?\n\nInferential Analysis — What do the model’s coefficients tell us about the relationship between predictors (e.g., credit score, income) and the probability of default?\n\n\n8.17.1 Predictive Analysis\nFrom a predictive standpoint, the model provides predicted probabilities for each customer. By applying a threshold (commonly 0.5), we can classify customers as predicted to default or not.\nWe then evaluate performance using:\n\n\nAccuracy: The proportion of correct predictions.\n\nSensitivity (Recall): How well the model detects actual defaults.\n\nSpecificity: How well the model detects non-defaults.\n\nROC Curve / AUC: Performance across all possible thresholds.\n\nFor example, in our loan dataset, the model correctly classifies a high proportion of non-defaulters, but sensitivity may be lower if defaults are relatively rare.\n\n8.17.2 Inferential Analysis\nFrom an inferential perspective, the coefficients give us odds ratios that describe how predictors affect default risk.\nFor instance, a coefficient of -0.01 for credit score corresponds to an odds ratio of 0.99 — meaning that for every 1-point increase in credit score, the odds of defaulting decrease by about 1%. Scaled up, a 50-point increase lowers the odds by roughly 40%.\nBy converting odds ratios into changes in probability at meaningful ranges of credit score, we provide a more intuitive interpretation for readers.\n\nTODOs for this section:\n\nAdd classification table (confusion matrix) showing accuracy, sensitivity, specificity.\nInclude ROC curve for predictive storytelling.\nProvide plain-language interpretation of at least one coefficient (credit score).\n\n\n\n\nR Code\nPython Code\n\n\n\n# Fit logistic regression model (this stays the same - base R)\nlogit_model &lt;- glm(defaulted ~ credit_score + income,\n                   data = BLR, family = binomial)\n\n# Summary for inference (stays the same)\nsummary(logit_model)\n\n# Odds ratios with confidence intervals (stays the same)\nexp(cbind(OR = coef(logit_model), confint(logit_model)))\n\n# ===================================\n# Predictive evaluation - TIDYMODELS\n# ===================================\nlibrary(yardstick)\nlibrary(dplyr)\n\n# Get predictions\npred_prob &lt;- predict(logit_model, type = \"response\")\npred_class &lt;- ifelse(pred_prob &gt; 0.5, 1, 0)\n\n# Create results dataframe for yardstick\nresults &lt;- tibble(\n  truth = factor(BLR$defaulted, levels = c(\"0\", \"1\")),\n  predicted = factor(pred_class, levels = c(\"0\", \"1\")),\n  pred_prob = pred_prob\n)\n\n# Confusion matrix\ncat(\"Confusion Matrix:\\n\")\nconf_mat(results, truth = truth, estimate = predicted)\n\n# Calculate classification metrics\ncat(\"\\nClassification Metrics:\\n\")\nmetrics &lt;- metric_set(accuracy, sensitivity, specificity, precision, f_meas)\nmetrics(results, truth = truth, estimate = predicted) %&gt;%\n  knitr::kable(digits = 3)\n\n# ===================================\n# ROC curve - TWO OPTIONS\n# ===================================\n\n# Option 1: Using pROC (traditional, great plots)\nlibrary(pROC)\nroc_curve &lt;- roc(BLR$defaulted, pred_prob)\nplot(roc_curve, main=\"ROC Curve for Loan Default Model\")\ncat(\"AUC:\", auc(roc_curve), \"\\n\")\n\n# Option 2: Using yardstick (tidymodels way)\nlibrary(yardstick)\nroc_data &lt;- roc_curve(results, truth = truth, pred_prob, event_level = \"second\")\nautoplot(roc_data) +\n  labs(title = \"ROC Curve for Loan Default Model\") +\n  theme_minimal()\n\n# AUC using yardstick\nroc_auc(results, truth = truth, pred_prob, event_level = \"second\")\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# Logistic regression\ndf = r.data['BLR'].copy()\ndf['defaulted'] = df['defaulted'].astype(int)\n\nX = sm.add_constant(df[['credit_score','income']])\ny = df['defaulted']\nlogit_model = sm.Logit(y, X).fit()\n\n# Inference: odds ratios\nparams = logit_model.params\nconf = logit_model.conf_int()\nodds_ratios = pd.DataFrame({\n    \"OR\": params.apply(lambda x: np.exp(x)),\n    \"2.5%\": conf[0].apply(lambda x: np.exp(x)),\n    \"97.5%\": conf[1].apply(lambda x: np.exp(x))\n})\nprint(odds_ratios)\n\n# Predictions\ndf['pred_prob'] = logit_model.predict(X)\ndf['pred_class'] = (df['pred_prob'] &gt; 0.5).astype(int)\n\n# Confusion matrix & report\nprint(confusion_matrix(y, df['pred_class']))\nprint(classification_report(y, df['pred_class']))\n\n# ROC Curve\nfpr, tpr, thresholds = roc_curve(y, df['pred_prob'])\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(6,4))\nplt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0,1], [0,1], color='grey', linestyle='--')\nplt.title(\"ROC Curve for Loan Default Model\", fontsize=14, fontweight='bold')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n8.17.3 Model Summary: Coefficients and Odds Ratios\nBelow is a comprehensive summary of our final model including log-odds coefficients, odds ratios, and 95% confidence intervals:\n\n\nR Output\nPython Output\n\n\n\n\n#| label: tbl-model-summary\n#| tbl-cap: \"Logistic Regression Model Summary with Odds Ratios\"\n#| echo: false\n\nlibrary(broom)\nlibrary(dplyr)\n\n# Fit the model\nlogit_model &lt;- glm(defaulted ~ credit_score + income,\n                   data = BLR, family = binomial)\n\n# Create comprehensive summary table\ncoef_summary &lt;- tidy(logit_model, conf.int = TRUE, conf.level = 0.95) %&gt;%\n  mutate(\n    odds_ratio = exp(estimate),\n    or_lower = exp(conf.low),\n    or_upper = exp(conf.high),\n    # Add significance stars\n    significance = case_when(\n      p.value &lt; 0.001 ~ \"***\",\n      p.value &lt; 0.01 ~ \"**\",\n      p.value &lt; 0.05 ~ \"*\",\n      p.value &lt; 0.10 ~ \".\",\n      TRUE ~ \"\"\n    )\n  ) %&gt;%\n  dplyr::select(term, estimate, std.error, statistic, p.value, \n                odds_ratio, or_lower, or_upper, significance)\n\n# Display table\nknitr::kable(\n  coef_summary,\n  digits = 4,\n  col.names = c(\"Predictor\", \"Log-Odds (β)\", \"SE\", \"z-value\", \"p-value\",\n                \"Odds Ratio\", \"OR Lower 95%\", \"OR Upper 95%\", \"Sig.\"),\n  caption = \"Logistic Regression Coefficients and Odds Ratios\",\n  align = c(\"l\", rep(\"r\", 8))\n)\n\n\nLogistic Regression Coefficients and Odds Ratios\n\n\n\n\n\n\n\n\n\n\n\n\nPredictor\nLog-Odds (β)\nSE\nz-value\np-value\nOdds Ratio\nOR Lower 95%\nOR Upper 95%\nSig.\n\n\n\n(Intercept)\n11.0420\n0.8348\n13.2275\n0\n62444.6652\n12844.5451\n340260.9478\n***\n\n\ncredit_score\n-0.0149\n0.0012\n-12.0308\n0\n0.9852\n0.9827\n0.9875\n***\n\n\nincome\n0.0000\n0.0000\n-6.5860\n0\n1.0000\n1.0000\n1.0000\n***\n\n\n\n\n\n\n\n\n\nTable 8.1: Logistic Regression Model Summary with Odds Ratios (Python)\n\n\n   Predictor  Log-Odds (β)       SE    z-value      p-value   Odds Ratio  OR Lower 95%  OR Upper 95% Sig.\n       const     11.042036 0.834779  13.227501 6.087766e-40 62444.665247  12159.912448 320671.405691  ***\ncredit_score     -0.014924 0.001240 -12.030829 2.446908e-33     0.985187      0.982795      0.987585  ***\n      income     -0.000029 0.000004  -6.586012 4.517966e-11     0.999971      0.999962      0.999980  ***\n\n\n\nSignificance codes: *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05, . p&lt;0.10\n\n\n\n\n\n\n\nKey Findings:\n\nCredit Score: Each 1-point increase reduces the odds of default by approximately 1% (OR ≈ 0.99). A 50-point increase (a common improvement after financial counseling) reduces odds by roughly 40%.\nIncome: The effect of income is marginal and not statistically significant (p &gt; 0.05), suggesting that once credit score is known, additional income information doesn’t substantially improve default prediction.\n\n\n8.17.4 Classification Performance\nTo evaluate predictive performance, we examine how well the model classifies borrowers using a 0.5 probability threshold:",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#storytelling",
    "href": "book/08-binary-logistic.html#storytelling",
    "title": "Binary Logistic Regression",
    "section": "\n8.20 Storytelling",
    "text": "8.20 Storytelling\n\n\n\n\n\n\nTip\n\n\n\nEffective storytelling connects statistical results to real-world meaning. Translate log-odds into “lenders are X times more likely to…” language.\n\n\nThe final step of our analysis is not just running the model, but communicating the findings clearly. Logistic regression results are often presented to stakeholders like lenders, policy makers, or managers, who may not be trained in statistics. For them, the story matters more than the math.\nLet’s revisit our loan default dataset. Suppose our logistic regression showed that credit score is strongly predictive of default:\n\nA 50-point increase in credit score reduces the odds of defaulting by about 40%.\nIn probability terms, this means that increasing credit score from 400 to 450 lowers the chance of default from roughly 70% to about 50%.\n\nNotice how we moved from the technical language of odds ratios to a plain-language probability story. This makes the model’s results accessible to a wider audience.\nVisuals amplify the story:\n\nA plot of predicted probabilities vs. credit score shows the smooth S-shaped curve replacing the jagged OLS line.\nA confusion matrix or ROC curve shows how well our model actually classifies defaulters.\nCalibration plots show whether predicted probabilities line up with observed rates of default.\n\nFinally, good storytelling means acknowledging limitations. For example, our dataset may suffer from class imbalance (fewer defaults than non-defaults), which could bias results. We should be clear about what the model can and cannot do.\nTODOs for this section:\n\nShow a ROC curve for the loan default model.\nAdd plain-language interpretations of coefficients (esp. odds ratio for credit_score).\nAdd a note about limitations (e.g., income and education may be correlated with credit score).\n\n\n\n\nR Code\nPython Code\n\n\n\n# Logistic regression on loan default data\nlogit_model &lt;- glm(defaulted ~ credit_score + income,\n                   data = BLR, family = binomial)\n\n# Predicted probabilities\nBLR$pred_prob &lt;- predict(logit_model, type = \"response\")\n\n# Storytelling visualization\nggplot(BLR, aes(x = credit_score, y = pred_prob, color = as.factor(defaulted))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"black\") +\n  labs(\n    title = \"Predicted Probability of Default vs. Credit Score\",\n    x = \"Credit Score\",\n    y = \"Predicted Probability\",\n    color = \"Actual Default\"\n  ) +\n  theme_minimal()\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Extract loan default data\ndf = r.data['BLR'].copy()\ndf['defaulted'] = df['defaulted'].astype(int)\n\n# Fit logistic regression\nX = sm.add_constant(df[['credit_score','income']])\ny = df['defaulted']\nlogit_model = sm.Logit(y, X).fit()\ndf['pred_prob'] = logit_model.predict(X)\n\n# Storytelling visualization\nfig, ax = plt.subplots(figsize=(6,4))\nscatter = ax.scatter(df['credit_score'], df['pred_prob'],\n                     c=df['defaulted'], cmap='coolwarm', alpha=0.6)\nax.set_title(\"Predicted Probability of Default vs. Credit Score\", fontsize=14, fontweight='bold')\nax.set_xlabel(\"Credit Score\")\nax.set_ylabel(\"Predicted Probability\")\nlegend1 = ax.legend(*scatter.legend_elements(), title=\"Actual Default\")\nax.add_artist(legend1)\nplt.show()\n\n\n\n\n8.20.1 Bringing the Model to Life\nStatistical models become most powerful when we can tell clear stories with them. Let’s visualize our findings in ways that stakeholders can understand.\nThe Default Risk Landscape\n\n\nR Code\nPython Code\n\n\n\n\nlibrary(ggplot2)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\n# Fit model\nlogit_model &lt;- glm(defaulted ~ credit_score + income,\n                   data = BLR, family = binomial)\n\n# Add predictions\nBLR$pred_prob &lt;- predict(logit_model, type = \"response\")\nBLR$risk_category &lt;- cut(BLR$pred_prob, \n                          breaks = c(0, 0.2, 0.5, 0.8, 1),\n                          labels = c(\"Low Risk\", \"Medium Risk\", \n                                     \"High Risk\", \"Very High Risk\"))\n\n# Create comprehensive visualization\np &lt;- ggplot(BLR, aes(x = credit_score, y = income/1000)) +\n  # Contour showing predicted probability\n  geom_density_2d_filled(aes(fill = after_stat(level)), alpha = 0.3) +\n  # Actual outcomes\n  geom_point(aes(color = as.factor(defaulted), \n                 shape = as.factor(defaulted)),\n             size = 2, alpha = 0.6) +\n  scale_color_manual(\n    values = c(\"0\" = \"#2ECC71\", \"1\" = \"#E74C3C\"),\n    labels = c(\"No Default\", \"Default\"),\n    name = \"Actual Outcome\"\n  ) +\n  scale_shape_manual(\n    values = c(\"0\" = 16, \"1\" = 17),\n    labels = c(\"No Default\", \"Default\"),\n    name = \"Actual Outcome\"\n  ) +\n  scale_fill_viridis_d(alpha = 0.3, name = \"Risk Zone\") +\n  labs(\n    title = \"The Default Risk Landscape\",\n    subtitle = \"How credit score and income jointly predict default probability\",\n    x = \"Credit Score\",\n    y = \"Annual Income ($1000s)\",\n    caption = \"Circles = No Default, Triangles = Default. Shaded regions show predicted risk zones.\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 15),\n    plot.subtitle = element_text(hjust = 0.5, size = 11),\n    legend.position = \"right\"\n  )\n\nprint(p)\n\n\n\n\n\n\nFigure 8.8: Default Risk Across Credit Score and Income\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.interpolate import griddata\n\n# Prepare data\ndf = r.BLR.copy()\ndf['defaulted'] = df['defaulted'].astype(int)\n\n# Fit model\nX = sm.add_constant(df[['credit_score', 'income']])\ny = df['defaulted']\nlogit_model = sm.Logit(y, X).fit(disp=0)\n\n# Get predictions\ndf['pred_prob'] = logit_model.predict(X)\n\n# Create grid for contour\ncredit_range = np.linspace(df['credit_score'].min(), df['credit_score'].max(), 100)\nincome_range = np.linspace(df['income'].min(), df['income'].max(), 100)\ncredit_grid, income_grid = np.meshgrid(credit_range, income_range)\n\n# Predict on grid\ngrid_points = np.c_[np.ones(credit_grid.ravel().shape), \n                     credit_grid.ravel(), \n                     income_grid.ravel()]\nprob_grid = logit_model.predict(grid_points).reshape(credit_grid.shape)\n\n# Create plot\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Contour plot\ncontour = ax.contourf(credit_grid, income_grid/1000, prob_grid, \n                      levels=15, cmap='RdYlGn_r', alpha=0.5)\ncbar = plt.colorbar(contour, ax=ax, label='Predicted Default Probability')\n\n# Scatter actual outcomes\nno_default = df[df['defaulted'] == 0]\ndefaulted = df[df['defaulted'] == 1]\n\nax.scatter(no_default['credit_score'], no_default['income']/1000, \n           c='#2ECC71', marker='o', s=30, alpha=0.6, \n           edgecolors='black', linewidth=0.5, label='No Default')\n\n&lt;matplotlib.collections.PathCollection object at 0x000001C1601905D0&gt;\n\nax.scatter(defaulted['credit_score'], defaulted['income']/1000, \n           c='#E74C3C', marker='^', s=40, alpha=0.8, \n           edgecolors='black', linewidth=0.5, label='Default')\n\n&lt;matplotlib.collections.PathCollection object at 0x000001C165E0D710&gt;\n\n# Labels and styling\nax.set_xlabel('Credit Score', fontsize=11)\n\nText(0.5, 0, 'Credit Score')\n\nax.set_ylabel('Annual Income ($1000s)', fontsize=11)\n\nText(0, 0.5, 'Annual Income ($1000s)')\n\nax.set_title('The Default Risk Landscape', \n             fontsize=15, fontweight='bold', pad=15)\n\nText(0.5, 1.0, 'The Default Risk Landscape')\n\nax.text(0.5, 1.02, 'How credit score and income jointly predict default probability',\n        transform=ax.transAxes, ha='center', fontsize=10)\n\nText(0.5, 1.02, 'How credit score and income jointly predict default probability')\n\nax.legend(loc='upper left', fontsize=10, framealpha=0.9)\n\n&lt;matplotlib.legend.Legend object at 0x000001C160109310&gt;\n\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nFigure 8.9: Default Risk Across Credit Score and Income (Python)\n\n\n\n\n\n\n\nThe Story: The visualization reveals the “safe zone” (green region, upper-right) where high credit scores and incomes make default unlikely, and the “danger zone” (red region, lower-left) where both factors signal high risk. Notice how credit score creates sharper boundaries than income, consistent with our earlier finding that credit score is the stronger predictor.\nModel Limitations and Caveats\nWhile our model performs well, we should acknowledge several limitations:\n\nCorrelation vs. Causation: The model shows credit score is associated with lower default risk, but doesn’t prove that improving someone’s credit score causes them to be more reliable borrowers. Credit scores may simply reflect other unmeasured factors like financial discipline.\nClass Imbalance: If defaults are rare in our dataset (e.g., &lt;10%), the model may be optimized for predicting non-defaults while missing many actual defaults. Consider stratified sampling or cost-sensitive learning approaches.\nMulticollinearity: Credit score, income, and education are likely correlated. Our finding that income is “not significant” may reflect this correlation rather than true irrelevance.\nMissing Variables: Important predictors like employment stability, existing debt, or past payment history are not included. The model’s predictive power is limited by available data.\nTemporal Validity: Credit risk patterns change over time (recessions, policy changes). A model trained on 2019 data may not perform well in 2024.\n\nDespite these limitations, the model provides valuable insights into default risk factors and generates interpretable predictions that can inform lending decisions when combined with human judgment and domain expertise.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#when-ols-gets-saucy-for-binary-outcomes",
    "href": "book/08-binary-logistic.html#when-ols-gets-saucy-for-binary-outcomes",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.2 When OLS Gets Saucy for Binary Outcomes",
    "text": "8.2 When OLS Gets Saucy for Binary Outcomes\nBefore diving into logistic regression, let’s stir the pot by applying what we already know — Ordinary Least Squares (OLS) — to a binary outcome.\nSuppose we try to predict whether a tumor is malignant (1) or benign (0) using a single variable like mean_radius. If we use OLS, we get predictions like this:\n\\[\n\\mathbb{E}(Y_i \\mid X_i) = p_i = \\beta_0 + \\beta_1 X_i\n\\]\nBut here’s the problem: this linear model can produce predicted values less than 0 or greater than 1 — which doesn’t make sense if we’re modeling a probability.\nOLS also assumes constant variance of the residuals, but binary data inherently violate this because the variance depends on the mean:\n\\[\n\\text{Var}(Y_i) = p_i(1 - p_i)\n\\]\nSo while OLS might seem like a quick and easy sauce, it’s the wrong recipe for binary outcomes. What we need instead is a model that: - Keeps predictions between 0 and 1, - Accounts for the binary nature of the outcome, - And lets us interpret the effects of predictors on probability in a principled way.\nThat’s where Binary Logistic Regression comes in.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#why-ordinary-least-squares-fails-for-binary-outcomes",
    "href": "book/08-binary-logistic.html#why-ordinary-least-squares-fails-for-binary-outcomes",
    "title": "Binary Logistic Regression",
    "section": "\n8.2 Why Ordinary Least Squares Fails for Binary Outcomes",
    "text": "8.2 Why Ordinary Least Squares Fails for Binary Outcomes\nTo understand the need for logistic regression, consider applying Ordinary Least Squares (OLS) to a binary outcome.\nSuppose we are trying to predict whether a student defaulted on a loan (1) or did not default (0) using a continuous predictor like credit_score.\nBefore diving into the technical reasons why OLS is inappropriate for binary outcomes, let’s look at a simplified version of the dataset we’ll use throughout this chapter:\n# Prepare data: numeric default variable and select predictor\nblr_data &lt;- BLR %&gt;%\n  select(credit_score, defaulted) %&gt;%\n  mutate(defaulted = as.numeric(defaulted))\nBelow is a sample of this dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\ncredit_score\nincome\neducation_years\nmarried\nowns_home\nage\ndefaulted\nsuccesses\ntrials\n\n\n\n850\n154100\n16\n0\n1\n56\n0\n7\n10\n\n\n606\n34100\n13\n1\n0\n31\n0\n7\n10\n\n\n846\n73000\n17\n0\n0\n50\n0\n7\n10\n\n\n702\n41300\n13\n0\n0\n41\n1\n5\n10\n\n\n843\n50800\n16\n1\n1\n45\n0\n7\n10\n\n\n610\n33400\n12\n0\n0\n25\n1\n3\n10\n\n\n572\n24600\n11\n0\n0\n28\n1\n3\n10\n\n\n795\n57700\n16\n1\n1\n40\n0\n7\n10\n\n\n796\n37100\n13\n1\n0\n50\n0\n8\n10\n\n\n690\n72300\n14\n0\n0\n38\n0\n8\n10\n\n\n\nNow, let’s narrow in on the key variables of interest for this section.\n\n\n\ncredit_score\ndefaulted\n\n\n\n850\n0\n\n\n606\n0\n\n\n846\n0\n\n\n702\n1\n\n\n843\n0\n\n\n610\n1\n\n\n572\n1\n\n\n795\n0\n\n\n796\n0\n\n\n690\n0\n\n\n\n\nThis dataset allows us to examine whether a student defaulted on a loan (defaulted = 1) based on their credit_score. It’s a classic binary outcome: a “yes” or “no” event.\nIf we use OLS, we estimate:\n\\[\n\\mathbb{E}(Y_i \\mid X_i) = \\pi_i = \\beta_0 + \\beta_1 X_i\n\\]\nThis means we’re treating the probability of default (\\(\\pi_i\\)) as a linear function of credit score.\nHowever, there are two fundamental issues with using OLS here:\n\nThe linear model can produce predicted values less than 0 or greater than 1 — which doesn’t make sense when modeling a probability.\nOLS assumes constant variance of the residuals, but binary outcomes inherently violate this assumption, because their variance depends on the mean:\n\n\\[\n\\text{Var}(Y_i) = \\pi_i(1 - \\pi_i)\n\\]\nTo see this issue in action, let’s try fitting an OLS regression model using our binary outcome and a continuous predictor. While OLS is designed to minimize squared error, it doesn’t account for the discrete nature of the response — which means it can return predicted probabilities below 0 or above 1. In the example below, we use a customer’s credit score to predict the probability of default, and you’ll see how the linear model fails to respect the fundamental constraints of probability.\n\n\n\n\n\nFigure 8.2: OLS fails to constrain predictions between 0 and 1, despite binary outcomes.\n\n\nThese limitations make OLS unsuitable for binary classification tasks.\nTo properly model binary outcomes, we need a method that:\n\nProduces predictions strictly between 0 and 1,\nAccounts for the fact that the variance depends on the mean, and\nConnects our predictors to the probability of the event in a way that is easy to interpret.\n\nThe key idea is to transform the probability so that it can be modeled as a linear function without breaking these rules.\nIn the next section, we’ll see how the logit transformation does exactly that.\n\n\nR Code\nPython Code\n\n\n\n# ⚠️ This OLS fit is not appropriate for binary outcomes.\n# We include it here only to illustrate why logistic regression is needed.\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Prepare data: numeric default variable and select predictor\nblr_data &lt;- BLR %&gt;%\n  select(credit_score, defaulted) %&gt;%\n  mutate(defaulted = as.numeric(defaulted))\n\n# Fit an OLS model (not ideal for binary outcome)\nols_model &lt;- lm(defaulted ~ credit_score, data = blr_data)\n\n# Add predicted values\nblr_data$predicted &lt;- predict(ols_model, newdata = blr_data)\n\n# Plot actual data and OLS-fitted line\nplot &lt;- ggplot(blr_data, aes(x = credit_score, y = defaulted)) +\n  geom_jitter(height = 0.05, width = 0, alpha = 0.4, color = \"black\") +\n  geom_line(aes(y = predicted), color = \"blue\", linewidth = 1.2) +\n  labs(\n    title = \"OLS Fitted Line on Binary Data\",\n    x = \"Credit Score\",\n    y = \"Predicted Probability of Default\"\n  ) +\n  coord_cartesian(ylim = c(-0.2, 1.2)) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\nplot\n\n\n# ⚠️ This OLS fit is not appropriate for binary outcomes.\n# We include it here only to illustrate why logistic regression is needed.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Extract relevant columns from the BLR dataset\nblr = r.data['BLR']\ndf = blr[['credit_score', 'defaulted']].copy()\ndf['defaulted'] = df['defaulted'].astype(int)\n\n# Fit an OLS model\nX = sm.add_constant(df['credit_score'])\nmodel = sm.OLS(df['defaulted'], X).fit()\ndf['predicted'] = model.predict(X)\n\n# Plot the actual binary outcomes and OLS predictions\nfig, ax = plt.subplots(figsize=(6, 4))\nax.scatter(df['credit_score'], df['defaulted'], alpha=0.4, color='black', label='Actual Data', s=20)\nax.plot(df['credit_score'], df['predicted'], color='blue', label='OLS Fit', linewidth=2)\nax.set_title(\"OLS Fitted Line on Binary Data\", fontsize=14, fontweight='bold')\nax.set_xlabel(\"Credit Score\")\nax.set_ylabel(\"Predicted Probability of Default\")\nax.set_ylim(-0.2, 1.2)\nax.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(-0.3, 1.2)",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#python-code",
    "href": "book/08-binary-logistic.html#python-code",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.3 Python Code",
    "text": "8.3 Python Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Sample data\nnp.random.seed(42)\nX = np.array([1000, 1200, 1500, 1800, 2000])\nY = np.array([200, 230, 250, 290, 310])\ndf = pd.DataFrame({'Size': X, 'Price': Y})\n\n# Fit the correct OLS model\nX_sm = sm.add_constant(df['Size'])\nmodel = sm.OLS(df['Price'], X_sm).fit()\ndf['Predicted_Correct'] = model.predict(X_sm)\n\n# Manually add the incorrect line\ndf['Predicted_Wrong'] = 110 + 0.08 * df['Size']\n\n# Reshape for plotting\ndf_long = pd.concat([\n    df[['Size', 'Predicted_Correct']].rename(columns={'Predicted_Correct': 'Price'}).assign(Line='Line A (Best Fit)'),\n    df[['Size', 'Predicted_Wrong']].rename(columns={'Predicted_Wrong': 'Price'}).assign(Line='Line B (Worse Fit)')\n])\n\n# Plot\nfig, ax = plt.subplots(figsize=(6, 4))\nax.scatter(df['Size'], df['Price'], color='black', label='Actual Data')\nfor label, group in df_long.groupby('Line'):\n    ax.plot(group['Size'], group['Price'], label=label)\nax.set_title(\"Comparing Regression Line Fits\", fontsize=14, fontweight='bold')\nax.set_xlabel(\"House Size (sq ft)\")\nax.set_ylabel(\"House Price (in $1000s)\")\nax.legend(title=\"Regression Line\", loc='lower right')\nplt.grid(True)\nplt.show()\n:::\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n8.3.1 Understanding Residuals\nFor each data point, the residual is the vertical distance between the actual \\(Y\\) value and the predicted \\(Y\\) value (denoted \\(\\hat{Y}\\)) on the line. In simple terms, it tells us how far off our prediction is for each point given the same \\(X\\) value. If a line fits well, these residuals will be small, meaning our predictions of the \\(Y\\) variable are close to the actual value.\nOLS quantifies how well a line fits the data by calculating the Sum of Squared Errors (SSE). The SSE is obtained by:\n\nComputing the residual for each data point.\nSquaring each residual (this ensures that errors do not cancel each other out).\nSumming all these squared values.\n\n\\[\nSSE=\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\n\\]\nA lower SSE indicates a line that is closer to the actual data points. OLS chooses the best line by finding the one with the smallest SSE.\n\n8.3.2 Quantifying the Fit with SSE\nWe can compare the two lines by computing their SSE. The code below calculates and prints the SSE for each line:\n\n\nR Code\nPython Code\n\n\n\n# Calculate the Sum of Squared Errors for the correct model (Blue)\nsse_correct &lt;- sum((df$Price - df$Predicted_Correct)^2)\n\n# Calculate the Sum of Squared Errors for the manually adjusted model (Red)\nsse_wrong &lt;- sum((df$Price - df$Predicted_Wrong)^2)\n\n# Print the SSEs for each line\ncat(\"SSE for Best-Fit Line (Blue line):\", sse_correct, \"\\n\")\ncat(\"SSE for Worse-Fit Line (Red line):\", sse_wrong, \"\\n\")\n\n\n# Calculate the Sum of Squared Errors for the correct model (Blue)\nsse_correct = np.sum((df['Price'] - df['Predicted_Correct']) ** 2)\n\n# Calculate the Sum of Squared Errors for the manually adjusted model (Red)\nsse_wrong = np.sum((df['Price'] - df['Predicted_Wrong']) ** 2)\n\n# Print the SSEs for each line\nprint(f\"SSE for Best-Fit Line (Blue line): {sse_correct}\")\nprint(f\"SSE for Worse-Fit Line (Red line): {sse_wrong}\")\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\nWhen you run this code, you’ll observe that the blue line (Line A) has a much lower SSE compared to the red line (Line B). This tells us that the blue line is a better fit for the data because its predictions are, on average, closer to the actual values.\nIn summary, OLS selects the “best line” by minimizing the sum of squared errors, ensuring that the total error between predicted and actual values is as small as possible.\n\n8.3.3 Why Squared Errors?\nWhen measuring how far off our predictions are, errors can be positive (if our prediction is too low) or negative (if it’s too high). If we simply added these errors together, they could cancel each other out, hiding the true size of the mistakes. By squaring each error, we convert all numbers to positive values so that every mistake counts.\nIn addition, squaring makes big errors count a lot more than small ones. This means that a large mistake will have a much bigger impact on the overall error, encouraging the model to reduce those large errors and improve its overall accuracy.\n\n8.3.4 The Mathematical Formulation of the OLS Model\nNow that we understand how OLS finds the best-fitting line by minimizing the differences between the actual and predicted values, let’s look at the math behind it.\nIn a simple linear regression with one predictor, we express the relationship between the outcome \\(Y\\) and the predictor \\(X\\) using the following equation. Note that OLS fits a straight line to the data, which is why the equation takes the familiar form of a straight line:\n\\[\nY=\\beta_0+\\beta_1X+\\epsilon\n\\]\nHere’s what each part of the equation means:\n\n\n\\(Y\\) is the dependent variable or the outcome we want to predict.\n\n\\(X\\) is the independent variable or the predictor that we believe influences \\(Y\\).\n\n\\(\\beta_0\\) is the intercept. It represents the predicted value of \\(Y\\) when \\(X=0\\).\n\n\\(\\beta_1\\) is the slope. It tells us how much \\(Y\\) is expected to change for each one-unit increase in \\(X\\).\n\n\\(\\epsilon\\) is the error term. It captures the random variation in \\(Y\\) that cannot be explained by \\(X\\).\n\nThis equation provides a clear mathematical framework for understanding how changes in \\(X\\) are expected to affect \\(Y\\), while also accounting for random variation. In the upcoming section, we will explore our toy dataset to showcase this equation and OLS in action.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#why-ordinary-least-squares-fails-for-binary-outcomes-2",
    "href": "book/08-binary-logistic.html#why-ordinary-least-squares-fails-for-binary-outcomes-2",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.3 Why Ordinary Least Squares Fails for Binary Outcomes 2",
    "text": "8.3 Why Ordinary Least Squares Fails for Binary Outcomes 2\nTo understand the need for logistic regression, consider applying Ordinary Least Squares (OLS) to a binary outcome. Suppose we are predicting whether a tumor is malignant (1) or benign (0) using a continuous variable such as mean_radius.\nUsing OLS, we estimate:\n\\[\n\\mathbb{E}(Y_i \\mid X_i) = p_i = \\beta_0 + \\beta_1 X_i\n\\]\nBut here’s the problem: this linear model can produce predicted values less than 0 or greater than 1 — which doesn’t make sense if we’re modeling a probability.\nOLS also assumes constant variance of the residuals, but binary data inherently violate this because the variance depends on the mean:\n\\[\n\\text{Var}(Y_i) = p_i(1 - p_i)\n\\]\nThese limitations make OLS unsuitable for binary classification tasks. To properly model binary outcomes, we need a method that:\n\nRestricts predictions to the [0, 1] interval,\nUses a likelihood-based estimation method appropriate for discrete outcomes, and\nProvides interpretable coefficients tied to the probability of an event.\n\nThis is exactly what Binary Logistic Regression offers.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#the-dataset",
    "href": "book/08-binary-logistic.html#the-dataset",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.5 8.5.1 The Dataset",
    "text": "8.5 8.5.1 The Dataset\n\nDescribe the Logistic_Regression dataset: predictors include credit_score, income, etc.\nOutcome: whether a customer defaulted (0/1)",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#the-problem-were-trying-to-solve",
    "href": "book/08-binary-logistic.html#the-problem-were-trying-to-solve",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.6 8.5.2 The Problem We’re Trying to Solve",
    "text": "8.6 8.5.2 The Problem We’re Trying to Solve\n\nPredict likelihood of default given customer info\nExplain real-world significance (lending, risk)",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#study-design",
    "href": "book/08-binary-logistic.html#study-design",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.7 8.5.3 Study Design",
    "text": "8.7 8.5.3 Study Design\n\nPredictor types: categorical, continuous\nOutcome: binary\nSample size, assumptions",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#applying-study-design-to-our-case-study",
    "href": "book/08-binary-logistic.html#applying-study-design-to-our-case-study",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.8 8.5.4 Applying Study Design to Our Case Study",
    "text": "8.8 8.5.4 Applying Study Design to Our Case Study\n\nData splits\nVariable encoding (e.g. factors)\nPreview what the model will do",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#sec-what-is-blr",
    "href": "book/08-binary-logistic.html#sec-what-is-blr",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.3 What is Binary Logistic Regression?",
    "text": "8.3 What is Binary Logistic Regression?\nBinary Logistic Regression (BLR) is a regression method used when the response variable is binary — it takes only two values: 0 or 1.\n\nThe goal is to model the probability of the event labeled 1, given some predictors.\nUnlike OLS, BLR uses the log-odds (also called the logit) to model the relationship:\n\n\\[\n\\text{logit}(p_i) = \\log\\left(\\frac{p_i}{1 - p_i}\\right) = \\beta_0 + \\beta_1 X_i\n\\]\nThis transformation ensures: - The model is linear in the log-odds, - Predicted probabilities stay between 0 and 1.\n\n\n\n\n\n\nTODO:\n\n\n\nAdd an annotated diagram showing how increasing X impacts odds, log-odds, and probabilities.\n\n\n\n\n8.3.1 The Logistic Function and Its Shape\n\nIntroduce the sigmoid function:\n\n\\[\np_i = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_i)}}\n\\]\n\nIt maps real-valued input to the [0, 1] interval.\nShape: S-curve — slow rise at ends, steep middle.\nContrast visually with a linear line from OLS.\n\n\n\nR Code\nPython Code\n\n\n\n\n# TODO: generate plot with logistic curve and linear fit for comparison\n\n\n\n\n# TODO: matplotlib plot of sigmoid and linear predictions",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#what-is-binary-logistic-regression",
    "href": "book/08-binary-logistic.html#what-is-binary-logistic-regression",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.5 What is Binary Logistic Regression?",
    "text": "8.5 What is Binary Logistic Regression?\nBinary Logistic Regression (BLR) is a regression method used when the response variable is binary — it takes only two values: 0 or 1.\n\nThe goal is to model the probability of the event labeled 1, given some predictors.\nUnlike OLS, BLR uses the log-odds (also called the logit) to model the relationship:\n\n\\[\n\\text{logit}(p_i) = \\log\\left(\\frac{p_i}{1 - p_i}\\right) = \\beta_0 + \\beta_1 X_i\n\\]\nThis transformation ensures: - The model is linear in the log-odds, - Predicted probabilities stay between 0 and 1.\n\n\n\n\n\n\nTODO:\n\n\n\nAdd an annotated diagram showing how increasing X impacts odds, log-odds, and probabilities.\n\n\n\n\n8.5.1 The Logistic Function and Its Shape\n\nIntroduce the sigmoid function:\n\n\\[\np_i = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_i)}}\n\\]\n\nIt maps real-valued input to the [0, 1] interval.\nShape: S-curve — slow rise at ends, steep middle.\nContrast visually with a linear line from OLS.\n\n\n\nR Code\nPython Code\n\n\n\n\n# TODO: generate plot with logistic curve and linear fit for comparison\n\n\n\n\n# TODO: matplotlib plot of sigmoid and linear predictions",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#case-study-understanding-financial-behaviors-1",
    "href": "book/08-binary-logistic.html#case-study-understanding-financial-behaviors-1",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.17 Case Study: Understanding Financial Behaviors",
    "text": "8.17 Case Study: Understanding Financial Behaviors\nTo demonstrate Ordinary Least Squares (OLS) in action, we will walk through a case study using a toy dataset. This case study will help us understand the financial behaviors of students and identify the factors that influence their Net_Money, the amount of money left over at the end of each month. We will approach this case study using the data science workflow described in a previous chapter, ensuring a structured approach to problem-solving and model building.\n\n8.17.1 The Dataset\nOur dataset captures various aspects of students’ financial lives. Each row represents a student, and the columns describe different characteristics. Below is a breakdown of the variables:\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\nHas_Job\nWhether the student has a job (0 = No, 1 = Yes).\n\n\nYear_of_Study\nThe student’s current year of study (e.g., 1st year, 2nd year, etc.).\n\n\nFinancially_Dependent\nWhether the student is financially dependent on someone else (0 = No, 1 = Yes).\n\n\nMonthly_Allowance\nThe amount of financial support the student receives each month.\n\n\nCooks_at_Home\nWhether the student prepares their own meals (0 = No, 1 = Yes).\n\n\nLiving_Situation\nThe student’s living arrangement (e.g., living with family, in a shared apartment, etc.).\n\n\nHousing_Type\nThe type of housing the student lives in (e.g., rented, owned, dormitory).\n\n\nGoes_Out_Spends_Money\nHow frequently the student goes out and spends money (1 = rarely, 5 = very often).\n\n\nDrinks_Alcohol\nWhether the student drinks alcohol (0 = No, 1 = Yes).\n\n\nNet_Money\nThe amount of money the student has left at the end of the month after income and expenses.\n\n\nMonthly_Earnings\nThe student’s earnings from any part-time jobs or other income sources.\n\n\n\nHere’s a sample of the dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHas_Job\nYear_of_Study\nFinancially_Dependent\nMonthly_Allowance\nCooks_at_Home\nLiving_Situation\nHousing_Type\nGoes_Out_Spends_Money\nDrinks_Alcohol\nNet_Money\nMonthly_Earnings\n\n\n\n0\n1\n0\n658.99\n0\n3\n1\n6\n0\n529.34\n0.00\n\n\n1\n3\n0\n592.55\n0\n3\n2\n3\n1\n992.72\n941.92\n\n\n1\n4\n1\n602.54\n0\n2\n2\n2\n1\n557.30\n876.57\n\n\n\n\nThis dataset provides a structured way to analyze the financial habits of students and determine which factors contribute most to their financial stability.\n\n8.17.2 The Problem We’re Trying to Solve\nOur goal in this case study is to understand which factors impact a student’s net money. Specifically, we aim to identify which characteristics, such as having a job, monthly earnings, or financial support, explain why some students have more money left over at the end of the month than others.\nThe key question we want to answer is:\n\nWhich factors have the biggest influence on a student’s net money?\n\nBy applying OLS to this dataset, we can:\n\nMeasure how much each factor contributes to variations in net money. For example, we can determine the increase in net money associated with a one-unit increase in monthly earnings.\nIdentify whether each factor has a positive or negative effect on net money.\nUnderstand the unique contribution of each variable while accounting for the influence of others. This helps us isolate the effect of, say, having a job from that of receiving financial support.\nPredict a student’s net money based on their characteristics. These insights could help institutions design targeted financial literacy programs or interventions to improve financial stability.\nEvaluate the overall performance of our model using statistical measures such as R-squared and p-values. This not only confirms the significance of our findings but also guides improvements in future analyses.\n\nIn summary, using OLS in this case study allows us to break down complex financial behaviors into understandable components. This powerful tool provides clear, actionable insights into which factors are most important, paving the way for more informed decisions and targeted interventions.\n\n8.17.3 Study Design\nNow that we’ve introduced our case study and dataset, it’s time to follow the data science workflow step by step. The first step is to define the main statistical inquiries we want to address. As mentioned earlier, our key question is:\n\nWhich factors have the biggest influence on a student’s net money?\n\nTo answer this question, we will adopt an inferential analysis approach rather than a predictive analysis approach. Let’s quickly review the difference between these two methods:\nInferential vs. Predictive Analysis\n\n\nInferential Analysis explores and quantifies the relationships between explanatory variables (e.g., student characteristics) and the response variable (Net_Money). For example, we might ask: Does having a part-time job significantly affect a student’s net money, and by how much? The goal here is to understand these effects and assess their statistical significance.\n\nPredictive Analysis focuses on accurately forecasting the response variable using new data. In this case, the question could be: Can we predict a student’s net money based on factors like monthly earnings, living situation, and spending habits? The emphasis is on building a model that produces reliable predictions, even if it doesn’t fully explain the underlying relationships.\n\n8.17.4 Applying Study Design to Our Case Study\nFor our case study, we are interested in understanding how factors such as Has_Job, Monthly_Earnings, and Spending_Habits affect a student’s Net_Money. This leads us to adopt an inferential approach. We aim to answer questions like:\n\nDoes having a part-time job lead to significantly higher net money?\nHow much do a student’s monthly earnings influence their financial situation?\nDo spending habits, like going out frequently, decrease a student’s net money?\n\nUsing OLS, we will estimate the impact of each factor and determine whether these effects are statistically significant. This inferential analysis will help us understand which variables have the greatest influence on students’ financial outcomes.\nIf our goal were instead to predict a student’s future Net Money based on their characteristics, we would adopt a predictive approach. Although our focus here is on inference, it’s important to recognize that OLS is versatile and can be applied in both contexts.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#sec-fit-blr",
    "href": "book/08-binary-logistic.html#sec-fit-blr",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.5 Fitting the Logistic Regression Model",
    "text": "8.5 Fitting the Logistic Regression Model\n\nUse glm(family = binomial) in R\nPython equivalent: Logit() in statsmodels\n\n\n\n\nR Code\nPython Code\n\n\n\n\n# TODO: Fit logistic regression using glm()\n\n\n\n\n# TODO: Fit logistic regression using statsmodels.Logit()\n\n\n\n\n\nDisplay coefficient table\nEmphasize coefficients = change in log-odds per unit increase",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#fitting-the-logistic-regression-model",
    "href": "book/08-binary-logistic.html#fitting-the-logistic-regression-model",
    "title": "Binary Logistic Regression",
    "section": "\n8.6 Fitting the Logistic Regression Model",
    "text": "8.6 Fitting the Logistic Regression Model\nWe model the log‑odds of default as a linear function of predictors. In practice:\n\n\nR: glm(..., family = binomial(link = \"logit\"))\n\n\nPython: statsmodels.Logit(...)\n\n\n\n\nR Code\nPython Code\n\n\n\n\n# Fit a simple, interpretable model\nfit_glm &lt;- glm(\n  defaulted ~ credit_score + income,\n  data = BLR,\n  family = binomial(link = \"logit\")\n)\n\n# Coefficients (log-odds) and odds ratios\ncoef_table &lt;- broom::tidy(fit_glm, conf.int = TRUE, conf.level = 0.95)\ncoef_table$odds_ratio &lt;- exp(coef_table$estimate)\ncoef_table$or_low     &lt;- exp(coef_table$conf.low)\ncoef_table$or_high    &lt;- exp(coef_table$conf.high)\n\nknitr::kable(\n  coef_table[, c(\"term\",\"estimate\",\"std.error\",\"statistic\",\"p.value\",\"odds_ratio\",\"or_low\",\"or_high\")],\n  digits = 3,\n  col.names = c(\"Term\",\"Log-Odds (β)\",\"SE\",\"z\",\"p\",\"Odds Ratio\",\"OR 2.5%\",\"OR 97.5%\")\n)\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\n\n# BLR was exported from R in the setup; use it directly\ndf = BLR.copy()\ndf['defaulted'] = df['defaulted'].astype(int)\n\nX = df[['credit_score','income']]\nX = sm.add_constant(X)\ny = df['defaulted']\n\nlogit_model = sm.Logit(y, X).fit(disp=False)\n\n# Build a coefficient table with odds ratios and CIs\nparams = logit_model.params\nbse    = logit_model.bse\nzvals  = params / bse\npvals  = logit_model.pvalues\nconf   = logit_model.conf_int()\nor_    = np.exp(params)\nor_lo  = np.exp(conf[0])\nor_hi  = np.exp(conf[1])\n\ncoef_table = pd.DataFrame({\n    \"Term\": params.index,\n    \"Log-Odds (β)\": params.values,\n    \"SE\": bse.values,\n    \"z\": zvals.values,\n    \"p\": pvals.values,\n    \"Odds Ratio\": or_.values,\n    \"OR 2.5%\": or_lo.values,\n    \"OR 97.5%\": or_hi.values\n})\n\ncoef_table\n\n\n\n\nNotes\n\nCoefficients are in log‑odds units.\nExponentiating a coefficient gives the odds ratio: \\(\\text{OR} = e^{\\beta}\\).\nE.g., \\(\\beta_{\\text{credit\\_score}}=-0.01\\Rightarrow \\text{OR}\\approx 0.99\\): each 1‑point increase in credit score multiplies the odds of default by ~0.99 (≈1% decrease).",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#from-log-odds-to-probabilities",
    "href": "book/08-binary-logistic.html#from-log-odds-to-probabilities",
    "title": "Binary Logistic Regression",
    "section": "\n8.9 From Log-Odds to Probabilities",
    "text": "8.9 From Log-Odds to Probabilities\n\nExplain inverse logit (sigmoid)\nConvert predictions to probabilities\nShow examples: low score, medium, high\nOptional table of predictor values and predicted probs\n\n\n# TODO: predict probabilities using `type = \"response\"`",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#model-evaluation",
    "href": "book/08-binary-logistic.html#model-evaluation",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.7 Model Evaluation",
    "text": "8.7 Model Evaluation\n\nConfusion matrix\nAccuracy, precision, recall\nROC curve, AUC\n\n\n# TODO: ROC curve and confusion matrix\n\n\n\n\n\n\n\nTODO:\n\n\n\nCompare against baseline model (predict all 0s)",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#summary",
    "href": "book/08-binary-logistic.html#summary",
    "title": "Binary Logistic Regression",
    "section": "\n8.23 Summary",
    "text": "8.23 Summary\nIn this chapter, we introduced binary logistic regression, a fundamental method for modeling outcomes with two possible categories. Unlike ordinary least squares, which can produce impossible probabilities, logistic regression uses the logit link function to transform probabilities into log-odds, ensuring predictions remain between 0 and 1.\n\n8.23.1 Key Takeaways\n\nThe logit transformation solves the problem of modeling bounded probabilities. By working on the log-odds scale, we can use linear regression techniques while respecting the [0,1] constraint on probabilities.\nCoefficients are on the log-odds scale, but we typically exponentiate them to obtain odds ratios, which are more interpretable. An odds ratio of 0.99 for credit score means that each 1-point increase reduces the odds of default by 1%.\nMaximum likelihood estimation replaces least squares. Because our outcome follows a Bernoulli distribution, we maximize the likelihood of observing our data rather than minimizing squared residuals.\nModel evaluation requires multiple perspectives: We used pseudo-R², AIC, BIC, deviance tests, confusion matrices, ROC curves, and diagnostic plots. No single metric tells the whole story.\nThe inference vs. prediction distinction is crucial. Coefficients and hypothesis tests answer “what factors matter?” (inference), while predicted probabilities and classification metrics answer “how well can we predict?” (prediction).\nParsimony often wins: In our loan default example, credit score alone was sufficient. Adding income and education years did not significantly improve model fit, demonstrating that more predictors do not always mean better models.\n\n8.23.2 Looking Ahead\nBinary logistic regression handles ungrouped data where each observation is a single trial (default or not default). But what if our data are aggregated? For example, suppose we observe “15 out of 20 students passed” for each combination of study hours and class attendance. This grouped data structure calls for a closely related method: Binomial Logistic Regression.\nIn Chapter 9, we extend binary logistic regression to handle binomial outcomes, where we observe counts of successes out of a known number of trials. The underlying principles remain the same (logit link, Bernoulli/Binomial distribution, maximum likelihood), but the data structure and interpretation differ slightly. We’ll see how grouped data provides more information per row and how overdispersion can complicate model fitting.\nBy mastering binary logistic regression, you’ve built a foundation for understanding not just binomial models but the entire family of generalized linear models (GLMs) that connect outcomes from different distributions (Poisson, Gamma, etc.) to linear predictors via link functions. The logic you’ve learned here, from the logit transformation to model diagnostics, will carry forward throughout the rest of the discrete outcome zone.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#exploratory-data-analysis-eda-todo",
    "href": "book/08-binary-logistic.html#exploratory-data-analysis-eda-todo",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.10 Exploratory Data Analysis (EDA) TODO",
    "text": "8.10 Exploratory Data Analysis (EDA) TODO\n\nSummarize variables\nCompare predictors by outcome\nUse tables, boxplots, and pairplots\n\n\n# TODO: Visualize distributions and relationships\n\n\n# TODO: EDA using seaborn/pandas/matplotlib",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#data-modelling-todo",
    "href": "book/08-binary-logistic.html#data-modelling-todo",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.11 Data Modelling TODO",
    "text": "8.11 Data Modelling TODO\n\nFit the binary logistic regression model\nInterpret coefficients\nDiscuss model form:\n\n\\[\n\\log\\left(\\frac{p_i}{1 - p_i}\\right) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_k X_k\n\\]\n\n\nR Code\nPython Code\n\n\n\n\n# TODO: Fit logistic regression model\nmodel &lt;- glm(defaulted ~ credit_score + income + education_years,\n             data = blr_data, family = binomial())\nsummary(model)\n\n\n\n\n# TODO: Fit logistic regression model\nimport statsmodels.api as sm\nX = df[['credit_score', 'income', 'education_years']]\nX = sm.add_constant(X)\ny = df['defaulted']\nmodel = sm.Logit(y, X).fit()\nprint(model.summary())",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#estimation-todo",
    "href": "book/08-binary-logistic.html#estimation-todo",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.12 Estimation TODO",
    "text": "8.12 Estimation TODO\n\nInterpret log-odds coefficients\nConvert to odds ratios using exp(β)\n\nConfidence intervals\n\n\n# TODO: Use broom or base R for odds ratios\n\n\n# TODO: Use np.exp() for odds ratios",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#goodness-of-fit-todo",
    "href": "book/08-binary-logistic.html#goodness-of-fit-todo",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.13 Goodness of Fit TODO",
    "text": "8.13 Goodness of Fit TODO\n\nDiscuss pseudo-$R^2$ and AIC\nAssess fit with residual plots (optional)\nUse ROC curve, confusion matrix, accuracy\n\n\n# TODO: Evaluate model fit with ROC, AUC\n\n\n# TODO: Evaluate model fit with ROC, AUC",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#results-todo",
    "href": "book/08-binary-logistic.html#results-todo",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.14 Results TODO",
    "text": "8.14 Results TODO\n\nPresent key findings\nTables or figures with coefficient interpretations\nHighlight statistically significant predictors\n\n\n# TODO: Summarize results",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#todo-2",
    "href": "book/08-binary-logistic.html#todo-2",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.15 8.12 TODO",
    "text": "8.15 8.12 TODO\n\nExplain results in plain language\nWhat do these findings suggest about financial behavior and default?\nWhat should a reader take away from this model?\n\n\n\n\n\n\n\nTip\n\n\n\nEffective storytelling connects statistical results to real-world meaning. Translate log-odds into “lenders are X times more likely to…” language.\n\n\n```",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#python-code-5",
    "href": "book/08-binary-logistic.html#python-code-5",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.16 Python Code",
    "text": "8.16 Python Code\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Sample data\nnp.random.seed(42)\nX = np.array([1000, 1200, 1500, 1800, 2000])\nY = np.array([200, 230, 250, 290, 310])\ndf = pd.DataFrame({'Size': X, 'Price': Y})\n\n# Fit the correct OLS model\nX_sm = sm.add_constant(df['Size'])\nmodel = sm.OLS(df['Price'], X_sm).fit()\ndf['Predicted_Correct'] = model.predict(X_sm)\n\n# Manually add the incorrect line\ndf['Predicted_Wrong'] = 110 + 0.08 * df['Size']\n\n# Reshape for plotting\ndf_long = pd.concat([\n    df[['Size', 'Predicted_Correct']].rename(columns={'Predicted_Correct': 'Price'}).assign(Line='Line A (Best Fit)'),\n    df[['Size', 'Predicted_Wrong']].rename(columns={'Predicted_Wrong': 'Price'}).assign(Line='Line B (Worse Fit)')\n])\n\n# Plot\nfig, ax = plt.subplots(figsize=(6, 4))\nax.scatter(df['Size'], df['Price'], color='black', label='Actual Data')\nfor label, group in df_long.groupby('Line'):\n    ax.plot(group['Size'], group['Price'], label=label)\nax.set_title(\"Comparing Regression Line Fits\", fontsize=14, fontweight='bold')\nax.set_xlabel(\"House Size (sq ft)\")\nax.set_ylabel(\"House Price (in $1000s)\")\nax.legend(title=\"Regression Line\", loc='lower right')\nplt.grid(True)\nplt.show()\n:::\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n8.16.1 Understanding Residuals\nFor each data point, the residual is the vertical distance between the actual \\(Y\\) value and the predicted \\(Y\\) value (denoted \\(\\hat{Y}\\)) on the line. In simple terms, it tells us how far off our prediction is for each point given the same \\(X\\) value. If a line fits well, these residuals will be small, meaning our predictions of the \\(Y\\) variable are close to the actual value.\nOLS quantifies how well a line fits the data by calculating the Sum of Squared Errors (SSE). The SSE is obtained by:\n\nComputing the residual for each data point.\nSquaring each residual (this ensures that errors do not cancel each other out).\nSumming all these squared values.\n\n\\[\nSSE=\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\n\\]\nA lower SSE indicates a line that is closer to the actual data points. OLS chooses the best line by finding the one with the smallest SSE.\n\n8.16.2 Quantifying the Fit with SSE\nWe can compare the two lines by computing their SSE. The code below calculates and prints the SSE for each line:\n\n\nR Code\nPython Code\n\n\n\n# Calculate the Sum of Squared Errors for the correct model (Blue)\nsse_correct &lt;- sum((df$Price - df$Predicted_Correct)^2)\n\n# Calculate the Sum of Squared Errors for the manually adjusted model (Red)\nsse_wrong &lt;- sum((df$Price - df$Predicted_Wrong)^2)\n\n# Print the SSEs for each line\ncat(\"SSE for Best-Fit Line (Blue line):\", sse_correct, \"\\n\")\ncat(\"SSE for Worse-Fit Line (Red line):\", sse_wrong, \"\\n\")\n\n\n# Calculate the Sum of Squared Errors for the correct model (Blue)\nsse_correct = np.sum((df['Price'] - df['Predicted_Correct']) ** 2)\n\n# Calculate the Sum of Squared Errors for the manually adjusted model (Red)\nsse_wrong = np.sum((df['Price'] - df['Predicted_Wrong']) ** 2)\n\n# Print the SSEs for each line\nprint(f\"SSE for Best-Fit Line (Blue line): {sse_correct}\")\nprint(f\"SSE for Worse-Fit Line (Red line): {sse_wrong}\")\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\nWhen you run this code, you’ll observe that the blue line (Line A) has a much lower SSE compared to the red line (Line B). This tells us that the blue line is a better fit for the data because its predictions are, on average, closer to the actual values.\nIn summary, OLS selects the “best line” by minimizing the sum of squared errors, ensuring that the total error between predicted and actual values is as small as possible.\n\n8.16.3 Why Squared Errors?\nWhen measuring how far off our predictions are, errors can be positive (if our prediction is too low) or negative (if it’s too high). If we simply added these errors together, they could cancel each other out, hiding the true size of the mistakes. By squaring each error, we convert all numbers to positive values so that every mistake counts.\nIn addition, squaring makes big errors count a lot more than small ones. This means that a large mistake will have a much bigger impact on the overall error, encouraging the model to reduce those large errors and improve its overall accuracy.\n\n8.16.4 The Mathematical Formulation of the OLS Model\nNow that we understand how OLS finds the best-fitting line by minimizing the differences between the actual and predicted values, let’s look at the math behind it.\nIn a simple linear regression with one predictor, we express the relationship between the outcome \\(Y\\) and the predictor \\(X\\) using the following equation. Note that OLS fits a straight line to the data, which is why the equation takes the familiar form of a straight line:\n\\[\nY=\\beta_0+\\beta_1X+\\epsilon\n\\]\nHere’s what each part of the equation means:\n\n\n\\(Y\\) is the dependent variable or the outcome we want to predict.\n\n\\(X\\) is the independent variable or the predictor that we believe influences \\(Y\\).\n\n\\(\\beta_0\\) is the intercept. It represents the predicted value of \\(Y\\) when \\(X=0\\).\n\n\\(\\beta_1\\) is the slope. It tells us how much \\(Y\\) is expected to change for each one-unit increase in \\(X\\).\n\n\\(\\epsilon\\) is the error term. It captures the random variation in \\(Y\\) that cannot be explained by \\(X\\).\n\nThis equation provides a clear mathematical framework for understanding how changes in \\(X\\) are expected to affect \\(Y\\), while also accounting for random variation. In the upcoming section, we will explore our toy dataset to showcase this equation and OLS in action.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#data-collection-and-wrangling-1",
    "href": "book/08-binary-logistic.html#data-collection-and-wrangling-1",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.18 Data Collection and Wrangling",
    "text": "8.18 Data Collection and Wrangling\nWith the statistical questions clearly defined, the next step is to ensure that the data is appropriately prepared for analysis. Although we already have the dataset, it is valuable to consider how this data could have been collected to better understand its context and potential limitations.\n\n8.18.1 Data Collection\nFor a study like ours, data on students’ financial behaviors could have been collected through various methods:\n\n\nSurveys: Students might have been asked about their employment status, earnings, and spending habits through structured questionnaires. While surveys can capture self-reported financial behaviors, they may suffer from recall bias or social desirability bias.\n\nAdministrative Data: Universities or employers may maintain records on student income and employment, providing a more objective source of financial information. However, access to such data may be limited due to privacy regulations.\n\nFinancial Tracking Apps: Digital financial management tools can offer detailed, real-time data on student income and spending patterns. While these apps provide high granularity, they may introduce selection bias, as only students who use such apps would be represented in the dataset.\n\nRegardless of the data collection method, each approach presents challenges, such as missing data, reporting errors, or sample biases. Addressing these issues is a critical aspect of data wrangling.\n\n8.18.2 Data Wrangling\nNow that our dataset is ready, the next step is to clean and organize it so that it’s in the best possible shape for analysis using OLS. Data wrangling involves several steps that ensure our data is accurate, consistent, and ready for modeling. Here are some key tasks:\nHandling Missing Data\nThe first task is to ensure data integrity by checking for missing values. Missing data can occur for various reasons, such as unrecorded responses or errors in data entry. When we find missing values—for example, if some students don’t have recorded earnings or net money—we must decide how to handle these gaps. Common strategies include:\n\n\nRemoving incomplete records: If the amount of missing data is minimal or missingness is random.\n\nImputing missing values: Using logical estimates or averages if missingness follows a systematic pattern.\n\nIn our toy dataset, there are no missing values, as confirmed by:\n\n\nR Code\nPython Code\n\n\n\n\ncolSums(is.na(data))\n\n\n\n\n# Count missing values in each column\ndata.isna().sum()\n\n\n\n\nEncoding Categorical Variables\nFor regression analysis, we need to convert categorical variables into numerical representations. In R, binary variables like Has_Job and Drinks_Alcohol should be transformed into factors so that the model correctly interprets them as categorical data rather than continuous numbers. For example:\n\n\nR Code\nPython Code\n\n\n\n\n# Convert binary categorical variables to factors\ndata &lt;- data |&gt;\n  mutate(Has_Job = as.factor(Has_Job),\n         Drinks_Alcohol = as.factor(Drinks_Alcohol),\n         Financially_Dependent = as.factor(Financially_Dependent),\n         Cooks_at_Home = as.factor(Cooks_at_Home))\n\n\n\n\n# Convert binary columns to categorical dtype\ncols_to_convert = [\"Has_Job\", \"Drinks_Alcohol\", \"Financially_Dependent\", \"Cooks_at_Home\"]\ndata[cols_to_convert] = data[cols_to_convert].astype(\"category\")\n\n\n\n\nDetecting and Handling Outliers\nOutliers in continuous variables like Monthly_Earnings and Net_Money can distort the regression analysis by skewing results. We use the Interquartile Range (IQR) method to identify these extreme values. Specifically, any observation falling below 1.5 times the IQR below the first quartile (Q1) or above 1.5 times the IQR above the third quartile (Q3) is flagged as an outlier. These outliers are then treated as missing values and removed:\n\n\nR Code\nPython Code\n\n\n\n\n# Using IQR method to filter out extreme values in continuous variables\nremove_outliers &lt;- function(x) {\n  Q1 &lt;- quantile(x, 0.25, na.rm = TRUE)\n  Q3 &lt;- quantile(x, 0.75, na.rm = TRUE)\n  IQR &lt;- Q3 - Q1\n  x[x &lt; (Q1 - 1.5 * IQR) | x &gt; (Q3 + 1.5 * IQR)] &lt;- NA\n  return(x)\n}\n\ndata &lt;- data |&gt;\n  mutate(across(c(Monthly_Earnings, Net_Money), remove_outliers))\n\n# Remove rows with newly introduced NAs due to outlier handling\ndata &lt;- na.omit(data)\n\n\n\n\n# Define the IQR outlier-removal function\ndef remove_outliers(series):\n    Q1 = series.quantile(0.25)\n    Q3 = series.quantile(0.75)\n    IQR = Q3 - Q1\n    return series.where((series &gt;= Q1 - 1.5 * IQR) & (series &lt;= Q3 + 1.5 * IQR))\n\n# Apply to specific continuous columns\ndata[\"Monthly_Earnings\"] = remove_outliers(data[\"Monthly_Earnings\"])\ndata[\"Net_Money\"] = remove_outliers(data[\"Net_Money\"])\n\n# Drop rows with any newly introduced NAs (from outliers)\ndata = data.dropna()\n\n\n\n\nSplitting the Data for Model Training\nTo ensure that our OLS model generalizes well to unseen data, we split the dataset into training and testing subsets. The training set is used to estimate the model parameters, and the testing set is used to evaluate the model’s performance. This split is typically done in an 80/20 ratio, as shown below:\n\n\nR Code\nPython Code\n\n\n\n\n# Splitting the dataset by row order: first 80% for training, last 20% for testing\nn &lt;- nrow(data)\nsplit_index &lt;- floor(0.8 * n)\ntrain_data &lt;- data[1:split_index, ]\ntest_data &lt;- data[(split_index + 1):n, ]\n\n\n\n\n# Splitting the dataset by row order: first 80% for training, last 20% for testing\nn = len(data)\nsplit_index = int(0.8 * n)\ntrain_data = data.iloc[:split_index]\ntest_data = data.iloc[split_index:]\n\n\n\n\nAlthough random sampling is generally preferred, since it helps ensure the training and testing sets are representative of the overall dataset, we deliberately split the data by row index here to produce consistent results across R and Python. This allows for reproducible comparisons between implementations in both languages.\nBy following these steps, checking for missing values, encoding categorical variables, handling outliers, and splitting the data, we ensure that our dataset is clean, well-organized, and ready for regression analysis using OLS.\nIt’s important to note, however, that these are just a few of the many techniques available during the data wrangling stage. Depending on the dataset and the specific goals of your analysis, you might also consider additional strategies such as feature scaling, normalization, advanced feature engineering, handling duplicate records, or addressing imbalanced data. Each of these techniques comes with its own set of solutions, and the optimal approach will depend on the unique challenges and objectives of your case.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#the-logit-function",
    "href": "book/08-binary-logistic.html#the-logit-function",
    "title": "Binary Logistic Regression",
    "section": "\n8.3 The Logit Function",
    "text": "8.3 The Logit Function\nIn Section 8.2, we saw that modeling probability directly with a linear equation can produce impossible values (less than 0 or greater than 1) and ignores the way variance changes with the mean.\nThe fix is to apply a transformation that:\n\nExpands the (0, 1) probability range to the entire real line,\nIs reversible, so we can convert back to probabilities, and\nPreserves the ordering of probabilities.\n\nOne transformation that checks all these boxes is the logit function.\n\n8.3.1 Logit: A Link Between Probability and Linear Predictors\nThe logit function transforms the probability \\(\\pi_i\\) into a value on the entire real line: \\[\n\\text{logit}(\\pi_i) = \\log\\!\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\beta_0 + \\beta_1 X_i\n\\]\nThis transformation:\n\nIs monotonic (it preserves order),\nMaps probabilities \\(\\pi_i \\in (0, 1)\\) to \\((-\\infty, \\infty)\\), and\nSolves the range issue of OLS by letting us fit a linear model in the transformed space.\n\nBy modeling the log-odds as a linear function of predictors and then inverting the transformation, we obtain valid predicted probabilities that remain within \\([0,1]\\).\n\n\n\n\n\n\nNote\n\n\n\nThe logit function is defined as: \\[\n\\text{logit}(\\pi_i) = \\log\\!\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right)\n\\] It models the log-odds of the outcome as a linear function of predictors.\n\n\n\n8.3.2 From Log-Odds Back to Probability\nWhile the model calculates linearly on the log-odds scale, we ultimately want to understand the probability of the outcome (\\(\\pi_i\\)). To do this, we invert the logit function.\nThe inverse of the logit is the Sigmoid function (also called the logistic function):\n\\[\n\\pi_i = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_i)}}\n\\]\nThis function takes our linear prediction (\\(\\beta_0 + \\beta_1 X_i\\)), which can range from \\(-\\infty\\) to \\(+\\infty\\), and “squashes” it into the range \\((0, 1)\\). This mathematical trick ensures that no matter how extreme our predictor values get, our predicted probability never exceeds 1 or drops below 0.\n\n\n\n\n\nFigure 8.3: The Logistic Function showing the S-shape curve\n\n\n\n8.3.3 Understanding the “S” Shape\nThe sigmoid function is not just a boundary enforcer; it fundamentally changes how we interpret the relationship between \\(X\\) and \\(Y\\). Unlike a straight line, the sigmoid curve is non-linear.\n1. The Steep Middle (High Sensitivity) When the probability is near 0.5 (the “tipping point”), the curve is steepest. This means that small changes in the predictor \\(X\\) result in large changes in the probability \\(\\pi_i\\).\n\n\nContext: If a borrower is borderline (e.g., 50/50 chance of default), a small improvement in their credit score can drastically reduce their default risk.\n\n2. The Flat Tails (Diminishing Returns) As the probability approaches 0 or 1, the curve flattens out (asymptotes). In these regions, even large changes in \\(X\\) result in tiny changes in \\(\\pi_i\\).\n\n\nContext: If a borrower has a near-perfect credit score (risk \\(\\approx 0.01\\%\\)), increasing their score further makes almost no difference to their probability of default. The model recognizes that they are already “maxed out” on safety.\n\nThis captures a realistic property of many binary outcomes: interventions are most effective when the outcome is uncertain.\nThis transformation lets us use a linear predictor on the log-odds scale and then recover valid probabilities.\nThis idea forms the basis of binary logistic regression, introduced next.\n\n\n\n\n\n\nVisualizing the Difference\n\n\n\nIn OLS (Linear Regression), the slope is constant (\\(\\beta_1\\)). In Logistic Regression, the “slope” (rate of change in probability) changes constantly—it is steepest at \\(\\pi = 0.5\\) and approaches zero at the extremes.\n\n\n\n8.3.4 The Logistic Function and Its Shape (Code)\nThe following plot illustrates this behavior. Notice how the linear model (red dashed line) marches blindly past 0 and 1, while the logistic model (blue solid line) respects the boundaries and shows the characteristic “S” shape.\n\n\n\n\n\nFigure 8.4: The Sigmoid ‘S-Curve’ vs. Linear Fit. The logistic model (blue solid line) respects the [0,1] probability bounds and captures the non-linear “diminishing returns” at the extremes, whereas the linear model (red dashed line) extends indefinitely.\n\n\n\n\nR Code\nPython Code\n\n\n\n# Logistic vs Linear fit demo\nset.seed(1)\n# Grid of linear predictor values (η)\neta &lt;- seq(-4, 4, length.out = 400)\n\n# Logistic sigmoid: π(η)\npi  &lt;- 1 / (1 + exp(-eta))\n\n# Naive linear \"probability\" for comparison\n# passes through (0, 0.5) with moderate slope\np_lin &lt;- 0.5 + 0.25 * eta\n\ndf &lt;- data.frame(\n  eta    = eta,\n  sigmoid = pi,\n  linear  = p_lin\n)\n\nlibrary(ggplot2)\n\nggplot(df, aes(x = eta)) +\n  # Logistic S-curve\n  geom_line(aes(y = sigmoid), linewidth = 1.4, color = \"steelblue\") +\n  # Naive linear line\n  geom_line(aes(y = linear), linewidth = 1.1, linetype = \"dashed\",\n            color = \"#E74C3C\") +\n  # Vertical marker for steepest change near π ≈ 0.5\n  geom_segment(aes(x = 0, xend = 0,\n                   y = plogis(-0.7), yend = plogis(0.7)),\n               colour = \"gray40\", linewidth = 0.7) +\n  annotate(\"text\", x = 0.7, y = 0.55,\n           label = \"Steepest change\\nat π ≈ 0.5\",\n           size = 3.5, hjust = 0, color = \"gray30\") +\n  coord_cartesian(ylim = c(0, 1)) +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +\n  labs(\n    x = \"Linear predictor (η = β0 + β1 X)\",\n    y = \"Probability (π)\",\n    title = \"The Sigmoid 'S-Curve' vs. Linear Fit\",\n    subtitle = \"Logistic regression captures diminishing returns at the extremes\",\n    caption = \"Blue: Logistic Sigmoid (Valid Probabilities). Red: Naive Linear (Violates Bounds).\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 11, colour = \"gray40\")\n  )\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import expit  # logistic / sigmoid\n\nnp.random.seed(1)\n\n# Grid of linear predictor values (η)\neta = np.linspace(-4, 4, 400)\n\n# Logistic sigmoid π(η)\npi = expit(eta)\n\n# Naive linear \"probability\" (same as R: 0.5 + 0.25η)\np_lin = 0.5 + 0.25 * eta\n\nplt.figure(figsize=(8, 5))\n\n# Logistic S-curve\nplt.plot(eta, pi, label=\"Logistic Sigmoid (Valid $\\pi$)\",\n         linewidth=2.0, color=\"steelblue\")\n\n# Naive linear line\nplt.plot(eta, p_lin, \"--\", label=\"Naive Linear (Violates Bounds)\",\n         linewidth=1.5, color=\"#E74C3C\")\n\n# Vertical marker around steepest change near π ≈ 0.5\ny_lo, y_hi = expit(-0.7), expit(0.7)\nplt.plot([0, 0], [y_lo, y_hi], color=\"gray40\", linewidth=1.0)\nplt.text(0.7, 0.55, \"Steepest change\\nat $\\pi \\\\approx 0.5$\",\n         fontsize=10, color=\"gray30\", ha=\"left\", va=\"center\")\n\nplt.ylim(0, 1)\nplt.yticks(np.arange(0, 1.01, 0.25))\nplt.xlabel(r\"Linear predictor ($\\eta = \\beta_0 + \\beta_1 X$)\")\nplt.ylabel(r\"Probability ($\\pi$)\")\nplt.title(\"The Sigmoid 'S-Curve' vs. Linear Fit\")\nplt.grid(True, alpha=0.3)\nplt.legend(loc=\"lower right\")\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#python-code-1",
    "href": "book/08-binary-logistic.html#python-code-1",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.4 Python Code",
    "text": "8.4 Python Code\n\n# TODO: matplotlib plot of sigmoid and linear predictions\n\n:::",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#todo-1",
    "href": "book/08-binary-logistic.html#todo-1",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.16 8.12 TODO",
    "text": "8.16 8.12 TODO\n\nExplain results in plain language\nWhat do these findings suggest about financial behavior and default?\nWhat should a reader take away from this model?\n\n\n\n\n\n\n\nTip\n\n\n\nEffective storytelling connects statistical results to real-world meaning. Translate log-odds into “lenders are X times more likely to…” language.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# What is Ordinary Least Squares (OLS)?\n\n**Ordinary Least Squares (OLS)** is a fundamental method in regression analysis for estimating the relationship between a dependent variable and one or more independent variables. In simple terms, OLS is like drawing the best straight line through a scatterplot of data points. Imagine you plotted students' net savings on a graph, and each point represents a student’s financial outcome. OLS finds the line that best follows the trend of these points by minimizing the overall distance (error) between what the line predicts and what the actual data shows.\n\nOLS is widely used because it is:\n\n- **Simple**: Easy to understand and compute.\n- **Clear**: Provides straightforward numbers (coefficients) that tell you how much each factor influences the outcome.\n- **Versatile**: Applicable in many fields, from economics to social sciences, to help make informed decisions.\n\nIn this chapter, we will break down how OLS works in plain language, explore its underlying assumptions, and discuss its practical applications and limitations. This will give you a solid foundation in regression analysis, paving the way for more advanced techniques later on.\n\n## The \"Best Line\"\n\nWhen using Ordinary Least Squares (OLS) to fit a regression line, our goal is to find the line that best represents the relationship between our dependent variable $Y$ and independent variable $X$. But what does “best” mean?\n\nImagine you have a scatter plot of data points. Now, consider drawing two different lines through this plot. Each one of these lines represent a set of predictions. They also represent a way to represent the relationship between the dependent variable $Y$ and independent variable $X$\n\n- **Line A (Blue)**: A line that follows the general trend of the data very well.\n- **Line B (Red)**: A line that doesn’t capture the trend as accurately.\n\n::: {.panel-tabset}\n\n## R Code\n```{.r}\n# Sample data\nset.seed(42)\nX &lt;- c(1000, 1200, 1500, 1800, 2000)\nY &lt;- c(200, 230, 250, 290, 310)\n\n# Create a data frame\ndf &lt;- data.frame(Size = X, Price = Y)\n\n# Fit the correct OLS model\ncorrect_model &lt;- lm(Price ~ Size, data = df)\n\n# Create predictions for the two lines\ndf$Predicted_Correct &lt;- predict(correct_model, newdata = df)\ndf$Predicted_Wrong &lt;- 110 + 0.08 * df$Size  # Adjusted manually\n\n# Reshape data for ggplot (to add legend)\ndf_long &lt;- data.frame(\n  Size = rep(df$Size, 2),\n  Price = c(df$Predicted_Correct, df$Predicted_Wrong),\n  Line = rep(c(\"Line A (Best Fit)\", \"Line B (Worse Fit)\"), each = nrow(df))\n)\n\n# Store the plot with a legend\nlibrary(ggplot2)\nplot &lt;- ggplot() +\n  geom_point(data = df, aes(x = Size, y = Price), size = 3, color = \"black\") +\n  geom_line(data = df_long, aes(x = Size, y = Price, color = Line), linewidth = 1.2) +\n  scale_color_manual(values = c(\"Line A (Best Fit)\" = \"blue\", \"Line B (Worse Fit)\" = \"red\")) +\n  labs(title = \"Comparing Regression Line Fits\",\n       x = \"House Size (sq ft)\",\n       y = \"House Price (in $1000s)\",\n       color = \"Regression Line\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        legend.position = \"bottom\")\n\nplot",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#the-logit-function-1",
    "href": "book/08-binary-logistic.html#the-logit-function-1",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.4 The Logit Function",
    "text": "8.4 The Logit Function\nWhen modeling binary outcomes, our goal is to estimate the probability that an event occurs, such as whether a customer will default on a loan:\n\\[\nY_i \\sim \\text{Bernoulli}(p_i), \\quad \\mathbb{E}(Y_i) = p_i\n\\]\nIn the OLS framework, we treated this probability linearly, but as we’ve seen, this leads to predictions outside the \\([0,1]\\) range.\nSo how can we model the probability in a way that:\n\nKeeps predictions in \\([0, 1]\\),\nAllows us to use linear predictors, and\nProvides meaningful interpretations?\n\nWe do this by modeling the log-odds — the logit function.\nBy modeling the log-odds of the outcome as a linear function of predictors, we avoid OLS’s range problem and correctly capture how variance depends on the mean.\nWhen we combine this transformation with a Bernoulli likelihood, we arrive at the full binary logistic regression model, which we introduce next.\n\n8.4.1 Logit: A Link Between Probability and Linear Predictors\nThe logit function transforms the probability \\(p\\_i\\) into a value on the entire real line:\n\\[\n\\text{logit}(p_i) = \\log\\left(\\frac{p_i}{1 - p_i}\\right) = \\beta_0 + \\beta_1 X_i\n\\]\nThis transformation:\n\nIs monotonic (it preserves order),\nMaps probabilities \\(p\\_i \\in (0, 1)\\) to \\((-\\infty, \\infty)\\), and\nSolves the range issue of OLS.\n\n\n\n\n\n\n\nNote\n\n\n\nThe logit function is defined as:\n\\[\n\\text{logit}(p_i) = \\log\\left(\\frac{p_i}{1 - p_i}\\right)\n\\]\n\n\nIt models the log-odds of the outcome as a linear function of predictors.\n\n8.4.2 From Log-Odds Back to Probability\nWe can invert the logit function to get predicted probabilities:\n\\[\np_i = \\frac{\\exp\\left(\\beta_0 + \\beta_1 X_i\\right)}{1 + \\exp\\left(\\beta_0 + \\beta_1 X_i\\right)} \\in (0,1)\n\\]\nThis sigmoid function outputs values strictly between 0 and 1, no matter what \\(X\\_\\) is — making it ideal for modeling probabilities.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#storytelling-1",
    "href": "book/08-binary-logistic.html#storytelling-1",
    "title": "\n8  Binary Logistic Regression\n",
    "section": "\n8.16 Storytelling",
    "text": "8.16 Storytelling\nThe final step in any analysis is not running the model — it’s telling the story of the results. Logistic regression is often used in applied settings like medicine, psychology, and business, where the people reading your report are not statisticians. This means the way you present your findings is just as important as the model itself.\nWhen reporting odds ratios, it is easy to confuse your audience. For example, if we say, “The odds ratio is 2.5,” a non-technical reader might mistakenly think the probability doubled by 2.5. In reality, the change in probability depends on baseline odds. Good storytelling means translating odds ratios into plain language:\n\n“Holding other variables constant, a one-unit increase in credit score reduces the odds of defaulting by 40%.”\nOr, even better, “Increasing credit score by 50 points lowers the chance of default from about 30% to 20%.”\n\n\nVisuals also help. ROC curves, calibration plots, or side-by-side bar plots of predicted vs. actual outcomes let readers see what the model is doing. These plots tell a story that complements the numbers.\nFinally, storytelling means being honest about limitations: Does the model generalize? Did we face class imbalance? Were some predictors weak or unreliable? A clear story includes both strengths and caveats.\nTODOs for this section:\n\nAdd figure showing predicted probabilities vs. actual defaults.\nInsert plain-language examples of coefficient interpretation from the case study.\nAdd ROC curve plot for visual storytelling.\n\n\n\n\nR Code\nPython Code\n\n\n\n# Example: storytelling with predicted probabilities\nlibrary(ggplot2)\n\n# Fit logistic regression model\nlogit_model &lt;- glm(defaulted ~ credit_score + income,\n                   data = BLR, family = binomial)\n\n# Add predictions\nBLR$pred_prob &lt;- predict(logit_model, type = \"response\")\n\n# Plot predicted vs actual\nggplot(BLR, aes(x = credit_score, y = pred_prob, color = as.factor(defaulted))) +\n  geom_point(alpha = 0.5) +\n  labs(\n    title = \"Predicted Probability of Default vs Credit Score\",\n    x = \"Credit Score\",\n    y = \"Predicted Probability\",\n    color = \"Actual Default\"\n  ) +\n  theme_minimal()\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Logistic regression with statsmodels\ndf = r.data['BLR'].copy()\ndf['defaulted'] = df['defaulted'].astype(int)\n\nX = sm.add_constant(df[['credit_score','income']])\ny = df['defaulted']\n\nlogit_model = sm.Logit(y, X).fit()\ndf['pred_prob'] = logit_model.predict(X)\n\n# Plot\nfig, ax = plt.subplots(figsize=(6,4))\nscatter = ax.scatter(df['credit_score'], df['pred_prob'],\n                     c=df['defaulted'], cmap='coolwarm', alpha=0.6)\nax.set_title(\"Predicted Probability of Default vs Credit Score\", fontsize=14, fontweight='bold')\nax.set_xlabel(\"Credit Score\")\nax.set_ylabel(\"Predicted Probability\")\nlegend1 = ax.legend(*scatter.legend_elements(), title=\"Actual Default\")\nax.add_artist(legend1)\nplt.show()",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#model-diagnostics",
    "href": "book/08-binary-logistic.html#model-diagnostics",
    "title": "Binary Logistic Regression",
    "section": "\n8.16 Model Diagnostics",
    "text": "8.16 Model Diagnostics\nOnce we’ve estimated our logistic regression model, it’s important to check whether the model is well-specified and whether there are any problematic observations influencing the results. Diagnostics help us assess whether our predictions are trustworthy and whether model assumptions are being violated.\n\n8.16.1 Deviance Residuals\nIn logistic regression, we don’t have “raw residuals” like in OLS. Instead, we use deviance residuals, which measure how far off each predicted probability is from the actual outcome. Large residuals may indicate observations the model struggles to predict — for instance, a borrower with a very high credit score who still defaulted.\nPlotting deviance residuals can help detect such outliers.\n\n8.16.2 Binned Residual Plots\nAnother way to check fit is with binned residual plots. Here, predicted probabilities are grouped (binned), and we compare average predicted probabilities with observed default rates in each bin. A well-calibrated model should show points lying close to the diagonal line (predicted = observed).\nFor our loan dataset, if the model predicts a 20% default rate for customers in a bin, then about 20% of those customers should actually have defaulted.\n\n8.16.3 Detecting Influential Points\nFinally, some individual cases may exert outsized influence on the model — often measured using statistics like Cook’s distance or leverage. For example, a single customer with an unusually low credit score but very high income may skew the coefficient estimates. Identifying such cases ensures that no single observation is disproportionately driving conclusions.\n\nTODOs for this section:\n\nAdd plot of deviance residuals vs. predicted probabilities.\nAdd binned residual plot for calibration.\nAdd influence plot (highlighting high-leverage or influential cases).\nProvide a short interpretation using the loan default dataset.\n\n\n\n\nR Code\nPython Code\n\n\n\n# Logistic regression model\nlogit_model &lt;- glm(defaulted ~ credit_score + income,\n                   data = BLR, family = binomial)\n\n# Deviance residuals\nresiduals_dev &lt;- residuals(logit_model, type = \"deviance\")\n\n# Plot deviance residuals vs. predicted probabilities\nBLR$pred_prob &lt;- predict(logit_model, type = \"response\")\nplot(BLR$pred_prob, residuals_dev,\n     xlab = \"Predicted Probability\",\n     ylab = \"Deviance Residuals\",\n     main = \"Deviance Residuals vs Predicted Probability\")\nabline(h = 0, col = \"red\", lty = 2)\n\n# Binned residual plot (using arm package)\nlibrary(arm)\nbinnedplot(BLR$pred_prob, residuals_dev,\n           xlab = \"Predicted Probability\",\n           ylab = \"Average Residual\",\n           main = \"Binned Residual Plot\")\n\n# Influence measures\ninfluence_measures &lt;- influence.measures(logit_model)\nsummary(influence_measures)\n\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Logistic regression\ndf = r.data['BLR'].copy()\ndf['defaulted'] = df['defaulted'].astype(int)\nX = sm.add_constant(df[['credit_score','income']])\ny = df['defaulted']\nlogit_model = sm.Logit(y, X).fit()\ndf['pred_prob'] = logit_model.predict(X)\n\n# Deviance residuals (via statsmodels residuals)\nresid_dev = logit_model.resid_dev\n\n# Plot deviance residuals vs predicted probabilities\nplt.scatter(df['pred_prob'], resid_dev, alpha=0.6)\nplt.axhline(0, color='red', linestyle='--')\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"Deviance Residuals\")\nplt.title(\"Deviance Residuals vs Predicted Probability\")\nplt.show()\n\n# Influence plot\nsm.graphics.influence_plot(logit_model, criterion=\"cooks\")\nplt.show()\n\n\n\n\n\nR Code\nPython Code\n\n\n\n\nlibrary(arm)  # For binned residual plots\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoading required package: lme4\n\n\n\narm (Version 1.14-4, built: 2024-4-1)\n\n\nWorking directory is C:/Users/aviv/Documents/GitHub/regression-cookbook/book\n\n\n\nAttaching package: 'arm'\n\n\nThe following object is masked from 'package:car':\n\n    logit\n\nlibrary(ggplot2)\n\n# Fit the logistic regression model\nlogit_model &lt;- glm(defaulted ~ credit_score + income,\n                   data = BLR, family = binomial)\n\n# Add predictions to dataset\nBLR$pred_prob &lt;- predict(logit_model, type = \"response\")\nBLR$pred_logit &lt;- predict(logit_model, type = \"link\")\n\n# ===================================\n# 1. DEVIANCE RESIDUALS\n# ===================================\nresiduals_dev &lt;- residuals(logit_model, type = \"deviance\")\nresiduals_pearson &lt;- residuals(logit_model, type = \"pearson\")\n\n# Plot deviance residuals vs predicted probabilities\npar(mfrow = c(2, 2))\n\n# Plot 1: Deviance residuals vs predicted probability\nplot(BLR$pred_prob, residuals_dev,\n     xlab = \"Predicted Probability\",\n     ylab = \"Deviance Residuals\",\n     main = \"Deviance Residuals vs Predicted Probability\",\n     pch = 16, col = rgb(0, 0, 0, 0.3))\nabline(h = 0, col = \"red\", lty = 2, lwd = 2)\nabline(h = c(-2, 2), col = \"orange\", lty = 3)\nlegend(\"topright\", legend = c(\"Reference\", \"±2 SD\"),\n       col = c(\"red\", \"orange\"), lty = c(2, 3), cex = 0.8)\n\n# Plot 2: Deviance residuals vs linear predictor\nplot(BLR$pred_logit, residuals_dev,\n     xlab = \"Linear Predictor (log-odds)\",\n     ylab = \"Deviance Residuals\",\n     main = \"Deviance Residuals vs Linear Predictor\",\n     pch = 16, col = rgb(0, 0, 0, 0.3))\nabline(h = 0, col = \"red\", lty = 2, lwd = 2)\nabline(h = c(-2, 2), col = \"orange\", lty = 3)\n\n# ===================================\n# 2. BINNED RESIDUAL PLOT\n# ===================================\n# Using the arm package\nbinnedplot(BLR$pred_prob, residuals_dev,\n           xlab = \"Predicted Probability\",\n           ylab = \"Average Residual\",\n           main = \"Binned Residual Plot\",\n           col.int = \"gray70\")\n\n# ===================================\n# 3. INFLUENTIAL POINTS\n# ===================================\n# Calculate influence measures\ninfluence_stats &lt;- influence.measures(logit_model)\n\n# Cook's Distance\ncooks_d &lt;- cooks.distance(logit_model)\n\nplot(cooks_d, type = \"h\",\n     ylab = \"Cook's Distance\",\n     xlab = \"Observation Index\",\n     main = \"Cook's Distance: Influential Points\",\n     col = ifelse(cooks_d &gt; 4/length(cooks_d), \"red\", \"gray\"))\nabline(h = 4/length(cooks_d), col = \"red\", lty = 2)\n# Identify influential points\ninfluential &lt;- which(cooks_d &gt; 4/length(cooks_d))\n\n# Only add labels if there are influential points\nif (length(influential) &gt; 0) {\n  text(x = influential,\n       y = cooks_d[influential],\n       labels = influential,\n       pos = 3, cex = 0.7)\n}\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n# ===================================\n# 4. GGPLOT VERSION\n# ===================================\n# Create a diagnostic dataframe\ndiag_df &lt;- data.frame(\n  pred_prob = BLR$pred_prob,\n  residuals_dev = residuals_dev,\n  cooks_d = cooks_d,\n  index = 1:nrow(BLR)\n)\n\n# Deviance residuals plot\np1 &lt;- ggplot(diag_df, aes(x = pred_prob, y = residuals_dev)) +\n  geom_point(alpha = 0.3) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_hline(yintercept = c(-2, 2), color = \"orange\", linetype = \"dotted\") +\n  labs(title = \"Deviance Residuals vs Predicted Probability\",\n       x = \"Predicted Probability\",\n       y = \"Deviance Residuals\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n# Cook's distance plot\np2 &lt;- ggplot(diag_df, aes(x = index, y = cooks_d)) +\n  geom_segment(aes(xend = index, yend = 0), \n               color = ifelse(diag_df$cooks_d &gt; 4/nrow(diag_df), \"red\", \"gray\")) +\n  geom_hline(yintercept = 4/nrow(diag_df), color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Cook's Distance: Identifying Influential Points\",\n       x = \"Observation Index\",\n       y = \"Cook's Distance\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\nprint(p1)\n\n\n\n\n\n\nprint(p2)\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Prepare data\ndf = r.BLR.copy()\ndf['defaulted'] = df['defaulted'].astype(int)\n\n# Fit logistic regression\nX = sm.add_constant(df[['credit_score', 'income']])\ny = df['defaulted']\nlogit_model = sm.Logit(y, X).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.366466\n         Iterations 7\n\n# Get predictions and residuals\ndf['pred_prob'] = logit_model.predict(X)\ndf['pred_logit'] = logit_model.predict(X, linear=True)\n\nC:\\Users\\aviv\\AppData\\Local\\R\\cache\\R\\RETICU~1\\uv\\cache\\ARCHIV~1\\9JLF1J~1\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:530: FutureWarning: linear keyword is deprecated, use which=\"linear\"\n  warnings.warn(msg, FutureWarning)\n\nresid_dev = logit_model.resid_dev\nresid_pearson = logit_model.resid_pearson  # From model, not influence object\n\n# ===================================\n# 1. DEVIANCE RESIDUALS PLOTS\n# ===================================\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Plot 1: Deviance residuals vs predicted probability\naxes[0, 0].scatter(df['pred_prob'], resid_dev, alpha=0.3, color='black')\naxes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\naxes[0, 0].axhline(y=2, color='orange', linestyle=':', linewidth=1)\naxes[0, 0].axhline(y=-2, color='orange', linestyle=':', linewidth=1)\naxes[0, 0].set_xlabel('Predicted Probability')\naxes[0, 0].set_ylabel('Deviance Residuals')\naxes[0, 0].set_title('Deviance Residuals vs Predicted Probability', fontweight='bold')\naxes[0, 0].grid(alpha=0.3)\n\n# Plot 2: Deviance residuals vs linear predictor\naxes[0, 1].scatter(df['pred_logit'], resid_dev, alpha=0.3, color='black')\naxes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\naxes[0, 1].axhline(y=2, color='orange', linestyle=':', linewidth=1)\naxes[0, 1].axhline(y=-2, color='orange', linestyle=':', linewidth=1)\naxes[0, 1].set_xlabel('Linear Predictor (log-odds)')\naxes[0, 1].set_ylabel('Deviance Residuals')\naxes[0, 1].set_title('Deviance Residuals vs Linear Predictor', fontweight='bold')\naxes[0, 1].grid(alpha=0.3)\n\n# ===================================\n# 2. BINNED RESIDUAL PLOT\n# ===================================\n# Create bins for predicted probabilities\nn_bins = 20\ndf_sorted = df.sort_values('pred_prob')\ndf_sorted['bin'] = pd.cut(df_sorted['pred_prob'], bins=n_bins, labels=False)\n\n# Calculate average residual per bin\nbinned_resids = df_sorted.groupby('bin').agg({\n    'pred_prob': 'mean',\n    'defaulted': lambda x: np.mean(x - logit_model.predict(X.loc[x.index]))\n}).reset_index()\n\n# Calculate standard error bounds\nse = 2 * np.sqrt(binned_resids['defaulted'].var() / len(binned_resids))\n\naxes[1, 0].scatter(binned_resids['pred_prob'], binned_resids['defaulted'], \n                   color='black', s=50, zorder=3)\naxes[1, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\naxes[1, 0].axhline(y=se, color='gray', linestyle=':', linewidth=1, alpha=0.7)\naxes[1, 0].axhline(y=-se, color='gray', linestyle=':', linewidth=1, alpha=0.7)\naxes[1, 0].fill_between([0, 1], -se, se, color='gray', alpha=0.2)\naxes[1, 0].set_xlabel('Predicted Probability (Binned)')\naxes[1, 0].set_ylabel('Average Residual')\naxes[1, 0].set_title('Binned Residual Plot', fontweight='bold')\naxes[1, 0].set_xlim(0, 1)\n\n(0.0, 1.0)\n\naxes[1, 0].grid(alpha=0.3)\n\n# ===================================\n# 3. COOK'S DISTANCE\n# ===================================\ninfluence = logit_model.get_influence()\ncooks_d = influence.cooks_distance[0]\n\n# Plot Cook's distance\naxes[1, 1].stem(range(len(cooks_d)), cooks_d, \n                linefmt='gray', markerfmt='o', basefmt=' ')\nthreshold = 4 / len(cooks_d)\naxes[1, 1].axhline(y=threshold, color='red', linestyle='--', linewidth=2)\n\n# Highlight influential points\ninfluential = np.where(cooks_d &gt; threshold)[0]\nif len(influential) &gt; 0:\n    axes[1, 1].stem(influential, cooks_d[influential],\n                    linefmt='red', markerfmt='ro', basefmt=' ')\n\naxes[1, 1].set_xlabel('Observation Index')\naxes[1, 1].set_ylabel(\"Cook's Distance\")\naxes[1, 1].set_title(\"Cook's Distance: Influential Points\", fontweight='bold')\naxes[1, 1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n# ===================================\n# 4. INFLUENCE PLOT\n# ===================================\n# Get influence measures\ninfluence = logit_model.get_influence()\ncooks_d = influence.cooks_distance[0]\nleverage = influence.hat_matrix_diag\n\n# Use residuals from MODEL not influence object\nresid_standardized = resid_pearson  # Already defined from logit_model above\n\n# Create the influence plot manually\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Bubble plot: x=leverage, y=residuals, size=Cook's distance\nbubble_size = cooks_d * 1000  # Scale for visibility\n\nscatter = ax.scatter(leverage, resid_standardized, \n                     s=bubble_size, alpha=0.5, c=cooks_d, \n                     cmap='Reds', edgecolors='black', linewidth=0.5)\n\n# Add colorbar to show Cook's distance\ncbar = plt.colorbar(scatter, ax=ax)\ncbar.set_label(\"Cook's Distance\", rotation=270, labelpad=20)\n\n# Add reference lines\nax.axhline(y=0, color='gray', linestyle='--', linewidth=1, alpha=0.5)\nax.axhline(y=2, color='red', linestyle=':', linewidth=1, alpha=0.5)\nax.axhline(y=-2, color='red', linestyle=':', linewidth=1, alpha=0.5)\n\n# Label most influential points\nthreshold = 4 / len(cooks_d)\ninfluential_idx = np.where(cooks_d &gt; threshold)[0]\n\nif len(influential_idx) &gt; 0:\n    for idx in influential_idx[:5]:  # Only label top 5\n        ax.annotate(str(idx), \n                   xy=(leverage[idx], resid_standardized[idx]),\n                   xytext=(5, 5), textcoords='offset points',\n                   fontsize=8, alpha=0.7)\n\nax.set_xlabel('Leverage', fontsize=11)\nax.set_ylabel('Standardized Residuals', fontsize=11)\nax.set_title(\"Influence Plot: Bubble size indicates Cook's Distance\", \n             fontweight='bold', fontsize=12)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n# Print summary of influential points\nif len(influential_idx) &gt; 0:\n    print(\"\\n\" + \"=\"*50)\n    print(\"Influential Observations\")\n    print(\"=\"*50)\n    print(f\"Number of influential points: {len(influential_idx)}\")\n    print(f\"Indices: {influential_idx}\")\n\n\n==================================================\nInfluential Observations\n==================================================\nNumber of influential points: 55\nIndices: [ 37  39  43  58  77  82  83 105 110 143 147 156 164 177 184 240 283 302\n 308 329 383 386 402 411 419 425 429 442 466 493 495 503 514 545 573 581\n 587 628 644 670 685 693 711 730 766 779 781 813 819 821 853 884 924 946\n 967]\n\n\n\n8.16.4 Interpreting the Diagnostics for the Loan Default Model\nFor our loan default dataset, the diagnostics suggest that the logistic model with credit score and income is reasonably well specified:\n\nThe deviance residual plots show most points clustered between about −2 and +2, with no obvious pattern as a function of predicted probability or the linear predictor. This suggests that the model is not systematically over- or under-predicting in any particular region.\nIn the binned residual plot, most bins lie close to the horizontal line at 0 and within the uncertainty band. That indicates the model is well calibrated: where it predicts, say, a 20–30% default probability, the observed default rate is roughly in that range.\nThe Cook’s distance / influence plots highlight a small number of observations with slightly higher influence, but no single point dominates the fit. These cases are worth inspecting (e.g., unusually low credit score but high income), yet they do not invalidate the overall model.\n\nOverall, these diagnostics give us confidence that the logistic regression model is capturing the main structure in the data and that its predictions are trustworthy for most borrowers.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#predictions",
    "href": "book/08-binary-logistic.html#predictions",
    "title": "Binary Logistic Regression",
    "section": "\n8.14 Predictions",
    "text": "8.14 Predictions\nOnce we’ve fit a logistic regression model, we can use it to generate predicted probabilities of default for each customer. These probabilities fall between 0 and 1 and tell us how likely the model thinks it is that a customer will default given their predictors.\n\n8.14.1 Predicted Probabilities vs. Predicted Classes\nPredicted probabilities can be turned into class predictions (default vs. no default) by applying a threshold, usually 0.5. Customers with probability ≥ 0.5 are classified as “default,” and those below as “no default.”\nBut in practice, the choice of threshold matters. If we lower the threshold to 0.3, we’ll catch more actual defaulters (higher sensitivity) but at the cost of more false alarms (lower specificity).\n\n8.14.2 Evaluating Performance\nTo judge prediction quality, we use metrics such as:\n\n\nAccuracy: proportion of correct predictions.\n\nSensitivity (recall): proportion of true defaults correctly identified.\n\nSpecificity: proportion of true non-defaults correctly identified.\n\nROC curve & AUC: performance across all thresholds, not just one.\n\nFor our loan dataset, we might find that the model predicts non-defaults very well (high specificity) but misses some defaults (lower sensitivity). This trade-off is a central theme in logistic regression applications.\nIn the code below, we compute a confusion matrix at a 0.5 threshold and plot the ROC curve with its AUC for our loan default model.\n\n\n\nR Code\nPython Code\n\n\n\n# Logistic regression model\nlogit_model &lt;- glm(defaulted ~ credit_score + income,\n                   data = BLR, family = binomial)\n\nlibrary(dplyr)\nlibrary(yardstick)\nlibrary(ggplot2)\n\n# Add predicted probabilities and classes to BLR\nBLR &lt;- BLR %&gt;%\n  mutate(\n    pred_prob  = predict(logit_model, type = \"response\"),\n    pred_class = if_else(pred_prob &gt;= 0.5, 1, 0)\n  )\n\n# Build results tibble for yardstick\nresults &lt;- BLR %&gt;%\n  transmute(\n    truth     = factor(defaulted, levels = c(0, 1)),\n    predicted = factor(pred_class, levels = c(0, 1)),\n    pred_prob = pred_prob\n  )\n\n# -----------------------------\n# 1. CONFUSION MATRIX & METRICS\n# -----------------------------\ncm &lt;- conf_mat(results, truth, predicted)\ncm          # 2x2 confusion matrix\n\n# Accuracy, sensitivity, specificity, precision, F1\nmetrics &lt;- metric_set(accuracy, sens, spec, precision, f_meas)\nmetrics(results, truth, predicted)\n\n# -----------------------------\n# 2. ROC CURVE & AUC\n# -----------------------------\nroc_df  &lt;- roc_curve(results, truth, pred_prob)\nroc_auc &lt;- roc_auc(results, truth, pred_prob)\nroc_auc  # prints the AUC value\n\nautoplot(roc_df) +\n  labs(\n    title = \"ROC Curve for Loan Default Model\",\n    subtitle = \"Area under the curve (AUC) summarizes performance across thresholds\"\n  )\n\n\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\ndf = r.data['BLR'].copy()\ndf['defaulted'] = df['defaulted'].astype(int)\n\nX = sm.add_constant(df[['credit_score','income']])\ny = df['defaulted']\nlogit_model = sm.Logit(y, X).fit()\ndf['pred_prob'] = logit_model.predict(X)\ndf['pred_class'] = (df['pred_prob'] &gt; 0.5).astype(int)\n\n# Confusion matrix & metrics\nprint(confusion_matrix(y, df['pred_class']))\nprint(classification_report(y, df['pred_class']))\n\n# ROC curve\nfpr, tpr, thresholds = roc_curve(y, df['pred_prob'])\nroc_auc = auc(fpr, tpr)\n\nplt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\nplt.plot([0,1], [0,1], linestyle=\"--\", color=\"grey\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve for Loan Default Model\")\nplt.legend()\nplt.show()\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n          Truth\nPrediction   0   1\n         0 657 106\n         1  66 171\n\n\n# A tibble: 5 × 3\n  .metric   .estimator .estimate\n  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy  binary         0.828\n2 sens      binary         0.909\n3 spec      binary         0.617\n4 precision binary         0.861\n5 f_meas    binary         0.884\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.109\n\n\n\n\n\n\nConfusion matrix (threshold 0.5):\n\n\n[[657  66]\n [106 171]]\n\n\n\nClassification report:\n\n\n              precision    recall  f1-score   support\n\n           0       0.86      0.91      0.88       723\n           1       0.72      0.62      0.67       277\n\n    accuracy                           0.83      1000\n   macro avg       0.79      0.76      0.77      1000\nweighted avg       0.82      0.83      0.82      1000\n\n\n\nAUC: 0.891",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#goodness-of-fit-model-selection",
    "href": "book/08-binary-logistic.html#goodness-of-fit-model-selection",
    "title": "Binary Logistic Regression",
    "section": "\n8.15 Goodness of Fit & Model Selection",
    "text": "8.15 Goodness of Fit & Model Selection\nEvaluating whether our model is a “good” fit is just as important as making predictions. For logistic regression, the diagnostics differ from OLS.\n\n8.15.1 Pseudo R-Squared Measures\nBecause we don’t have the same notion of variance explained as in OLS, we use pseudo R² measures (e.g., McFadden’s R²). These are useful for comparison, but don’t carry the same interpretation as R² in linear regression.\n\n8.15.2 Analysis of Deviance\nWe can compare models using the deviance statistic, which measures how well the model fits relative to a saturated model. Lower deviance indicates better fit. Nested models (e.g., one with credit_score only vs. one with credit_score + income) can be compared using a likelihood ratio test.\n\n8.15.3 Information Criteria\nAnother approach is to use information criteria such as:\n\nAIC (Akaike Information Criterion)\nBIC (Bayesian Information Criterion)\n\nBoth balance fit and complexity: lower AIC or BIC means a better trade-off. AIC tends to favor more complex models; BIC penalizes complexity more heavily.\nFor the loan default dataset, the diagnostics tell a clear story. The model with credit score + income (Model 2) fits substantially better than the credit-score-only model (Model 1): the likelihood ratio test is highly significant, and both AIC and BIC are lower for Model 2.\nAdding education_years (Model 3) yields only a very small additional improvement in fit. AIC changes little and BIC actually prefers the simpler two-predictor model, since it penalizes extra parameters more heavily. In practice, we would typically choose Model 2 as a good balance between predictive performance and parsimony.\n\n\n\nR Code\nPython Code\n\n\n\n# Fit models\nmodel1 &lt;- glm(defaulted ~ credit_score, data = BLR, family = binomial)\nmodel2 &lt;- glm(defaulted ~ credit_score + income, data = BLR, family = binomial)\n\n# Compare deviance (likelihood ratio test)\nanova(model1, model2, test = \"Chisq\")\n\n# Pseudo R-squared\nlibrary(pscl)\npR2(model2)\n\n# AIC and BIC\nAIC(model1, model2)\nBIC(model1, model2)\n\n\nimport statsmodels.api as sm\n\ndf = r.data['BLR'].copy()\ndf['defaulted'] = df['defaulted'].astype(int)\n\n# Model 1: credit score only\nX1 = sm.add_constant(df[['credit_score']])\nmodel1 = sm.Logit(df['defaulted'], X1).fit()\n\n# Model 2: credit score + income\nX2 = sm.add_constant(df[['credit_score','income']])\nmodel2 = sm.Logit(df['defaulted'], X2).fit()\n\n# Likelihood ratio test\nLR_stat = 2 * (model2.llf - model1.llf)\ndf_diff = model2.df_model - model1.df_model\nfrom scipy.stats import chi2\np_value = chi2.sf(LR_stat, df_diff)\n\nprint(\"Likelihood Ratio Test:\", LR_stat, \"df:\", df_diff, \"p:\", p_value)\n\n# AIC & BIC\nprint(\"Model 1 AIC/BIC:\", model1.aic, model1.bic)\nprint(\"Model 2 AIC/BIC:\", model2.aic, model2.bic)\n\n\n\n\n\nR Code\nPython Code\n\n\n\n\nlibrary(pscl)  # For pseudo R-squared\n\nClasses and Methods for R originally developed in the\nPolitical Science Computational Laboratory\nDepartment of Political Science\nStanford University (2002-2015),\nby and under the direction of Simon Jackman.\nhurdle and zeroinfl functions by Achim Zeileis.\n\nlibrary(broom)\n\n# Fit models for comparison\nmodel1 &lt;- glm(defaulted ~ credit_score, data = BLR, family = binomial)\nmodel2 &lt;- glm(defaulted ~ credit_score + income, data = BLR, family = binomial)\nmodel3 &lt;- glm(defaulted ~ credit_score + income + education_years, \n              data = BLR, family = binomial)\n\n# ===================================\n# 1. PSEUDO R-SQUARED MEASURES\n# ===================================\ncat(\"\\n=== Pseudo R-Squared Measures ===\\n\")\n\n\n=== Pseudo R-Squared Measures ===\n\npr2_model2 &lt;- pR2(model2)\n\nfitting null model for pseudo-r2\n\nprint(pr2_model2)\n\n         llh      llhNull           G2     McFadden         r2ML         r2CU \n-366.4661306 -590.0975621  447.2628631    0.3789737    0.3606242    0.5205456 \n\n# ===================================\n# 2. DEVIANCE COMPARISON (Analysis of Deviance)\n# ===================================\ncat(\"\\n=== Analysis of Deviance ===\\n\")\n\n\n=== Analysis of Deviance ===\n\nanova(model1, model2, model3, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: defaulted ~ credit_score\nModel 2: defaulted ~ credit_score + income\nModel 3: defaulted ~ credit_score + income + education_years\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       998     787.73                          \n2       997     732.93  1    54.80 1.334e-13 ***\n3       996     730.30  1     2.63    0.1049    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# ===================================\n# 3. INFORMATION CRITERIA (AIC & BIC)\n# ===================================\ncat(\"\\n=== Model Comparison: AIC and BIC ===\\n\")\n\n\n=== Model Comparison: AIC and BIC ===\n\n# Create comparison table\nmodel_comparison &lt;- data.frame(\n  Model = c(\"Model 1: credit_score only\",\n            \"Model 2: + income\",\n            \"Model 3: + education_years\"),\n  AIC = c(AIC(model1), AIC(model2), AIC(model3)),\n  BIC = c(BIC(model1), BIC(model2), BIC(model3)),\n  Deviance = c(deviance(model1), deviance(model2), deviance(model3)),\n  Df = c(model1$df.residual, model2$df.residual, model3$df.residual)\n)\n\nknitr::kable(model_comparison, digits = 2,\n             caption = \"Model Comparison: AIC, BIC, and Deviance\")\n\n\nModel Comparison: AIC, BIC, and Deviance\n\nModel\nAIC\nBIC\nDeviance\nDf\n\n\n\nModel 1: credit_score only\n791.73\n801.55\n787.73\n998\n\n\nModel 2: + income\n738.93\n753.66\n732.93\n997\n\n\nModel 3: + education_years\n738.30\n757.93\n730.30\n996\n\n\n\n\n# Find best model by each criterion\ncat(\"\\nBest model by AIC:\", model_comparison$Model[which.min(model_comparison$AIC)])\n\n\nBest model by AIC: Model 3: + education_years\n\ncat(\"\\nBest model by BIC:\", model_comparison$Model[which.min(model_comparison$BIC)])\n\n\nBest model by BIC: Model 2: + income\n\n# ===================================\n# 4. VISUALIZATION\n# ===================================\nlibrary(ggplot2)\n\n# Reshape for plotting\ncomparison_long &lt;- model_comparison %&gt;%\n  tidyr::pivot_longer(cols = c(AIC, BIC), \n                      names_to = \"Criterion\", \n                      values_to = \"Value\")\n\nggplot(comparison_long, aes(x = Model, y = Value, fill = Criterion)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_fill_manual(values = c(\"AIC\" = \"#3498db\", \"BIC\" = \"#e74c3c\")) +\n  labs(title = \"Model Comparison: AIC vs BIC\",\n       subtitle = \"Lower values indicate better model fit\",\n       x = NULL, y = \"Information Criterion Value\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 15, hjust = 1),\n        plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2\n\n\n# Prepare data\ndf = r.BLR.copy()\ndf['defaulted'] = df['defaulted'].astype(int)\n\n# Fit three models for comparison\n# Model 1: credit_score only\nX1 = sm.add_constant(df[['credit_score']])\nmodel1 = sm.Logit(df['defaulted'], X1).fit(disp=0)\n\n# Model 2: credit_score + income\nX2 = sm.add_constant(df[['credit_score', 'income']])\nmodel2 = sm.Logit(df['defaulted'], X2).fit(disp=0)\n\n# Model 3: credit_score + income + education_years\nX3 = sm.add_constant(df[['credit_score', 'income', 'education_years']])\nmodel3 = sm.Logit(df['defaulted'], X3).fit(disp=0)\n\n# ===================================\n# 1. PSEUDO R-SQUARED MEASURES\n# ===================================\nprint(\"\\n\" + \"=\"*50)\n\n\n==================================================\n\nprint(\"Pseudo R-Squared Measures (Model 2)\")\n\nPseudo R-Squared Measures (Model 2)\n\nprint(\"=\"*50)\n\n==================================================\n\nprint(f\"McFadden's R²: {model2.prsquared:.4f}\")\n\nMcFadden's R²: 0.3790\n\nprint(f\"Log-Likelihood: {model2.llf:.2f}\")\n\nLog-Likelihood: -366.47\n\nprint(f\"Null Log-Likelihood: {model2.llnull:.2f}\")\n\nNull Log-Likelihood: -590.10\n\n# ===================================\n# 2. LIKELIHOOD RATIO TESTS\n# ===================================\nprint(\"\\n\" + \"=\"*50)\n\n\n==================================================\n\nprint(\"Likelihood Ratio Tests\")\n\nLikelihood Ratio Tests\n\nprint(\"=\"*50)\n\n==================================================\n\n# Model 1 vs Model 2\nlr_12 = 2 * (model2.llf - model1.llf)\ndf_12 = model2.df_model - model1.df_model\np_12 = chi2.sf(lr_12, df_12)\n\nprint(f\"\\nModel 1 vs Model 2:\")\n\n\nModel 1 vs Model 2:\n\nprint(f\"  LR Statistic: {lr_12:.4f}, df: {df_12}, p-value: {p_12:.4f}\")\n\n  LR Statistic: 54.8003, df: 1.0, p-value: 0.0000\n\n# Model 2 vs Model 3\nlr_23 = 2 * (model3.llf - model2.llf)\ndf_23 = model3.df_model - model2.df_model\np_23 = chi2.sf(lr_23, df_23)\n\nprint(f\"\\nModel 2 vs Model 3:\")\n\n\nModel 2 vs Model 3:\n\nprint(f\"  LR Statistic: {lr_23:.4f}, df: {df_23}, p-value: {p_23:.4f}\")\n\n  LR Statistic: 2.6295, df: 1.0, p-value: 0.1049\n\n# ===================================\n# 3. INFORMATION CRITERIA (AIC & BIC)\n# ===================================\nprint(\"\\n\" + \"=\"*50)\n\n\n==================================================\n\nprint(\"Model Comparison: AIC and BIC\")\n\nModel Comparison: AIC and BIC\n\nprint(\"=\"*50)\n\n==================================================\n\ncomparison = pd.DataFrame({\n    'Model': ['Model 1: credit_score only',\n              'Model 2: + income',\n              'Model 3: + education_years'],\n    'AIC': [model1.aic, model2.aic, model3.aic],\n    'BIC': [model1.bic, model2.bic, model3.bic],\n    'Pseudo R²': [model1.prsquared, model2.prsquared, model3.prsquared]\n})\n\nprint(comparison.round(2))\n\n                        Model     AIC     BIC  Pseudo R²\n0  Model 1: credit_score only  791.73  801.55       0.33\n1           Model 2: + income  738.93  753.66       0.38\n2  Model 3: + education_years  738.30  757.93       0.38\n\nprint(f\"\\nBest model by AIC: {comparison.loc[comparison['AIC'].idxmin(), 'Model']}\")\n\n\nBest model by AIC: Model 3: + education_years\n\nprint(f\"Best model by BIC: {comparison.loc[comparison['BIC'].idxmin(), 'Model']}\")\n\nBest model by BIC: Model 2: + income\n\n# ===================================\n# 4. VISUALIZATION\n# ===================================\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# AIC comparison\nax1.bar(range(3), comparison['AIC'], color='#3498db', alpha=0.7)\nax1.set_xticks(range(3))\nax1.set_xticklabels(['Model 1', 'Model 2', 'Model 3'])\nax1.set_ylabel('AIC')\nax1.set_title('AIC Comparison\\n(Lower is Better)', fontweight='bold')\nax1.grid(axis='y', alpha=0.3)\n\n# BIC comparison\nax2.bar(range(3), comparison['BIC'], color='#e74c3c', alpha=0.7)\nax2.set_xticks(range(3))\nax2.set_xticklabels(['Model 1', 'Model 2', 'Model 3'])\nax2.set_ylabel('BIC')\nax2.set_title('BIC Comparison\\n(Lower is Better)', fontweight='bold')\nax2.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#inference",
    "href": "book/08-binary-logistic.html#inference",
    "title": "Binary Logistic Regression",
    "section": "\n8.12 Inference",
    "text": "8.12 Inference\nAfter estimating a logistic regression model, we often want to know whether predictors are statistically significant — i.e., whether they have a meaningful relationship with the probability of default. In logistic regression, inference is based on the likelihood framework.\n\n8.12.1 Wald Tests\nThe Wald test checks whether an individual coefficient is significantly different from zero. For example, we can test whether credit_score has a nonzero effect on the odds of default. The test statistic is the ratio of the estimated coefficient to its standard error. In our loan-default example, the model summary below reports Wald tests for both credit_score and income.\n\n8.12.2 Likelihood Ratio Tests\nWe can also compare nested models (e.g., model with credit_score only vs. model with credit_score + income) using the likelihood ratio (LR) test. This evaluates whether adding predictors significantly improves model fit. The R and Python output below show an LR test comparing a model with only credit_score to a model with both credit_score and income.\n\n8.12.3 Confidence Intervals\nFinally, we often report confidence intervals for odds ratios. For example, if the odds ratio for credit score is 0.99 with a 95% CI [0.98, 0.995], we can say with confidence that higher credit scores reduce the odds of default.\n\n\n\nR Code\nPython Code\n\n\n\n# Logistic regression\nlogit_model &lt;- glm(defaulted ~ credit_score + income,\n                   data = BLR, family = binomial)\n\n# Wald test results are included in summary\nsummary(logit_model)\n\n# Confidence intervals for odds ratios\nexp(cbind(OR = coef(logit_model), confint(logit_model)))\n\n# Likelihood ratio test for nested models\nmodel1 &lt;- glm(defaulted ~ credit_score, data = BLR, family = binomial)\nanova(model1, logit_model, test = \"Chisq\")\n\n\nimport statsmodels.api as sm\nimport numpy as np\nfrom scipy.stats import chi2\n\ndf = r.data['BLR'].copy()\ndf['defaulted'] = df['defaulted'].astype(int)\n\n# Full model\nX_full = sm.add_constant(df[['credit_score','income']])\ny = df['defaulted']\nmodel_full = sm.Logit(y, X_full).fit()\n\n# Wald test (coeff / SE)\nwald_stats = (model_full.params / model_full.bse)**2\nprint(\"Wald test chi2 values:\\n\", wald_stats)\n\n# Confidence intervals for odds ratios\nconf = model_full.conf_int()\nodds_ratios = np.exp(model_full.params)\nconf_exp = np.exp(conf)\nprint(\"Odds Ratios:\\n\", pd.DataFrame({\"OR\": odds_ratios,\n                                      \"2.5%\": conf_exp[0],\n                                      \"97.5%\": conf_exp[1]}))\n\n# Likelihood ratio test vs simpler model\nX_simple = sm.add_constant(df[['credit_score']])\nmodel_simple = sm.Logit(y, X_simple).fit()\n\nLR_stat = 2 * (model_full.llf - model_simple.llf)\ndf_diff = model_full.df_model - model_simple.df_model\np_value = chi2.sf(LR_stat, df_diff)\nprint(f\"LR Test: chi2={LR_stat:.2f}, df={df_diff}, p={p_value:.4f}\")\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\nlibrary(lmtest)\nlibrary(broom)\n\n# Fit the logistic regression model\nlogit_model &lt;- glm(defaulted ~ credit_score + income,\n                   data = BLR, family = binomial)\n\n# ===================================\n# 1. WALD TESTS (from model summary)\n# ===================================\nsummary(logit_model)\n\n\nCall:\nglm(formula = defaulted ~ credit_score + income, family = binomial, \n    data = BLR)\n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.104e+01  8.348e-01  13.228  &lt; 2e-16 ***\ncredit_score -1.492e-02  1.240e-03 -12.031  &lt; 2e-16 ***\nincome       -2.907e-05  4.414e-06  -6.586 4.52e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1180.20  on 999  degrees of freedom\nResidual deviance:  732.93  on 997  degrees of freedom\nAIC: 738.93\n\nNumber of Fisher Scoring iterations: 6\n\n# ===================================\n# 2. ODDS RATIOS WITH CONFIDENCE INTERVALS\n# ===================================\n# Calculate odds ratios and confidence intervals\ncoef_table &lt;- tidy(logit_model, conf.int = TRUE, conf.level = 0.95)\ncoef_table &lt;- coef_table %&gt;%\n  mutate(\n    odds_ratio = exp(estimate),\n    or_lower = exp(conf.low),\n    or_upper = exp(conf.high)\n  )\n\n# Display formatted table\nknitr::kable(\n  coef_table %&gt;% \n    select(term, estimate, std.error, p.value, odds_ratio, or_lower, or_upper),\n  digits = 4,\n  col.names = c(\"Term\", \"Log-Odds (β)\", \"SE\", \"p-value\", \n                \"Odds Ratio\", \"OR 2.5%\", \"OR 97.5%\"),\n  caption = \"Logistic Regression Coefficients with Odds Ratios\"\n)\n\n\nLogistic Regression Coefficients with Odds Ratios\n\n\n\n\n\n\n\n\n\n\nTerm\nLog-Odds (β)\nSE\np-value\nOdds Ratio\nOR 2.5%\nOR 97.5%\n\n\n\n(Intercept)\n11.0420\n0.8348\n0\n62444.6652\n12844.5451\n340260.9478\n\n\ncredit_score\n-0.0149\n0.0012\n0\n0.9852\n0.9827\n0.9875\n\n\nincome\n0.0000\n0.0000\n0\n1.0000\n1.0000\n1.0000\n\n\n\n\n# ===================================\n# 3. LIKELIHOOD RATIO TEST\n# ===================================\n# Compare nested models\nmodel_simple &lt;- glm(defaulted ~ credit_score, data = BLR, family = binomial)\nmodel_full &lt;- glm(defaulted ~ credit_score + income, data = BLR, family = binomial)\n\n# LR test\nlr_test &lt;- anova(model_simple, model_full, test = \"Chisq\")\nprint(lr_test)\n\nAnalysis of Deviance Table\n\nModel 1: defaulted ~ credit_score\nModel 2: defaulted ~ credit_score + income\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       998     787.73                          \n2       997     732.93  1     54.8 1.334e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Alternative: using lmtest package\nlrtest(model_simple, model_full)\n\nLikelihood ratio test\n\nModel 1: defaulted ~ credit_score\nModel 2: defaulted ~ credit_score + income\n  #Df  LogLik Df Chisq Pr(&gt;Chisq)    \n1   2 -393.87                        \n2   3 -366.47  1  54.8  1.334e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\nfrom scipy.stats import chi2\n\n# Prepare data\ndf = r.BLR.copy()\ndf['defaulted'] = df['defaulted'].astype(int)\n\n# Fit the logistic regression model\nX = sm.add_constant(df[['credit_score', 'income']])\ny = df['defaulted']\nlogit_model = sm.Logit(y, X).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.366466\n         Iterations 7\n\n# ===================================\n# 1. MODEL SUMMARY (includes Wald tests)\n# ===================================\nprint(logit_model.summary())\n\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:              defaulted   No. Observations:                 1000\nModel:                          Logit   Df Residuals:                      997\nMethod:                           MLE   Df Model:                            2\nDate:                Sun, 07 Dec 2025   Pseudo R-squ.:                  0.3790\nTime:                        12:24:37   Log-Likelihood:                -366.47\nconverged:                       True   LL-Null:                       -590.10\nCovariance Type:            nonrobust   LLR p-value:                 7.553e-98\n================================================================================\n                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nconst           11.0420      0.835     13.228      0.000       9.406      12.678\ncredit_score    -0.0149      0.001    -12.031      0.000      -0.017      -0.012\nincome       -2.907e-05   4.41e-06     -6.586      0.000   -3.77e-05   -2.04e-05\n================================================================================\n\n# ===================================\n# 2. ODDS RATIOS WITH CONFIDENCE INTERVALS\n# ===================================\n# Extract parameters and confidence intervals\nparams = logit_model.params\nconf = logit_model.conf_int()\nconf.columns = ['Lower', 'Upper']\n\n# Calculate odds ratios\nodds_ratios = pd.DataFrame({\n    'Log-Odds (β)': params,\n    'OR': np.exp(params),\n    'OR Lower 95%': np.exp(conf['Lower']),\n    'OR Upper 95%': np.exp(conf['Upper']),\n    'p-value': logit_model.pvalues\n})\n\nprint(\"\\nOdds Ratios with 95% Confidence Intervals:\")\n\n\nOdds Ratios with 95% Confidence Intervals:\n\nprint(odds_ratios.round(4))\n\n              Log-Odds (β)          OR  OR Lower 95%  OR Upper 95%  p-value\nconst              11.0420  62444.6652    12159.9124   320671.4057      0.0\ncredit_score       -0.0149      0.9852        0.9828        0.9876      0.0\nincome             -0.0000      1.0000        1.0000        1.0000      0.0\n\n# ===================================\n# 3. LIKELIHOOD RATIO TEST\n# ===================================\n# Fit simpler model for comparison\nX_simple = sm.add_constant(df[['credit_score']])\nmodel_simple = sm.Logit(y, X_simple).fit(disp=0)\n\n# Calculate LR statistic\nLR_stat = 2 * (logit_model.llf - model_simple.llf)\ndf_diff = logit_model.df_model - model_simple.df_model\np_value = chi2.sf(LR_stat, df_diff)\n\nprint(\"\\n\" + \"=\"*50)\n\n\n==================================================\n\nprint(\"Likelihood Ratio Test: Simple vs Full Model\")\n\nLikelihood Ratio Test: Simple vs Full Model\n\nprint(\"=\"*50)\n\n==================================================\n\nprint(f\"LR Statistic: {LR_stat:.4f}\")\n\nLR Statistic: 54.8003\n\nprint(f\"Degrees of Freedom: {df_diff}\")\n\nDegrees of Freedom: 1.0\n\nprint(f\"p-value: {p_value:.4f}\")\n\np-value: 0.0000\n\nprint(f\"Decision: {'Reject H0' if p_value &lt; 0.05 else 'Fail to reject H0'}\")\n\nDecision: Reject H0\n\nprint(f\"Interpretation: Income {'significantly' if p_value &lt; 0.05 else 'does not'} improve model fit\")\n\nInterpretation: Income significantly improve model fit\n\n\n\n\n\n\n8.12.4 Interpreting the Inference Results\nThe statistical tests above help us answer a fundamental question: Are our predictors truly associated with default risk, or could the observed patterns have occurred by chance?\nWald Tests: Individual Predictor Significance\nThe model summary shows that both credit score and income are highly significant (p &lt; 0.001).\n\nThe coefficient for credit score is approximately −0.0149.\nThis means that for each 1-point increase in credit score, the log-odds of default decrease by about 0.015, holding income constant.\nOn the odds scale this corresponds to an odds ratio of about 0.985 per point, i.e., roughly a 1.5% reduction in the odds of default for each extra credit-score point.\nThe coefficient for income is about −2.9 × 10⁻⁵ per dollar.\nOn a more interpretable scale, a $10,000 increase in income changes the log-odds by about −0.29, giving an odds ratio of about 0.75.\nIn other words, each additional $10,000 of income reduces the odds of default by roughly 25%, holding credit score fixed.\n\nThe very small p-values for both predictors give strong evidence that these relationships are real, not artifacts of random variation.\nLikelihood Ratio Test: Is the Full Model Better?\nThe likelihood ratio test compares our two-predictor model (credit score + income) against a simpler model with credit score alone. The test yields a chi-squared statistic of about 54.8 on 1 degree of freedom (p ≈ 1.3 × 10⁻¹³).\nInterpretation: The tiny p-value indicates that adding income does significantly improve model fit beyond credit score alone. In other words, even after accounting for credit score, income still provides extra information about default risk.\nConfidence Intervals for Odds Ratios\nThe 95% confidence interval for the credit score odds ratio is approximately [0.983, 0.988] per 1-point increase. Because this interval lies entirely below 1.0, we can be confident that higher credit scores have a genuine protective effect.\nScaling to a more meaningful change:\n\nA 50-point increase in credit score multiplies the odds of default by about0.47, with a 95% CI roughly [0.42, 0.53].\nSo we are 95% confident that increasing a borrower’s credit score by 50 points cuts their odds of default by about half.\n\n\nFor income, a $10,000 increase corresponds to an odds ratio of about 0.75, with a 95% CI roughly [0.69, 0.82]. Again, the interval lies entirely below 1.0, indicating a clear protective effect of higher income on default risk.\nPractical Implications\nThese results suggest that both credit score and income play important roles in predicting default, with credit score exerting the larger effect. From a lender’s perspective:\n\nCredit score provides a strong, easily interpretable signal: borrowers with higher scores are much less likely to default.\nIncome adds meaningful additional information: among borrowers with the same credit score, those with higher incomes still have substantially lower default risk.\n\nIn practice, a lender might prefer to include both variables in their risk model to balance predictive performance with interpretability.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#coefficient-interpretation",
    "href": "book/08-binary-logistic.html#coefficient-interpretation",
    "title": "Binary Logistic Regression",
    "section": "\n8.13 Coefficient Interpretation",
    "text": "8.13 Coefficient Interpretation\nOnce we’ve established that predictors matter, the next step is to interpret the coefficients in a meaningful way.\n\n8.13.1 Odds Ratios and Their Meaning\nLogistic regression coefficients are expressed in log-odds units. To make them interpretable, we exponentiate them to obtain odds ratios.\n\nIn our loan-default model, the estimated coefficient for credit score is about −0.0149, which corresponds to an odds ratio of roughly 0.985 per 1-point increase.\nThat means each extra point in credit score reduces the odds of default by about 1.5%.\n\n\nScaling makes interpretation clearer:\n\nA 50-point increase in credit score multiplies the odds of default by about 0.47, i.e., it roughly cuts the odds of default in half.\n\nFor income, the coefficient per dollar is very small, but over a $10,000 increase it corresponds to an odds ratio of about 0.75.\nSo each extra $10,000 of income lowers the odds of default by about 25%.\n\n\n8.13.2 Pitfalls in Interpretation\nIt’s important to remember that odds ratios are multiplicative, not additive. This means the effect on probability depends on the baseline. For example:\n\nGoing from a 40% chance of default to 30% is a big shift,\nBut the same odds ratio may translate into a much smaller change if the baseline probability is already low (e.g., from 5% to 4%).\n\nClear communication requires translating odds ratios back into probability changes for meaningful scenarios.\n\n8.13.3 Example from Loan Default Dataset\nIn our fitted model (see the odds-ratio table), we estimate that:\n\nA 50-point increase in credit score multiplies the odds of default by about 0.47\n(roughly a 53% reduction in the odds of default).\nA $10,000 increase in income multiplies the odds of default by about 0.75\n(about a 25% reduction in the odds), holding credit score constant.\n\nWe can present this in plain English:\n\n“Compared to an otherwise similar borrower with a credit score 50 points lower, a higher-score borrower has about half the odds of default.\nLikewise, borrowers who earn $10,000 more per year have roughly 25% lower odds of default, even after accounting for credit score.”\n\n\n\n\nR Code\nPython Code\n\n\n\n# Logistic regression \nlogit_model &lt;- glm(defaulted ~ credit_score + income, data = BLR, family = binomial) \n\n# Odds ratios and CI \nodds_ratios &lt;- exp(cbind(OR = coef(logit_model), confint(logit_model))) odds_ratios \n\n# Example: probability at credit_score = 600 vs 700 \nnew_data &lt;- data.frame(credit_score = c(600, 700), income = mean(BLR$income)) predict(logit_model, newdata = new_data, type = \"response\")\n\n\n# Odds ratios\nparams = model_full.params\nconf = model_full.conf_int()\nodds_ratios = np.exp(params)\nconf_exp = np.exp(conf)\nprint(pd.DataFrame({\"OR\": odds_ratios,\n                    \"2.5%\": conf_exp[0],\n                    \"97.5%\": conf_exp[1]}))\n\n# Example: probability at 600 vs 700 credit score\ntest_data = pd.DataFrame({\n    \"const\": 1,\n    \"credit_score\": [600, 700],\n    \"income\": [df['income'].mean(), df['income'].mean()]\n})\nprint(model_full.predict(test_data))\n\n\n\n\n8.13.4 Visualizing Predicted Probabilities\nTo make these odds ratios more concrete, we can plot predicted probabilities across the range of credit scores. This shows the characteristic S-shaped curve of logistic regression.\n\n\nR Code\nPython Code\n\n\n\n\n# Fit simple model with credit score only for clear visualization\nmodel_simple &lt;- glm(defaulted ~ credit_score, data = BLR, family = binomial)\n\n# Create sequence of credit scores for prediction\ncredit_seq &lt;- seq(min(BLR$credit_score), max(BLR$credit_score), length.out = 200)\npred_df &lt;- data.frame(credit_score = credit_seq)\n\n# Get predicted probabilities with confidence intervals\npred_df$prob &lt;- predict(model_simple, newdata = pred_df, type = \"response\")\n\n# Calculate confidence intervals on the linear predictor scale, then transform\npred_link &lt;- predict(model_simple, newdata = pred_df, type = \"link\", se.fit = TRUE)\npred_df$lower &lt;- plogis(pred_link$fit - 1.96 * pred_link$se.fit)\npred_df$upper &lt;- plogis(pred_link$fit + 1.96 * pred_link$se.fit)\n\n# Add reference lines for key credit scores\nref_scores &lt;- c(400, 500, 600, 700, 800)\nref_probs &lt;- predict(model_simple, \n                     newdata = data.frame(credit_score = ref_scores),\n                     type = \"response\")\n\n# Create the plot\nggplot(pred_df, aes(x = credit_score, y = prob)) +\n  # Confidence interval ribbon\n  geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"#3498db\", alpha = 0.2) +\n  # Predicted probability curve\n  geom_line(color = \"#2C3E50\", linewidth = 1.5) +\n  # Actual data points (jittered for visibility)\n  geom_jitter(data = BLR, \n              aes(x = credit_score, y = as.numeric(as.character(defaulted))),\n              height = 0.02, alpha = 0.2, size = 0.8, color = \"gray30\") +\n  # Reference lines for key credit scores\n  geom_segment(data = data.frame(x = ref_scores, y = ref_probs),\n               aes(x = x, xend = x, y = 0, yend = y),\n               linetype = \"dotted\", color = \"#E74C3C\", alpha = 0.6) +\n  geom_point(data = data.frame(x = ref_scores, y = ref_probs),\n             aes(x = x, y = y), color = \"#E74C3C\", size = 3) +\n  # Annotations for key points\n  annotate(\"text\", x = 400, y = ref_probs[1] + 0.08, \n           label = sprintf(\"Score 400\\nP(default) = %.1f%%\", ref_probs[1]*100),\n           size = 3, color = \"#E74C3C\", fontface = \"bold\") +\n  annotate(\"text\", x = 700, y = ref_probs[4] - 0.08, \n           label = sprintf(\"Score 700\\nP(default) = %.1f%%\", ref_probs[4]*100),\n           size = 3, color = \"#E74C3C\", fontface = \"bold\") +\n  # Labels and theme\n  labs(\n    title = \"The S-Curve: How Credit Score Affects Default Probability\",\n    subtitle = \"Shaded area shows 95% confidence interval\",\n    x = \"Credit Score\",\n    y = \"Predicted Probability of Default\"\n  ) +\n  scale_y_continuous(labels = scales::percent_format(), limits = c(0, 1)) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 14),\n    plot.subtitle = element_text(hjust = 0.5, size = 10, color = \"gray40\")\n  )\n\nWarning: Removed 492 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\nFigure 8.5: Predicted Probability of Default by Credit Score\n\n\n\n\n\n\n\n#| label: fig-prob-curve-credit-py\n#| fig-cap: \"Predicted Probability of Default by Credit Score (Python)\"\n#| fig-width: 8\n#| fig-height: 5\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom scipy.special import expit\n\n# Prepare data\ndf = r.BLR.copy()\ndf['defaulted'] = df['defaulted'].astype(int)\n\n# Fit simple model with credit score only\nX_simple = sm.add_constant(df[['credit_score']])\ny = df['defaulted']\nmodel_simple = sm.Logit(y, X_simple).fit(disp=0)\n\n# Create sequence for prediction\ncredit_seq = np.linspace(df['credit_score'].min(), df['credit_score'].max(), 200)\npred_df = pd.DataFrame({'const': 1, 'credit_score': credit_seq})\n\n# Get predictions (probabilities directly)\npred_probs = model_simple.predict(pred_df)\n\n# Calculate confidence intervals manually\n# Get linear predictor (log-odds) and its standard error\npred_logit = model_simple.predict(pred_df, linear=True)\n\nC:\\Users\\aviv\\AppData\\Local\\R\\cache\\R\\RETICU~1\\uv\\cache\\ARCHIV~1\\9JLF1J~1\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:530: FutureWarning: linear keyword is deprecated, use which=\"linear\"\n  warnings.warn(msg, FutureWarning)\n\n# Get variance-covariance matrix and calculate SE\nX_pred = pred_df.values\nvcov = model_simple.cov_params()\nse_logit = np.sqrt(np.diag(X_pred @ vcov @ X_pred.T))\n\n# Transform to probability scale using inverse logit\nlower_ci = expit(pred_logit - 1.96 * se_logit)\nupper_ci = expit(pred_logit + 1.96 * se_logit)\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(8, 5))\n\n# Confidence interval\nax.fill_between(credit_seq, lower_ci, upper_ci,\n                alpha=0.2, color='#3498db', label='95% CI')\n\n# Predicted probability curve\nax.plot(credit_seq, pred_probs, color='#2C3E50', linewidth=2.5, \n        label='Predicted Probability')\n\n# Actual data points (jittered)\nnp.random.seed(42)\njitter = np.random.normal(0, 0.02, size=len(df))\nax.scatter(df['credit_score'], df['defaulted'] + jitter, \n           alpha=0.2, s=10, color='gray', label='Actual Data')\n\n# Reference points for key credit scores\nref_scores = np.array([400, 500, 600, 700, 800])\nref_df = pd.DataFrame({'const': 1, 'credit_score': ref_scores})\nref_probs = model_simple.predict(ref_df)\n\n# Add reference lines\nfor score, prob in zip(ref_scores, ref_probs):\n    ax.plot([score, score], [0, prob], ':', color='#E74C3C', alpha=0.6, linewidth=1)\n    ax.plot(score, prob, 'o', color='#E74C3C', markersize=8)\n\n# Annotations for extreme points\nax.text(400, ref_probs[0] + 0.08, \n        f'Score 400\\nP(default) = {ref_probs[0]*100:.1f}%',\n        ha='center', fontsize=9, color='#E74C3C', fontweight='bold')\nax.text(700, ref_probs[3] - 0.08, \n        f'Score 700\\nP(default) = {ref_probs[3]*100:.1f}%',\n        ha='center', fontsize=9, color='#E74C3C', fontweight='bold')\n\n# Labels and styling\nax.set_xlabel('Credit Score', fontsize=11)\nax.set_ylabel('Predicted Probability of Default', fontsize=11)\nax.set_title('The S-Curve: How Credit Score Affects Default Probability', \n             fontsize=14, fontweight='bold', pad=15)\nax.text(0.5, 1.02, 'Shaded area shows 95% confidence interval',\n        transform=ax.transAxes, ha='center', fontsize=9, color='gray')\nax.set_ylim(0, 1)\n\n(0.0, 1.0)\n\nax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0%}'))\nax.legend(loc='upper right', framealpha=0.9)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nInterpretation: The plot reveals the nonlinear relationship between credit score and default probability. In our fitted model, a borrower with a credit score of 400 has an estimated default probability of about 98%, whereas a borrower with a score of 700 has a default probability of only about 25%.\nNotice how the curve is steepest in the middle of the credit-score range: in that region, relatively small improvements in credit score translate into large reductions in default risk. At the extremes (very low or very high scores), additional changes in credit score have diminishing marginal effects on the predicted probability.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#fitting-the-binary-logistic-regression-model",
    "href": "book/08-binary-logistic.html#fitting-the-binary-logistic-regression-model",
    "title": "Binary Logistic Regression",
    "section": "\n8.5 Fitting the Binary Logistic Regression Model",
    "text": "8.5 Fitting the Binary Logistic Regression Model\nNow that we’ve prepared the data, we fit the model. Our goal is to estimate the unknown parameters (\\(\\beta\\)) that link our predictors (like credit_score) to the probability of default.\n\n\n8.5.1 Model Specification\nWe define the relationship between the predictors and the probability of default \\(\\pi_i\\) as:\n\\[\n\\log\\!\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\beta_0 + \\beta_1\\,\\text{credit\\_score}_i + \\beta_2\\,\\text{income}_i + \\dots + \\beta_k X_{ki}\n\\]\nHere is what the parameters entail:\n\n\n\\(\\pi_i\\): The probability that the \\(i\\)-th student defaults.\n\n\\(\\beta_0\\) (The Intercept): The expected log-odds of default when all predictors are zero. (Often theoretical, as no one has 0 credit score).\n\n\\(\\beta_1, \\dots, \\beta_k\\) (The Slopes): The change in the log-odds of default for a one-unit increase in the predictor, holding all other variables constant.\n\n8.5.2 Fitting the Model\nTo estimate these \\(\\beta\\) parameters, we use Maximum Likelihood Estimation (MLE).\nIn R, we use the glm() function (Generalized Linear Model). Crucially, we must specify family = binomial.\n\nWhy family = binomial? Standard regression (OLS) assumes the target variable follows a Normal (Gaussian) distribution. However, our target is binary (0/1), which follows a Bernoulli/Binomial distribution. This argument tells the software to switch from “Least Squares” to “Maximum Likelihood” using the binomial distribution.\nWhy Logit? By default, family = binomial uses the logit link function. While other links exist (like probit), the logit is preferred for its interpretability (odds ratios) and mathematical properties.\n\n\n\nR Code\nPython Code\n\n\n\n# Fit the model using Generalized Linear Model (glm)\n# family = binomial automatically defaults to link = \"logit\"\nfit_glm &lt;- glm(defaulted ~ credit_score + income + age + loan_amount + student,\n               data = BLR,\n               family = binomial)\n\nsummary(fit_glm)\n\n\nimport statsmodels.api as sm\n\n# 1. Define Predictors (X) and Target (y)\n# Note: We access the R dataframe 'BLR' using 'r.BLR'\ndf = r.BLR\nX = df[['credit_score', 'income', 'age', 'loan_amount', 'student']]\ny = df['defaulted']\n\n# 2. Add Intercept manually (Statsmodels does not add it by default)\nX = sm.add_constant(X)\n\n# 3. Fit the Logit model\nmodel = sm.Logit(y, X).fit()\n\nprint(model.summary())\n\n\n\n\n8.5.3 Model Output Interpretation\nThe output table provides the Estimate (the \\(\\beta\\) coefficients) for each predictor.\n\n\nPositive Coefficient (+): As this variable increases, the log-odds of default increase (Higher Risk).\n\nNegative Coefficient (-): As this variable increases, the log-odds of default decrease (Lower Risk).\n\nIn the next section, we will convert these raw log-odds into Odds Ratios, which are much easier to interpret for business decisions.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#from-logodds-to-probabilities",
    "href": "book/08-binary-logistic.html#from-logodds-to-probabilities",
    "title": "Binary Logistic Regression",
    "section": "\n8.7 From Log‑Odds to Probabilities",
    "text": "8.7 From Log‑Odds to Probabilities\nThe inverse‑logit (a.k.a. logistic/sigmoid) maps any linear predictor \\(\\eta\\) back to a valid probability:\n\\[\np = \\text{logit}^{-1}(\\eta) = \\frac{1}{1 + e^{-\\eta}}\n\\]\nBelow we:\n\nget linear predictors (\\(\\eta\\)),\ntransform to probabilities \\(p\\), and\ncompute example probabilities at low/median/high credit scores (holding income at its mean).\n\n\n\nR Code\nPython Code\n\n\n\n\n# 1) Linear predictor (η) and probability (p) for observed rows\nBLR$eta_hat &lt;- predict(fit_glm, type = \"link\")         # η = Xβ\nBLR$p_hat   &lt;- predict(fit_glm, type = \"response\")     # p = logistic(η)\n\n# 2) Example scenarios at low / median / high credit score\nqs &lt;- quantile(BLR$credit_score, probs = c(0.25, 0.5, 0.75), na.rm = TRUE)\nnewdat &lt;- tibble::tibble(\n  credit_score = as.numeric(qs),\n  income = mean(BLR$income, na.rm = TRUE),\n  label = c(\"Low (25th %)\",\"Median (50th %)\",\"High (75th %)\")\n)\n\nnewdat$eta   &lt;- predict(fit_glm, newdata = newdat, type = \"link\")\nnewdat$prob  &lt;- predict(fit_glm, newdata = newdat, type = \"response\")\n\nknitr::kable(\n  newdat[, c(\"label\",\"credit_score\",\"income\",\"eta\",\"prob\")],\n  digits = 4,\n  col.names = c(\"Scenario\",\"Credit Score\",\"Income (mean)\",\"η (linear predictor)\",\"Probability\")\n)\n\n\n\n\nimport numpy as np\nimport pandas as pd\n\n# 1) Linear predictor and probabilities for observed rows\neta_hat = np.dot(X, logit_model.params)      # η = Xβ\np_hat   = 1 / (1 + np.exp(-eta_hat))         # logistic(η)\n\ndf_out = df[['credit_score','income','defaulted']].copy()\ndf_out['eta_hat'] = eta_hat\ndf_out['p_hat']   = p_hat\n\n# 2) Example scenarios at low / median / high credit score (income at mean)\nqs = df['credit_score'].quantile([0.25, 0.5, 0.75]).values\ninc_mean = df['income'].mean()\n\nnewX = pd.DataFrame({\n    'const': 1.0,\n    'credit_score': qs,\n    'income': [inc_mean, inc_mean, inc_mean]\n}, index=[\"Low (25th %)\",\"Median (50th %)\",\"High (75th %)\"])\n\neta = np.dot(newX, logit_model.params)\nprob = 1 / (1 + np.exp(-eta))\n\npd.DataFrame({\n    \"Scenario\": newX.index,\n    \"Credit Score\": newX['credit_score'].values,\n    \"Income (mean)\": newX['income'].values,\n    \"η (linear predictor)\": eta,\n    \"Probability\": prob\n})\n\n\n\n\nTakeaway\n\nThe model fits a straight line on the log‑odds scale, then the inverse‑logit maps it to valid probabilities in \\([0,1]\\).\nYou can read coefficients as changes in log‑odds, or exponentiate to odds ratios; but to communicate, show probabilities for realistic scenarios (like low/median/high credit score).",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#fitting-the-logistic-regression-model-1",
    "href": "book/08-binary-logistic.html#fitting-the-logistic-regression-model-1",
    "title": "Binary Logistic Regression",
    "section": "\n8.8 Fitting the Logistic Regression Model",
    "text": "8.8 Fitting the Logistic Regression Model\n\nUse glm(family = binomial) in R\nPython equivalent: Logit() in statsmodels\n\n\n\n\nR Code\nPython Code\n\n\n\n\n# TODO: Fit logistic regression using glm()\n\n\n\n\n# TODO: Fit logistic regression using statsmodels.Logit()\n\n\n\n\n\nDisplay coefficient table\nEmphasize coefficients = change in log-odds per unit increase",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#estimation-1",
    "href": "book/08-binary-logistic.html#estimation-1",
    "title": "Binary Logistic Regression",
    "section": "\n8.12 Estimation",
    "text": "8.12 Estimation\nUnder a model with \\(k\\) regressors, the coefficients \\(\\beta_0,\\beta_1,\\ldots,\\beta_k\\) are unknown and must be estimated from the data.\n\n8.12.1 Maximum Likelihood Estimation (MLE)\nBecause our outcome \\(Y_i\\) follows a Bernoulli distribution with probability \\(\\pi_i\\), we cannot use “Least Squares.” Instead, we use Maximum Likelihood Estimation.\nWe seek the values of \\(\\beta\\) that maximize the likelihood of observing the data we actually collected. The likelihood function is:\n\\[\nL(\\boldsymbol\\beta)=\\prod_{i=1}^n \\pi_i^{y_i}(1-\\pi_i)^{1-y_i}\n\\]Where \\(\\pi_i\\) is connected to our regressors via the inverse-logit:\n\\[\\\\pi\\_i=\\\\frac{e^{\\\\beta\\_0 + \\\\dots + \\\\beta\\_k X\\_{ki}}}{1+e^{\\\\beta\\_0 + \\\\dots + \\\\beta\\_k X\\_{ki}}}\n\\]The computer solves this using an iterative algorithm (Newton-Raphson) to find the \\(\\hat{\\beta}\\) values that make the observed defaults most probable.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#r-code",
    "href": "book/08-binary-logistic.html#r-code",
    "title": "Binary Logistic Regression",
    "section": "\n8.3 R Code",
    "text": "8.3 R Code\n#| label: fig-ols-failure\n#| fig-cap: \"OLS (blue) predicts probabilities &gt; 1 and &lt; 0. Logistic Regression (red) respects the boundaries.\"\n\n# Fit OLS\nols_model &lt;- lm(defaulted ~ credit_score, data = blr_data)\n# Fit Logistic for comparison\nlog_model &lt;- glm(defaulted ~ credit_score, data = blr_data, family = \"binomial\")\n\n# Create a grid for smooth plotting\ngrid &lt;- data.frame(credit_score = seq(min(blr_data$credit_score), max(blr_data$credit_score), length.out = 200))\ngrid$ols_pred &lt;- predict(ols_model, newdata = grid)\ngrid$log_pred &lt;- predict(log_model, newdata = grid, type = \"response\")\n\nggplot(blr_data, aes(x = credit_score, y = defaulted)) +\n  # The data points\n  geom_point(alpha = 0.3, size = 2) +\n  # The OLS Line\n  geom_line(data = grid, aes(y = ols_pred), color = \"blue\", linewidth = 1, linetype = \"dashed\") +\n  # The Logistic Curve\n  geom_line(data = grid, aes(y = log_pred), color = \"red\", linewidth = 1) +\n  # Threshold lines\n  geom_hline(yintercept = c(0, 1), linetype = \"dotted\", alpha = 0.5) +\n  labs(title = \"Linear vs. Logistic Regression\",\n       subtitle = \"Linear regression violates probability bounds [0,1]\",\n       y = \"Probability of Default\") +\n  theme_minimal()\nThese limitations make OLS unsuitable for binary classification tasks.\nTo properly model binary outcomes, we need a method that:\n\nProduces predictions strictly between 0 and 1,\nAccounts for the fact that the variance depends on the mean, and\nConnects our predictors to the probability of the event in a way that is easy to interpret.\n\nThe key idea is to transform the probability so that it can be modeled as a linear function without breaking these rules.\nIn the next section, we’ll see how the logit transformation does exactly that.\n\n\nR Code\nPython Code\n\n\n\n# ⚠️ This OLS fit is not appropriate for binary outcomes.\n# We include it here only to illustrate why logistic regression is needed.\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Prepare data: numeric default variable and select predictor\nblr_data &lt;- BLR %&gt;%\n  select(credit_score, defaulted) %&gt;%\n  mutate(defaulted = as.numeric(defaulted))\n\n# Fit an OLS model (not ideal for binary outcome)\nols_model &lt;- lm(defaulted ~ credit_score, data = blr_data)\n\n# Add predicted values\nblr_data$predicted &lt;- predict(ols_model, newdata = blr_data)\n\n# Plot actual data and OLS-fitted line\nplot &lt;- ggplot(blr_data, aes(x = credit_score, y = defaulted)) +\n  geom_jitter(height = 0.05, width = 0, alpha = 0.4, color = \"black\") +\n  geom_line(aes(y = predicted), color = \"blue\", linewidth = 1.2) +\n  labs(\n    title = \"OLS Fitted Line on Binary Data\",\n    x = \"Credit Score\",\n    y = \"Predicted Probability of Default\"\n  ) +\n  coord_cartesian(ylim = c(-0.2, 1.2)) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\nplot\n\n\n# ⚠️ This OLS fit is not appropriate for binary outcomes.\n# We include it here only to illustrate why logistic regression is needed.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Extract relevant columns from the BLR dataset\nblr = r.data['BLR']\ndf = blr[['credit_score', 'defaulted']].copy()\ndf['defaulted'] = df['defaulted'].astype(int)\n\n# Fit an OLS model\nX = sm.add_constant(df['credit_score'])\nmodel = sm.OLS(df['defaulted'], X).fit()\ndf['predicted'] = model.predict(X)\n\n# Plot the actual binary outcomes and OLS predictions\nfig, ax = plt.subplots(figsize=(6, 4))\nax.scatter(df['credit_score'], df['defaulted'], alpha=0.4, color='black', label='Actual Data', s=20)\nax.plot(df['credit_score'], df['predicted'], color='blue', label='OLS Fit', linewidth=2)\nax.set_title(\"OLS Fitted Line on Binary Data\", fontsize=14, fontweight='bold')\nax.set_xlabel(\"Credit Score\")\nax.set_ylabel(\"Predicted Probability of Default\")\nax.set_ylim(-0.2, 1.2)\nax.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nR Output TODO\nPython Output TODO",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#in-the-next-section-we-will-convert-these-raw-log-odds-into-odds-ratios-which-are-much-easier-to-interpret-for-business-decisions.",
    "href": "book/08-binary-logistic.html#in-the-next-section-we-will-convert-these-raw-log-odds-into-odds-ratios-which-are-much-easier-to-interpret-for-business-decisions.",
    "title": "Binary Logistic Regression",
    "section": "\n8.6 In the next section, we will convert these raw log-odds into Odds Ratios, which are much easier to interpret for business decisions.",
    "text": "8.6 In the next section, we will convert these raw log-odds into Odds Ratios, which are much easier to interpret for business decisions.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#interpreting-model-results-log-odds-and-odds-ratios",
    "href": "book/08-binary-logistic.html#interpreting-model-results-log-odds-and-odds-ratios",
    "title": "Binary Logistic Regression",
    "section": "\n8.6 Interpreting Model Results: Log-Odds and Odds Ratios",
    "text": "8.6 Interpreting Model Results: Log-Odds and Odds Ratios\nIn the previous section, we fit a model with several predictors. To clearly demonstrate how to interpret the coefficients, let’s focus on a simplified model using just two key variables: credit_score and income.\n\n8.6.1 The “Log-Odds” Output\nBy default, logistic regression software outputs coefficients (\\(\\hat{\\beta}\\)) in log-odds.\n\n\nR: The estimate column.\n\nPython: The coef column.\n\nWhile mathematically necessary, log-odds are difficult for humans to visualize. (e.g., “What does a -0.004 change in log-odds mean?”).\n\n8.6.2 The “Odds Ratio” (OR)\nTo make these numbers interpretable, we exponentiate the coefficients: \\[\\text{OR} = e^{\\hat{\\beta}}\\]\n\n\nIf \\(\\text{OR} &gt; 1\\): The predictor is associated with higher odds of default.\n\nIf \\(\\text{OR} &lt; 1\\): The predictor is associated with lower odds of default.\n\nIf \\(\\text{OR} = 1\\): No association.\n\nBelow, we fit the simplified model and calculate the Odds Ratios with 95% Confidence Intervals.\n\n\nR Code\nPython Code\n\n\n\n\n# Fit a simplified model for clear interpretation\nfit_simple &lt;- glm(\n  defaulted ~ credit_score + income,\n  data = BLR,\n  family = binomial(link = \"logit\")\n)\n\n# We use broom::tidy to easily extract coefficients and convert them\nlibrary(broom)\ncoef_table &lt;- tidy(fit_simple, conf.int = TRUE, conf.level = 0.95)\n\n# Calculate Odds Ratios (OR) by exponentiating estimates and CIs\ncoef_table$odds_ratio &lt;- exp(coef_table$estimate)\ncoef_table$or_low     &lt;- exp(coef_table$conf.low)\ncoef_table$or_high    &lt;- exp(coef_table$conf.high)\n\n# Display the table\nknitr::kable(\n  coef_table[, c(\"term\",\"estimate\",\"std.error\",\"p.value\",\"odds_ratio\",\"or_low\",\"or_high\")],\n  digits = 4,\n  col.names = c(\"Term\",\"Log-Odds (β)\",\"SE\",\"p\",\"Odds Ratio\",\"OR 2.5%\",\"OR 97.5%\")\n)\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\n\n# Access the data from R\ndf = r.BLR\nX = sm.add_constant(df[['credit_score', 'income']])\ny = df['defaulted']\n\n# Fit the simplified model\nlogit_model = sm.Logit(y, X).fit(disp=False)\n\n# 1. Extract Parameters (Log-Odds)\nparams = logit_model.params\nconf   = logit_model.conf_int()\n\n# 2. Calculate Odds Ratios (OR)\n# We exponentiate the log-odds to get the OR\nodds_ratios = np.exp(params)\nconf_or     = np.exp(conf)\n\n# 3. Create a clean summary table\nresults_table = pd.DataFrame({\n    \"Log-Odds (β)\": params,\n    \"p-value\": logit_model.pvalues,\n    \"Odds Ratio\": odds_ratios,\n    \"OR 2.5%\": conf_or[0],\n    \"OR 97.5%\": conf_or[1]\n})\n\nprint(results_table.round(4))\n\n\n\n\nInterpretation of the Output\n\n\nIntercept: usually ignored in interpretation.\n\ncredit_score: If the coefficient is \\(\\beta \\approx -0.01\\), then \\(\\text{OR} \\approx 0.99\\).\n\n\nMeaning: For every 1-point increase in credit score, the odds of default are multiplied by 0.99 (a ~1% decrease), holding income constant.\n\n\n\nincome: If the \\(\\text{OR} \\approx 1.00\\), the effect per dollar is tiny. In such cases, it is often useful to rescale the variable (e.g., income in $10k units) to see a meaningful effect size.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#from-log-odds-to-probabilities-simple-model",
    "href": "book/08-binary-logistic.html#from-log-odds-to-probabilities-simple-model",
    "title": "Binary Logistic Regression",
    "section": "\n8.7 From Log-Odds to Probabilities (Simple Model)",
    "text": "8.7 From Log-Odds to Probabilities (Simple Model)\nTo fully understand how the model converts abstract “log-odds” into a tangible “probability,” it is best to start with the simplest case: one predictor.\nIn a simple binary logistic regression, the relationship is:\n\\[\n\\hat{\\pi} = \\frac{1}{1 + e^{-(\\hat{\\beta}_0 + \\hat{\\beta}_1 X)}}\n\\]\nBelow, we fit a model using only credit_score to predict default. We will then calculate the predicted probability (\\(\\hat{\\pi}\\)) for a student with a Low, Median, and High credit score.\n\n\nR Code\nPython Code\n\n\n\n\n# 1. Fit a Simple Model (1 Predictor)\nfit_simple &lt;- glm(defaulted ~ credit_score, data = BLR, family = binomial)\n\n# 2. Define Scenarios: Low, Median, and High Credit Score\n# We pick the 25th, 50th, and 75th percentiles\nqs &lt;- quantile(BLR$credit_score, probs = c(0.25, 0.5, 0.75), na.rm = TRUE)\n\nscenarios &lt;- data.frame(\n  scenario = c(\"Low (25th %)\", \"Median (50th %)\", \"High (75th %)\"),\n  credit_score = as.numeric(qs)\n)\n\n# 3. Calculate Linear Predictor (Log-Odds) and Probability\n# type = \"link\" gives log-odds (eta)\n# type = \"response\" gives probability (pi)\nscenarios$log_odds &lt;- predict(fit_simple, newdata = scenarios, type = \"link\")\nscenarios$prob_pi  &lt;- predict(fit_simple, newdata = scenarios, type = \"response\")\n\n# Display Results\nknitr::kable(\n  scenarios,\n  digits = 3,\n  col.names = c(\"Scenario\", \"Credit Score\", \"Log-Odds (η)\", \"Probability (π)\")\n)\n\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nimport numpy as np\n\n# 1. Fit a Simple Model (1 Predictor)\n# We define X with only credit_score\nX_simple = sm.add_constant(df[['credit_score']])\ny = df['defaulted']\nmodel_simple = sm.Logit(y, X_simple).fit(disp=0)\n\n# 2. Define Scenarios\nqs = df['credit_score'].quantile([0.25, 0.5, 0.75]).values\nscenarios = pd.DataFrame({\n    'const': 1.0, \n    'credit_score': qs\n}, index=[\"Low (25th %)\", \"Median (50th %)\", \"High (75th %)\"])\n\n# 3. Calculate Linear Predictor and Probability manually\n# eta = beta0 + beta1 * score\neta = np.dot(scenarios, model_simple.params)\n# pi = 1 / (1 + e^-eta)\npi  = 1 / (1 + np.exp(-eta))\n\n# Display Results\nresults = pd.DataFrame({\n    \"Credit Score\": scenarios['credit_score'],\n    \"Log-Odds (η)\": eta,\n    \"Probability (π)\": pi\n})\n\nprint(results.round(3))\n\n\n\n\nInterpretation\n\n\nAt Low Credit Score: The log-odds are likely positive (or less negative), resulting in a higher probability (\\(\\pi\\)) of default.\n\nAt High Credit Score: The log-odds become strongly negative. Because of the exponent in the denominator (\\(e^{-\\text{negative}}\\) is large), the probability \\(\\pi\\) shrinks toward 0.\n\nThis demonstrates the inverse relationship: as credit score goes up, the probability of default goes down, following the S-curve shape we saw earlier.\nNext Steps: Now that we understand the mechanics of the simple model, we can expand our view. Real-world problems are rarely caused by a single factor. In the following sections, we will add more variables (like income and student status) to build a multiple logistic regression model.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#data-preparation-and-wrangling",
    "href": "book/08-binary-logistic.html#data-preparation-and-wrangling",
    "title": "Binary Logistic Regression",
    "section": "\n8.8 Data Preparation and Wrangling",
    "text": "8.8 Data Preparation and Wrangling\nBefore we can train a model, we must bridge the gap between how data is collected and how a regression model consumes it. Raw data is often messy, incomplete, or stored in text formats that mathematical algorithms cannot process.\nData wrangling ensures our inputs are reliable (free of errors), complete (no missing values), and structured (numerical matrices). Without this step, even the most sophisticated algorithm will fail—a principle known as “Garbage In, Garbage Out.”\n\n8.8.1 Handling Missing Data\nReal-world datasets frequently have gaps—a borrower might leave the “Income” field blank. Logistic regression requires a complete set of observations; it cannot calculate a coefficient for a “blank” space.\nWe must first check the extent of the problem. If only a few rows have missing data, we typically remove them.\n\n\n\n\n\n\nStrategies for Missing Values\n\n\n\nWhen data is missing, you have three main options:\n\n\nDeletion (Complete Case Analysis): If missingness is rare (&lt;5% of data) and random, simply remove those rows. This is the most common approach in introductory texts.\n\nImputation: Replace missing values with the mean, median, or a value predicted by other variables. This preserves sample size but adds complexity.\n\nFlagging: Create a new binary variable (e.g., income_missing = 1) to model the missingness explicitly.\n\n\n\n\n8.8.2 Encoding Categorical Variables\nLogistic regression works with matrices of numbers. It cannot mathematically multiply a coefficient \\(\\beta\\) by the text string “Married.”\nTo fix this, we perform One-Hot Encoding (or Dummy Encoding). This converts a categorical variable into a binary 0/1 flag.\n\n\nOriginal: student = \"Yes\"\n\n\nEncoded: student_yes = 1\n\n\nNote on Multicollinearity: If a variable has two categories (Student: Yes/No), we only need one column (is_student). If we included both student_yes and student_no, they would be perfectly correlated, causing the math to break (the “Dummy Variable Trap”).\n\n8.8.3 Splitting the Data\nFinally, as discussed in the study design, we split our clean, encoded dataset into training and testing sets to evaluate performance fairly.\n\n\nR Code\nPython Code\n\n\n\nlibrary(dplyr)\nlibrary(rsample)\n\n# 1. Handling Missing Data\n# We check for NAs and, for this chapter, remove incomplete rows\nsum(is.na(BLR)) \nBLR_clean &lt;- na.omit(BLR)\n\n# 2. Encoding\n# R handles factor encoding automatically in glm(), \n# but it is good practice to ensure variables are factors.\nBLR_clean &lt;- BLR_clean %&gt;%\n  mutate(\n    student = as.factor(student),\n    married = as.factor(married),\n    owns_home = as.factor(owns_home)\n  )\n\n# 3. Splitting (Stratified to handle class imbalance)\nset.seed(123)\nsplit &lt;- initial_split(BLR_clean, prop = 0.7, strata = defaulted)\ntrain_data &lt;- training(split)\ntest_data  &lt;- testing(split)\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# 1. Handling Missing Data\n# Drop rows with any missing values\ndf_clean = df.dropna().copy()\n\n# 2. Encoding Categorical Variables\n# We use pd.get_dummies to convert text categories to 0s and 1s.\n# drop_first=True prevents the \"Dummy Variable Trap\" (e.g., keeps 'student_Yes', drops 'student_No')\ndf_clean = pd.get_dummies(df_clean, columns=['student', 'married', 'owns_home'], drop_first=True)\n\n# Display the new columns to verify\nprint(df_clean.head())\n\n# 3. Splitting (Stratified)\nX = df_clean.drop(columns=['defaulted'])\ny = df_clean['defaulted']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=123, stratify=y\n)",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#r-output-3",
    "href": "book/08-binary-logistic.html#r-output-3",
    "title": "Binary Logistic Regression",
    "section": "\n8.18 R Output",
    "text": "8.18 R Output\n\n\n\nAttaching package: 'yardstick'\n\n\nThe following object is masked from 'package:readr':\n\n    spec\n\n\nConfusion Matrix:\n\n\n          Truth\nPrediction   0   1\n         0 657 106\n         1  66 171\n\n\n\nTable 8.2: Confusion Matrix and Classification Metrics\n\n\n\nClassification Performance Metrics (Threshold = 0.5)\n\nPerformance Metric\nValue\n\n\n\nAccuracy\n0.828\n\n\nSensitivity (Recall)\n0.617\n\n\nSpecificity\n0.909\n\n\nPrecision (PPV)\n0.722\n\n\nF1 Score\n0.665\n\n\nBalanced Accuracy\n0.763",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#python-output-3",
    "href": "book/08-binary-logistic.html#python-output-3",
    "title": "Binary Logistic Regression",
    "section": "\n8.19 Python Output",
    "text": "8.19 Python Output\n\n\nTable 8.3: Confusion Matrix and Classification Metrics (Python)\n\n\nConfusion Matrix:\n\n\n                    Pred: No Default  Pred: Default\nActual: No Default               657             66\nActual: Default                  106            171\n\n\n              Metric    Value\n            Accuracy 0.828000\nSensitivity (Recall) 0.617329\n         Specificity 0.908714\n     Precision (PPV) 0.721519\n            F1 Score 0.665370\n\n\n\n\n:::\nInterpretation: The model achieves high overall accuracy (typically &gt;85%), but pay close attention to sensitivity (recall). If sensitivity is low (e.g., &lt;60%), this means the model fails to identify many actual defaulters. For lenders, this matters because false negatives (predicting no default when default occurs) are costly. The threshold of 0.5 can be adjusted based on the cost-benefit ratio of false positives vs. false negatives.\n\n\n8.19.1 ROC Curve and AUC\nThe Receiver Operating Characteristic (ROC) curve visualizes the trade-off between true positive rate (sensitivity) and false positive rate (1 - specificity) across all possible thresholds:\n\n\nR Output\nPython Output\n\n\n\n\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n# Calculate ROC\nroc_obj &lt;- roc(BLR$defaulted, pred_prob)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\nauc_value &lt;- auc(roc_obj)\n\n# Plot ROC curve\nplot(roc_obj, \n     col = \"#2C3E50\", \n     lwd = 2.5,\n     main = \"ROC Curve: Loan Default Prediction\",\n     print.auc = TRUE,\n     print.auc.x = 0.6,\n     print.auc.y = 0.2,\n     print.auc.cex = 1.2,\n     legacy.axes = TRUE,\n     grid = TRUE)\n\n# Add diagonal reference line\nabline(a = 0, b = 1, lty = 2, col = \"gray50\")\n\n# Add legend\nlegend(\"bottomright\", \n       legend = c(\n         sprintf(\"Logistic Model (AUC = %.3f)\", auc_value),\n         \"Random Classifier (AUC = 0.5)\"\n       ),\n       col = c(\"#2C3E50\", \"gray50\"),\n       lty = c(1, 2),\n       lwd = c(2.5, 1),\n       cex = 0.9)\n\n\n\n\n\n\nFigure 8.6: ROC Curve for Loan Default Model\n\n\n\n\n\n\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# Calculate ROC\nfpr, tpr, thresholds = roc_curve(y, pred_prob)\nroc_auc = auc(fpr, tpr)\n\n# Create plot\nfig, ax = plt.subplots(figsize=(7, 6))\n\n# ROC curve\nax.plot(fpr, tpr, color='#2C3E50', linewidth=2.5, \n        label=f'Logistic Model (AUC = {roc_auc:.3f})')\n\n[&lt;matplotlib.lines.Line2D object at 0x0000010788829790&gt;]\n\n# Diagonal reference line\nax.plot([0, 1], [0, 1], 'k--', linewidth=1, color='gray', \n        label='Random Classifier (AUC = 0.5)')\n\n&lt;string&gt;:3: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"k--\" (-&gt; color='k'). The keyword argument will take precedence.\n[&lt;matplotlib.lines.Line2D object at 0x000001078DC6D810&gt;]\n\n# Styling\nax.set_xlabel('False Positive Rate (1 - Specificity)', fontsize=11)\n\nText(0.5, 0, 'False Positive Rate (1 - Specificity)')\n\nax.set_ylabel('True Positive Rate (Sensitivity)', fontsize=11)\n\nText(0, 0.5, 'True Positive Rate (Sensitivity)')\n\nax.set_title('ROC Curve: Loan Default Prediction', \n             fontsize=14, fontweight='bold')\n\nText(0.5, 1.0, 'ROC Curve: Loan Default Prediction')\n\nax.legend(loc='lower right', fontsize=10)\n\n&lt;matplotlib.legend.Legend object at 0x00000107945E6310&gt;\n\nax.grid(True, alpha=0.3)\nax.set_xlim([0, 1])\n\n(0.0, 1.0)\n\nax.set_ylim([0, 1])\n\n(0.0, 1.0)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nFigure 8.7: ROC Curve for Loan Default Model (Python)\n\n\n\n\n\n\n\nInterpretation: An AUC (Area Under the Curve) of 0.75-0.85 indicates good discriminative ability. The model does substantially better than random guessing (AUC = 0.5). However, perfect discrimination (AUC = 1.0) is rare in real-world applications. The ROC curve shows that we can trade sensitivity for specificity by adjusting the classification threshold.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#when-to-use-binary-logistic-regression",
    "href": "book/08-binary-logistic.html#when-to-use-binary-logistic-regression",
    "title": "Binary Logistic Regression",
    "section": "\n8.21 When to Use Binary Logistic Regression",
    "text": "8.21 When to Use Binary Logistic Regression\nBinary logistic regression is the appropriate tool when you have a binary outcome (two mutually exclusive categories) and want to model the probability of one outcome occurring as a function of predictor variables.\n\n8.21.1 Ideal Use Cases\nBinary logistic regression is well-suited for problems where:\n\nThe outcome is truly binary: Success/failure, yes/no, default/no default, pass/fail, click/no click. The outcome must have exactly two levels.\nYou have ungrouped data: Each row represents a single observation (one person, one transaction, one trial). If you have grouped data (e.g., “15 successes out of 20 trials”), use Binomial Logistic Regression (Chapter 9) instead.\nYou want interpretable probabilities: Unlike some machine learning classifiers that only produce class predictions, logistic regression outputs calibrated probabilities that can inform decision-making under uncertainty.\nYou need inferential insight: When the goal is to understand which predictors matter and how strongly they affect the outcome, logistic regression provides coefficients, odds ratios, and hypothesis tests.\nYou have sufficient sample size: A rough guideline is at least 10-15 events (occurrences of the rarer outcome) per predictor variable. With fewer events, estimates become unstable.\n\n8.21.2 When NOT to Use Binary Logistic Regression\nBinary logistic regression is not appropriate when:\n\nYour outcome has more than two categories: For unordered categories (e.g., choice of transportation: car, bus, train), use Multinomial Logistic Regression (Chapter 14). For ordered categories (e.g., satisfaction: low, medium, high), use Ordinal Logistic Regression (Chapter 15).\nYour outcome is a count: If you’re modeling the number of events (e.g., number of accidents, number of purchases), use Poisson Regression (Chapter 10) or Negative Binomial Regression (Chapter 11).\nYour outcome is continuous: Use OLS Regression (Chapter 3), Gamma Regression (Chapter 4), or Beta Regression (Chapter 5) depending on the range and distribution of your outcome.\nYou have grouped/aggregated data: If your data structure is “X successes out of N trials” for each row, use Binomial Logistic Regression (Chapter 9).\nComplete separation exists: When a predictor perfectly predicts the outcome (all 1s above a threshold, all 0s below), maximum likelihood estimation fails. Solutions include Firth’s penalized likelihood or exact logistic regression.\n\n8.21.3 Assumptions to Check\nBefore fitting a binary logistic regression, verify:\n\nIndependence of observations: Each observation should be independent. Repeated measures or clustered data require mixed-effects models.\nLinear relationship with log-odds: Continuous predictors should have a linear relationship with the log-odds of the outcome. Check this with lowess curves or categorize continuous predictors and examine trends.\nNo perfect multicollinearity: Predictors should not be perfectly correlated with each other. High multicollinearity (VIF &gt; 10) can inflate standard errors.\nSufficient sample size: Particularly important for rare events. With very few occurrences of the outcome, consider Firth’s correction or exact methods.\nNo extreme influential points: Check Cook’s distance and leverage plots (as shown in Model Diagnostics) to ensure no single observation drives the results.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#limitations-and-extensions",
    "href": "book/08-binary-logistic.html#limitations-and-extensions",
    "title": "Binary Logistic Regression",
    "section": "\n8.22 Limitations and Extensions",
    "text": "8.22 Limitations and Extensions\nWhile binary logistic regression is a powerful and interpretable method, it has important limitations. Understanding these helps you know when to look for alternative approaches or extensions.\n\n8.22.1 Limitations\n1. Assumes Linear Log-Odds\nLogistic regression assumes that the relationship between continuous predictors and the log-odds of the outcome is linear. If the true relationship is U-shaped, exponential, or otherwise nonlinear, the model will fit poorly.\nExample: In our loan default model, we assumed credit score affects log-odds linearly. But perhaps default risk drops steeply from 300-500, then more gradually from 500-850. A linear term would miss this.\nSolutions: - Add polynomial terms (e.g., credit_score + credit_score²) - Use splines or generalized additive models (GAMs) - Categorize continuous predictors into meaningful bins\n2. Sensitive to Outliers and Influential Points\nUnlike robust methods, logistic regression can be heavily influenced by a small number of extreme observations. A single borrower with unusual characteristics (e.g., very low income but very high credit score) can skew coefficient estimates.\nSolutions: - Examine diagnostic plots (Cook’s distance, leverage) - Consider removing or investigating influential cases - Use robust standard errors\n3. Requires Sufficient Sample Size\nWith small samples or rare events (e.g., only 5 defaults in 100 loans), maximum likelihood estimates become unstable or even fail to converge. Confidence intervals become very wide, and hypothesis tests lose power.\nSolutions: - Collect more data - Use Firth’s penalized likelihood for small samples - Consider exact logistic regression for very small samples\n4. Cannot Handle Perfect Separation\nWhen a predictor perfectly distinguishes the two outcomes (e.g., everyone with income &gt; $100k never defaults, everyone below always defaults), the MLE algorithm fails because coefficients approach infinity.\nSolutions: - Use Firth’s penalized likelihood - Use exact logistic regression - Combine categories or use Bayesian methods with informative priors\n5. Multicollinearity Inflates Standard Errors\nWhen predictors are highly correlated (e.g., income and credit score both measure financial health), standard errors increase, making it harder to detect significant effects even when they exist.\nSolutions: - Remove redundant predictors - Use principal components or other dimension reduction - Interpret coefficients cautiously as “effect holding other predictors constant”\n\n8.22.2 Extensions\nWhen binary logistic regression’s assumptions are violated or additional complexity is needed, consider these extensions:\nMixed-Effects Logistic Regression\nWhen observations are clustered (e.g., students within schools, patients within hospitals), use mixed-effects models to account for within-cluster correlation.\nR packages: lme4::glmer(), glmmTMB::glmmTMB()\nGeneralized Additive Models (GAMs)\nGAMs allow nonlinear relationships between predictors and log-odds without pre-specifying the functional form.\nR packages: mgcv::gam()\nPenalized Logistic Regression (Ridge, Lasso, Elastic Net)\nWhen you have many predictors or multicollinearity, penalized methods shrink coefficients toward zero, improving prediction and sometimes performing variable selection.\nR packages: glmnet::glmnet()\nBayesian Logistic Regression\nBayesian methods incorporate prior information and handle small samples and separation issues more gracefully than maximum likelihood.\nR packages: rstanarm::stan_glm(), brms::brm()",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#r-output-4",
    "href": "book/08-binary-logistic.html#r-output-4",
    "title": "Binary Logistic Regression",
    "section": "\n8.18 R Output",
    "text": "8.18 R Output\n\n\nConfusion Matrix:\n\n\n          Truth\nPrediction   0   1\n         0 657 106\n         1  66 171\n\n\n\nTable 8.2: Confusion Matrix and Classification Metrics\n\n\n\nClassification Performance Metrics (Threshold = 0.5)\n\nPerformance Metric\nValue\n\n\n\nAccuracy\n0.828\n\n\nSensitivity (Recall)\n0.617\n\n\nSpecificity\n0.909\n\n\nPrecision (PPV)\n0.722\n\n\nF1 Score\n0.665\n\n\nBalanced Accuracy\n0.763",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html#python-output-4",
    "href": "book/08-binary-logistic.html#python-output-4",
    "title": "Binary Logistic Regression",
    "section": "\n8.19 Python Output",
    "text": "8.19 Python Output\n\n\nTable 8.3: Confusion Matrix and Classification Metrics (Python)\n\n\nConfusion Matrix:\n\n\n                    Pred: No Default  Pred: Default\nActual: No Default               657             66\nActual: Default                  106            171\n\n\n              Metric    Value\n            Accuracy 0.828000\nSensitivity (Recall) 0.617329\n         Specificity 0.908714\n     Precision (PPV) 0.721519\n            F1 Score 0.665370\n\n\n\n\n:::\nInterpretation: The model achieves high overall accuracy (typically &gt;85%), but pay close attention to sensitivity (recall). If sensitivity is low (e.g., &lt;60%), this means the model fails to identify many actual defaulters. For lenders, this matters because false negatives (predicting no default when default occurs) are costly. The threshold of 0.5 can be adjusted based on the cost-benefit ratio of false positives vs. false negatives.\n\n\n8.19.1 ROC Curve and AUC\nThe Receiver Operating Characteristic (ROC) curve visualizes the trade-off between true positive rate (sensitivity) and false positive rate (1 - specificity) across all possible thresholds:\n\n\nR Output\nPython Output\n\n\n\n\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n# Calculate ROC\nroc_obj &lt;- roc(BLR$defaulted, pred_prob)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\nauc_value &lt;- auc(roc_obj)\n\n# Plot ROC curve\nplot(roc_obj, \n     col = \"#2C3E50\", \n     lwd = 2.5,\n     main = \"ROC Curve: Loan Default Prediction\",\n     print.auc = TRUE,\n     print.auc.x = 0.6,\n     print.auc.y = 0.2,\n     print.auc.cex = 1.2,\n     legacy.axes = TRUE,\n     grid = TRUE)\n\n# Add diagonal reference line\nabline(a = 0, b = 1, lty = 2, col = \"gray50\")\n\n# Add legend\nlegend(\"bottomright\", \n       legend = c(\n         sprintf(\"Logistic Model (AUC = %.3f)\", auc_value),\n         \"Random Classifier (AUC = 0.5)\"\n       ),\n       col = c(\"#2C3E50\", \"gray50\"),\n       lty = c(1, 2),\n       lwd = c(2.5, 1),\n       cex = 0.9)\n\n\n\n\n\n\nFigure 8.6: ROC Curve for Loan Default Model\n\n\n\n\n\n\n\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# Calculate ROC\nfpr, tpr, thresholds = roc_curve(y, pred_prob)\nroc_auc = auc(fpr, tpr)\n\n# Create plot\nfig, ax = plt.subplots(figsize=(7, 6))\n\n# ROC curve\nax.plot(fpr, tpr, color='#2C3E50', linewidth=2.5, \n        label=f'Logistic Model (AUC = {roc_auc:.3f})')\n\n[&lt;matplotlib.lines.Line2D object at 0x000001C16010ADD0&gt;]\n\n# Diagonal reference line\nax.plot([0, 1], [0, 1], 'k--', linewidth=1, color='gray', \n        label='Random Classifier (AUC = 0.5)')\n\n&lt;string&gt;:3: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"k--\" (-&gt; color='k'). The keyword argument will take precedence.\n[&lt;matplotlib.lines.Line2D object at 0x000001C152B5EC90&gt;]\n\n# Styling\nax.set_xlabel('False Positive Rate (1 - Specificity)', fontsize=11)\n\nText(0.5, 0, 'False Positive Rate (1 - Specificity)')\n\nax.set_ylabel('True Positive Rate (Sensitivity)', fontsize=11)\n\nText(0, 0.5, 'True Positive Rate (Sensitivity)')\n\nax.set_title('ROC Curve: Loan Default Prediction', \n             fontsize=14, fontweight='bold')\n\nText(0.5, 1.0, 'ROC Curve: Loan Default Prediction')\n\nax.legend(loc='lower right', fontsize=10)\n\n&lt;matplotlib.legend.Legend object at 0x000001C160154590&gt;\n\nax.grid(True, alpha=0.3)\nax.set_xlim([0, 1])\n\n(0.0, 1.0)\n\nax.set_ylim([0, 1])\n\n(0.0, 1.0)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nFigure 8.7: ROC Curve for Loan Default Model (Python)\n\n\n\n\n\n\n\nInterpretation: An AUC (Area Under the Curve) of 0.75-0.85 indicates good discriminative ability. The model does substantially better than random guessing (AUC = 0.5). However, perfect discrimination (AUC = 1.0) is rare in real-world applications. The ROC curve shows that we can trade sensitivity for specificity by adjusting the classification threshold.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Binary Logistic Regression</span>"
    ]
  }
]