[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Regression Cookbook",
    "section": "",
    "text": "Preface\n\nLet the regression cooking begin!\n\nThroughout my journey as a postdoctoral fellow in the Master of Data Science (MDS) at the University of British Columbia, I became aware of the fascinating overlap between machine learning and statistics. Many data science students usually come across common machine learning/statistics concepts or ideas that might only differ in names. For instance, simple terms such as weights in supervised learning (and their equivalent statistical counterpart as regression coefficients) might be misleading for students starting their data science formation. On the other hand, from an instructor’s perspective in a data science program that subsets its courses in machine learning in Python and statistics in R, regression courses in R also demand the inclusion of Python-related packages as alternative tools. In my data science teaching experience, this is especially critical for students whose career plan leans towards industry where Python is more heavily used.\nAs a data science educator, I view this field as a substantial synergy between machine learning and statistics. Nevertheless, many gaps between both disciplines still need to be addressed. Thus, closing these critical gaps is imperative in a domain with accelerated growth, such as data science. In this regard, the MDS Stat-ML dictionary inspired me to write this textbook. It basically consists of common ground between foundational supervised learning models from machine learning and regression models commonly used in statistics. I strive to explore linear modelling approaches as a primary step while highlighting different terminology found in both fields. Furthermore, this discussion is more comprehensive than a simple conceptual exploration. Hence, the second step is hands-on practice via the corresponding Python packages for machine learning and R for statistics.\n\n\n\n\n\n\nFun fact!\n\n\n\nWhile thinking about possible names for this work, I was planning to name it “Machine Learning and Statistics: A Common Ground.” Nevertheless, it was quite plain and boring! That said, this whole textbook idea sounded analogous to a cookbook, given its heavily applied focus.\n\nHence, the cookbook name idea!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "book/audience-scope.html",
    "href": "book/audience-scope.html",
    "title": "Audience and Scope",
    "section": "",
    "text": "This book mainly focuses on regression analysis and its supervised learning counterpart. Thus, it is not introductory statistics and machine learning material. Also, some coding background on R (R Core Team 2024) and/or Python (Van Rossum and Drake 2009) is recommended. That said, the following topics are suggested as fundamental reviews:\n\n\n\nMutivariable differential calculus and linear algebra. Certain sections of each chapter pertain to modelling estimation. Therefore, topics such as partial derivatives and matrix algebra are a great asset. You can find helpful learning resources on the MDS webpage.\nBasic Python programming. When necessary, Python {pandas} (The Pandas Development Team 2024) library will be used to perform data wrangling. The MDS course DSCI 511 (Programming for Data Science) is an ideal example of a quick review.\n\n\n\n\n\nImage by Lubos Houska via Pixabay.\n\n\n\n\n\nBasic R programming. Knowledge of data wrangling and plotting through R {tidyverse} (Wickham et al. 2019) is recommended for hands-on practice via the cases provided in each one of the chapters of this book. The MDS courses DSCI 523 (Programming for Data Manipulation) and DSCI 531 (Data Visualization I) are ideal examples of a quick review.\nFoundations of probability and basic distributional knowledge. The reader should be familiar with elemental discrete and continuous distributions since they are a vital component of any given regression or supervised learning model. The MDS course DSCI 551 (Descriptive Statistics and Probability for Data Science) is an ideal example of a quick review.\nFoundations of frequentist statistical inference. One of the data science paradigms to be covered in this book is statistical inference, i.e., identifying relationships between different variables in a given population or system of interest via a sampled dataset. I only aim to cover a frequentist approach using inferential tools such as parameter estimation, hypothesis testing, and confidence intervals. The MDS course DSCI 552 (Statistical Inference and Computation I) is an ideal example of a quick review.\nFoundations of supervised learning. The second data science paradigm to be covered pertains to prediction, which is core in machine learning. The reader should be familiar with basic terminology, such as training and testing data, overfitting, underfitting, cross-validation, etc. The MDS course DSCI 571 (Machine Learning I) provides these foundations.\nFoundations of feature and model selection. This prerequisite also relates to machine learning and its corresponding prediction paradigm. Basic knowledge of prediction accuracy and variable selection tools is recommended. The MDS course DSCI 573 (Feature and Model Selection) is an ideal example of a quick review.\n\n\n\n\n\n\n\nA further remark on probability and statistical inference\n\n\n\nIn case the reader is not 100% familiar with probabilistic and inferential topics, as discussed above, I will provide a fundamental refresher in 1  Getting Ready for Regression Cooking! with crucial points that are needed to follow along the statistical way each one of the chapters is delivered (more specifically for modelling estimation/training matters!).\n\nFurthermore, this refresher will be integrated into the three big pillars that will be fully expanded in this book: a data science workflow, the right workflow flavour (inferential or predictive), and a regression toolbox.\n\n\n\n\n\n\nR Core Team. 2024. “R: A Language and Environment for Statistical Computing.” Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nThe Pandas Development Team. 2024. “Pandas-Dev/Pandas: Pandas.” Zenodo. https://doi.org/10.5281/zenodo.3509134.\n\n\nVan Rossum, Guido, and Fred L. Drake. 2009. Python 3 Reference Manual. Scotts Valley, CA: CreateSpace.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Audience and Scope"
    ]
  },
  {
    "objectID": "book/01-intro.html",
    "href": "book/01-intro.html",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "",
    "text": "1.1 The ML-Stats Dictionary\nThe above admonition for a definition will pave the way to a complimentary aspect of this textbook that I have had in mind since I started teaching statistics (and, more especially, regression analysis) in a data science context. Machine learning and statistics usually overlap across many subjects, and regression modelling is no exception. Topics we teach in an utterly regression-based course, under a purely statistical framework, also appear in machine learning-based courses such as fundamental supervised learning, but often with different terminology. On this basis, the Master of Data Science (MDS) program at the University of British Columbia (UBC) provides the MDS Stat-ML dictionary (Gelbart 2017) under the following premises:\nIndeed, both disciplines have a tremendous amount of jargon and terminology. Furthermore, as I previously emphasized in the Preface, machine learning and statistics construct a substantial synergy that is reflected in data science. Even with this, people in both fields could encounter miscommunication issues when working together. This should not happen if we build solid bridges between both disciplines. Hence, a comprehensive ML-Stats dictionary (ML stands for Machine Learning) is imperative, and this textbook offers a perfect opportunity to build this resource. Primarily, this dictionary clarifies any potential confusion between statistics and machine learning regarding terminology within supervised learning and regression analysis contexts.\nFinally, Appendix A will be the section in this book where the reader can find all those statistical and machine learning-related terms in alphabetical order. Notable terms (either statistical or machine learning-related) will include an admonition identifying which terms (again, either statistical or machine learning-related) are equivalent (or maybe NOT equivalent!). Take as an example the statistical term dependent variable:\nThen, the above definition will be followed by this admonition:\nNote we have identified four equivalent terms for the term dependent variable. Furthermore, according to our already defined colour scheme, these terms can be statistical or machine learning-related.\nNow, let us proceed to a quick review on probability and statistics in a frequentist framework. This review will be important to understanding the philosophy of modelling parameter estimation, mainly in relation to statistical inference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-ml-stats-dictionary",
    "href": "book/01-intro.html#sec-ml-stats-dictionary",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "",
    "text": "This document is intended to help students navigate the large amount of jargon, terminology, and acronyms encountered in the MDS program and beyond.\n\n\nThis section covers terms that have different meanings in different contexts, specifically statistics vs. machine learning (ML).\n\n\n\n\n\n\n\n\nNote 1.1: Heads-up on terminology highlights!\n\n\n\nFollowing the spirit of the ML-Stats dictionary, throughout the book, all statistical terms will be highlighted in magenta whereas the machine learning terms will be highlighted in orange. This colour scheme strives to combine this terminology so we can switch from one field to another in an easier way. With practice and time, we should be able to jump back and forth when using these concepts.\n\n\n\n\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\n\n\n\n\n\n\nEquivalent to:\n\n\n\nResponse, outcome, output or target.\n\n\n\n\n\n\n\n\n\nNote 1.2: Heads-up on the use of terminology!\n\n\n\nThroughout this book, I will interchangeably use specific terms when explaining the different regression approaches in each chapter. Whenever confusion arises about using these interchangeable terms, it is highly recommended to consult their definitions and equivalences (or non-equivalences) in Appendix A.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-quick-review",
    "href": "book/01-intro.html#sec-quick-review",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "\n1.2 A Quick Review on Probability and Frequentist Statistical Inference",
    "text": "1.2 A Quick Review on Probability and Frequentist Statistical Inference\n\n\nWhen I was an undergraduate student and took my very first course in probability and statistics (inference included!) in an industrial engineering context, I used to feel quite overwhelmed by the large amount of jargon and formulas one had to grasp and use regularly for primary engineering fields such as quality control in a manufacturing facility. Population parameters, hypothesis testing, tests statistics, significance level, \\(p\\)-values, and confidence intervals (do not worry, our statistical/machine learning scheme will come in later in this quick review) were appearing here and there! And to my frustration, I could never find a statistical connection between all these inferential tools! Instead, I relied on mechanistic procedures when solving assignments or exam problems.\n\n\n\nImage by OpenClipart-Vectors via Pixabay.\n\n\n\nFor instance, when performing hypothesis testing for a two-sample \\(t\\)-test, I never reflected on what the hypotheses were trying to indicate for the corresponding population parameters(s) nor how the test statistic was related to these hypotheses. Moreover, my interpretation of the resulting \\(p\\)-value and/or confidence interval was purely mechanical with the inherent claim:\n\nWith a significance level \\(\\alpha = 0.05\\), we reject (or fail to reject, if that is the case!) the null hypothesis given that…\n\nHonestly, I am not proud of this whole mechanical way of doing statistics now that I reflect on it after many years of practice, teaching, and research. Then, of course, the above situation should not happen when we learn key statistical topics for the very first time as undergraduate students. That is why we will dig into a more intuitive way of viewing probability and its crucial role in statistical inference. This matter will help us deliver more coherent storytelling when presenting our results in practice during any regression analysis to our peers or stakeholders. Note that the role of probability also extends to model training when it comes to supervised learning and not just in regard to statistical inference.\nHaving said all this, it is time to introduce a statement that I ended up reasoning the very first time I taught hypothesis testing in the introductory statistical inference course in the MDS program at UBC:\n\nIn statistical inference, everything always boils down to randomness and how we can control it!\n\nThat is quite a bold statement! Nonetheless, once one starts teaching statistical topics to audiences not entirely familiar with the usual field jargon, the idea of randomness always persists across many different tools. And, of course, regression analysis is not an exception at all since it also involves inference on population parameters of interest! This is why I have allocated this section in the textbook to explain core probabilistic and inferential concepts to pave the way to its role in regression analysis.\n\n\n\n\n\n\nNote 1.3: Heads-up on why we mean as a non-ideal mechanical analysis!\n\n\n\nThe reader might get a contradictory impression of why the above mechanical way of performing hypothesis testing is viewed as something not ideal, whereas the word cookbook appears in this book’s title. Certainly, the cookbook idea refers to some class of recipe to perform data modelling (which is reflected in the workflow from Figure 1.2). Still, there is a critical difference between a non-ideal mechanical way of performing hypothesis testing versus our data modelling workflow.\nOn the one hand, the non-ideal mechanical way refers to the use of a tool without understanding the rationale of what this tool stands for, resulting in vacuous and standard statements that we would not be able to explain any way further, such as the statement I previously indicated:\n\nWith a significance level \\(\\alpha = 0.05\\), we reject (or fail to reject, if that is the case!) the null hypothesis given that…\n\nWhat if a stakeholder of our analysis asks us in plain words what a significance level means? Why are we phrasing our conclusion on the null hypothesis and not directly on the alternative one? As a data scientist, one should be able to deliver the corresponding explanations of why the whole inference process is yielding that statement without misleading the stakeholder’s understanding. For sure, this also implicates appropriate communication skills that cater to general audiences rather than just statistical.\nOn the other hand, as we will elaborate in further detail, the workflow in Figure 1.2 has stages that demand a thorough and precise understanding of what exactly we are executing in our analysis. Moving forward from one current stage in the workflow to the next (without a complete understanding of what is going on in the current one) would risk carrying over false insights that might become faulty data storytelling of the whole analysis.\n\n\nFinally, even though this book has suggested reviews related to the basics of probability via different distributions and the fundamentals of frequentist statistical inference as stated in Audience and Scope, we will retake essential concepts as follows:\n\nThe role of random variables and probability distributions and the governance of population (or system) parameters (i.e., the so-called Greek letters we usually see in statistical inference and regression analysis). Section 1.2.1 will explore these topics more in detail while connecting them to the subsequent inferential terrain under a frequentist context.\nWhen delving into supervised learning and regression analysis, we might wonder how randomness is incorporated into model fitting (i.e., parameter estimation). That is quite a fascinating aspect, implemented via a crucial statistical tool known as maximum likelihood estimation. This tool is heavily related to the concept of loss function in supervised learning. Section 1.2.2 will explore these matters in more detail and how the concept of random sample is connected to this estimation tool.\n\nSection 1.2.3 will explore the basics of hypothesis testing and its intrinsic components such as null and alternative hypotheses, type I and type II errors, test statistic, standard error, \\(p\\)-value, and confidence interval.\nFinally, Section 1.2.4 will briefly discuss the connections between supervised learning and regression analysis regarding terminology.\n\nWithout further ado, let us start with reviewing core concepts in probability via quite a tasty example.\n\n1.2.1 Basics of Probability\nIn terms of regression analysis and its supervised analysis counterpart (either on an inferential or predictive framework), probability can be viewed as the solid foundation on which more complex tools, including estimation and hypothesis testing, are built upon. Under this foundation, our data is coming from a given population or system of interest. Moreover, the population or system is assumed to be governed by parameters which, as data scientists or researchers, they are of their best interest to study. That said, the terms population and parameter will pave the way to our first statistical definitions.\n\n\n\n\n\n\nImportant 1.1: Definition of population\n\n\n\nIt is a whole collection of individuals or items that share distinctive attributes. As data scientists or researchers, we are interested in studying these attributes, which we assume are governed by parameters. In practice, we must be as precise as possible when defining our given population such that we would frame our entire data modelling process since its very early stages. Examples of a population could be the following:\n\n\n\nChildren between the ages of 5 and 10 years old in states of the American West Coast.\nCustomers of musical vinyl records in the Canadian provinces of British Columbia and Alberta.\nAvocado trees grown in the Mexican state of Michoacán.\nAdult giant pandas in the Southwestern Chinese province of Sichuan.\nMature açaí palm trees from the Brazilian Amazonian jungle.\n\n\n\n\nImage by Eak K. via Pixabay.\n\n\n\nNote that the term population could be exchanged for the term system, given that certain contexts do not particularly refer to individuals or items. Instead, these contexts could refer to processes whose attributes are also governed by parameters. Examples of a system could be the following:\n\nThe production of cellular phones in a set of manufacturing facilities.\nThe sale process in the Vancouver franchises of a well-known ice cream parlour.\nThe transit cycle of the twelve lines of Mexico City’s subway.\n\n\n\n\n\n\n\n\n\nImportant 1.2: Definition of parameter\n\n\n\nIt is a characteristic (numerical or even non-numerical, such as a distinctive category) that summarizes the state of our population or system of interest. Examples of a population parameter can be described as follows:\n\n\n\nThe average weight of children between the ages of 5 and 10 years old in states of the American west coast.\nThe variability in the height of the mature açaí palm trees from the Brazilian Amazonian jungle.\nThe proportion of defective items in the production of cellular phones in a set of manufacturing facilities.\nThe average customer waiting time to get their order in the Vancouver franchises of a well-known ice cream parlour.\n\n\n\n\nImage by meineresterampe via Pixabay.\n\n\n\nNote the standard mathematical notation for a population parameters are Greek letters. Moreover, in practice, these population parameter(s) of interest will be unknown to the data scientist or researcher. Instead, they would use formal statistical inference to estimate them.\n\n\nThe parameter definition in Important 1.2 points out a crucial fact in investigating any given population or system:\n\nOur parameter(s) of interest are usually unknown!\n\nGiven this fact, it would be pretty unfortunate and inconvenient if we eventually wanted to discover any significant insights about the population or system. Therefore, let us proceed to our so-called tasty example so we can dive into the need for statistical inference and why probability is our perfect ally in this parameter quest.\n\n\nImagine you are the owner of a large fleet of ice cream carts, around 500, to be exact. These ice cream carts operate across different parks in the following Canadian cities: Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal. In the past, to optimize operational costs, you decided to limit ice cream cones to only two items: vanilla and chocolate flavours, as in Figure 1.1.\nNow, let us point this whole case onto a more statistical and probabilistic field; suppose you have a well-defined overall population of interest for those above eight Canadian cities: children between 4 and 11 years old attending these parks during the Summer weekends. Of course, Summer time is coming this year, and you would like to know which ice cream cone flavour is the favourite one for this population (and by how much). As a business owner, knowing these facts would allow you to plan your summer restocks more carefully with your corresponding suppliers. Therefore, you decided to meet with all your regional managers to discuss how to tackle this demand query.\n\n\n\n\n\n\nFigure 1.1: The two flavours of the ice cream cone you sell across all your ice cream carts: vanilla and chocolate. Image by tomekwalecki via Pixabay.\n\n\n\n\nDuring the meeting with the regional managers, it was decided to run a comprehensive market study on the population of interest across the eight Canadian cities right before next Summer (suppose we are currently in Spring). Surprisingly, when discussing study requirements for the marketing firm who would be in charge of it, one of the managers dares to state the following:\n\nSince we’re already planning to collect consumer data on these cities, let’s mimic a census-type study to ensure we can have the MOST precise results on their preferences.\n\n\n\n\n\n\n\nImportant 1.3: Definition of probability\n\n\n\n\n\n\n\n1.2.2 What is Maximum Likelihood Estimation?\n\n1.2.3 Basics of Frequentist Statistical Inference\n\n1.2.4 Supervised Learning and Regression Analysis",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-ds-workflow",
    "href": "book/01-intro.html#sec-ds-workflow",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "\n1.3 The Data Science Workflow",
    "text": "1.3 The Data Science Workflow\nIt is time to review the so-called data science workflow. Each one of these three pillars is heavily connected since a general Data Science workflow is applied in each one of these regression models, which aims to help in our learning (i.e., we would be able to know what exact stage to expect in our data analysis regardless of the regression model we are being exposed to). Therefore, a crucial aspect of the practice of Regression Analysis is the need for this systematic Data Science workflow that will allow us to solve our respective inquiries in a reproducible way. Figure 1.2 shows this workflow which has the following general stages (I briefly define each one of them; note a broader delivery will be done in subsequent subsections):\n\nStudy design:\nData collection and wrangling:\nExploratory data analysis:\nData modelling:\nEstimation:\nGoodness of fit:\nResults:\nStorytelling\n\n\n\n\n\n\n\nWhat if there is no formal structure in our regression analysis?\n\n\n\nSince very early learning stages in data analysis, it is crucial tp\nNow, suppose we do not follow a predefined workflow in practice. In that case, we might be at stake in incorrectly addressing our inquiries, translating into meaningless results outside the context of the problem we aim to solve. This is why the formation of a Data Scientist must stress this workflow from the very introductory learning stages.\n\n\n\n\n\n\n\nFigure 1.2: Data science workflow for inferential and predictive inquiries in regression analysis and supervised learning, respectively.\n\n\n\n1.3.1 Study Design\nThe first stage of this workflow is heavily related to the main statistical inquiries we aim to address throughout the whole data analysis process. As a data scientist, it is your task to primarily translate these inquiries from the stakeholders of the problem as inferential or predictive. Roughly speaking, this primary classification can be explained as follows:\n\n\nInferential. The main objective is to untangle relationships of association or causation between the regressors (i.e., explanatory variables) and the corresponding response in the context of the problem of interest. Firstly, we would assess whether there is a statistical relationship between them. Then, if significant, we would quantify by how much.\n\nPredictive. The main objective is to deliver response predictions on further observations of regressors, having estimated a given model via a current training dataset. Unlike inferential inquiries, assessing a statistically significant association or causation between our variables of interest is not a primary objective but accurate predictions. This is one of the fundamental paradigms of machine learning.\n\n\n\n\n\n\n\nFigure 1.3: Study design stage from the data science workflow in Figure 1.2. This stage is directly followed by data collection and wrangling.\n\n\n\n1.3.2 Data Collection and Wrangling\nOnce we have defined our main statistical inquiries, it is time to collect our data. Note we have to be careful about the way we collect this data since it might have a particular impact on the quality of our statistical practice:\n\nRegarding inferential inquiries, recall we are approaching populations or systems of interest governed by unknown and fixed distributional parameters. Thus, via sampled data, we aim to estimate these distributional parameters. This is why a proper sampling method on this population or system of interest is critical to obtaining representative data for appropriate hypothesis testing.\n\n\n\n\n\n\n\nTip 1.1: A Quick Debrief on Sampling!\n\n\n\nSampling topics are out of the scope of this book. Nevertheless, we still need to stress that a proper sampling method is also key in inferential inquiries to assess association and/or causation between the regressors and your response of interest. That said, depending on the context of the problem, we could apply either one of the following methods of sampling:\n\n\nSimple random sampling.\n\nSystematic sampling.\n\nStratified sampling.\n\nClustered sampling.\nEtc.\n\nAs in the case of Regression Analysis, statistical sampling is a vast field, and we could spend a whole course on it. If you are more interested in these topics, Sampling: design and analysis by Lohr offers great foundations.\n\n\n\nIn practice, regarding predictive inquiries, we would likely have to deal with databases given that our trained models will not be used to make inference and parameter interpretations.\n\n\n\n\n\n\nFigure 1.4: Data collection and wrangling stage from the data science workflow in Figure 1.2. This stage is directly followed by exploratory data analysis and preceded by study design.\n\n\n\n1.3.3 Exploratory Data Analysis\n\n\n\n\n\nFigure 1.5: Exploratory data analysis stage from the data science workflow in Figure 1.2. This stage is directly followed by data modelling and preceded by data collection and wrangling.\n\n\n\n1.3.4 Data Modelling\n\n\n\n\n\nFigure 1.6: Data modelling stage from the data science workflow in Figure 1.2. This stage is directly preceded by exploratory data analysis. On the other hand, it is directly followed by estimation but indirectly with goodness of fit. If necessary, the goodness of fit stage could retake the process to data modelling.\n\n\n\n1.3.5 Estimation\n\n\n\n\n\nFigure 1.7: Estimation stage from the data science workflow in Figure 1.2. This stage is directly preceded by data modelling and followed by goodness of fit. If necessary, the goodness of fit stage could retake the process to data modelling and then to estimation.\n\n\n\n1.3.6 Goodness of Fit\n\n\n\n\n\nFigure 1.8: Goodness of fit stage from the data science workflow in Figure 1.2. This stage is directly preceded by estimation and followed by results. If necessary, the goodness of fit stage could retake the process to data modelling and then to estimation.\n\n\n\n1.3.7 Results\n\n\n\n\n\nFigure 1.9: Results stage from the data science workflow in Figure 1.2. This stage is directly followed by storytelling and preceded by goodness of fit.\n\n\n\n1.3.8 Storytelling\n\n\n\n\n\nFigure 1.10: Storytelling stage from the data science workflow in Figure 1.2. This stage preceded by results.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-regression-mindmap",
    "href": "book/01-intro.html#sec-regression-mindmap",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "\n1.4 Mindmap of Regression Analysis",
    "text": "1.4 Mindmap of Regression Analysis\nHaving defined the necessary statistical aspects to execute a proper supervised learning analysis, either inferential or predictive across its seven sequential phases, we must dig into the different approaches we might encounter in practice as regression models. The nature of our outcome of interest will dictate any given modelling approach to apply, depicted as clouds in Figure 1.11. Note these regression models can be split into two sets depending on whether the outcome of interest is continuous or discrete. Therefore, under a probabilistic view, identifying the nature of a given random variable is crucial in regression analysis.\n\n\n\n\n\nFigure 1.11: Regression analysis mindmap depicting all modelling techniques to be explored in this book. These techniques are split into two big sets: continuous and discrete outcomes.\n\n\nThat said, we will go beyond OLS regression and explore further regression techniques. In practice, these techniques have been developed in the statistical literature to address practical cases where the OLS modelling framework and assumptions are not suitable anymore. Thus, throughout this block, we will cover (at least) one new regression model per lecture.\nAs we can see in the clouds of Figure 1.11, there are 13 regression models: 8 belonging to discrete outcomes and 5 to continuous outcomes. Each of these models is contained in a chapter of this book, beginning with the most basic regression tool known as ordinary least-squares in Chapter 2. We must clarify that the current statistical literature is not restricted to these 13 regression models. The field of regression analysis is vast, and one might encounter more complex models to target certain specific inquiries. Nonetheless, I consider these models the fundamental regression approaches that any data scientist must be familiar with in everyday practice.\nEven though this book comprises 13 chapters, each depicting a different regression model, we have split these chapters into two major subsets: those with continuous outcomes and those with discrete outcomes.\n\n\n\n\nGelbart, Michael. 2017. “Data Science Terminology.” UBC MDS. Master of Data Science at the University of British Columbia. https://ubc-mds.github.io/resources_pages/terminology/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/references.html",
    "href": "book/references.html",
    "title": "References",
    "section": "",
    "text": "Gelbart, Michael. 2017. “Data Science Terminology.” UBC\nMDS. Master of Data Science at the University of British Columbia.\nhttps://ubc-mds.github.io/resources_pages/terminology/.\n\n\nR Core Team. 2024. “R: A Language and Environment for Statistical\nComputing.” Vienna, Austria: R Foundation for Statistical\nComputing. https://www.R-project.org/.\n\n\nThe Pandas Development Team. 2024. “Pandas-Dev/Pandas:\nPandas.” Zenodo. https://doi.org/10.5281/zenodo.3509134.\n\n\nVan Rossum, Guido, and Fred L. Drake. 2009. Python 3 Reference\nManual. Scotts Valley, CA: CreateSpace.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "book/A-dictionary.html",
    "href": "book/A-dictionary.html",
    "title": "Appendix A — The ML-Stats Dictionary",
    "section": "",
    "text": "D",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#d",
    "href": "book/A-dictionary.html#d",
    "title": "Appendix A — The ML-Stats Dictionary",
    "section": "",
    "text": "Dependent variable\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\n\n\n\n\nEquivalent to:\n\n\n\nResponse, outcome, output or target.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#o",
    "href": "book/A-dictionary.html#o",
    "title": "Appendix A — The ML-Stats Dictionary",
    "section": "O",
    "text": "O\nOutcome\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\n\n\n\n\nEquivalent to:\n\n\n\nDependent variable, response, output or target.\n\n\nOutput\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\n\n\n\n\nEquivalent to:\n\n\n\nDependent variable, response, outcome or target.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#p",
    "href": "book/A-dictionary.html#p",
    "title": "Appendix A — The ML-Stats Dictionary",
    "section": "P",
    "text": "P\nParameter\nIt is a characteristic (numerical or even non-numerical, such as a distinctive category) that summarizes the state of our population or system of interest. Examples of a population parameter can be described as follows:\n\nThe average weight of children between the ages of 5 and 10 years old in states of the American West Coast.\nThe variability in the height of the mature açaí palm trees from the Brazilian Amazonian jungle.\nThe proportion of defective items in the production of cellular phones in a set of manufacturing facilities.\nThe average customer waiting time to get their order in the Vancouver franchises of a well-known ice cream parlour.\n\nNote the standard mathematical notation for a population parameters are Greek letters. Moreover, in practice, these population parameter(s) of interest will be unknown to the data scientist or researcher. Instead, they would use formal statistical inference to estimate them.\nPopulation\nIt is a whole collection of individuals or items that share distinctive attributes. As data scientists or researchers, we are interested in studying these attributes, which we assume are governed by parameters. In practice, we must be as precise as possible when defining our given population such that we would frame our entire data modelling process since its very early stages. Examples of a population could be the following:\n\nChildren between the ages of 5 and 10 years old in states of the American west coast.\nCustomers of musical vinyl records in the Canadian provinces of British Columbia and Alberta.\nAvocado trees grown in the Mexican state of Michoacán.\nAdult giant pandas in the Southwestern Chinese province of Sichuan.\nMature açaí palm trees from the Brazilian Amazonian jungle.\n\nNote that the term population could be exchanged for the term system, given that certain contexts do not specifically refer to individuals or items. Instead, these contexts could refer to processes whose attributes are also governed by parameters. Examples of a system could be the following:\n\nThe production of cellular phones in a set of manufacturing facilities.\nThe sale process in the Vancouver franchises of a well-known ice cream parlour.\nThe transit cycle of the twelve lines of Mexico City’s subway.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#r",
    "href": "book/A-dictionary.html#r",
    "title": "Appendix A — The ML-Stats Dictionary",
    "section": "R",
    "text": "R\nResponse\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\n\n\n\n\nEquivalent to:\n\n\n\nDependent variable, outcome, outpur or target.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#t",
    "href": "book/A-dictionary.html#t",
    "title": "Appendix A — The ML-Stats Dictionary",
    "section": "T",
    "text": "T\nTarget\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\n\n\n\n\nEquivalent to:\n\n\n\nDependent variable, response, outcome or output.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/B-greek-alphabet.html",
    "href": "book/B-greek-alphabet.html",
    "title": "Appendix B — Greek Alphabet",
    "section": "",
    "text": "Statistical notation can be pretty particular and different from usual mathematical notation. One of these particularities is the constant use of Greek letters to denote unknown population parameters in modelling setup, estimation, and statistical inference. In that spirit, throughout this book, we use diverse Greek letters to denote our regression parameters across each of the outlined models in every chapter.\nDuring the early learning stages of regression modelling, we may feel overwhelmed by these new letters, which could be unfamiliar. Therefore, whenever confusion arises in any of the main chapters in this book regarding the names of these letters, we recommend checking out the Greek alphabet from table Table B.1. Note that frequentist statistical inference mostly uses lowercase letters. Finally, with practice over time, you would likely end up memorizing most of this alphabet.\n\n\nTable B.1: Greek alphabet composed of 24 letters, from left to right you can find the name of letter along with its corresponding uppercase and lowercase forms.\n\n\n\nName\nUppercase\nLowercase\n\n\n\nAlpha\n\\(\\text{A}\\)\n\\(\\alpha\\)\n\n\nBeta\n\\(\\text{B}\\)\n\\(\\beta\\)\n\n\nGamma\n\\(\\Gamma\\)\n\\(\\gamma\\)\n\n\nDelta\n\\(\\Delta\\)\n\\(\\delta\\)\n\n\nEpsilon\n\\(\\text{E}\\)\n\\(\\epsilon\\)\n\n\nZeta\n\\(\\text{Z}\\)\n\\(\\zeta\\)\n\n\nEta\n\\(\\text{H}\\)\n\\(\\eta\\)\n\n\nTheta\n\\(\\Theta\\)\n\\(\\theta\\)\n\n\nIota\n\\(\\text{I}\\)\n\\(\\iota\\)\n\n\nKappa\n\\(\\text{K}\\)\n\\(\\kappa\\)\n\n\nLambda\n\\(\\Lambda\\)\n\\(\\lambda\\)\n\n\nMu\n\\(\\text{M}\\)\n\\(\\mu\\)\n\n\nNu\n\\(\\text{N}\\)\n\\(\\nu\\)\n\n\nXi\n\\(\\Xi\\)\n\\(\\xi\\)\n\n\nO\n\\(\\text{O}\\)\n\\(\\text{o}\\)\n\n\nPi\n\\(\\Pi\\)\n\\(\\pi\\)\n\n\nRho\n\\(\\text{P}\\)\n\\(\\rho\\)\n\n\nSigma\n\\(\\Sigma\\)\n\\(\\sigma\\)\n\n\nTau\n\\(\\text{T}\\)\n\\(\\tau\\)\n\n\nUpsilon\n\\(\\Upsilon\\)\n\\(\\upsilon\\)\n\n\nPhi\n\\(\\Phi\\)\n\\(\\phi\\)\n\n\nChi\n\\(\\text{X}\\)\n\\(\\chi\\)\n\n\nPsi\n\\(\\Psi\\)\n\\(\\psi\\)\n\n\nOmega\n\\(\\Omega\\)\n\\(\\omega\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Greek Alphabet</span>"
    ]
  }
]