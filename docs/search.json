[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Regression Cookbook",
    "section": "",
    "text": "Preface\nData science is a field in which we become aware of the fascinating overlap between machine learning and statistics. Many data science students usually come across everyday machine learning and statistics concepts or ideas that might only differ in names. For instance, simple terms such as weights in supervised learning (and their statistical counterpart as regression coefficients) might be misleading for students starting their data science formation. On the other hand, from an instructor’s perspective in a data science program that subsets its courses in machine learning in Python and statistics in R, regression courses in R also demand the inclusion of Python-related packages as alternative tools. Furthermore, in a graduate program such as the Master of Data Science (MDS) at the University of British Columbia, this is especially critical for students whose career plan leans towards the industry job market where Python is more heavily used.\nThat said, we can state that data science is a substantial synergy between machine learning and statistics. Nevertheless, many gaps between both disciplines still need to be addressed. Thus, closing these critical gaps is imperative in a domain with accelerated growth, such as data science. In this regard, the MDS Stat-ML dictionary has inspired us to write this textbook. It basically consists of common ground between foundational supervised learning models from machine learning and regression models commonly used in statistics. We strive to explore linear modelling approaches as a primary step while highlighting different terminology found in both fields. Furthermore, this discussion is more comprehensive than a simple conceptual exploration. Hence, the second step is hands-on practice via the corresponding Python packages for machine learning and R for statistics.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#g.-alexi-rodríguez-arelis",
    "href": "index.html#g.-alexi-rodríguez-arelis",
    "title": "The Regression Cookbook",
    "section": "G. Alexi Rodríguez-Arelis",
    "text": "G. Alexi Rodríguez-Arelis\n\n\n\n\n\n\n\n\n\n\n\nI'm an Assistant Professor of Teaching in the Department of Statistics and Master of Data Science at the University of British Columbia. Throughout my academic and professional journey, I've been involved in diverse fields, such as credit risk management, statistical consulting, and data science teaching. My doctoral research in statistics is primarily focused on computer experiments that emulate scientific and engineering systems via Gaussian stochastic processes (i.e., kriging regression). I'm incredibly passionate about teaching regression topics while combining statistical and machine learning contexts.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#andy-tai",
    "href": "index.html#andy-tai",
    "title": "The Regression Cookbook",
    "section": "Andy Tai",
    "text": "Andy Tai\n\n\n\n\n\n\n\n\n\n\n\nI'm a Postdoctoral Teaching and Learning Fellow in the Department of Statistics and Master of Data Science at the University of British Columbia. Throughout my academic and professional journey, I've been involved in diverse fields, such as addiction psychiatry, machine learning, and data science teaching. My doctoral research in neuroscience primarily focused on using machine learning to predict the risk of fatal overdose. I am interested in leveraging data science and machine learning to solve complex problems, and I strive to inspire others to explore the vast potential of these fields.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#ben-chen",
    "href": "index.html#ben-chen",
    "title": "The Regression Cookbook",
    "section": "Ben Chen",
    "text": "Ben Chen\n\n\n\n\n\n\n\n\n\n\n\nI hold a Master's degree in Data Science from the University of British Columbia, and I am passionate about educating others in the fields of statistics and data science. With experience teaching students how to use statistical methods and data science tools, I also enjoy sharing my knowledge through writing. My blog focuses on making complex statistical concepts accessible to everyone. Additionally, I've worked on a variety of data science projects, ranging from developing recommendation systems to building Generative Adversarial Network (GAN) models.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "The Regression Cookbook",
    "section": "",
    "text": "Special thanks to Jonathan Graves, who mentioned the cookbook term when this textbook was conceptualized during very early stages.↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "book/audience-scope.html",
    "href": "book/audience-scope.html",
    "title": "Audience and Scope",
    "section": "",
    "text": "This book mainly focuses on regression analysis and its supervised learning counterpart. Thus, it is not introductory statistics and machine learning material. Also, some coding background on R (R Core Team 2024) and/or Python (Van Rossum and Drake 2009) is recommended. That said, the following topics are suggested as fundamental reviews:\n\nMutivariable differential calculus and linear algebra. Certain sections of each chapter pertain to modelling estimation. Therefore, topics such as partial derivatives and matrix algebra are a great asset. You can find helpful learning resources on the MDS webpage.\nBasic Python programming. When necessary, Python {pandas} (The Pandas Development Team 2024) library will be used to perform data wrangling. The MDS course DSCI 511 (Programming for Data Science) is an ideal example of a quick review.\n\n\n\n\nImage by Lubos Houska via Pixabay.\n\n\n\nBasic R programming. Knowledge of data wrangling and plotting through R {tidyverse} (Wickham et al. 2019) is recommended for hands-on practice via the cases provided in each one of the chapters of this book. The MDS courses DSCI 523 (Programming for Data Manipulation) and DSCI 531 (Data Visualization I) are ideal examples of a quick review.\nFoundations of probability and basic distributional knowledge. The reader should be familiar with elemental discrete and continuous distributions since they are a vital component of any given regression or supervised learning model. The MDS course DSCI 551 (Descriptive Statistics and Probability for Data Science) is an ideal example of a quick review.\nFoundations of frequentist statistical inference. One of the data science paradigms to be covered in this book is statistical inference, i.e., identifying relationships between different variables in a given population or system of interest via a sampled dataset. I only aim to cover a frequentist approach using inferential tools such as parameter estimation, hypothesis testing, and confidence intervals. The MDS course DSCI 552 (Statistical Inference and Computation I) is an ideal example of a quick review.\nFoundations of supervised learning. The second data science paradigm to be covered pertains to prediction, which is core in machine learning. The reader should be familiar with basic terminology, such as training and testing data, overfitting, underfitting, cross-validation, etc. The MDS course DSCI 571 (Machine Learning I) provides these foundations.\nFoundations of feature and model selection. This prerequisite also relates to machine learning and its corresponding prediction paradigm. Basic knowledge of prediction accuracy and variable selection tools is recommended. The MDS course DSCI 573 (Feature and Model Selection) is an ideal example of a quick review.\n\n\n\nA further remark on probability and statistical inference\n\n\nIn case the reader is not 100% familiar with probabilistic and inferential topics, as discussed above, we will provide a fundamental refresher in 2  Basic Cuisine: A Review on Probability and Frequentist Statistical Inference with crucial points that are needed to follow along the statistical way each one of the chapters is delivered (more specifically for modelling estimation/training matters!).\n\nFurthermore, this refresher will be integrated into the three big pillars that will be fully expanded in this book, more concretely in 1  Getting Ready for Regression Cooking!: a data science workflow, the right workflow flavour (inferential or predictive), and a regression toolbox.\n\n\n\n\n\n\nR Core Team. 2024. “R: A Language and Environment for Statistical Computing.” Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nThe Pandas Development Team. 2024. “Pandas-Dev/Pandas: Pandas.” Zenodo. https://doi.org/10.5281/zenodo.3509134.\n\n\nVan Rossum, Guido, and Fred L. Drake. 2009. Python 3 Reference Manual. Scotts Valley, CA: CreateSpace.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Audience and Scope"
    ]
  },
  {
    "objectID": "book/01-intro.html",
    "href": "book/01-intro.html",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "",
    "text": "1.1 The ML-Stats Dictionary\nMachine learning and statistics usually overlap across many subjects, and regression modelling is no exception. Topics we teach in an utterly regression-based course, under a purely statistical framework, also appear in machine learning-based courses such as fundamental supervised learning, but often with different terminology. On this basis, the Master of Data Science (MDS) program at the University of British Columbia (UBC) provides the MDS Stat-ML dictionary (Gelbart 2017) under the following premises:\nIndeed, both disciplines have a tremendous amount of jargon and terminology. Furthermore, as previously emphasized in the Preface, machine learning and statistics construct a substantial synergy reflected in data science. Even with this, people in both fields could encounter miscommunication issues when working together. This should not happen if we build solid bridges between both disciplines. Therefore, the above definition callout box will pave the way to a complimentary resource called the ML-Stats dictionary (ML stands for Machine Learning). This comprehensive ML-Stats dictionary is imperative, and our textbook offers a perfect opportunity to build this resource. Primarily, this dictionary will clarify any potential confusion between statistics and machine learning regarding terminology within supervised learning and regression analysis contexts.\nFinally, Appendix A will be the section in this book where the reader can find all those statistical and machine learning-related terms in alphabetical order. Notable terms (either statistical or machine learning-related) will include an admonition identifying which terms (again, either statistical or machine learning-related) are equivalent (or NOT equivalent if that is the case). Take as an example the statistical term dependent variable:\nThen, the above definition will be followed by this admonition:\nAbove, we have identified four equivalent terms for the term dependent variable. Furthermore, according to our already defined colour scheme, these terms can be statistical or machine learning-related. Finally, note we will start using this colour scheme in Chapter 2.\nNow, let us proceed to explaining the three pillars on which this textbook is built upon: a data science workflow, the right workflow flavour (inferential or predictive), and a regression toolbox.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-ml-stats-dictionary",
    "href": "book/01-intro.html#sec-ml-stats-dictionary",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "",
    "text": "This document is intended to help students navigate the large amount of jargon, terminology, and acronyms encountered in the MDS program and beyond.\n\n\nThis section covers terms that have different meanings in different contexts, specifically statistics vs. machine learning (ML).\n\n\n\n\nHeads-up on terminology highlights!\n\n\nFollowing the spirit of the ML-Stats dictionary, throughout the book, all statistical terms will be highlighted in blue whereas the machine learning terms will be highlighted in magenta. This colour scheme strives to combine this terminology so we can switch from one field to another in an easier way. With practice and time, we should be able to jump back and forth when using these concepts.\n\n\n\n\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\n\n\nEquivalent to:\n\n\nResponse variable, outcome, output or target.\n\n\n\n\n\nHeads-up on the use of terminology!\n\n\nThroughout this book, we will interchangeably use specific terms when explaining the different regression approaches in each chapter. Whenever confusion arises about using these interchangeable terms, it is highly recommended to consult their definitions and equivalences (or non-equivalences) in Appendix A.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-ds-workflow",
    "href": "book/01-intro.html#sec-ds-workflow",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "\n1.2 The Data Science Workflow",
    "text": "1.2 The Data Science Workflow\nUnderstanding the data science workflow is essential for mastering regression analysis. This workflow serves as a blueprint that guides us through each stage of our analysis, ensuring that we apply a systematic approach to solving our inquiries in a reproducible way. Each of the three pillars of this textbook—data science workflow, the right workflow flavor (inferential or predictive), and a regression toolbox—are deeply interconnected. Regardless of the regression model we explore, this general workflow provides a consistent framework that helps us navigate our data analysis with clarity and purpose. As shown in Figure 1.1, the data science workflow is composed of the following stages (each of which will be discussed in more detail in subsequent subsections):\n\n\nStudy design: Define the research question, objectives, and variables of interest to ensure the analysis is purpose-driven and aligned with the problem at hand.\n\nData collection and wrangling: Gather and clean data, addressing issues such as missing values, outliers, and inconsistencies to transform it into a usable format.\n\nExploratory data analysis: Explore the data through statistical summaries and visualizations to identify patterns, trends, and potential anomalies.\n\nData modelling: Apply statistical or machine learning models to uncover relationships between variables or make predictions based on the data.\n\nEstimation: Calculate model parameters to quantify relationships between variables and assess the accuracy and reliability of the model.\n\nGoodness of fit: Evaluate the model’s performance using metrics and diagnostic checks to determine how well it explains the data.\n\nResults: Interpret the model’s outputs to derive meaningful insights and provide answers to the original research question.\n\nStorytelling Communicate the findings through a clear, engaging narrative that is accessible to a non-technical audience.\n\nBy adhering to this workflow, we ensure that our regression analyses are not only systematic and thorough but also capable of producing results that are meaningful within the context of the problem we aim to solve.\n\n\n\n\n\n\nThe Importance of a Formal Structure in Regression Analysis\n\n\n\nFrom the earliest stages of learning data analysis, understanding the importance of a structured workflow is crucial. If we do not adhere to a predefined workflow, we risk misinterpreting the data, leading to incorrect conclusions that fail to address the core questions of our analysis. Such missteps can result in outcomes that are not only meaningless but potentially misleading when taken out of the problem’s context. Therefore, it is essential for aspiring data scientists to internalize this workflow from the very beginning of their education. A systematic approach ensures that each stage of the analysis is conducted with precision, ultimately producing reliable and contextually relevant results.\n\n\n\n\n\n\n\nFigure 1.1: Data science workflow for inferential and predictive inquiries in regression analysis and supervised learning, respectively.\n\n\n\n1.2.1 Study Design\nThe first stage of this workflow is centered around defining the main statistical inquiries we aim to address throughout the data analysis process. As a data scientist, your primary task is to translate these inquiries from the stakeholders into one of two categories: inferential or predictive. This classification determines the direction of your analysis and the methods you will use.\n\nInferential: The objective here is to explore and quantify relationships of association or causation between explanatory variables (regressors) and the response variable within the context of the problem at hand. For example, you may seek to determine whether a specific marketing campaign (regressor) significantly impacts sales revenue (response) and, if so, by how much.\nPredictive: In this case, the focus is on making accurate predictions about the response variable based on future observations of the regressors. Unlike inferential inquiries, where understanding the relationship between variables is key, the primary goal here is to maximize prediction accuracy. This approach is fundamental in machine learning. For instance, you might build a model to predict future sales revenue based on past marketing expenditures, without necessarily needing to understand the underlying relationship between the two.\n\nExample: Predicting Housing Prices\nTo illustrate the study design stage, let’s consider a simple example: predicting housing prices in a specific city.\n\nIf our goal is inferential, we might be interested in understanding the relationship between various factors (like square footage, number of bedrooms, and proximity to schools) and housing prices. Specifically, we would ask questions like, “How much does the number of bedrooms affect the price of a house, after accounting for other factors?”\nIf our goal is predictive, we would focus on creating a model that can accurately predict the price of a house based on its features, regardless of whether we fully understand how each factor contributes to the price.\n\nIn both cases, the study design stage involves clearly defining these objectives and determining the appropriate methods to address them. This stage sets the foundation for all subsequent steps in the data science workflow, as illustrated in Figure 1.2. Once the study design is established, the next stage is data collection and wrangling.\n\n\n\n\n\nFigure 1.2: Study design stage from the data science workflow in Figure 1.1. This stage is directly followed by data collection and wrangling.\n\n\n\n1.2.2 Data Collection and Wrangling\nOnce we have clearly defined our statistical questions, the next crucial step is to collect the data that will form the basis of our analysis. The way we collect this data is vital because it directly affects the accuracy and reliability of our results:\n\n\nFor inferential inquiries, we focus on understanding large groups or systems (populations) that we cannot fully observe. These populations are governed by characteristics (parameters) that we want to estimate. Because we can’t study every individual in the population, we collect a smaller, representative subset called a sample. The method we use to collect this sample—known as sampling—is crucial. A proper sampling method ensures that our sample reflects the larger population, allowing us to make accurate generalizations (inferences) about the entire group. After collecting the sample, it’s common practice to randomly split the data into training and test sets. This split allows us to build and validate our models, ensuring that the findings are robust and not overly tailored to the specific data at hand.\n\n\n\n\n\n\n\nTip 1.1: A Quick Debrief on Sampling!\n\n\n\nAlthough this book does not cover sampling methods in detail, it’s important to know that the way you collect your sample can greatly influence your results. Depending on the problem, you might use different techniques:\n\n\nSimple Random Sampling: Every individual in the population has an equal chance of being selected.\n\nSystematic Sampling: You select individuals at regular intervals from a list of the population.\n\nStratified Sampling: You divide the population into subgroups (strata) and take a proportional sample from each subgroup.\n\nClustered Sampling: You divide the population into clusters and randomly select whole clusters for your sample.\nEtc.\n\nAs in the case of Regression Analysis, statistical sampling is a vast field, and we could spend a whole course on it. If you’re interested in learning more about these methods, Sampling: design and analysis by Lohr is a great resource.\n\n\n\n\nFor predictive inquiries, our goal is often to use existing data to make predictions about future events or outcomes. In these cases, we usually work with large datasets (databases) that have already been collected. Instead of focusing on whether the data represents a population (as in inferential inquiries), we focus on cleaning and preparing the data so that it can be used to train models that make accurate predictions. After wrangling the data, it is typically split into training, validation, and test sets. The training set is used to build the model, the validation set is used to tune model parameters, and the test set evaluates the model’s final performance on unseen data.\n\nExample: Collecting Data for Housing Price Predictions\nLet’s continue with our housing price prediction example to illustrate these concepts:\n\nInferential Approach: Suppose we want to understand how the number of bedrooms affects housing prices in a city. To do this, we would collect a sample of house sales that accurately represents the city’s entire housing market. For instance, we might use stratified sampling to ensure that we include houses from different neighborhoods in proportion to how common they are. After collecting the data, we would split it into training and test sets. The training set helps us build our model and estimate the relationship between variables, while the test set allows us to evaluate how well our findings generalize to new data.\nPredictive Approach: If our goal is to predict the selling price of a house based on its features (like size, number of bedrooms, and location), we would gather a large dataset of recent house sales. This data might come from a real estate database that tracks the details of each sale. Before we can use this data to train a model, we would clean it by filling in any missing information, converting data to a consistent format, and making sure all variables are ready for analysis. After preprocessing, we would split the data into training, validation, and test sets. The training set would be used to fit the model, the validation set to fine-tune it, and the test set to assess how well the model can predict prices for houses it hasn’t seen before.\n\nAs shown in Figure 1.3, the data collection and wrangling stage is fundamental to the workflow. It directly follows the study design and sets the stage for exploratory data analysis.\n\n\n\n\n\nFigure 1.3: Data collection and wrangling stage from the data science workflow in Figure 1.1. This stage is directly followed by exploratory data analysis and preceded by study design.\n\n\n\n1.2.3 Exploratory Data Analysis\nBefore diving into data modeling, it’s crucial to develop a deep understanding of the relationships between the variables in your training data. This is where the third stage of the data science workflow—Exploratory Data Analysis (EDA)—comes into play. EDA serves as a vital process that allows you to visualize and summarize your data, uncover patterns, detect anomalies, and test key assumptions that will inform your modeling decisions.\nThe first step in EDA is to classify your variables according to their types. This classification is essential because it guides your choice of analysis techniques and models. Specifically, you need to determine whether each variable is discrete or continuous, and whether it has any specific characteristics such as being bounded or unbounded.\n\n\nResponse Variable:\n\nDetermine if your response variable is discrete (e.g., binary, count-based, categorical) or continuous.\nIf it is continuous, consider whether it is bounded (e.g., percentages that range between 0 and 100) or unbounded (e.g., a variable like temperature that can take on a wide range of values).\n\n\n\nRegressors:\n\nFor each regressor, identify whether it is discrete or continuous.\nIf a regressor is discrete, classify it further as binary, count-based, or categorical.\nIf a regressor is continuous, determine whether it is bounded or unbounded.\n\n\n\nThis classification step ensures that you are prepared to choose the correct visualization and statistical methods for your analysis, as different types of variables often require different approaches.\nThis classification step ensures that you are prepared to choose the correct visualization and statistical methods for your analysis, as different types of variables often require different approaches.\nAfter classifying your variables, the next step is to create visualizations and calculate descriptive statistics using your training data. This involves coding plots that can reveal the underlying distribution of each variable and the relationships between them. For instance, you might create histograms to visualize distributions, scatter plots to explore relationships between continuous variables, and box plots to compare categorical variables against the response variable.\nAlongside these visualizations, it is important to calculate key descriptive statistics such as the mean, median, and standard deviation. These statistics provide a numerical summary of your data, offering insights into central tendency and variability. You might also use a correlation matrix to assess the strength of relationships between continuous variables.\nOnce you have generated these plots and statistics, they should be displayed in a clear and logical manner. The goal here is to interpret the data and draw preliminary conclusions about the relationships between variables. Presenting these findings effectively helps to uncover key insights and prepares you for the modeling stage.\nFinally, the insights gained from this exploratory analysis must be clearly articulated. This involves summarizing the key findings and considering their implications for the next stage of the workflow—data modeling. Observing patterns, correlations, and potential outliers in this stage will inform your modeling approach and ensure that it is grounded in a thorough and informed analysis.\nThis structured approach to EDA is visually summarized in Figure 1.4, illustrating the sequential steps from variable classification to the delivery of exploratory insights.\nExample: EDA for Housing Price Predictions\nTo illustrate the EDA process, let’s apply it to the example of predicting housing prices.\nWe start with variable classification:\n\nThe response variable is the sale price of a house, a continuous and unbounded variable.\nThe regressors include:\n\n\nNumber of bedrooms: Discrete, count-based.\n\nSquare footage: Continuous and unbounded.\n\nNeighborhood type: Discrete, categorical (e.g., urban, suburban, rural).\n\nProximity to schools: Continuous, potentially bounded by distance.\n\n\n\nOnce the variables are classified, we move on to coding plots and calculating descriptive statistics. Here are a couple of visualizations that can be helpful in this context:\n\nA histogram of sale prices helps visualize the distribution and spot any outliers.\nA scatter plot of square footage versus sale price shows the relationship, typically revealing a positive correlation.\n\nBox plots compare sale prices across different neighborhood types, highlighting any variations in median prices.\n\nDescriptive statistics like the mean and standard deviation provide a numerical summary, while a correlation matrix helps assess relationships between continuous variables like square footage and sale price.\n\nFinally, in displaying and interpreting results, these plots and statistics guide us in understanding the data:\n\nThe histogram might show most houses fall within a mid-range price.\nThe scatter plot could confirm that larger houses generally sell for more.\nBox plots may reveal that urban homes tend to have higher prices.\n\nThese exploratory insights help identify key predictors like square footage and neighborhood type, and highlight any outliers that may require further attention during modeling.\nBy following these steps, the EDA process in the housing price prediction example lays a solid foundation for effective modeling, ensuring that the key variables and their relationships are well understood.\n\n\n\n\n\nFigure 1.4: Exploratory data analysis stage from the data science workflow in Figure 1.1. This stage is directly followed by data modelling and preceded by data collection and wrangling.\n\n\n\n1.2.4 Data Modelling\n\n\n\n\n\nFigure 1.5: Data modelling stage from the data science workflow in Figure 1.1. This stage is directly preceded by exploratory data analysis. On the other hand, it is directly followed by estimation but indirectly with goodness of fit. If necessary, the goodness of fit stage could retake the process to data modelling.\n\n\n\n1.2.5 Estimation\n\n\n\n\n\nFigure 1.6: Estimation stage from the data science workflow in Figure 1.1. This stage is directly preceded by data modelling and followed by goodness of fit. If necessary, the goodness of fit stage could retake the process to data modelling and then to estimation.\n\n\n\n1.2.6 Goodness of Fit\n\n\n\n\n\nFigure 1.7: Goodness of fit stage from the data science workflow in Figure 1.1. This stage is directly preceded by estimation and followed by results. If necessary, the goodness of fit stage could retake the process to data modelling and then to estimation.\n\n\n\n1.2.7 Results\n\n\n\n\n\nFigure 1.8: Results stage from the data science workflow in Figure 1.1. This stage is directly followed by storytelling and preceded by goodness of fit.\n\n\n\n1.2.8 Storytelling\n\n\n\n\n\nFigure 1.9: Storytelling stage from the data science workflow in Figure 1.1. This stage preceded by results.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-regression-mindmap",
    "href": "book/01-intro.html#sec-regression-mindmap",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "\n1.3 Mindmap of Regression Analysis",
    "text": "1.3 Mindmap of Regression Analysis\nHaving defined the necessary statistical aspects to execute a proper supervised learning analysis, either inferential or predictive across its seven sequential phases, we must dig into the different approaches we might encounter in practice as regression models. The nature of our outcome of interest will dictate any given modelling approach to apply, depicted as clouds in Figure 1.10. Note these regression models can be split into two sets depending on whether the outcome of interest is continuous or discrete. Therefore, under a probabilistic view, identifying the nature of a given random variable is crucial in regression analysis.\n\n\n\n\n\nFigure 1.10: Regression analysis mindmap depicting all modelling techniques to be explored in this book. These techniques are split into two big sets: continuous and discrete outcomes.\n\n\nThat said, we will go beyond OLS regression and explore further regression techniques. In practice, these techniques have been developed in the statistical literature to address practical cases where the OLS modelling framework and assumptions are not suitable anymore. Thus, throughout this block, we will cover (at least) one new regression model per lecture.\nAs we can see in the clouds of Figure 1.10, there are 13 regression models: 8 belonging to discrete outcomes and 5 to continuous outcomes. Each of these models is contained in a chapter of this book, beginning with the most basic regression tool known as ordinary least-squares in Chapter 3. We must clarify that the current statistical literature is not restricted to these 13 regression models. The field of regression analysis is vast, and one might encounter more complex models to target certain specific inquiries. Nonetheless, I consider these models the fundamental regression approaches that any data scientist must be familiar with in everyday practice.\nEven though this book comprises 13 chapters, each depicting a different regression model, we have split these chapters into two major subsets: those with continuous outcomes and those with discrete outcomes.\n\n\n\n\nGelbart, Michael. 2017. “Data Science Terminology.” UBC MDS. Master of Data Science at the University of British Columbia. https://ubc-mds.github.io/resources_pages/terminology/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/02-stats-review.html",
    "href": "book/02-stats-review.html",
    "title": "2  Basic Cuisine: A Review on Probability and Frequentist Statistical Inference",
    "section": "",
    "text": "2.1 Basics of Probability\nIn terms of regression analysis and its supervised learning counterpart (either on an inferential or predictive framework), probability can be viewed as the solid foundation on which more complex tools, including estimation and hypothesis testing, are built upon. Under this foundation, our data is coming from a given population or system of interest. Moreover, the population or system is assumed to be governed by parameters which, as data scientists or researchers, they are of their best interest to study. That said, the terms population and parameter will pave the way to our first statistical definitions.\nThe parameter definition points out a crucial fact in investigating any given population or system:\nGiven this fact, it would be pretty unfortunate and inconvenient if we eventually wanted to discover any significant insights about the population or system. Therefore, let us proceed to our so-called tasty example so we can dive into the need for statistical inference and why probability is our perfect ally in this parameter quest.\nImagine you are the owner of a large fleet of ice cream carts, around 900 to be exact. These ice cream carts operate across different parks in the following Canadian cities: Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal. In the past, to optimize operational costs, you decided to limit ice cream cones to only two items: vanilla and chocolate flavours, as in Figure 2.1.\nNow, let us direct this whole case onto a more statistical and probabilistic field; suppose you have a well-defined overall population of interest for those above eight Canadian cities: children between 4 and 11 years old attending these parks during the Summer weekends. Of course, Summer time is coming this year, and you would like to know which ice cream cone flavour is the favourite one for this population (and by how much!). As a business owner, investigating ice cream flavour preferences would allow you to plan Summer restocks more carefully with your corresponding suppliers. Therefore, it would be essential to start collecting consumer data so the company can tackle this demand query.\nAlso, suppose there is a second query. For the sake of our case, we will call it a time query. As a critical component of demand planning, besides estimating which cone flavour is the most preferred one (and by how much!) for the above population of interest, the operations area is currently requiring a realistic estimation of the average waiting time from one customer to the next one in any given cart during Summer weekends. This average waiting time would allow the operations team to plan carefully how much stock each cart should have so there will not be any waste or shortage.\nNote that the nature of the aforementioned time query is more related to a larger population. Therefore, we can define it as all our ice cream customers during the Summer weekends. Furthermore, this second definition would limit this query to our corresponding general ice cream customers, given the requirements of our operations team, and not all the children between 4 and 11 years old attending the parks during Summer weekends. Consequently, it is crucial to note that the nature of our queries will dictate how we define our population and our subsequent data modelling and statistical inference.\nSummer time represents the most profitable season from a business perspective, thus solving these above two queries is a significant priority for your company. Hence, you decide to organize a meeting with your eight general managers (one per Canadian city). Finally, during the meeting with the general managers, it was decided to do the following:\nSurprisingly, when discussing study requirements for the marketing firm who would be in charge of it for the demand query, Vancouver’s general manager dares to state the following:\nOn the other hand, when agreeing on the specific operations protocol to start recording waiting times for all the 900 vending carts this upcoming Summer, Ottawa’s general manager provides a comment for further statistical food for thought:\nBingo! Ottawa’s general manager just nailed the probabilistic way of making inference on our population parameter of interest for the time query. Indeed, their comment was primarily framed from a business perspective of optimizing operational costs. Still, this fact does not take away a crucial insight on which statistical inference is built: a random sample (as in Important 2.1). As for Vancouver’s general manager, ironically, their statement is NOT PRECISE at all! Mimicking a census-type study might not be the most optimal decision for the demand query given the time constraint and the potential size of its target population.\nWe can state that probability is viewed as the language to decode random phenomena that occur in any given population or system of interest. In our example, we have two random phenomena:\nHence, let us finally define what we mean by probability along with the inherent concept of sample space.\nNote the definition of the probability for an event \\(A\\) in the definition of probability specifically highlights the following:\nThe “infinity” term is key when it comes to an understanding the philosophy behind the frequentist school of statistical thinking in contrast to its Bayesian counterpart. In general, the frequentist way of practicing statistics in terms of probability and inference is the approach we usually learn in introductory courses, more specifically when it comes to hypothesis testing and confidence intervals which will be explored in Section 2.3. That said, the Bayesian approach is another way of practicing statistical inference. Its philosophy differs in what information is used to infer our population parameters of interest. Below, we briefly define both schools of thinking.\nLet us put the definitions for the above schools of statistical thinking into a more concrete example. We can use the demand query from our ice cream case as a starting point. More concretely, we can dig more into a standalone population parameter such as the probability that a randomly selected child between 4 and 11 years old, attending the parks of the above eight Canadian cities during the Summer weekends, prefers the chocolate-flavoured ice cream cone over the vanilla one. Think about the following two hypothetical questions:\nEquation 2.3\n\\[\nP(X = x \\mid \\pi) = \\pi^x (1 - \\pi)^{1 - x} \\quad \\text{for} \\quad x = 0, 1.\n\\tag{2.3}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span>"
    ]
  },
  {
    "objectID": "book/02-stats-review.html#sec-basics-prob",
    "href": "book/02-stats-review.html#sec-basics-prob",
    "title": "2  Basic Cuisine: A Review on Probability and Frequentist Statistical Inference",
    "section": "",
    "text": "Definition of population\n\n\nIt is a whole collection of individuals or items that share distinctive attributes. As data scientists or researchers, we are interested in studying these attributes, which we assume are governed by parameters. In practice, we must be as precise as possible when defining our given population such that we would frame our entire data modelling process since its very early stages. Examples of a population could be the following:\n\nChildren between the ages of 5 and 10 years old in states of the American West Coast.\nCustomers of musical vinyl records in the Canadian provinces of British Columbia and Alberta.\nAvocado trees grown in the Mexican state of Michoacán.\nAdult giant pandas in the Southwestern Chinese province of Sichuan.\nMature açaí palm trees from the Brazilian Amazonian jungle.\n\n\n\nImage by Eak K. via Pixabay.\n\nNote that the term population could be exchanged for the term system, given that certain contexts do not particularly refer to individuals or items. Instead, these contexts could refer to processes whose attributes are also governed by parameters. Examples of a system could be the following:\n\nThe production of cellular phones from a given model in a set of manufacturing facilities.\nThe sale process in the Vancouver franchises of a well-known ice cream parlour.\nThe transit cycle during rush hours on weekdays in the twelve lines of Mexico City’s subway.\n\n\n\n\n\nDefinition of parameter\n\n\nIt is a characteristic (numerical or even non-numerical, such as a distinctive category) that summarizes the state of our population or system of interest. Examples of a population parameter can be described as follows:\n\nThe average weight of children between the ages of 5 and 10 years old in states of the American west coast (numerical).\nThe variability in the height of the mature açaí palm trees from the Brazilian Amazonian jungle (numerical).\nThe proportion of defective items in the production of cellular phones in a set of manufacturing facilities (numerical).\nThe average customer waiting time to get their order in the Vancouver franchises of a well-known ice cream parlour (numerical).\nThe most favourite pizza topping of vegetarian adults between the ages of 30 and 40 years old in Edmonton (non-numerical).\n\n\n\nImage by meineresterampe via Pixabay.\n\nNote the standard mathematical notation for population parameters are Greek letters. Moreover, in practice, these population parameter(s) of interest will be unknown to the data scientist or researcher. Instead, they would use formal statistical inference to estimate them.\n\n\n\n\nOur parameter(s) of interest are usually unknown!\n\n\n\n\n\n\n\n\nFigure 2.1: The two flavours of the ice cream cone you sell across all your ice cream carts: vanilla and chocolate. Image by tomekwalecki via Pixabay.\n\n\n\n\n\n\nImage by Icons8 Team via Unsplash.\n\n\n\n\nFor the demand query, a comprehensive market study will be run on the population of interest across the eight Canadian cities right before next Summer; suppose we are currently in Spring.\nFor the time query, since the operations team has not previously recorded any historical data, ALL vendor staff from 900 carts will start collecting data on the waiting time in seconds between each customer this upcoming Summer.\n\n\n\nSince we’re already planning to collect consumer data on these cities, let’s mimic a census-type study to ensure we can have the MOST PRECISE results on their preferences.\n\n\n\nThe operations protocol for recording waiting times in the 900 vending carts looks too cumbersome to implement straightforwardly this upcoming Summer. Why don’t we select A SMALLER GROUP of ice cream carts across the eight cities to have a more efficient process implementation that would allow us to optimize operational costs?\n\n\n\nRealistically, there is no cheap and efficient way to conduct a census-type study for any of the two queries!\n\n\n\nFor the demand query, a phenomenon can be represented by the preferred ice cream cone flavour of any randomly selected child between 4 and 11 years old attending the parks of the above eight Canadian cities during the Summer weekends.\nRegarding the time query, a phenomenon of this kind can be represented by any randomly recorded waiting time between two customers during a Summer weekend in any of the above eight Canadian cities.\n\n\n\n\nDefinition of probability\n\n\nLet \\(A\\) be an event of interest in a random phenomenon, in a population or system of interest, whose all possible outcomes belong to a given sample space \\(S\\). Generally, the probability for this event \\(A\\) happening can be mathematically depicted as \\(P(A)\\). Moreover, suppose we observe the random phenomenon \\(n\\) times such as we were running some class of experiment, then \\(P(A)\\) is defined as the following ratio:\n\\[\nP(A) = \\frac{\\text{Number of times event $A$ is observed}}{n},\n\\tag{2.1}\\]\nas the \\(n\\) times we observe the random phenomenon goes to infinity.\nEquation 2.1 will always put \\(P(A)\\) in the following numerical range:\n\\[\n0 \\leq P(A) \\leq 1.\n\\]\n\n\n\n\nDefinition of sample space\n\n\nLet \\(A\\) be an event of interest in a random phenomenon in a population or system of interest. The sample space \\(S\\) of event \\(A\\) denotes the set of all the possible random outcomes we might encounter every time we randomly observe \\(A\\) such as we were running some class of experiment.\nNote each of these outcomes has a determined probability associated with them. If we add up all these probabilities, the probability of the sample \\(S\\) will be one, i.e.,\n\\[\nP(S) = 1.\n\\tag{2.2}\\]\n\n\n\n\n… as the \\(n\\) times we observe the random phenomenon goes to infinity.\n\n\n\n\nDefinition of frequentist statistics\n\n\nThis statistical school of thinking heavily relies on the frequency of events to estimate specific parameters of interest in a population or system. This frequency of events is reflected in the repetition of \\(n\\) experiments involving a random phenomenon within this population or system.\nUnder the umbrella of this approach, we assume that our governing parameters are fixed. Note that, within the philosophy of this school of thinking, we can only make precise and accurate predictions as long as we repeat our \\(n\\) experiments as many times as possible, i.e.,\n\\[\nn \\rightarrow \\infty.\n\\]\n\n\n\n\nDefinition of Bayesian statistics\n\n\nThis statistical school of thinking also relies on the frequency of events to estimate specific parameters of interest in a population or system. Nevertheless, unlike frequentist statistics, Bayesian statisticians use prior knowledge on the population parameters to update their estimations on them along with the current evidence they can gather. This evidence is in the form of the repetition of \\(n\\) experiments involving a random phenomenon. All these ingredients allow Bayesian statisticians to make inference by conducting appropriate hypothesis testings, which are designed differently from their mainstream frequentist counterpart.\n\n\nThe unique known portrait of Reverend Thomas Bayes according to O’Donnell, T. (1936), even though Bellhouse (2004) argues it might not be a Bayes’ portrait.\n\nUnder the umbrella of this approach, we assume that our governing parameters are random; i.e., they have their own sample space and probabilities associated to their corresponding outcomes. The statistical process of inference is heavily backed up by probability theory mostly in the form of the Bayes theorem (named after Reverend Thomas Bayes, an English statistician from the 18th century). This theorem uses our current evidence along with our prior beliefs to deliver a posterior distribution of our random parameter(s) of interest.\n\n\n\n\nFrom a frequentist point of view, what is the estimated probability of preferring chocolate over vanilla after randomly surveying \\(n = 100\\) children from our population of interest?\nUsing a Bayesian approach, suppose the marketing team has found ten prior market studies on similar children populations on their preferred ice cream flavour (between chocolate and vanilla). Therefore, along with our actual random survey of \\(n = 100\\) children from our population of interest, what is the posterior estimation of the probability of preferring chocolate over vanilla?\n\n\n\n\n\n\n\nNote 2.1: Heads-up on the difference between frequentist and Bayesian statistics!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant 2.1: Definition of random sample",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span>"
    ]
  },
  {
    "objectID": "book/02-stats-review.html#sec-mle",
    "href": "book/02-stats-review.html#sec-mle",
    "title": "2  Basic Cuisine: A Review on Probability and Frequentist Statistical Inference",
    "section": "\n2.2 What is Maximum Likelihood Estimation?",
    "text": "2.2 What is Maximum Likelihood Estimation?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span>"
    ]
  },
  {
    "objectID": "book/02-stats-review.html#sec-basics-inf",
    "href": "book/02-stats-review.html#sec-basics-inf",
    "title": "2  Basic Cuisine: A Review on Probability and Frequentist Statistical Inference",
    "section": "\n2.3 Basics of Frequentist Statistical Inference",
    "text": "2.3 Basics of Frequentist Statistical Inference",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span>"
    ]
  },
  {
    "objectID": "book/02-stats-review.html#sec-sup-learning-regression",
    "href": "book/02-stats-review.html#sec-sup-learning-regression",
    "title": "2  Basic Cuisine: A Review on Probability and Frequentist Statistical Inference",
    "section": "\n2.4 Supervised Learning and Regression Analysis",
    "text": "2.4 Supervised Learning and Regression Analysis\n\n\n\n\nBellhouse, D. R. 2004. “The Reverend Thomas Bayes, FRS: A Biography to Celebrate the Tercentenary of His Birth.” Statistical Science 19 (1): 3–43. https://doi.org/10.1214/088342304000000189.\n\n\nO’Donnell, T. 1936. History of Life Insurance in Its Formative Years. Compiled from Approved Sources by T. O’Donnell. Chicago.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html",
    "href": "book/03-ols.html",
    "title": "3  Zestylicious Ordinary Least-squares Regression",
    "section": "",
    "text": "3.1 Learning outcomes",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Zestylicious Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#learning-outcomes",
    "href": "book/03-ols.html#learning-outcomes",
    "title": "3  Zestylicious Ordinary Least-squares Regression",
    "section": "",
    "text": "Fun fact!\n\n\nZestylicious! That mouth-puckering, lemon-squirted, totally tangy kick!\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 3.1",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Zestylicious Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html",
    "href": "book/04-gamma.html",
    "title": "4  Smoketastic Gamma Regression",
    "section": "",
    "text": "Fun fact!\n\n\nSmoketastic! For foods that seem to have been grilled by a campfire enthusiast.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 4.1",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Smoketastic Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/05-beta.html",
    "href": "book/05-beta.html",
    "title": "5  Soup-erb Beta Regression",
    "section": "",
    "text": "Fun fact!\n\n\nSoup-erb! Soup that’s so heartwarming it feels like a cozy hug.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 5.1",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Soup-erb Beta Regression</span>"
    ]
  },
  {
    "objectID": "book/06-parametric-survival.html",
    "href": "book/06-parametric-survival.html",
    "title": "6  Crunchified Parametric Survival Regression",
    "section": "",
    "text": "Fun fact!\n\n\nCrunchified! Extra crunchy, borderline noisy; could probably shatter glass.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 6.1",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Crunchified Parametric Survival Regression</span>"
    ]
  },
  {
    "objectID": "book/07-semiparametric-survival.html",
    "href": "book/07-semiparametric-survival.html",
    "title": "7  Butteryfied Semiparametric Survival Regression",
    "section": "",
    "text": "Fun fact!\n\n\nButteryfied! So rich and buttery it practically slides off the plate.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 7.1",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Butteryfied Semiparametric Survival Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html",
    "href": "book/08-binary-logistic.html",
    "title": "8  Sauce-sational Binary Logistic Regression",
    "section": "",
    "text": "Fun fact!\n\n\nSauce-sational! When the sauce is so good, it’s basically soup.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 8.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sauce-sational Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/09-binomial-logistic.html",
    "href": "book/09-binomial-logistic.html",
    "title": "9  Cheesified Binomial Logistic Regression",
    "section": "",
    "text": "Fun fact!\n\n\nCheesified! Oozing with cheese in every crevice; a cheese lover’s paradise.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma &lt;br/&gt;Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 9.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cheesified Binomial Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/10-classical-poisson.html",
    "href": "book/10-classical-poisson.html",
    "title": "10  Bubblarious Classical Poisson Regression",
    "section": "",
    "text": "Fun fact!\n\n\nBubblarious! For all the boba, fizzy drinks, and seltzers that go pop!\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 10.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Bubblarious Classical Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/11-negative-binomial.html",
    "href": "book/11-negative-binomial.html",
    "title": "11  Umami-zing Negative Binomial Regression",
    "section": "",
    "text": "Fun fact!\n\n\nUmami-zing! Savory to the point where you start craving a second plate… and a third.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 11.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Umami-zing Negative Binomial Regression</span>"
    ]
  },
  {
    "objectID": "book/12-zero-inflated-poisson.html",
    "href": "book/12-zero-inflated-poisson.html",
    "title": "12  Spicetacular Zero-Inflated Poisson Regression",
    "section": "",
    "text": "Fun fact!\n\n\nSpicetacular! The kind of spicy that requires a fire extinguisher at the ready.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n        {{Overdispersed or &lt;br/&gt;Underdispersed &lt;br/&gt;Data}}\n        {{Zero Inflated &lt;br/&gt;Data}}\n          )Chapter 12: &lt;br/&gt;Zero Inflated Poisson &lt;br/&gt;Regression(\n            (Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 12.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Spicetacular Zero-Inflated Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/13-generalized-poisson.html",
    "href": "book/13-generalized-poisson.html",
    "title": "13  Herbalicious Generalized Poisson Regression",
    "section": "",
    "text": "Fun fact!\n\n\nHerbalicious! Loaded with herbs and greens; like chewing through a botanical garden.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: &lt;br/&gt;Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n        {{Zero Inflated &lt;br/&gt;Data}}\n          )Chapter 12: &lt;br/&gt;Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed or &lt;br/&gt;Underdispersed &lt;br/&gt;Data}}\n          )Chapter 13: &lt;br/&gt;Generalized &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Generalized &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 13.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Herbalicious Generalized Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/14-multinomial-logistic.html",
    "href": "book/14-multinomial-logistic.html",
    "title": "14  Picklified Multinomial Logistic Regression",
    "section": "",
    "text": "Fun fact!\n\n\nPicklified! When everything, even dessert, tastes a bit pickled!\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n        {{Zero Inflated &lt;br/&gt;Data}}\n          )Chapter 12: &lt;br/&gt;Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed or &lt;br/&gt;Underdispersed &lt;br/&gt;Data}}\n          )Chapter 13: &lt;br/&gt;Generalized &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Generalized &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n      {{Categorical &lt;br/&gt;Outcome Y}}\n        {{Nominal &lt;br/&gt;Outcome Y}}\n          )Chapter 14: &lt;br/&gt;Multinomial &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Multinomial &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 14.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Picklified Multinomial Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/15-ordinal-logistic.html",
    "href": "book/15-ordinal-logistic.html",
    "title": "15  Tang-tastic Ordinal Logistic Regression",
    "section": "",
    "text": "Fun fact!\n\n\nTang-tastic! So tangy it could wake you up better than coffee.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n        {{Overdispersed or &lt;br/&gt;Underdispersed &lt;br/&gt;Data}}\n          )Chapter 13: &lt;br/&gt;Generalized &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Generalized &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n        {{Zero Inflated &lt;br/&gt;Data}}\n          )Chapter 12: &lt;br/&gt;Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n      {{Categorical &lt;br/&gt;Outcome Y}}\n        {{Nominal &lt;br/&gt;Outcome Y}}\n          )Chapter 14: &lt;br/&gt;Multinomial &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Multinomial &lt;br/&gt;Outcome Y)\n        {{Ordinal &lt;br/&gt;Outcome Y}}\n          )Chapter 15: &lt;br/&gt;Ordinal &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Logistic &lt;br/&gt;Distributed &lt;br/&gt;Cumulative Outcome &lt;br/&gt;Probability)\n\n\n\n\n\n\n\n\nFigure 15.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Tang-tastic Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/references.html",
    "href": "book/references.html",
    "title": "References",
    "section": "",
    "text": "Bellhouse, D. R. 2004. “The Reverend Thomas\nBayes, FRS: A Biography to Celebrate the Tercentenary of His\nBirth.” Statistical Science 19 (1): 3–43. https://doi.org/10.1214/088342304000000189.\n\n\nGelbart, Michael. 2017. “Data Science Terminology.” UBC\nMDS. Master of Data Science at the University of British Columbia.\nhttps://ubc-mds.github.io/resources_pages/terminology/.\n\n\nO’Donnell, T. 1936. History of Life Insurance\nin Its Formative Years. Compiled from Approved Sources by T.\nO’Donnell. Chicago.\n\n\nR Core Team. 2024. “R: A Language and Environment for Statistical\nComputing.” Vienna, Austria: R Foundation for Statistical\nComputing. https://www.R-project.org/.\n\n\nThe Pandas Development Team. 2024. “Pandas-Dev/Pandas:\nPandas.” Zenodo. https://doi.org/10.5281/zenodo.3509134.\n\n\nVan Rossum, Guido, and Fred L. Drake. 2009. Python 3 Reference\nManual. Scotts Valley, CA: CreateSpace.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "book/A-dictionary.html",
    "href": "book/A-dictionary.html",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "",
    "text": "A",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#a",
    "href": "book/A-dictionary.html#a",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "",
    "text": "Attribute\n\n\nEquivalent to:\n\n\nCovariate, exogeneous variable, explanatory variable, feature, independent variable, input, predictor or regressor.\n\n\nAverage\n\n\nEquivalent to:\n\n\nExpected value or mean.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#b",
    "href": "book/A-dictionary.html#b",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "B",
    "text": "B\nBayesian statistics\nThis statistical school of thinking also relies on the frequency of events to estimate specific parameters of interest in a population or system. Nevertheless, unlike frequentist statistics, Bayesian statisticians use prior knowledge on the population parameters to update their estimations on them along with the current evidence they can gather. This evidence is in the form of the repetition of \\(n\\) experiments involving a random phenomenon. All these ingredients allow Bayesian statisticians to make inference by conducting appropriate hypothesis testings, which are designed differently from their mainstream frequentist counterpart.\nUnder the umbrella of this approach, we assume that our governing parameters are random; i.e., they have their own sample space and probabilities associated to their corresponding outcomes. The statistical process of inference is heavily backed up by probability theory mostly in the form of the Bayes theorem (named after Reverend Thomas Bayes, an English statistician from the 18th century). This theorem uses our current evidence along with our prior beliefs to deliver a posterior distribution of our random parameter(s) of interest.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#c",
    "href": "book/A-dictionary.html#c",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "C",
    "text": "C\nContinuous random variable\nCovariate\n\n\nEquivalent to:\n\n\nAttribute, exogeneous variable, explanatory variable, feature, independent variable, input, predictor or regressor.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#d",
    "href": "book/A-dictionary.html#d",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "D",
    "text": "D\nDependent variable\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\nEquivalent to:\n\n\nEndogeneous variable, response variable, outcome, output or target.\n\n\nDiscrete random variable",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#e",
    "href": "book/A-dictionary.html#e",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "E",
    "text": "E\nEquidispersion\nExpected value\n\n\nEquivalent to:\n\n\nAverage or mean.\n\n\nExogeneous variable\n\n\nEquivalent to:\n\n\nAttribute, covariate, explanatory variable, feature, independent variable, input, predictor or regressor.\n\n\nExplanatory variable\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, feature, independent variable, input, predictor or regressor.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#f",
    "href": "book/A-dictionary.html#f",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "F",
    "text": "F\nFeature\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, explanatory variable, independent variable, input, predictor or regressor.\n\n\nFrequentist statistics\nThis statistical school of thinking heavily relies on the frequency of events to estimate specific parameters of interest in a population or system. This frequency of events is reflected in the repetition of \\(n\\) experiments involving a random phenomenon within this population or system.\nUnder the umbrella of this approach, we assume that our governing parameters are fixed. Note that, within the philosophy of this school of thinking, we can only make precise and accurate predictions as long as we repeat our \\(n\\) experiments as many times as possible, i.e.,\n\\[\nn \\rightarrow \\infty.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#g",
    "href": "book/A-dictionary.html#g",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "G",
    "text": "G\nGeneralized linear model",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#h",
    "href": "book/A-dictionary.html#h",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "H",
    "text": "H\nHypothesis testing",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#i",
    "href": "book/A-dictionary.html#i",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "I",
    "text": "I\nIndependence\nIndependent variable\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, explanatory variable, feature, input, predictor or regressor.\n\n\nInput\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, explanatory variable, feature, independent variable, predictor or regressor.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#m",
    "href": "book/A-dictionary.html#m",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "M",
    "text": "M\nMean\n\n\nEquivalent to:\n\n\nAverage or expected value.\n\n\nMeasure of central tendency\nMeasure of uncertainty",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#o",
    "href": "book/A-dictionary.html#o",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "O",
    "text": "O\nOutcome\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\nEquivalent to:\n\n\nDependent variable, response variable, output or target.\n\n\nOutput\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\nEquivalent to:\n\n\nDependent variable, response variable, outcome or target.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#p",
    "href": "book/A-dictionary.html#p",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "P",
    "text": "P\nParameter\nIt is a characteristic (numerical or even non-numerical, such as a distinctive category) that summarizes the state of our population or system of interest. Examples of a population parameter can be described as follows:\n\nThe average weight of children between the ages of 5 and 10 years old in states of the American west coast (numerical).\nThe variability in the height of the mature açaí palm trees from the Brazilian Amazonian jungle (numerical).\nThe proportion of defective items in the production of cellular phones in a set of manufacturing facilities (numerical).\nThe average customer waiting time to get their order in the Vancouver franchises of a well-known ice cream parlour (numerical).\nThe most favourite pizza topping of vegetarian adults between the ages of 30 and 40 years old in Edmonton (non-numerical).\n\nNote the standard mathematical notation for population parameters are Greek letters. Moreover, in practice, these population parameter(s) of interest will be unknown to the data scientist or researcher. Instead, they would use formal statistical inference to estimate them.\nPopulation\nIt is a whole collection of individuals or items that share distinctive attributes. As data scientists or researchers, we are interested in studying these attributes, which we assume are governed by parameters. In practice, we must be as precise as possible when defining our given population such that we would frame our entire data modelling process since its very early stages. Examples of a population could be the following:\n\nChildren between the ages of 5 and 10 years old in states of the American west coast.\nCustomers of musical vinyl records in the Canadian provinces of British Columbia and Alberta.\nAvocado trees grown in the Mexican state of Michoacán.\nAdult giant pandas in the Southwestern Chinese province of Sichuan.\nMature açaí palm trees from the Brazilian Amazonian jungle.\n\nNote that the term population could be exchanged for the term system, given that certain contexts do not specifically refer to individuals or items. Instead, these contexts could refer to processes whose attributes are also governed by parameters. Examples of a system could be the following:\n\nThe production of cellular phones in a set of manufacturing facilities.\nThe sale process in the Vancouver franchises of a well-known ice cream parlour.\nThe transit cycle of the twelve lines of Mexico City’s subway.\nPredictor\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, explanatory variable, feature, independent variable, input or regressor.\n\n\nProbability\nLet \\(A\\) be an event of interest in a random phenomenon, in a population or system of interest, whose all possible outcomes belong to a given sample space \\(S\\). Generally, the probability for this event \\(A\\) happening can be mathematically depicted as \\(P(A)\\). Moreover, suppose we observe the random phenomenon \\(n\\) times such as we were running some class of experiment, then \\(P(A)\\) is defined as the following ratio:\n\\[\nP(A) = \\frac{\\text{Number of times event $A$ is observed}}{n},\n\\tag{A.1}\\]\nas the \\(n\\) times we observe the random phenomenon goes to infinity.\nEquation A.1 will always put \\(P(A)\\) in the following numerical range:\n\\[\n0 \\leq P(A) \\leq 1.\n\\]\nProbability distribution\nProbability mass function (PMF)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#r",
    "href": "book/A-dictionary.html#r",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "R",
    "text": "R\nRandom Variable\nRegression Analysis\nRegressor\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, explanatory variable, feature, independent variable, input or predictor.\n\n\nResponse variable\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\nEquivalent to:\n\n\nDependent variable, outcome, output or target.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#s",
    "href": "book/A-dictionary.html#s",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "S",
    "text": "S\nSample space\nLet \\(A\\) be an event of interest in a random phenomenon in a population or system of interest. The sample space \\(S\\) of event \\(A\\) denotes the set of all the possible random outcomes we might encounter every time we randomly observe \\(A\\) such as we were running some class of experiment.\nNote each of these outcomes has a determined probability associated with them. If we add up all these probabilities, the probability of the sample \\(S\\) will be one, i.e.,\n\\[\nP(S) = 1.\n\\tag{A.2}\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#t",
    "href": "book/A-dictionary.html#t",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "T",
    "text": "T\nTarget\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\nEquivalent to:\n\n\nDependent variable, response variable, outcome or output.\n\n\nTraining dataset",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#v",
    "href": "book/A-dictionary.html#v",
    "title": "Appendix A — The Fusionified ML-Stats Dictionary",
    "section": "V",
    "text": "V\nVariance",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The Fusionified ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/B-greek-alphabet.html",
    "href": "book/B-greek-alphabet.html",
    "title": "Appendix B — The Snazzalicious Greek Alphabet",
    "section": "",
    "text": "Fun fact!\n\n\nSnazzalicious! Food that’s dressed up, fancy, and begging for a photo.\n\n\nStatistical notation can be pretty particular and different from usual mathematical notation. One of these particularities is the constant use of Greek letters to denote unknown population parameters in modelling setup, estimation, and statistical inference. In that spirit, throughout this book, we use diverse Greek letters to denote our regression parameters across each of the outlined models in every chapter.\n\n\nImage by meineresterampe via Pixabay.\n\nDuring early learning stages of regression modelling, we may feel overwhelmed by these new letters, which could be unfamiliar. Therefore, whenever confusion arises in any of the main chapters in this book regarding the names of these letters, we recommend checking out the Greek alphabet from Table B.1. Note that frequentist statistical inference mostly uses lowercase letters. With practice over time, you would likely end up memorizing most of this alphabet.\n\n\nTable B.1: Greek alphabet composed of 24 letters, from left to right you can find the name of letter along with its corresponding uppercase and lowercase forms.\n\n\n\nName\nUppercase\nLowercase\n\n\n\nAlpha\n\\(\\text{A}\\)\n\\(\\alpha\\)\n\n\nBeta\n\\(\\text{B}\\)\n\\(\\beta\\)\n\n\nGamma\n\\(\\Gamma\\)\n\\(\\gamma\\)\n\n\nDelta\n\\(\\Delta\\)\n\\(\\delta\\)\n\n\nEpsilon\n\\(\\text{E}\\)\n\\(\\epsilon\\)\n\n\nZeta\n\\(\\text{Z}\\)\n\\(\\zeta\\)\n\n\nEta\n\\(\\text{H}\\)\n\\(\\eta\\)\n\n\nTheta\n\\(\\Theta\\)\n\\(\\theta\\)\n\n\nIota\n\\(\\text{I}\\)\n\\(\\iota\\)\n\n\nKappa\n\\(\\text{K}\\)\n\\(\\kappa\\)\n\n\nLambda\n\\(\\Lambda\\)\n\\(\\lambda\\)\n\n\nMu\n\\(\\text{M}\\)\n\\(\\mu\\)\n\n\nNu\n\\(\\text{N}\\)\n\\(\\nu\\)\n\n\nXi\n\\(\\Xi\\)\n\\(\\xi\\)\n\n\nO\n\\(\\text{O}\\)\n\\(\\text{o}\\)\n\n\nPi\n\\(\\Pi\\)\n\\(\\pi\\)\n\n\nRho\n\\(\\text{P}\\)\n\\(\\rho\\)\n\n\nSigma\n\\(\\Sigma\\)\n\\(\\sigma\\)\n\n\nTau\n\\(\\text{T}\\)\n\\(\\tau\\)\n\n\nUpsilon\n\\(\\Upsilon\\)\n\\(\\upsilon\\)\n\n\nPhi\n\\(\\Phi\\)\n\\(\\phi\\)\n\n\nChi\n\\(\\text{X}\\)\n\\(\\chi\\)\n\n\nPsi\n\\(\\Psi\\)\n\\(\\psi\\)\n\n\nOmega\n\\(\\Omega\\)\n\\(\\omega\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>The Snazzalicious Greek Alphabet</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html",
    "href": "book/C-distributional-mind-map.html",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "",
    "text": "D Discrete Random Variables\nLet us recall what a discrete random variable is. This type of variable is defined to take on a set of countable values. In other words, these values belong to a finite set. Figure C.1 delves into the following specific probability distributions:\nTable D.1 outlines the parameter(s), support, mean, and variance for each discrete probability distribution utilized to model the target \\(Y\\) in a specific regression tool explained in this book.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#bernoulli",
    "href": "book/C-distributional-mind-map.html#bernoulli",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nD.1 Bernoulli",
    "text": "D.1 Bernoulli\nLet \\(Y\\) be a discrete random variable that is part of a random process or system. \\(Y\\) can only take on the following values:\n\\[\nY =\n\\begin{cases}\n1 \\; \\; \\; \\; \\text{if there is a success},\\\\\n0 \\; \\; \\; \\; \\mbox{otherwise}.\n\\end{cases}\n\\tag{D.1}\\]\nNote that the support of \\(Y\\) in Equation D.1 makes it binary with these outcomes: \\(1\\) for success and \\(0\\) for failure. Then, \\(Y\\) is said to have a Bernoulli distribution with parameter \\(\\pi\\):\n\\[Y \\sim \\text{Bern}(\\pi).\\]\n\nD.1.1 Probability Mass Function\nThe probability mass function (PMF) of \\(Y\\) is the following:\n\\[\nP \\left( Y = y \\mid \\pi \\right) = \\pi^y (1 - \\pi)^{1 - y} \\quad \\text{for $y \\in \\{ 0, 1 \\}$.}\n\\tag{D.2}\\]\nParameter \\(\\pi \\in [0, 1]\\) refers to the probability of success. We can verify Equation D.2 is a proper probability distribution (i.e., all the standalone probabilities over the support of \\(Y\\) add up to one) given that:\n\nProof. \\[\n\\begin{align*}\n\\sum_{y = 0}^1 P \\left( Y = y \\mid \\pi \\right) &=  \\sum_{y = 0}^1 \\pi^y (1 - \\pi)^{1 - y}  \\\\\n&= \\underbrace{\\pi^0}_{1} (1 - \\pi) + \\pi \\underbrace{(1 - \\pi)^{0}}_{1} \\\\\n&= (1 - \\pi) + \\pi \\\\\n&= 1. \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\n\nD.1.2 Expected Value\nVia Equation C.3, the expected value or mean of a Bernoulli-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\sum_{y = 0}^1 y P \\left( Y = y \\mid \\pi \\right) \\\\\n&= \\sum_{y = 0}^1 y \\left[ \\pi^y (1 - \\pi)^{1 - y} \\right] \\\\\n&= \\underbrace{(0) \\left[ \\pi^0 (1 - \\pi) \\right]}_{0} + (1) \\left[ \\pi (1 - \\pi)^{0} \\right] \\\\\n&= 0 + \\pi \\\\\n&= \\pi. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\n\nD.1.3 Variance\nVia Equation C.5 and the Equation C.3 of a discrete expected value, the variance of a Bernoulli-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E}(Y^2) - \\left[ \\mathbb{E}(Y)\\right]^2 \\\\\n&= \\mathbb{E}(Y^2) - \\pi^2 \\qquad \\text{since $\\mathbb{E}(Y) = \\pi$} \\\\\n&= \\sum_{y = 0}^1 y^2 P \\left( Y = y \\mid \\pi \\right) - \\pi^2 \\\\\n&= \\left\\{ \\underbrace{(0^2) \\left[ \\pi^0 (1 - \\pi) \\right]}_{0} + \\underbrace{(1^2) \\left[ \\pi (1 - \\pi)^{0} \\right]}_{\\pi} \\right\\} - \\pi^2 \\\\\n&= (0 + \\pi) - \\pi^2 \\\\\n&= \\pi - \\pi^2 \\\\\n&= \\pi (1 - \\pi). \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#binomial",
    "href": "book/C-distributional-mind-map.html#binomial",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nD.2 Binomial",
    "text": "D.2 Binomial\nSuppose you execute \\(n\\) independent Bernoulli trials, each one with a probability of success \\(\\pi\\). Let \\(Y\\) be the number of successes obtained within these \\(n\\) Bernoulli trials. Then, \\(Y\\) is said to have a Binomial distribution with parameters \\(n\\) and \\(\\pi\\):\n\\[Y \\sim \\text{Bin}(n, \\pi).\\]\n\nD.2.1 Probability Mass Function\nThe PMF of \\(Y\\) is the following:\n\\[\n\\begin{align*}\nP \\left( Y = y \\mid n, \\pi \\right) &= {n \\choose y} \\pi^y (1 - \\pi)^{n - y} \\\\\n& \\qquad \\qquad \\qquad \\text{for $y \\in \\{ 0, 1, \\dots, n \\}$.}\n\\end{align*}\n\\tag{D.3}\\]\nParameters \\(\\pi \\in [0, 1]\\) and \\(n \\in \\mathbb{N}\\) refer to the probability of success and number of trials, respectively. On the other hand, the term \\({n \\choose y}\\) indicates the total number of possible combinations for \\(y\\) successes out of our \\(n\\) trials:\n\\[\n{n \\choose y} = \\frac{n!}{y!(n - y)!}.\n\\tag{D.4}\\]\n\nHow can we verify that Equation D.3 is a proper PMF (i.e., all the standalone probabilities over the support of \\(Y\\) add up to one)?\n\nTo elaborate on this, we need to use a handy mathematical result called the binomial theorem.\n\nTheorem D.1 (Binomial Theorem) This theorem is associated to the Pascal’s identity, and it defines the pattern of coefficients in the expansion of a polynomial in the form \\((u + v)^m\\). More specifically, the binomial theorem indicates that if \\(m\\) is a non-negative integer, then the polynomial \\((u + v)^m\\) can be expanded via the following series:\n\\[\n\\begin{align*}\n(u + v)^m &= u^m + {m \\choose 1} u^{m - 1} v + {m \\choose 2} u^{m - 2} v^2 + \\dots + \\\\\n& \\qquad {m \\choose r} u^{m - r} v^r + \\dots + \\\\\n& \\qquad {m \\choose m - 1} u v^{m - 1} + v^m \\\\\n&= \\underbrace{{m \\choose 0}}_1 u^m + {m \\choose 1} u^{m - 1} v + {m \\choose 2} u^{m - 2} v^2 + \\dots + \\\\\n& \\qquad {m \\choose r} u^{m - r} v^r + \\dots + \\\\\n& \\qquad {m \\choose m - 1} u v^{m - 1} + \\underbrace{{m \\choose m}}_1 v^m \\\\\n&= \\sum_{i = 0}^m {m \\choose i} u^{m - i} v^i.\n\\end{align*}\n\\tag{D.5}\\]\n\n\n\nTip on the binomial theorem and Pascal’s identity\n\n\nLet us dig into the proof of the binomial theorem from Equation D.5. This proof will require another important result called the Pascal’s identity. This identity states that for any integers \\(m\\) and \\(k\\), with \\(k \\in \\{ 1, \\dots, m \\}\\), it follows that:\n\nProof. \\[\n\\begin{align*}\n{m \\choose k - 1} + {m \\choose k} &= \\left[ \\frac{m!}{(k - 1)! (m - k + 1)!} \\right] \\\\\n& \\qquad + \\left[ \\frac{m!}{k! (m - k)!} \\right] \\\\\n&= m! \\biggl\\{ \\left[ \\frac{1}{(k - 1)! (m - k + 1)!} \\right] + \\\\\n& \\qquad \\left[ \\frac{1}{k! (m - k)!} \\right] \\biggl\\} \\\\\n&= m! \\Biggl\\{ \\Biggr[ \\frac{k}{\\underbrace{k (k - 1)!}_{k!} (m - k + 1)!} \\Biggr] + \\\\\n& \\qquad \\Biggr[ \\frac{m - k + 1}{k! \\underbrace{(m - k + 1)(m - k)!}_{(m - k + 1)!}} \\Biggr] \\Biggl\\}  \\\\\n&= m! \\left[ \\frac{k + m - k + 1}{k! (m - k + 1)!} \\right] \\\\\n&= m! \\left[ \\frac{m + 1}{k! (m - k + 1)!} \\right] \\\\\n&= \\frac{(m + 1)!}{k! (m + 1 - k)!} \\\\\n&= {m + 1 \\choose k }. \\qquad \\qquad \\qquad \\qquad \\square\n\\end{align*}\n\\tag{D.6}\\]\n\nNow, we will use mathematical induction to prove the binomial theorem from Equation D.5. Firstly, on the left-hand side of the theorem, note that when \\(m = 0\\) we have:\n\\[\n(u + v)^0 = 1.\n\\]\nNow, when \\(m = 0\\), for the right-hand side of this equation, we have that\n\\[\n\\sum_{i = 0}^m {m \\choose i} u^{m - i} v^i  = \\sum_{i = 0}^0 {0 \\choose i} u^i v^{i} = {0 \\choose 0} u^0 v^0 = 1.\n\\]\nHence, the binomial theorem holds when \\(m = 0\\). This is what we call the base case in mathematical induction.\nThat said, let us proceed with the inductive hypothesis. We aim to prove that the binomial theorem\n\\[\n\\begin{align*}\n(u + v)^j &= u^j + {j \\choose 1} u^{j - 1} v + {j \\choose 2} u^{j - 2} v^2 + \\dots + \\\\\n& \\qquad {j \\choose r} u^{j - r} v^r + \\dots + \\\\\n& \\qquad {j \\choose j - 1} u v^{j - 1} + v^j \\\\\n&= \\underbrace{{j \\choose 0}}_1 u^j + {j \\choose 1} u^{j - 1} v + {j \\choose 2} u^{j - 2} v^2 + \\dots + \\\\\n& \\qquad {j \\choose r} u^{j - r} v^r + \\dots + \\\\\n& \\qquad {j \\choose j - 1} u v^{j - 1} + \\underbrace{{j \\choose j}}_1 v^j \\\\\n&= \\sum_{i = 0}^j {j \\choose i} u^{j - i} v^i\n\\end{align*}\n\\tag{D.7}\\]\nholds when integer \\(j \\geq 1\\). This is our inductive hypothesis.\nThen, we pave the way to the inductive step. Let us consider the following expansion:\n\\[\n\\begin{align*}\n(u + v)^{j + 1} &= (u + v) (u + v)^j \\\\\n&= (u + v) \\times \\\\\n& \\qquad \\bigg[ u^j + {j \\choose 1} u^{j - 1} v + {j \\choose 2} u^{j - 2} v^2 + \\dots + \\\\\n& \\qquad {j \\choose r} u^{j - r} v^r + \\dots + {j \\choose j - 1} u v^{j - 1} + v^j \\bigg] \\\\\n&= \\bigg[u^{j + 1} + {j \\choose 1} u^j v + {j \\choose 2} u^{j - 1} v^2 + \\dots + \\\\\n& \\qquad {j \\choose r} u^{j - r + 1} v^r + \\dots + \\\\\n& \\qquad {j \\choose j - 1} u^2 v^{j - 1} + u v^j \\bigg] + \\\\\n& \\qquad \\bigg[ u^j v + {j \\choose 1} u^{j - 1} v^2 + {j \\choose 2} u^{j - 2} v^3 + \\dots + \\\\\n& \\qquad {j \\choose r} u^{j - r} v^{r + 1} + \\dots + \\\\\n& \\qquad {j \\choose j - 1} u v^j + {j \\choose j} v^{j + 1} \\bigg] \\\\\n&= u^{j + 1} + \\left[ {j \\choose 0} + {j \\choose 1} \\right] u^j v + \\\\\n& \\qquad \\left[ {j \\choose 1} + {j \\choose 2} \\right] u^{j - 1} v^2 + \\dots + \\\\\n& \\qquad \\left[ {j \\choose r - 1} + {j \\choose r} \\right] u^{j - r + 1} v^r + \\dots + \\\\\n& \\qquad \\left[ {j \\choose j - 1} + {j \\choose j} \\right] u v^j + v^{j + 1}.\n\\end{align*}\n\\tag{D.8}\\]\nLet us plug in the Pascal’s identity from Equation D.6 into Equation D.8:\n\\[\n\\begin{align*}\n(u + v)^{j + 1} &= u^{j + 1} + {j + 1 \\choose 1} u^j v + \\\\\n& \\qquad {j + 1 \\choose 2} u^{j - 1} v^2 + \\dots + \\\\\n& \\qquad {j + 1 \\choose r} u^{j - r + 1} v^r + \\dots + \\\\\n& \\qquad {j + 1 \\choose j} u v^j + v^{j + 1} \\\\\n&= \\underbrace{{j + 1 \\choose 0}}_1 u^{j + 1} + {j + 1 \\choose 1} u^j v + \\\\\n& \\qquad {j + 1 \\choose 2} u^{j - 1} v^2 + \\dots + \\\\\n& \\qquad {j + 1 \\choose r} u^{j - r + 1} v^r + \\dots + \\\\\n& \\qquad {j + 1 \\choose j} u v^j + \\underbrace{{j + 1 \\choose j + 1}}_1 v^{j + 1} \\\\\n&= \\sum_{i = 0}^{j + 1} {j + 1 \\choose i} u^{j + 1 - i} v^i. \\qquad \\quad \\square\n\\end{align*}\n\\tag{D.9}\\]\nNote that the result for \\(j\\) in Equation D.7 also holds for \\(j + 1\\) in Equation D.9. Therefore, by induction, the binomial theorem from Equation D.5 is true for all positive integers \\(m\\).\n\n\nAfter the above fruitful digression on the binomial theorem, let us use it to show that our Binomial PMF in Equation D.3 actually adds up to one all over the support of the random variable:\n\nProof. \\[\n\\begin{align*}\n\\sum_{y = 0}^n P \\left( Y = y \\mid n, \\pi \\right) &= \\sum_{y = 0}^n {n \\choose y} \\pi^y (1 - \\pi)^{n - y} \\\\\n&= \\sum_{y = 0}^n {n \\choose y} (1 - \\pi)^{n - y} \\pi^y \\\\\n& \\quad \\qquad \\text{rearranging factors.}\n\\end{align*}\n\\]\nNow, by using the binomial theorem in Equation D.5, let:\n\\[\n\\begin{gather*}\nm  = n\\\\\ni = y \\\\\nu = 1 - \\pi \\\\\nv = \\pi.\n\\end{gather*}\n\\]\nThe above arrangement yields the following result:\n\\[\n\\begin{align*}\n\\sum_{y = 0}^n P \\left( Y = y \\mid n, \\pi \\right) &= (1 - \\pi + \\pi)^n \\\\\n&= 1^n = 1. \\qquad \\square\n\\end{align*}\n\\tag{D.10}\\]\nIndeed, the Binomial PMF is a proper probability distribution!\n\n\nD.2.2 Expected Value\nVia Equation C.3, the expected value or mean of a Binomial-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\sum_{y = 0}^n y P \\left( Y = y \\mid n, \\pi \\right) \\\\\n&= \\sum_{y = 1}^n y P \\left( Y = y \\mid n, \\pi \\right) \\\\\n& \\quad \\qquad \\text{for $y = 0$, the addend is equal to zero} \\\\\n&= \\sum_{y = 1}^n y \\left[ {n \\choose y} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n&= \\sum_{y = 1}^n y \\left[ \\frac{n!}{y! (n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n&= \\sum_{y = 1}^n \\left[ \\frac{y n!}{y (y - 1)!(n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{in the denominator, $y! = y (y - 1)!$}\\\\\n&= \\sum_{y = 1}^n \\left[ \\frac{n (n - 1)!}{(y - 1)!(n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{in the numerator, $n! = n (n - 1)!$} \\\\\n&= \\sum_{y = 1}^n \\left[ \\frac{n (n - 1)!}{(y - 1)!(n - y)!} \\pi^{y + 1 - 1} (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{note $\\pi^y = \\pi^{y + 1 - 1}$} \\\\\n&= n \\sum_{y = 1}^n \\left[ \\frac{(n - 1)!}{(y - 1)!(n - y)!} \\pi \\pi^{y - 1} (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{rearranging terms} \\\\\n&= n \\pi \\sum_{y = 1}^n \\left[ \\frac{(n - 1)!}{(y - 1)!(n - y)!} \\pi^{y - 1} (1 - \\pi)^{n - y} \\right]\n\\end{align*}\n\\tag{D.11}\\]\nNow, let us make the following variable rearrangement:\n\\[\n\\begin{gather*}\nm = n - 1 \\\\\nz = y - 1 \\\\\nm - z = n - y.\n\\end{gather*}\n\\]\nGoing back to Equation D.11, and applying our above variable rearrangement within the summation, we have:\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= n \\pi \\sum_{z = 0}^m \\left[ \\frac{m!}{z!(m - z)!} \\pi^{z} (1 - \\pi)^{m - z} \\right] \\\\\n&= n \\pi \\sum_{z = 0}^m \\left[ {m \\choose z}\\pi^{z} (1 - \\pi)^{m - z} \\right]\n\\end{align*}\n\\tag{D.12}\\]\n\nNote that, in the summation of Equation D.12, we encounter the PMF of a random variable \\(Z\\) as follows:\n\\[\nZ \\sim \\text{Bin}(m, \\pi).\n\\]\nSince the summation, where this Binomial PMF of \\(Z\\) is depicted, goes from \\(z = 0\\) to \\(m\\), we can apply our result from Equation D.10:\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= n \\pi \\underbrace{\\sum_{z = 0}^m \\left[ {m \\choose z}\\pi^{z} (1 - \\pi)^{m - z} \\right]}_{1} \\\\\n& \\quad \\qquad \\text{the summation adds up to 1} \\\\\n&= n \\pi. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\nD.2.3 Variance\nVia Equation C.5 and the Equation C.3 of a discrete expected value, the variance of a Binomial-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E}(Y^2) - \\left[ \\mathbb{E}(Y)\\right]^2 \\\\\n&= \\mathbb{E}(Y^2) - (n \\pi)^2 \\qquad \\text{since $\\mathbb{E}(Y) = n \\pi$.}\n\\end{align*}\n\\tag{D.13}\\]\nUnlike the Bernoulli random variable, finding \\(\\mathbb{E}(Y^2)\\) is not quite straightforward. We need to play around with its expected value expression as follows:\n\\[\n\\begin{align*}\n\\mathbb{E}(Y^2) &= \\mathbb{E} \\left[ Y (Y - 1) \\right] + \\mathbb{E}(Y) \\\\\n&= \\mathbb{E} \\left[ Y (Y - 1) \\right] + n \\pi \\qquad \\text{since $\\mathbb{E}(Y) = n \\pi$.}\n\\end{align*}\n\\tag{D.14}\\]\nNow, to find \\(\\mathbb{E} \\left[ Y (Y - 1) \\right]\\), we make the following derivation via Equation C.1 when \\(g(Y) = y (y - 1)\\):\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= \\sum_{y = 0}^n y (y - 1) P \\left( Y = y \\mid n, \\pi \\right) \\\\\n&= \\sum_{y = 2}^n y (y - 1) P \\left( Y = y \\mid n, \\pi \\right) \\\\\n& \\quad \\qquad \\text{for $y = 0, 1$; the addends are equal to zero} \\\\\n&= \\sum_{y = 2}^n y (y - 1) \\left[ {n \\choose y} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n&= \\sum_{y = 2}^n y (y - 1) \\left[ \\frac{n!}{y! (n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n&= \\sum_{y = 2}^n \\left[ \\frac{y (y - 1) n!}{y (y - 1) (y - 2)! (n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\\\\n&= \\sum_{y = 2}^n \\left[ \\frac{n (n - 1) (n - 2)!}{(y - 2)! (n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{in the numerator, $n! = n (n - 1) (n - 2)!$} \\\\\n&= \\sum_{y = 2}^n \\left[ \\frac{n (n - 1) (n - 2)!}{(y - 2)! (n - y)!} \\pi^{y + 2 - 2} (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{note $\\pi^y = \\pi^{y + 2 - 2}$} \\\\\n&= n (n - 1) \\times \\\\\n& \\qquad \\sum_{y = 2}^n \\left[ \\frac{(n - 2)!}{(y - 2)! (n - y)!} \\pi^2 \\pi^{y - 2} (1 - \\pi)^{n - y} \\right] \\\\\n& \\qquad \\qquad \\text{rearranging terms} \\\\\n&= n (n - 1) \\pi^2 \\times \\\\\n& \\qquad \\sum_{y = 2}^n \\left[ \\frac{(n - 2)!}{(y - 2)! (n - y)!} \\pi^{y - 2} (1 - \\pi)^{n - y} \\right] \\\\\n& \\qquad \\qquad \\text{rearranging terms}\n\\end{align*}\n\\tag{D.15}\\]\nThen, we make the following variable rearrangement:\n\\[\n\\begin{gather*}\nm = n - 2 \\\\\nz = y - 2 \\\\\nm - z = n - y.\n\\end{gather*}\n\\]\nGoing back to Equation D.15, and applying our above variable rearrangement within the summation, we have:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= n (n - 1) \\pi^2 \\sum_{z = 0}^m \\left[ \\frac{m!}{z! (m - z)!} \\pi^{z} (1 - \\pi)^{m - z} \\right].\n\\end{align*}\n\\tag{D.16}\\]\nNote that, in the summation of Equation D.16, we encounter the PMF of a random variable \\(Z\\) as follows:\n\\[\nZ \\sim \\text{Bin}(m, \\pi).\n\\]\nSince the summation, where this Binomial PMF of \\(Z\\) is depicted, goes from \\(z = 0\\) to \\(m,\\) we can apply our result from Equation D.10:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= n (n - 1) \\pi^2 \\underbrace{\\sum_{z = 0}^m \\left[ \\frac{m!}{z! (m - z)!} \\pi^{z} (1 - \\pi)^{m - z} \\right]}_{1} \\\\\n& \\qquad \\qquad \\qquad \\quad \\text{the summation adds up to 1} \\\\\n&= n (n - 1) \\pi^2.\n\\end{align*}\n\\]\nLet us go back to Equation D.14 and plug in the above result:\n\\[\n\\begin{align*}\n\\mathbb{E}(Y^2) &= \\mathbb{E} \\left[ Y (Y - 1) \\right] + n \\pi \\\\\n&= n (n - 1) \\pi^2 + n \\pi. \\\\\n\\end{align*}\n\\]\n\nFinally, we plug in \\(\\mathbb{E}(Y^2)\\) in Equation D.13:\n\\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E}(Y^2) - (n \\pi)^2 \\\\\n&= n (n - 1) \\pi^2 + n \\pi - n^2 \\pi^2 \\\\\n&= n^2 \\pi^2 - n \\pi^2 + n \\pi - n^2 \\pi^2 \\\\\n&= n \\pi - n \\pi^2 \\\\\n&= n \\pi (1 - \\pi). \\qquad \\qquad \\qquad \\square\n\\end{align*}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#negative-binomial",
    "href": "book/C-distributional-mind-map.html#negative-binomial",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nD.3 Negative Binomial",
    "text": "D.3 Negative Binomial",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#classical-poisson",
    "href": "book/C-distributional-mind-map.html#classical-poisson",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nD.4 Classical Poisson",
    "text": "D.4 Classical Poisson",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#generalized-poisson",
    "href": "book/C-distributional-mind-map.html#generalized-poisson",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nD.5 Generalized Poisson",
    "text": "D.5 Generalized Poisson",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#zero-inflated-poisson",
    "href": "book/C-distributional-mind-map.html#zero-inflated-poisson",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nD.6 Zero-inflated Poisson",
    "text": "D.6 Zero-inflated Poisson",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#exponential",
    "href": "book/C-distributional-mind-map.html#exponential",
    "title": "Appendix C — The Chocolified Distributional Mind Map",
    "section": "\nE.1 Exponential",
    "text": "E.1 Exponential",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>The Chocolified Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/D-regression-mind-map.html",
    "href": "book/D-regression-mind-map.html",
    "title": "Appendix D — The Sugartastic Regression Mind Map",
    "section": "",
    "text": "Fun fact!\n\n\nSugartastic! So sweet, it could power a carnival’s worth of cotton candy machines.\n\n\nThe regression mind map is a key component of the philosophy behind this book, besides the data science workflow from Section 1.2 and the ML-Stats dictionary found in Appendix A. Figure D.1 shows this regression mind map split in two zones by colour: discrete and continuous. This regression mind map is handy when executing the data modelling stage from the data science workflow, as explained in Section 1.2.4. Recall the first step in this stage is to choose a suitable regression model, and we made this decision in the function of the type of outcome \\(Y\\) we are dealing with. That said, the distributional mind map from Figure C.1 complements the regression mind map when identifying the correct type of outcome \\(Y\\).\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n        {{Overdispersed or &lt;br/&gt;Underdispersed &lt;br/&gt;Data}}\n          )Chapter 13: &lt;br/&gt;Generalized &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Generalized &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n        {{Zero Inflated &lt;br/&gt;Data}}\n          )Chapter 12: &lt;br/&gt;Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n      {{Categorical &lt;br/&gt;Outcome Y}}\n        {{Nominal &lt;br/&gt;Outcome Y}}\n          )Chapter 14: &lt;br/&gt;Multinomial &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Multinomial &lt;br/&gt;Outcome Y)\n        {{Ordinal &lt;br/&gt;Outcome Y}}\n          )Chapter 15: &lt;br/&gt;Ordinal &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Logistic &lt;br/&gt;Distributed &lt;br/&gt;Cumulative Outcome &lt;br/&gt;Probability)\n\n\n\n\n\n\n\n\nFigure D.1: Regression analysis mind map depicting all modelling techniques to be explored in this book. Depending on the type of outcome \\(Y\\), these techniques are split into two large zones: discrete and continuous.\n\n\nSuppose we start reading this regression map clockwise in the continuous zone. In that case, note this zone starts the cloud corresponding to Chapter 3 on the classical regression model called Ordinary Least-squares (OLS). Moreover, we can see that OLS is meant for an unbounded outcome \\(Y\\) (i.e., \\(-\\infty &lt; Y &lt; \\infty\\)). Then, we can proceed to the distributional assumption on \\(Y\\) in OLS, which would be Normal. Following up with the cloud corresponding to Chapter 4 on Gamma regression, we can see this model is meant for a nonnegative outcome \\(Y\\) (i.e., \\(0 \\leq Y &lt; \\infty\\)) where we assume a Gamma distribution for \\(Y\\). This way of reading the continuous zone in the mind map will persist until the survival analysis models: Chapter 6 and Chapter 7.\nThen, we can proceed to the discrete zone with the cloud corresponding to Chapter 8 on the generalized linear model (GLM) called Binary Logistic regression which aims to model a binary outcome \\(Y\\) (i.e., it can only take on the values \\(0\\) and \\(1\\)). Note that the Binary Logistic regression model is meant for ungrouped data where each row in the training dataset contains unique feature values. Hence, in this modelling case, we assume the outcome \\(Y\\) as a Bernoulli trial where \\(1\\) indicates a success and \\(0\\) indicates a failure. Then, suppose we take another clockwise case such as Chapter 10 on the GLM Classical Poisson regression, this model is suitable for count-type outcomes where equidispersion is present (i.e., the mean of the counts is equal to its corresponding variance). Finally, this model assumes that the outcome is Poisson-distributed. This way of reading the discrete zone in the mind map will persist until Chapter 15 on Ordinal Logistic Regression.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>The Sugartastic Regression Mind Map</span>"
    ]
  },
  {
    "objectID": "book/continuous-zone.html",
    "href": "book/continuous-zone.html",
    "title": "Continuous Cuisine",
    "section": "",
    "text": "mindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 1: Initial regression analysis mind map where, depending on the type of outcome \\(Y\\), we will split out modelling techniques into two large zones: discrete and continuous.",
    "crumbs": [
      "Continuous Cuisine"
    ]
  },
  {
    "objectID": "book/discrete-zone.html",
    "href": "book/discrete-zone.html",
    "title": "Discrete Cuisine",
    "section": "",
    "text": "mindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 1",
    "crumbs": [
      "Discrete Cuisine"
    ]
  }
]