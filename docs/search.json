[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Regression Cookbook",
    "section": "",
    "text": "Preface\n\nLet the regression cooking begin!\n\nThroughout my journey as a postdoctoral fellow in the Master of Data Science (MDS) at the University of British Columbia, I became aware of the fascinating overlap between machine learning and statistics. Many data science students usually come across common machine learning/statistics concepts or ideas that might only differ in names. For instance, simple terms such as weights in supervised learning (and their equivalent statistical counterpart as regression coefficients) might be misleading for students starting their data science formation. On the other hand, from an instructor’s perspective in a data science program that subsets its courses in machine learning in Python and statistics in R, regression courses in R also demand the inclusion of Python-related packages as alternative tools. In my MDS teaching experience, this is especially critical for students whose career plans lean towards industry where Python is more heavily used.\nAs a data science educator, I view this field as a substantial synergy between machine learning and statistics. Nevertheless, many gaps between both disciplines still need to be addressed. Thus, closing these critical gaps is imperative in a domain with accelerated growth, such as data science. In this regard, the MDS Stat-ML dictionary inspired me to write this textbook. It basically consists of common ground between foundational supervised learning models from machine learning and regression models commonly used in statistics. I strive to explore linear modelling approaches as a primary step while highlighting different terminology found in both fields. Furthermore, this discussion is more comprehensive than a simple conceptual exploration. Hence, the second step is hands-on practice via the corresponding Python packages for machine learning and R for statistics.\n\n\n\n\n\n\nFun Fact!\n\n\n\nWhile thinking about possible names for this work, I was planning to name it “Machine Learning and Statistics: A Common Ground.” Nevertheless, it was quite plain and boring! That said, this whole textbook idea sounded analogous to a cookbook, given its heavily applied focus.\n\nHence, the cookbook name idea!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "book/audience-scope.html",
    "href": "book/audience-scope.html",
    "title": "Audience and Scope",
    "section": "",
    "text": "This book mainly focuses on regression analysis and its supervised learning counterpart. Thus, it is not introductory statistics and machine learning material. Instead, the following topics are suggested as prerequisites:\n\n\n\nMutivariable differential calculus and linear algebra. Certain sections of each chapter pertain to modelling estimation. Therefore, topics such as partial derivatives and matrix algebra are a great asset. You can find helpful learning resources on the MDS webpage.\nBasic Python programming. When necessary, Python {pandas} library will be used to perform data wrangling. The MDS course DSCI 511 (Programming for Data Science) is an ideal example of this prerequisite.\n\n\n  \n\n\n\nBasic R programming. Knowledge of data wrangling and plotting through R {tidyverse} is recommended for hands-on practice via the examples provided in each one of the chapters of this book. The MDS courses DSCI 523 (Programming for Data Manipulation) and DSCI 531 (Data Visualization I) are ideal examples of this prerequisite.\nFoundations of probability and basic distributional knowledge. The reader should be familiar with elemental discrete and continuous distributions since they are a vital component of any given regression or supervised learning model. The MDS course DSCI 551 (Descriptive Statistics and Probability for Data Science) is an ideal example of this prerequisite.\nFoundations of frequentist statistical inference. One of the data science paradigms to be covered in this book is statistical inference, i.e., identifying relationships between different variables in a given population or system of interest via a sampled dataset. I only aim to cover a frequentist approach using inferential tools such as parameter estimation, hypothesis testing, and confidence intervals. The MDS course DSCI 552 (Statistical Inference and Computation I) is an ideal example of this prerequisite.\nFoundations of supervised learning. The second data science paradigm to be covered pertains to prediction, which is core in machine learning. The reader should be familiar with basic terminology, such as training and testing data, overfitting, underfitting, cross-validation, etc. The MDS course DSCI 571 (Machine Learning I) provides these foundations.\nFoundations of feature and model selection. This prerequisite also relates to machine learning and its corresponding prediction paradigm. Basic knowledge of prediction accuracy and variable selection tools is recommended. The MDS course DSCI 573 (Feature and Model Selection) is an ideal example of this prerequisite.\n\n\n\n\n\n\n\nA Further Remark on Probability and Statistical Inference\n\n\n\nIn case the reader is not 100% familiar with probabilistic and inferential topics, as discussed above, I will provide a quick refresher 1  Getting Ready for Regression Cooking! with crucial points that are needed to follow along the statistical way each one of the chapters is delivered (more specifically for modelling estimation/training matters!).\n\nFurthermore, this refresher will be integrated into the three big pillars that will be fully expanded in this book: a data science workflow, the right workflow flavour (inferential or predictive), and a regression toolbox.",
    "crumbs": [
      "Audience and Scope"
    ]
  },
  {
    "objectID": "book/01-intro.html",
    "href": "book/01-intro.html",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "",
    "text": "1.1 The ML-Stats Dictionary\nThe above example definition in Important 1.1 will pave the way to a complimentary aspect of this textbook that I have had in mind since I started teaching statistics (and, more specifically, regression analysis!) in a data science program. Machine learning and statistics usually overlap across many subjects, and regression modelling is no exception. Topics we teach in an utterly regression-based course, under a purely statistical framework, also appear in machine learning-based courses such as fundamental supervised learning, but often with different terminology. On this basis, the Master of Data Science (MDS) Program at the University of British Columbia provides the MDS Stat-ML dictionary (Gelbart 2017) under the following premises:\nIndeed, both disciplines have a tremendous amount of jargon and terminology. Furthermore, as I previously emphasized in the Preface, machine learning and statistics construct a substantial synergy that is reflected in data science. Even with this, people in both fields could encounter miscommunication issues when working together. This should not happen if we build solid bridges between both disciplines. Hence, a comprehensive ML-Stats dictionary (ML stands for Machine Learning) is imperative as in , and this textbook offers a perfect opportunity to build this resource. Primarily, this dictionary attempts to clarify any potential confusion between statistics and machine learning regarding terminology within supervised learning and regression analysis contexts.\nFinally, this Appendix A will be one of the appendices of this book where the reader can find all those statistical and machine learning-related terms in alphabetical order. Specific terms (either statistical or machine learning-related) will include an admonition identifying which terms (either statistical or machine learning-related) are equivalent. Take as an example the statistical term dependent variable:\nThen, the above definition will be followed by this admonition:\nNote we have identified four equivalent terms for the term dependent variable. Furthermore, according to our already defined colour scheme, these terms can be statistical or machine learning-related.\nNow, let us proceed to a quick review of probability and statistics in a frequentist framework. This review will be especially essential to understanding the philosophy of modelling parameter estimation, specifically in relation to statistical inference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-ml-stats-dictionary",
    "href": "book/01-intro.html#sec-ml-stats-dictionary",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "",
    "text": "This document is intended to help students navigate the large amount of jargon, terminology, and acronyms encountered in the MDS program and beyond.\n\n\nThis section covers terms that have different meanings in different contexts, specifically statistics vs. machine learning (ML).\n\n\n\n\n\n\n\n\nHeads-up on terminology highlights!\n\n\n\nFollowing the spirit of the ML-Stats dictionary, throughout the book, all statistical terms will be highlighted in magenta whereas the machine learning terms will be highlighted in orange. This colour scheme strives to combine this terminology so we can switch from one field to another in an easier way. With practice and time, we should be able to jump back and forth when using these concepts.\n\n\n\n\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\n\n\n\n\n\n\nEquivalent to:\n\n\n\nResponse, outcome, output or target.\n\n\n\n\n\n\n\n\n\nHeads-up on the use of terminology!\n\n\n\nThroughout this book, I will interchangeably use specific terms when explaining the different regression approaches in each subsequent chapter. Whenever confusion arises about using these interchangeable terms, it is highly recommended to consult their definitions and equivalences in Appendix A.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-quick-review",
    "href": "book/01-intro.html#sec-quick-review",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "\n1.2 A Quick Review on Probability and Frequentist Statistical Inference",
    "text": "1.2 A Quick Review on Probability and Frequentist Statistical Inference\n\nIn Statistics, everything always boils down to randomness!\n\nI know! That is quite a bold statement. Nonetheless, once one starts teaching statistical topics to audiences not quite familiar with the usual field jargon, the randomness idea always persists across many different topics. And regression analysis is not an exception at all! This is why I have an allocated this special section in the textbook to explain core probabilistic and inferential concepts to pave the way to its role in the context of regression analysis.\nOn the other hand, even though this book has prerequisites related to the basics of probability via different distributions and the fundamentals of frequentist statistical inference, Chapter 1 also provides a handy summary of all these topics. We will check essential concepts such as random variables, sampling, parameter estimation via maximum likelihood, the process of hypothesis testing, delivery of confidence intervals, and how to obtain outcome predictions. Note these concepts are heavily related to the data science workflow depicted in Section 1.3 across its different phases.\n\n1.2.1 Basics of Probability\nUnder this foundation, our data is coming from this given population or system of interest. Moreover, the population or system is assumed to be governed by parameters. In practice, as a data scientist, you will never parameters\n\n1.2.2 Basics of Frequentist Statistical Inference\n\n1.2.3 What is Maximum Likelihood Estimation?\n\n1.2.4 Supervised Learning and Regression Analysis",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-ds-workflow",
    "href": "book/01-intro.html#sec-ds-workflow",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "\n1.3 The Data Science Workflow",
    "text": "1.3 The Data Science Workflow\nIt is time to review the so-called data science workflow. Each one of these three pillars is heavily connected since a general Data Science workflow is applied in each one of these regression models, which aims to help in our learning (i.e., we would be able to know what exact stage to expect in our data analysis regardless of the regression model we are being exposed to). Therefore, a crucial aspect of the practice of Regression Analysis is the need for this systematic Data Science workflow that will allow us to solve our respective inquiries in a reproducible way. Figure 1.1 shows this workflow which has the following general stages (I briefly define each one of them; note a broader delivery will be done in subsequent subsections):\n\nStudy design:\nData collection and wrangling:\nExploratory data analysis:\nData modelling:\nEstimation:\nGoodness of fit:\nResults:\nStorytelling\n\n\n\n\n\n\n\nWhat if there is no formal structure in our regression analysis?\n\n\n\nSince very early learning stages in data analysis, it is crucial tp\nNow, suppose we do not follow a predefined workflow in practice. In that case, we might be at stake in incorrectly addressing our inquiries, translating into meaningless results outside the context of the problem we aim to solve. This is why the formation of a Data Scientist must stress this workflow from the very introductory learning stages.\n\n\n\n\n\n\n\nFigure 1.1: Data science workflow for inferential and predictive inquiries in regression analysis and supervised learning, respectively.\n\n\n\n1.3.1 Study Design\nThe first stage of this workflow is heavily related to the main statistical inquiries we aim to address throughout the whole data analysis process. As a data scientist, it is your task to primarily translate these inquiries from the stakeholders of the problem as inferential or predictive. Roughly speaking, this primary classification can be explained as follows:\n\n\nInferential. The main objective is to untangle relationships of association or causation between the regressors (i.e., explanatory variables) and the corresponding response in the context of the problem of interest. Firstly, we would assess whether there is a statistical relationship between them. Then, if significant, we would quantify by how much.\n\nPredictive. The main objective is to deliver response predictions on further observations of regressors, having estimated a given model via a current training dataset. Unlike inferential inquiries, assessing a statistically significant association or causation between our variables of interest is not a primary objective but accurate predictions. This is one of the fundamental paradigms of machine learning.\n\n\n\n\n\n\n\nFigure 1.2: Study design stage from the data science workflow in Figure 1.1. This stage is directly followed by data collection and wrangling.\n\n\n\n1.3.2 Data Collection and Wrangling\nOnce we have defined our main statistical inquiries, it is time to collect our data. Note we have to be careful about the way we collect this data since it might have a particular impact on the quality of our statistical practice:\n\nRegarding inferential inquiries, recall we are approaching populations or systems of interest governed by unknown and fixed distributional parameters. Thus, via sampled data, we aim to estimate these distributional parameters. This is why a proper sampling method on this population or system of interest is critical to obtaining representative data for appropriate hypothesis testing.\n\n\n\n\n\n\n\nTip 1.3: A Quick Debrief on Sampling!\n\n\n\nThis stage is coloured in gray in {numref}data-science-workflow, unlike the other ones coloured in yellow. This is because sampling topics are out of the scope of this course and MDS in general. Nevertheless, we still need to stress that a proper sampling method is also key in inferential inquiries to assess association and/or causation between the regressors and your response of interest. That said, depending on the context of the problem, we could apply either one of the following methods of sampling:\n\n\nSimple random sampling.\n\nSystematic sampling.\n\nStratified sampling.\n\nClustered sampling.\nEtc.\n\nAs in the case of Regression Analysis, statistical sampling is a vast field, and we could spend a whole course on it. If you are more interested in these topics, Sampling: design and analysis by Lohr offers great foundations.\n\n\n\nIn practice, regarding predictive inquiries, we would likely have to deal with databases given that our trained models will not be used to make inference and parameter interpretations.\n\n\n\n\n\n\nFigure 1.3: Data collection and wrangling stage from the data science workflow in Figure 1.1. This stage is directly followed by exploratory data analysis and preceded by study design.\n\n\n\n1.3.3 Exploratory Data Analysis\n\n\n\n\n\nFigure 1.4: Exploratory data analysis stage from the data science workflow in Figure 1.1. This stage is directly followed by data modelling and preceded by data collection and wrangling.\n\n\n\n1.3.4 Data Modelling\n\n\n\n\n\nFigure 1.5: Data modelling stage from the data science workflow in Figure 1.1. This stage is directly preceded by exploratory data analysis. On the other hand, it is directly followed by estimation but indirectly with goodness of fit. If necessary, the goodness of fit stage could retake the process to data modelling.\n\n\n\n1.3.5 Estimation\n\n\n\n\n\nFigure 1.6: Estimation stage from the data science workflow in Figure 1.1. This stage is directly preceded by data modelling and followed by goodness of fit. If necessary, the goodness of fit stage could retake the process to data modelling and then to estimation.\n\n\n\n1.3.6 Goodness of Fit\n\n\n\n\n\nFigure 1.7: Goodness of fit stage from the data science workflow in Figure 1.1. This stage is directly preceded by estimation and followed by results. If necessary, the goodness of fit stage could retake the process to data modelling and then to estimation.\n\n\n\n1.3.7 Results\n\n\n\n\n\nFigure 1.8: Results stage from the data science workflow in Figure 1.1. This stage is directly followed by storytelling and preceded by goodness of fit.\n\n\n\n1.3.8 Storytelling\n\n\n\n\n\nFigure 1.9: Storytelling stage from the data science workflow in Figure 1.1. This stage preceded by results.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-regression-mindmap",
    "href": "book/01-intro.html#sec-regression-mindmap",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "\n1.4 Mindmap of Regression Analysis",
    "text": "1.4 Mindmap of Regression Analysis\nHaving defined the necessary statistical aspects to execute a proper supervised learning analysis, either inferential or predictive across its seven sequential phases, we must dig into the different approaches we might encounter in practice as regression models. The nature of our outcome of interest will dictate any given modelling approach to apply, depicted as clouds in Figure 1.10. Note these regression models can be split into two sets depending on whether the outcome of interest is continuous or discrete. Therefore, under a probabilistic view, identifying the nature of a given random variable is crucial in regression analysis.\n\n\n\n\n\nFigure 1.10: Regression analysis mindmap depicting all modelling techniques to be explored in this book. These techniques are split into two big sets: continuous and discrete outcomes.\n\n\nThat said, we will go beyond OLS regression and explore further regression techniques. In practice, these techniques have been developed in the statistical literature to address practical cases where the OLS modelling framework and assumptions are not suitable anymore. Thus, throughout this block, we will cover (at least) one new regression model per lecture.\nAs we can see in the clouds of Figure 1.10, there are 13 regression models: 8 belonging to discrete outcomes and 5 to continuous outcomes. Each of these models is contained in a chapter of this book, beginning with the most basic regression tool known as ordinary least-squares in Chapter 2. We must clarify that the current statistical literature is not restricted to these 13 regression models. The field of regression analysis is vast, and one might encounter more complex models to target certain specific inquiries. Nonetheless, I consider these models the fundamental regression approaches that any data scientist must be familiar with in everyday practice.\nEven though this book comprises 13 chapters, each depicting a different regression model, we have split these chapters into two major subsets: those with continuous outcomes and those with discrete outcomes.\n\n\n\n\nGelbart, Michael. 2017. “Data Science Terminology.” UBC MDS. Master of Data Science at the University of British Columbia. https://ubc-mds.github.io/resources_pages/terminology/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/references.html",
    "href": "book/references.html",
    "title": "References",
    "section": "",
    "text": "Gelbart, Michael. 2017. “Data Science Terminology.” UBC\nMDS. Master of Data Science at the University of British Columbia.\nhttps://ubc-mds.github.io/resources_pages/terminology/.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "book/A-dictionary.html",
    "href": "book/A-dictionary.html",
    "title": "Appendix A — The ML-Stats Dictionary",
    "section": "",
    "text": "D",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#d",
    "href": "book/A-dictionary.html#d",
    "title": "Appendix A — The ML-Stats Dictionary",
    "section": "",
    "text": "Dependent variable\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\n\n\n\n\nEquivalent to:\n\n\n\nResponse, outcome, output or target.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#o",
    "href": "book/A-dictionary.html#o",
    "title": "Appendix A — The ML-Stats Dictionary",
    "section": "O",
    "text": "O\nOutcome\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\n\n\n\n\nEquivalent to:\n\n\n\nDependent variable, response, output or target.\n\n\nOutput\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\n\n\n\n\nEquivalent to:\n\n\n\nDependent variable, response, outcome or target.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#r",
    "href": "book/A-dictionary.html#r",
    "title": "Appendix A — The ML-Stats Dictionary",
    "section": "R",
    "text": "R\nResponse\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\n\n\n\n\nEquivalent to:\n\n\n\nDependent variable, outcome, outpur or target.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#t",
    "href": "book/A-dictionary.html#t",
    "title": "Appendix A — The ML-Stats Dictionary",
    "section": "T",
    "text": "T\nTarget\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\n\n\n\n\nEquivalent to:\n\n\n\nDependent variable, response, outcome or output.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>The ML-Stats Dictionary</span>"
    ]
  }
]