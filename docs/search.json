[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Regression Cookbook (in development)",
    "section": "",
    "text": "Preface\nData science is a field in which we become aware of the fascinating overlap between machine learning and statistics. Many data science students usually come across everyday machine learning and statistics concepts or ideas that might only differ in names. For instance, simple terms such as weights in supervised learning (and their statistical counterpart as regression coefficients) might be misleading for students starting their data science formation. On the other hand, from an instructor’s perspective in a data science program that subsets its courses in machine learning in Python and statistics in R, regression courses in R also demand the inclusion of Python-related packages as alternative tools. Furthermore, in a graduate program such as the Master of Data Science (MDS) at the University of British Columbia (UBC), this is especially critical for students whose career plan leans towards the industry job market where Python is more heavily used.\nThat said, we can state that data science is a substantial synergy between machine learning and statistics. Nevertheless, many gaps between both disciplines still need to be addressed. Thus, closing these critical gaps is imperative in a domain with accelerated growth, such as data science. In this regard, the MDS Stat-ML dictionary has inspired us to write this textbook. It basically consists of common ground between foundational supervised learning models from machine learning and regression models commonly used in statistics. We strive to explore linear modelling approaches as a primary step while explaining different terminology found in both fields. Furthermore, this discussion is more comprehensive than a simple conceptual exploration. Hence, the second step is hands-on practice via the corresponding Python packages for machine learning and R for statistics.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "The Regression Cookbook (in development)",
    "section": "",
    "text": "Special thanks to Jonathan Graves, who mentioned the cookbook term when this textbook was conceptualized during very early stages.↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "book/meet-the-team.html",
    "href": "book/meet-the-team.html",
    "title": "Meet the Team",
    "section": "",
    "text": "Authors",
    "crumbs": [
      "Meet the Team"
    ]
  },
  {
    "objectID": "book/meet-the-team.html#g.-alexi-rodríguez-arelis",
    "href": "book/meet-the-team.html#g.-alexi-rodríguez-arelis",
    "title": "Meet the Team",
    "section": "G. Alexi Rodríguez-Arelis",
    "text": "G. Alexi Rodríguez-Arelis\n\n\n\n\n\nI’m an Assistant Professor of Teaching in the Department of Statistics and Master of Data Science at the University of British Columbia. Throughout my academic and professional journey, I’ve been involved in diverse fields, such as credit risk management, statistical consulting, and data science teaching. My doctoral research in statistics is primarily focused on computer experiments that emulate scientific and engineering systems via Gaussian stochastic processes (i.e., kriging regression). I’m incredibly passionate about teaching regression topics while combining statistical and machine learning contexts.",
    "crumbs": [
      "Meet the Team"
    ]
  },
  {
    "objectID": "book/meet-the-team.html#andy-tai",
    "href": "book/meet-the-team.html#andy-tai",
    "title": "Meet the Team",
    "section": "Andy Tai",
    "text": "Andy Tai\n\n\n\n\n\nI’m a Postdoctoral Teaching and Learning Fellow in the Department of Statistics and Master of Data Science at the University of British Columbia. Throughout my academic and professional journey, I’ve been involved in diverse fields, such as addiction psychiatry, machine learning, and data science teaching. My doctoral research in neuroscience primarily focused on using machine learning to predict the risk of fatal overdose. I am interested in leveraging data science and machine learning to solve complex problems, and I strive to inspire others to explore the vast potential of these fields.",
    "crumbs": [
      "Meet the Team"
    ]
  },
  {
    "objectID": "book/meet-the-team.html#ben-chen",
    "href": "book/meet-the-team.html#ben-chen",
    "title": "Meet the Team",
    "section": "Ben Chen",
    "text": "Ben Chen\n\n\n\n\n\nI hold a Master’s degree in Data Science from the University of British Columbia, and I am passionate about educating others in the fields of statistics and data science. With experience teaching students how to use statistical methods and data science tools, I also enjoy sharing my knowledge through writing. My blog focuses on making complex statistical concepts accessible to everyone. Additionally, I’ve worked on a variety of data science projects, ranging from developing recommendation systems to building Generative Adversarial Network (GAN) models.",
    "crumbs": [
      "Meet the Team"
    ]
  },
  {
    "objectID": "book/meet-the-team.html#payman-nickchi",
    "href": "book/meet-the-team.html#payman-nickchi",
    "title": "Meet the Team",
    "section": "Payman Nickchi",
    "text": "Payman Nickchi\n\n\n\n\n\nI am a Postdoctoral Research and Teaching Fellow in the Department of Statistics and the Master of Data Science (MDS) program at the University of British Columbia (UBC). I completed my PhD in Statistics at Simon Fraser University (SFU), where my research focused on biostatistics and goodness-of-fit tests using empirical distribution functions. I am currently teaching statistical courses in the MDS program at UBC. My passion for statistics, teaching, and data science led me to this role. Outside of work, I enjoy swimming and capturing the night sky through astrophotography.",
    "crumbs": [
      "Meet the Team"
    ]
  },
  {
    "objectID": "book/meet-the-team.html#olivia-liu",
    "href": "book/meet-the-team.html#olivia-liu",
    "title": "Meet the Team",
    "section": "Olivia Liu",
    "text": "Olivia Liu\n\n\n\n\n\nI’m a PhD candidate in Statistics at the University of British Columbia, specializing in computational statistics and convex optimization. My research focuses on convex optimization methods, particularly trend filtering with Poisson loss, and algorithms applied to various domains such as epidemiology. I have two years of part-time experience in consulting through the Applied Statistics and Data Science Group (ASDa) at UBC. I also have served as a teaching assistant for STAT 306 (Finding Relationships in Data) across multiple terms, as well as for the Vancouver Summer Program with ASDa in 2024 and 2025.",
    "crumbs": [
      "Meet the Team"
    ]
  },
  {
    "objectID": "book/meet-the-team.html#aviv-milner",
    "href": "book/meet-the-team.html#aviv-milner",
    "title": "Meet the Team",
    "section": "Aviv Milner",
    "text": "Aviv Milner\n\n\n\n\nAviv Milner is an undergraduate mathematics student at the University of British Columbia with a focus on statistics and quantitative methods for psychology research. After several years working as a product manager in software, he returned to academia to work on problems at the intersection of data analysis, reproducible science, and statistical education. He has completed multiple quantitative psychology research projects and earned undergraduate research awards for work on methodological and replication-focused topics. Aviv’s interests include rock climbing, hiking, camping and exploring the beautiful nature of BC.",
    "crumbs": [
      "Meet the Team"
    ]
  },
  {
    "objectID": "book/meet-the-team.html#michael-sekatchev",
    "href": "book/meet-the-team.html#michael-sekatchev",
    "title": "Meet the Team",
    "section": "Michael Sekatchev",
    "text": "Michael Sekatchev\n\n\n\n\n\nI hold a Bachelor’s and Master’s degree in Physics from the University of British Columbia (UBC), and began my PhD in Nuclear Engineering at the University of California, Berkeley in August 2025. My research focuses on dark matter in the form of axions. I have a strong interest in open education resources (OER) as a way to facilitate equitable access to education, with a particular focus on the development of open-access textbooks: in addition to this project, I designed mechanics problems for the “OER Mechanics Project”, a collaboration between UBC and Douglas College to develop an open-source mechanics textbook. I am also co-authoring “Speaking and Writing Physics 101: The Language of Solving First-year Physics Problems”, an open-source textbook designed to help non-native English speakers better understand physics concepts and strengthen their communication skills in scientific English.",
    "crumbs": [
      "Meet the Team"
    ]
  },
  {
    "objectID": "book/privacy-policy.html",
    "href": "book/privacy-policy.html",
    "title": "Website Privacy Policy",
    "section": "",
    "text": "Information Collection and Use\nWe use Google Analytics, a web analytics service provided by Google, LLC. (“Google”). Google Analytics uses cookies to help analyze how students interact with the textbook, including tracking which sections are accessed most frequently. Information generated by cookies about your use of our website (including IP address) will be transmitted to and stored by Google on servers in the United States.\nGoogle will use this information solely for evaluating textbook usage, compiling usage reports to enhance the educational effectiveness of the textbook, and providing related services.\nYou may refuse the use of cookies by selecting the appropriate settings in your browser; however, please note this may affect your textbook browsing experience.",
    "crumbs": [
      "Website Privacy Policy"
    ]
  },
  {
    "objectID": "book/privacy-policy.html#personal-information",
    "href": "book/privacy-policy.html#personal-information",
    "title": "Website Privacy Policy",
    "section": "Personal Information",
    "text": "Personal Information\nWe do not collect personally identifiable information through Google Analytics. Any personally identifiable information, such as your name and email address, would only be collected if voluntarily submitted for specific educational purposes (e.g., feedback or course-related inquiries). We will never sell or distribute your personal information to third parties.\nFor any questions or concerns, please contact us at alexrod@stat.ubc.ca.",
    "crumbs": [
      "Website Privacy Policy"
    ]
  },
  {
    "objectID": "book/audience-scope.html",
    "href": "book/audience-scope.html",
    "title": "Audience and Scope",
    "section": "",
    "text": "This book mainly focuses on statistical regression analysis with connections to its corresponding supervised learning counterpart. Thus, it is not introductory statistics and machine learning material. Also, some coding background on R (R Core Team 2024) and/or Python (Van Rossum and Drake 2009) is recommended. That said, the following topics are suggested:\n\nMutivariable differential calculus. Certain sections of each chapter pertain to modelling estimation. Therefore, topics such as partial derivatives are a good asset. You can find helpful learning resources on the Master of Data Science (MDS) webpage.\nBasic Python programming. When necessary, Python {pandas} (The Pandas Development Team 2024) library will be used to perform data wrangling. The MDS course DSCI 511 (Programming for Data Science) is an ideal example of a quick review.\n\n\n\n\nImage by Lubos Houska via Pixabay.\n\n\n\nBasic R programming. Knowledge of data wrangling through R {tidyverse} (Wickham et al. 2019) is recommended for hands-on practice via the cases provided in each one of the chapters of this book. The MDS course DSCI 523 (Programming for Data Manipulation) is an ideal example of a quick review.\nFoundations of supervised learning. A fundamental data science paradigm to be covered pertains to prediction, which is core in machine learning. The reader should be familiar with basic terminology, such as training and testing data, overfitting, underfitting, etc. The MDS course DSCI 571 (Machine Learning I) provides these foundations.\nFoundations of feature and model selection. This prerequisite also relates to machine learning and its corresponding prediction paradigm. Basic knowledge of prediction accuracy and model selection tools is recommended. The MDS course DSCI 573 (Feature and Model Selection) is an ideal example of a quick review.\n\n\n\nA Crucial Remark on Probability and Statistical Inference\n\n\nIf you are not fully familiar with introductory statistical concepts, particularly topics related to probability and inference, we suggest two pathways for review. The first pathway involves revisiting the following course materials:\n\nFoundations of probability and basic distributional knowledge: The MDS course DSCI 551 (Descriptive Statistics and Probability for Data Science) covers fundamental discrete and continuous probability distributions, which are essential components of any regression or supervised learning model.\nFoundations of frequentist statistical inference: The MDS course DSCI 552 (Statistical Inference and Computation I) addresses statistical inference, a key paradigm in this book. This involves identifying relationships between different variables within a population or system of interest using a sampled dataset. We focus exclusively on a frequentist approach utilizing tools such as parameter estimation, hypothesis testing, and confidence intervals.\n\nThe second pathway entails an in-depth review of the refresher material provided in Chapter 2, which covers critical points needed to grasp the statistical concepts presented in each of the core thirteen regression chapters. This refresher chapter aims to address the same topics outlined in the above bullet points through a practical example, with the necessary theoretical background to understand the foundations of generative modeling and statistical inference.\n\n\n\n\n\n\nR Core Team. 2024. “R: A Language and Environment for Statistical Computing.” Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nThe Pandas Development Team. 2024. “Pandas-Dev/Pandas: Pandas.” Zenodo. https://doi.org/10.5281/zenodo.3509134.\n\n\nVan Rossum, Guido, and Fred L. Drake. 2009. Python 3 Reference Manual. Scotts Valley, CA: CreateSpace.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Audience and Scope"
    ]
  },
  {
    "objectID": "book/01-intro.html",
    "href": "book/01-intro.html",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "",
    "text": "1.1 The ML-Stats Dictionary\nMachine learning and statistics often overlap, especially in regression modelling. Topics covered in a regression-focused course, under a purely statistical framework, can also appear in machine learning-based courses on supervised learning, but the terminology can differ. Recognizing this overlap, the Master of Data Science (MDS) program at the University of British Columbia (UBC) provides the MDS Stat-ML dictionary (Gelbart 2017) under the following premises:\nBoth disciplines have a tremendous amount of jargon and terminology. As mentioned in the Preface, machine learning and statistics construct a substantial synergy reflected in data science. Despite this overlap, misunderstandings can still happen due to differences in terminology. To prevent this, we need clear bridges between these disciplines via a ML-Stats dictionary (ML stands for Machine Learning).\nThe above appendix will be the section in this book where the reader can find all those statistical and machine learning-related terms in alphabetical order. Notable terms (either statistical or machine learning-related) will include an admonition identifying which terms (again, either statistical or machine learning-related) are equivalent or somewhat equivalent (or even not equivalent if that is the case). For instance, consider the statistical term called dependent variable:\nThen, the above definition will be followed by this admonition:\nNote that we have identified four equivalent terms for the term dependent variable. Furthermore, these terms can be statistical or machine learning-related.\nNext, we will introduce the three main foundations of this textbook: a data science workflow, choosing the correct workflow flavour (inferential or predictive), and building your regression toolbox.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-ml-stats-dictionary",
    "href": "book/01-intro.html#sec-ml-stats-dictionary",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "",
    "text": "This document is intended to help students navigate the large amount of jargon, terminology, and acronyms encountered in the MDS program and beyond.\n\n\nThis section covers terms that have different meanings in different contexts, specifically statistics vs. machine learning (ML).\n\n\n\n\nHeads-up on how the ML-Stats Dictionary is built and structured!\n\n\nThe complete ML-Stats dictionary can be found in Appendix A. This resource builds upon the concepts introduced in the definition callout box throughout the fifteen main chapters of this textbook. The dictionary aims to clarify terminology that varies between statistics and machine learning, specifically in the context of supervised learning and regression analysis.\n\n\nImage by manfredsteger via Pixabay.\n\nTerms in this dictionary related to statistics will be highlighted in blue, while terms related to machine learning will be highlighted in magenta. This color scheme is designed to help readers easily navigate between the two disciplines. With practice, you will become proficient in applying concepts from both fields.\n\n\n\n\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, in a statistical inference framework, the variable we are trying explain.\n\n\n\n\nEquivalent to:\n\n\nResponse variable, outcome, output or target.\n\n\n\n\n\nHeads-up on the use of terminology!\n\n\nThroughout this book, we will use specific concepts interchangeably while explaining different regression methods. If confusion arises, you must always check definitions and equivalences (or non-equivalences) in Appendix A.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-ds-workflow",
    "href": "book/01-intro.html#sec-ds-workflow",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "\n1.2 The Data Science Workflow",
    "text": "1.2 The Data Science Workflow\n\n\nImage by manfredsteger via Pixabay.\n\nUnderstanding the data science workflow is essential for mastering regression analysis. This workflow serves as a blueprint that guides us through each stage of our analysis, ensuring that we apply a systematic approach to solving our inquiries in a reproducible way. Each of the three pillars of this textbook—data science workflow, the right workflow flavour (inferential or predictive), and a regression toolbox—are deeply interconnected. Regardless of the regression model we explore, this general workflow provides a consistent framework that helps us navigate our data analysis with clarity and purpose.\nAs shown in Figure 1.1, the data science workflow is composed of the following eight stages (each of which will be discussed in more detail in subsequent subsections):\n\n\nStudy design: Define the research question, objectives, and variables of interest to ensure the analysis is purpose-driven and aligned with the problem at hand.\n\nData collection and wrangling: Gather and clean data, addressing issues such as missing values, outliers, and inconsistencies to transform it into a usable format.\n\nExploratory data analysis (EDA): Explore the data through statistical summaries and visualizations to identify patterns, trends, and potential anomalies.\n\nData modelling: Apply statistical or machine learning models to uncover relationships between variables or make predictions based on the data.\n\nEstimation: Calculate model parameters to quantify relationships between variables and assess the accuracy and reliability of the model.\n\nGoodness of fit: Evaluate the model’s performance using metrics and model diagnostic checks to determine how well it explains the data.\n\nResults: Interpret the model’s outputs to derive meaningful insights and provide answers to the original research question.\n\nStorytelling Communicate the findings through a clear, engaging narrative that is accessible to a non-technical audience.\n\nBy adhering to this workflow, we ensure that our regression analysis are not only systematic and thorough but also capable of producing results that are meaningful within the context of the problem we aim to solve.\n\n\nHeads-up on the importance of a formal structure in regression analysis!\n\n\nFrom the earliest stages of learning data analysis, understanding the importance of a structured workflow is crucial. If we do not adhere to a predefined workflow, we risk misinterpreting the data, leading to incorrect conclusions that fail to address the core questions of our analysis. Such missteps can result in outcomes that are not only meaningless but potentially misleading when taken out of the problem’s context.\nTherefore, it is essential for aspiring data scientists to internalize this workflow from the very beginning of their education. A systematic approach ensures that each stage of the analysis is conducted with precision, ultimately producing reliable and contextually relevant results.\n\n\n\n\n\n\n\nFigure 1.1: Data science workflow for inferential and predictive inquiries in regression analysis and supervised learning, respectively. The workflow is structured in eight stages: study design, data collection and wrangling, exploratory data analysis, data modelling, estimation, goodness of fit, results, and storytelling.\n\n\n\n1.2.1 Study Design\nThe first stage of this workflow is centred around defining the main statistical inquiries we aim to address throughout the data analysis process. As a data scientist, your primary task is to translate these inquiries from the stakeholders into one of two categories: inferential or predictive. This classification determines the direction of your analysis and the methods you will use:\n\n\nInferential: The objective here is to explore and quantify relationships of association or causation between explanatory variables (referred to as regressors in the models discussed in this textbook) and the response variable within the context of the specific problem at hand. For example, you might statistically seek to determine whether a particular marketing campaign (the regressor) significantly influences sales revenue (the response), and if it does, by how much.\n\nPredictive: In this case, the focus is on making accurate predictions about the response variable based on future observations of the regressors. Unlike inferential inquiries, where understanding the relationship between variables is key, the primary goal here is to maximize prediction accuracy. This approach is fundamental in machine learning. For instance, you might build a model to predict future sales revenue based on past marketing expenditures, without necessarily needing to understand the underlying relationship between the two.\n\n\n\nImage by Manfred Steger via Pixabay.\n\n\n\nHeads-up on the inquiry focus of this book!\n\n\nIn the regression chapters of this book, we will emphasize both types of inquiries. As we follow the workflow from Figure 1.1, we will explore the two pathways identified by the decision points concerning inference and prediction.\n\n\nExample: Housing Sale Prices\nTo illustrate the study design stage, let us consider a simple example involving housing sale prices in a specific city:\n\nIf our goal is inferential, we might be interested in understanding the relationship between various factors—such as square footage, number of bedrooms, and proximity to schools—and housing sale prices. Specifically, we would ask questions like:\n\n\nHow does the number of bedrooms affect the price of a house, once we account for other factors?\n\n\nIf our goal is predictive, we would focus on estimating a model that can accurately predict the price of a house based on its features (i.e., the characteristics of a given house), regardless of whether we fully understand how each feature contributes to the price. Hence, we would be able to answer questions such as:\n\n\nWhat would be the predicted price of a house with 3,500 square feet and 3 bedrooms located on a block where the closest school is at 2.5 km?\n\n\n\nImage by Tomislav Kaučić via Pixabay.\n\nIn both cases, the study design stage involves clearly defining these objectives and determining the appropriate data modelling methods to address them. This stage sets the foundation for all subsequent steps in the data science workflow. After establishing the study design, the next step is data collection and wrangling, as shown in Figure 1.2.\n\n\n\n\n\nFigure 1.2: Study design stage from the data science workflow in Figure 1.1. This stage is directly followed by data collection and wrangling.\n\n\n\n1.2.2 Data Collection and Wrangling\n\n\nImage by Manfred Steger via Pixabay.\n\nOnce we have clearly defined our statistical questions, the next crucial step is to collect the data that will form the basis of our analysis. The way we collect this data is vital because it directly affects the accuracy and reliability of our results:\n\nFor inferential inquiries, we focus on understanding populations or systems that we cannot fully observe. These populations are governed by characteristics (referred to as parameters) that we want to estimate. Because we cannot study every individual in the population or system, we collect a smaller, representative subset called a sample. The method we use to collect this sample—known as sampling—is crucial. A proper sampling method ensures that our sample reflects the larger population or system, allowing us to make accurate and precise generalizations (i.e., inferences) about the entire population or system. After collecting the sample, it is common practice to randomly split the data into training and test sets. This split allows us to build and assess our models, ensuring that the findings are robust and not overly tailored to the specific data at hand.\n\nFor predictive inquiries, our goal is often to use existing data to make predictions about future events or outcomes. In these cases, we usually work with large datasets (databases) that have already been collected. Instead of focusing on whether the data represents a population (as in inferential inquiries), we focus on cleaning and preparing the data so that it can be used to train models that make accurate predictions. After wrangling the data, it is typically split into training, validation (if necessary, depending on our chosen modelling strategy), and test sets. The training set is used to build the model, the validation set is used to tune model parameters, and the test set evaluates the model’s final performance on unseen data.\n\n\n\nTip on sampling techniques!\n\n\nCareful attention to sampling design is a crucial step in any research aimed at supporting valid regression-based inference. The selection of an appropriate sampling design should be guided by the structural characteristics of the population as well as the specific goals of the analysis. A well-designed sampling strategy enhances the accuracy, precision, and generalizability of parameter estimates derived from regression models, particularly when the intention is to extend model-based conclusions beyond the observed data to the whole population or system.\nBelow, we summarize some commonly used probability-based sampling designs, each of which has distinct implications for model validity and estimation efficiency:\n\n\nSimple random sampling: Every unit in the population has an equal probability of selection. While this method is straightforward to implement and analyze, it may be inefficient or impractical for populations with heterogeneous subgroups.\n\nSystematic sampling: Sampling occurs at fixed intervals from an ordered list, starting from a randomly chosen point. This design can improve efficiency under certain ordering schemes, but caution is necessary to avoid biases related to periodicity.\n\nStratified sampling: The population is divided into mutually exclusive strata based on key characteristics (e.g., age, income, region, etc.). Samples are drawn within each stratum, often in proportion to the strata sizes or based on optimal allocation. This approach increases precision for subgroup estimates and enhances overall model efficiency.\n\nCluster sampling: The population is divided into naturally occurring clusters (e.g., households, schools, geographic units, etc.), and entire clusters are sampled randomly. This design is often preferred for cost efficiency, but it typically requires adjustments for intracluster correlation during analysis.\n\nIn the context of our regression-based inferential framework, it is necessary to carefully plan data collection and preparation around the sampling strategy. The choice of sampling design can influence not only model estimation but also the interpretation and generalizability of the results. While this textbook does not provide an exhaustive treatment of sampling theory, we recommend Lohr (2021) for an in-depth reference. Their work offers both theoretical insights and applied examples that are highly relevant for data scientists engaged in model-based inference.\n\n\nExample: Collecting Data for Housing Inference and Predictions\nLet us continue with our housing example to illustrate the above concepts:\n\n\nInferential Approach: Suppose we want to understand how the number of bedrooms is associated with the housing sale prices in a city. To do this, we would collect a sample of house sales that accurately represents the city’s entire housing market. For instance, we might use stratified sampling to ensure that we include houses from different neighbourhoods in proportion to how common they are. After collecting the data, we would split it into training and test sets. The training set helps us build our model and estimate the relationship between variables, while the test set allows us to evaluate how well our findings generalize to new data.\n\nPredictive Approach: If our goal is to predict the selling price of a house based on its features (such as size, number of bedrooms, and location), we would gather a large dataset of recent house sales. This data might come from a real estate database that tracks the details of each sale. Before we can use this data to train a model, we would clean it by filling in any missing information, converting data to a consistent format, and making sure all variables are ready for analysis. After preprocessing, we would split the data into training, validation, and test sets. The training set would be used to fit the model, the validation set to fine-tune it, and the test set to assess how well the model can predict prices for houses it has not seen before.\n\n\n\nImage by Stefan via Pixabay.\n\nAs shown in Figure 1.3, the data collection and wrangling stage is fundamental to the workflow. It directly follows the study design and sets the stage for exploratory data analysis.\n\n\n\n\n\nFigure 1.3: Data collection and wrangling stage from the data science workflow in Figure 1.1. This stage is directly followed by exploratory data analysis and preceded by study design.\n\n\n\n1.2.3 Exploratory Data Analysis\nBefore diving into data modelling, it is crucial to develop a deep understanding of the relationships between the variables in our training data. This is where the third stage of the data science workflow comes into play: exploratory data analysis (EDA). EDA serves as a vital process that allows us to visualize and summarize our data, uncover patterns, detect anomalies, and test key assumptions that will inform our modelling decisions.\n\n\nImage by Manfred Steger via Pixabay.\n\nThe first step in EDA is to classify our variables according to their types. This classification is essential because it guides our choice of analysis techniques and models. Specifically, we need to determine whether each variable is discrete or continuous, and whether it has any specific characteristics such as being bounded or unbounded.\n\n\nResponse (i.e., the \\(Y\\)):\n\nDetermine if the response variable is discrete (e.g., binary, count-based, categorical) or continuous.\nIf it is continuous, let us consider whether it is bounded (e.g., percentages that range between \\(0\\) and \\(100\\)) or unbounded (e.g., a variable like company profits/losses that can take on a wide range of values).\n\n\n\nRegressors (i.e., the \\(x\\)s):\n\nFor each regressor, we must identify whether it is discrete or continuous.\nIf a regressor is discrete, let us classify it further as binary, count-based, or categorical.\nIf a regressor is continuous, let us determine whether it is bounded or unbounded.\n\n\n\nThis classification scheme helps us select the appropriate visualization and statistical methods for our analysis, as different variable types often need different approaches. It ensures that we are well-equipped to make the right choices in our analyses.\nAfter classifying your variables, the next step is to create visualizations and calculate descriptive statistics using our training data. This involves coding plots that can reveal the underlying distribution of each variable and the relationships between them. For instance, we might create histograms to visualize distributions, scatter plots to explore relationships between continuous variables, and box plots to compare discrete and categorical variables against a continuous variable.\nAlongside these visualizations, it is important to calculate key descriptive statistics such as the mean, median, and standard deviation if our variables are numeric. These statistics provide a summary of our data, offering insights into central tendency and variability. We might also use a correlation matrix to assess the strength of relationships between continuous variables.\n\n\nImage by Manfred Stege via Pixabay.\n\nOnce we have generated these plots and statistics, they should be displayed in a clear and logical manner. The goal here is to interpret the data and draw preliminary conclusions about the relationships between the observed variables. Presenting these findings effectively helps to uncover key descriptive insights and prepares you for the subsequent modelling stage. Finally, the insights gained from our EDA must be clearly articulated. This involves summarizing the key findings and considering their implications for the next stage of the workflow—data modelling. Observing patterns, correlations, and potential outliers in this stage will inform your modelling approach and ensure that it is grounded in a thorough and informed analysis.\n\n\nHeads-up on the use of EDA to deliver inferential conclusions!\n\n\nEDA plays a critical role in uncovering patterns, detecting anomalies, and generating hypotheses. However, it is important to emphasize that the results of EDA should not be generalized beyond the specific sample data being analyzed. EDA is inherently descriptive and focused on the sample, and it is not intended to support inferential claims about larger populations. The insights gained from EDA are contingent on the specific sample and may not accurately reflect systematic relationships within the broader population. Nevertheless, EDA can provide valuable information to inform our modelling decisions.\n\n\nImage by Manfred Stege via Pixabay.\n\nGeneralizing findings to a larger population requires formal statistical inference, which takes into account sampling variability, model uncertainty, and the precision of estimates. This is particularly important in regression analysis, where extending patterns observed in a sample to the wider population needs rigorous modelling assumptions, estimation procedures, and a quantification of uncertainty (e.g., through confidence intervals). Treating EDA findings as if they were inferential conclusions can lead to misleading interpretations throughout our data science workflow.\n\n\nExample: EDA for Housing Data\nTo illustrate the EDA process, we will follow it within the context of the housing example used in the previous two workflow stages, utilizing simulated data. Suppose we have a sample of \\(n = 2,000\\) houses drawn from various Canadian cities through cluster sampling. As shown in Table 1.1, our earlier inferential and predictive inquiries focus on housing sale price in CAD as our response variable in a regression context. Note that this numeric response cannot be negative, which classifies it as positively unbounded. Additionally, Table 1.1 provides the relevant details for the regressors in this case: the number of bedrooms, square footage, neighbourhood type, and proximity to schools. Note that we also indicate the coding names of all the variables involved.\n\n\nTable 1.1: Classification table for variables in housing data.\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nScale\nModel Role\nCoding Name\n\n\n\nHousing Sale Price (CAD)\nContinuous\nPositively unbounded\nResponse\nsale_price\n\n\nNumber of Bedrooms\nDiscrete\nCount\nRegressor\nbedrooms\n\n\nSquare Footage\nContinuous\nPositively unbounded\nRegressor\nsqft\n\n\nNeighbourhood Type (Rural, Suburban or Urban)\nDiscrete\nCategorical\nRegressor\nneighbourhood\n\n\nProximity to Schools (km)\nContinuous\nPositively unbounded\nRegressor\nschool_distance\n\n\n\n\n\n\nBefore continuing with this housing example, let us make a quick note on this textbook’s coding delivery.\n\n\nHeads-up on coding tabs!\n\n\nYou might be wondering:\n\nWhere do we begin with some R or Python code?\n\nIt is time to introduce our very first lines of code and provide some explanations about the coding approach in this book. Our goal is to make this book “bilingual,” meaning that all hands-on coding practices can be performed in either R or Python. Whenever we present a specific proof of concept or data modelling exercise, you will find two tabs: one for R and another for Python. We will first show the input code, followed by the output.\n\n\nImage by Manfred Stege via Pixabay.\n\nWith this format, you can choose your coding journey based on your language preferences and interests as you progress throughout the book.\n\n\nHaving clarified the bilingual nature of this book with respect to coding, let us load this sample of \\(n = 2,000\\) houses in both R and Python. For Python, we will need the {pandas} library. Table 1.2 and Table 1.3 show the first 100 rows of this full dataset and R and Python, respectively.\n\n\nR Code\nPython Code\n\n\n\n# Loading dataset\nhousing_data &lt;- read.csv(\"data/housing_data.csv\")\n\n# Showing the first 100 houses of the full dataset\nhead(housing_data, n = 100)\n\n\n# Importing library\nimport pandas as pd\n\n# Loading dataset\nhousing_data = pd.read_csv(\"data/housing_data.csv\")\n\n# Showing the first 100 houses of the full dataset\nprint(housing_data.head(100))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\nTable 1.2: First 100 rows of full housing data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1.3: First 100 rows of full housing data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip on this simulated housing data!\n\n\nThe housing_data mentioned above is not an actual dataset; it is a simulated one designed to effectively illustrate our data science workflow in this chapter. This simulated dataset will somehow enable us to meet the assumptions of the chosen model during the data modelling stage outlined in Section 1.2.4. If you would like to learn more about this generative modelling process, you can refer to the provided R script.\n\n\nImage by Manfred Stege via Pixabay.\n\n\n\nNow, we will randomly split the sampled data into training and testing sets for both inferential and predictive inquiries. Specifically, 20% of the data will be allocated to the training set, while the remaining 80% will serve as the testing set. For the predictive analysis, we will not create a validation set because our chosen modelling strategy (to be discussed in Section 1.2.4) does not require it. The below codes do the following:\n\n\nR: This code executes an 80/20 random split of the housing_data dataset using the {rsample} package (Frick et al. 2025). The set.seed() function ensures reproducibility, while initial_split() partitions the data into training and testing subsets. The resulting split object is then passed to training() and testing() to extract the corresponding datasets. A sanity check follows, where dim() and nrow() are used to inspect the shapes of each subset and to compute their observed proportions, confirming that the split aligns with the intended allocation.\n\nPython: This code performs an analogous 80/20 partition of housing_data using train_test_split() from {scikit-learn} (Pedregosa et al. 2011), with random_state ensuring reproducibility. The function returns the training and testing subsets directly. A subsequent sanity check uses .shape and len() to inspect the size of each subset and to verify the observed proportions of the split, ensuring that the partitioning matches the expected configuration before proceeding with further modelling steps. Note that we also use the {numpy} library (Harris et al. 2020).\n\n\n\nHeads-up on the different training and testing sets obtained via R and Python!\n\n\nIt turns out that both the {rsample} package in R and {scikit-learn} in Python utilize different pseudo-random number generators. As a result, they produce different training and testing data splits, even when using the same seed values.\n\n\nImage by Manfred Steger via Pixabay.\n\n\n\n\n\nR Code\nPython Code\n\n\n\n# Loading library\nlibrary(rsample)\n\n# Seed for reproducibility\nset.seed(123)\n\n# Randomly splitting into training and testing sets\nhousing_data_splitting &lt;- initial_split(housing_data,\n  prop = 0.2\n)\n\n# Assigning data points to training and testing sets\ntraining_data &lt;- training(housing_data_splitting)\ntesting_data &lt;- testing(housing_data_splitting)\n\n# Dimension check\ncat(\"Training shape:\", dim(training_data), \"\\n\")\ncat(\"Testing shape:\", dim(testing_data), \"\\n\\n\")\n\n# Proportion check\nn_total &lt;- nrow(housing_data)\nn_train &lt;- nrow(training_data)\nn_test  &lt;- nrow(testing_data)\ncat(\"Training proportion:\", round(n_train / n_total, 3), \"\\n\")\ncat(\"Testing proportion:\",  round(n_test  / n_total, 3), \"\\n\")\n\n\n# Importing libraries\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Seed for reproducibility\nrandom_state = 123\n\n# Randomly splitting into training and testing sets\ntraining_data, testing_data = train_test_split(\n    housing_data,\n    test_size=0.8,\n    random_state=random_state\n)\n\n# Dimension check\nprint(\"Training shape:\", training_data.shape)\nprint(\"Testing shape:\", testing_data.shape, \"\\n\")\n\n# Proportion check\nn_total = len(housing_data)\nn_train = len(training_data)\nn_test  = len(testing_data)\nprint(\"Training proportion:\", round(n_train/n_total, 3))\nprint(\"Testing proportion:\",  round(n_test/n_total, 3))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\nTraining shape: 400 5 \n\n\nTesting shape: 1600 5 \n\n\nTraining proportion: 0.2 \n\n\nTesting proportion: 0.8 \n\n\n\n\n\n\nTraining shape: (400, 5)\n\n\nTesting shape: (1600, 5) \n\n\nTraining proportion: 0.2\n\n\nTesting proportion: 0.8\n\n\n\n\n\nIn addition, the code below displays the first 100 rows of our training data, which is a subset of size equal to 400 data points.\n\n\nR Code\nPython Code\n\n\n\n# Showing the first 100 houses of the training set\nhead(training_data, n = 100)\n\n\n# Showing the first 100 houses of the training set\nprint(training_data.head(100))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\nTable 1.4: First 100 rows of training data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1.5: First 100 rows of training data.\n\n\n\n\n\n\n\n\n\n\n\n\nDue to the use of different pseudo-random number generators for data splitting in R and Python, the training_data in the tables above differs. Now, let us make a necessary clarification about why we need to split the data in inferential inquiries.\n\n\nHeads-up on data splitting for inferential inquiries!\n\n\nIn machine learning, data splitting is a foundational practice designed to prevent data leakage in predictive inquiries. However, you may wonder:\n\nWhy should we also split the data for inferential inquiries?\n\nIn the context of statistical inference, especially when making claims about population parameters, data splitting plays a different but important role: it helps prevent double dipping. Double dipping refers to the misuse of the same data for both exploring hypotheses (as in EDA) and formally testing those hypotheses. This practice undermines the validity of inferential claims by increasing the probability of Type I errors—incorrectly rejecting the null hypothesis \\(H_0\\) when it is actually true for the population under study.\n\n\nImage by Manfred Steger via Pixabay.\n\nTo illustrate this, consider conducting a one-sample \\(t\\)-test in a double-dipping scenario for a population mean \\(\\mu\\). Suppose we first observe a sample mean of \\(\\bar{x} = 9.5\\) (i.e., an EDA summary statistic), and then decide to test the null hypothesis\n\\[\\text{$H_0$: } \\mu \\geq 10\\]\nagainst the alternative hypothesis\n\\[\\text{$H_1$: } \\mu &lt; 10,\\]\nafter performing EDA on the same data. If we proceed with the formal \\(t\\)-test using that same data, we are essentially tailoring the hypothesis to fit our sample. Empirical simulations can show that such practices lead to inflated false positive rates, which threaten the reproducibility and integrity of statistical inference.\nUnlike predictive modelling, data splitting is not a routine practice in statistical inference. However, it becomes relevant when the line between exploration and formal testing is blurred. For more information on double dipping in statistical inference, Chapter 6 of Reinhart (2015) offers in-depth insights and some real-life examples.\n\n\nAfter classifying the variables and splitting our data, we will move on to coding the plots and calculating the summary statistics.\n\n\nHeads-up on the use of R-generated training and testing for the rest of the data science workflow!\n\n\nWe have clarified that both R and Python produce different random data splits, even when using the same seeds. Therefore, in all the following Python code snippets related to this housing price case, we will be utilizing both the training and testing sets generated by the R-based data splitting. This approach ensures consistency in our coding outputs.\n\n\nImage by manfredsteger via Pixabay.\n\nIf you want to reproduce all these outputs in Python using Quarto (Allaire et al. 2025), while utilizing the R-generated sets, you can import these datasets from the R environment using the {reticulate} package (Ushey, Allaire, and Tang 2025).\n\n\nAs we move forward, we provide a list of plots and summary statistics, along with their corresponding EDA outputs and interpretations. This is based on our training data, which has a size of 400. Note that we are not providing the code to generate all of the EDA output directly (though you can find the R source here). However, subsequent chapters will include both R and Python code snippets to generate the corresponding EDA insights. Below is the list:\n\nA histogram of housing sale prices, as in Figure 1.4, shows the response’s distribution and helps identify any outliers. The training set reveals a fairly symmetric distribution of sale prices, with a noticeable concentration of sales between \\(\\$200,000\\) and \\(\\$400,000\\). However, there are a few outliers. Even with just 20% of the total data, this plot provides valuable graphical insights into central tendency and variability.\n\n\n\n\n\n\n\n\nFigure 1.4: Histogram of housing sale prices via training set.\n\n\n\n\n\nSide-by-side jitter plots, as in Figure 1.5, visualize the distribution of sale prices across different bedroom counts, highlighting spread. Overall, these plots indicate a positive association between the number of bedrooms and housing sale price. Note that the average price (represented by red diamonds) tends to increase with the addition of more bedrooms. The training set predominantly has homes with 3 to 5 bedrooms, and there are some high-priced outliers present even among mid-sized homes.\n\n\n\n\n\n\n\n\nFigure 1.5: Side-by-side jitter plots of housing sale prices by number of bedrooms via training set (red diamonds indicate sale price means by number of bedrooms).\n\n\n\n\n\nA scatter plot displaying the relationship between square footage and housing sale price, as in Figure 1.6, illustrates how these two continuous variables interact. There is a clear upward trend in the training data, indicated by the fitted solid red line of the simple linear regression (which is a preliminary regression fit used by different plotting tools in R or Python, via the model from Chapter 3). Although the variability increases with larger square footage, the overall positive linear pattern is still clear.\n\n\n\n\n\n\n\n\nFigure 1.6: Scatter plot of square footage versus housing sale prices via training set (solid red line indicates a simple linear regression fitting).\n\n\n\n\n\nSide-by-side box plots, as in Figure 1.7, are used to compare housing sale prices across different types of neighbourhoods, highlighting variations in median prices. The training data reveals neighbourhood-specific price patterns: urban homes tend to have higher prices, while rural homes are generally less expensive. However, from a graphical perspective, we do not observe major differences in price spreads between these types of neighbourhoods.\n\n\n\n\n\n\n\n\nFigure 1.7: Side-by-side box plots of housing sale prices by neighbourhood type via training set.\n\n\n\n\n\nThe scatter plot showing the relationship between proximity to schools and housing sale price, as in Figure 1.8, reveals an almost flat trend in the training data. This observation is supported by the fitted solid red line of the simple linear regression (same model from Chapter 3), indicating a weak graphical relationship between these two variables.\n\n\n\n\n\n\n\n\nFigure 1.8: Scatter plot of proximity to schools versus housing sale prices via training set.\n\n\n\n\n\n\nDescriptive statistics from Table 1.6, such as the mean and standard deviation, summarize continuous variables. In addition, a Pearson correlation matrix from Table 1.7 numerically assesses the relationships between these variables. Note that square footage is positively correlated with housing sale price, while proximity to schools has a negative association.\n\n\n\n\nTable 1.6: Descriptive statistics of housing data via training set.\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1.7: Pearson correlation matrix of housing data, via training set, for numeric variables.\n\n\n\n\n\n\n\n\n\n\n\nIn displaying and interpreting results, the plots and statistics will guide us in understanding the data. In this specific example, these exploratory insights help identify key factors, such as square footage and neighbourhood type, that influence housing sale prices. They also highlight any outliers that may need further attention during modelling. By following this EDA process, we will establish a solid descriptive foundation for effective data modelling, ensuring that the key variables and their relationships are well understood.\nFinally, this structured approach to EDA is visually summarized in Figure 1.9, which shows the sequential steps from variable classification to the delivery of exploratory insights.\n\n\n\n\n\nFigure 1.9: Exploratory data analysis stage from the data science workflow in Figure 1.1. This stage is directly followed by data modelling and preceded by data collection and wrangling.\n\n\n\n1.2.4 Data Modelling\nThe previous EDA provides a solid descriptive foundation regarding the identified types of data for our response variable and regressors, as well as their graphical relationships. This information will guide us in selecting a suitable regression model based on the following factors:\n\nThe response type (e.g., whether it is continuous, bounded or unbounded, count, binary, categorical, etc.).\nThe flexibility of the chosen model (e.g., its ability to handle extreme values or outliers).\nIts interpretability (i.e., can we effectively communicate our statistical findings to stakeholders?).\n\n\n\nImage by Manfred Steger via Pixabay.\n\nIn statistical literature, we often encounter classical linear regression models, such as the Ordinary Least-squares (OLS) model discussed in Chapter 3. This model enables us to explain our continuous response variable of interest, denoted as a random variable \\(Y\\), in the form of a linear combination of a specified set of regressors (the observed \\(x\\) variables). A linear combination is essentially an additive relationship where \\(Y\\) depends on the \\(x\\) variables, which are multiplied by regression coefficients. Alternatively, for both continuous and discrete response variables, we can utilize more complex models that establish a non-linear relationship between \\(Y\\) and the \\(x\\) variables. Some of these models are referred to as generalized linear models (GLMs).\nFor this workflow stage, whether using a classical linear regression model like OLS or a more complex one such as a GLM (a type of model that is covered in this book along other models that explain survival time responses), we need to establish modelling equations that align with both theoretical and data-driven considerations. These modelling equations will need definitions for the parameters, link functions (if applicable as in the case of GLMs), and any relevant distributional assumptions based on the chosen model. Then, once we have defined our modelling equation(s), we can proceed to the estimation stage. Note that this data modelling stage is iterative, as illustrated in Figure 1.10. The process will depend heavily on the results obtained during the goodness-of-fit stage.\n\n\n\n\n\nFigure 1.10: Data modelling stage from the data science workflow in Figure 1.1. This stage is directly preceded by exploratory data analysis. On the other hand, it is directly followed by estimation but indirectly with goodness of fit. If necessary, the goodness-of-fit stage could retake the process to data modelling.\n\n\nExample: OLS Regression Model for Housing Data\nLet us continue with our housing example, where our response of interest is the sale price of a house in CAD, as shown in Table 1.1. During the study design stage outlined in Section 1.2.1, we identified two key inquiries: inferential and predictive. The inferential inquiry focuses on understanding the statistical associations between the sale price and other variables, such as square footage, number of bedrooms, and proximity to schools. In contrast, the predictive inquiry involves fitting a suitable model to obtain estimates that will enable us to predict housing sale prices based on these same features.\nBefore selecting a model, we need to define our mathematical notation for all the variables involved. Let \\(Y_i\\) represent the continuous sale price of the \\(i\\)th house in CAD from a dataset of size \\(n\\) used to estimate a chosen model in general, where \\(i = 1, 2, \\ldots, n\\). For the observed explanatory variables, we define the following:\n\n\n\\(x_{i, 1}\\) is the number of bedrooms in the \\(i\\)th house, which is a count-type variable.\n\n\\(x_{i, 2}\\) is the continuous square footage of the \\(i\\)th house.\n\n\\(x_{i, 3}\\) is the continuous proximity to schools for the \\(i\\)th house in km.\n\nTo mathematically represent the categorical and nominal neighbourhood types to which the \\(i\\)th house could belong, we need more than one variable \\(x\\). In regression analysis involving nominal explanatory variables, we typically use binary dummy variables. In this example, these dummy variables will help us identify the neighbourhood type of each house. Generally, for a nominal variable with \\(u\\) categories, we need to define \\(u - 1\\) dummy variables, as shown in Table 1.8.\n\n\nTable 1.8: Dummy variable arrangement for a categorical and nominal \\(x\\) with \\(u\\) levels.\n\n\n\nLevel\n\\(x_{i, 1}\\)\n\\(x_{i, 2}\\)\n\\(\\cdots\\)\n\\(x_{i, u - 1}\\)\n\n\n\n\\(1\\)\n\\(0\\)\n\\(0\\)\n\\(\\cdots\\)\n\\(0\\)\n\n\n\\(2\\)\n\\(1\\)\n\\(0\\)\n\\(\\cdots\\)\n\\(0\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\n\n\\(u\\)\n\\(0\\)\n\\(0\\)\n\\(\\cdots\\)\n\\(1\\)\n\n\n\n\n\n\n\n\nHeads-up on how to use dummy variables!\n\n\nIn Table 1.8, note that level \\(1\\) is considered the baseline (reference) level. If the \\(i\\)th observation belongs to level \\(1\\), then all the dummy variables \\(x_{i, 1}, \\ldots, x_{i, u - 1}\\) will take the value of \\(0\\). The choice of baseline affects how we interpret the estimated regression coefficients later in our data science workflow.\n\n\nTable 1.9 shows the dummy variable arrangement for our housing example regarding the neighbourhood type where rural is the baseline level. Since we have three levels (rural, suburban, and urban), our chosen model will have two binary dummy variables for the \\(i\\)th house:\n\\[\nx_{i, 4} =\n\\begin{cases}\n1 \\quad \\text{if the house belongs to a suburban neighbourhood},\\\\\n0 \\quad \\text{otherwise};\n\\end{cases}\n\\tag{1.1}\\]\nand\n\\[\nx_{i, 5} =\n\\begin{cases}\n1 \\quad \\text{if the house belongs to an urban neighbourhood},\\\\\n0 \\quad \\text{otherwise}.\n\\end{cases}\n\\tag{1.2}\\]\n\n\nTable 1.9: Dummy variable arrangement for the categorical and nominal neighbourhood type with \\(3\\) levels.\n\n\n\nLevel\n\\(x_{i,4}\\)\n\\(x_{i,5}\\)\n\n\n\n\\(\\text{Rural}\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\text{Suburban}\\)\n\\(1\\)\n\\(0\\)\n\n\n\\(\\text{Urban}\\)\n\\(0\\)\n\\(1\\)\n\n\n\n\n\n\nWith the mathematical notation for our data variables defined, it is time to choose a suitable regression model to address our inferential and predictive inquiries. Since the nature of \\(Y_i\\) is continuous, we may consider using OLS regression, as outlined in Chapter 3, although there is an important distributional matter to be highlighted at the end of this section. OLS is typically the first regression model to explore because it is a widely used model that is easy to understand and communicate to stakeholders. We refer to OLS as a parametric model, a distinction that other models, such as the GLMs, also have. Let us define this type of model below.\n\n\nDefinition of parametric model\n\n\nA parametric model is a type of model that assumes a specific functional relationship between the response variable of interest, \\(Y\\), which is considered a random variable, and one or more observed explanatory variables, \\(x\\). This relationship is characterized by a finite set of parameters and can often be expressed as a linear combination of the observed \\(x\\) variables, which favours interpretability.\nMoreover, since \\(Y\\) is a random variable, there is room to make further assumptions on it in the form of a probability distribution, independence or even homoscedasticity (the condition where all responses in the population have the same variance). It is essential to test these assumptions after fitting this type of models, as any deviations may result in misleading or biased estimates, predictions, and inferential conclusions.\n\n\nA parametric model, as previously mentioned, allows us to prioritize interpretability in our regression analysis, and OLS offers this advantageous characteristic. The classical setup of OLS describes the relationship between the response variable \\(Y\\) and the observed variables \\(x\\) as a linear combination, represented by the following equation for \\(i = 1, 2, \\ldots, n\\) in this housing price example:\n\\[\nY_i = \\underbrace{\\beta_0 + \\beta_1 x_{i, 1} + \\beta_2 x_{i, 2} + \\beta_3 x_{i, 3} + \\beta_4 x_{i, 4} + \\beta_5 x_{i, 5}}_{\\text{Systematic Component}} + \\underbrace{\\varepsilon_i.}_{\\substack{\\text{Random} \\\\ \\text{Component}}}\n\\tag{1.3}\\]\nEquation 1.3 indicates two important components in this regression model on its righ-hand side:\n\n\nSystematic Component: This component includes six fixed and unknown regression parameters (\\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), and \\(\\beta_5\\)) that we will estimate in the next stage using our training data. Note that this component represents the expected value of the response variable \\(Y\\), conditioned on the observed values of the regressors and it is also the result of the assumptions on the random component below:\n\n\\[\n\\begin{align*}\n\\mathbb{E}(Y_i \\mid x_{i, 1}, \\ldots, x_{i, 5}) &= \\beta_0 + \\beta_1 x_{i, 1} + \\beta_2 x_{i, 2} + \\\\\n& \\qquad \\beta_3 x_{i, 3} + \\beta_4 x_{i, 4} + \\beta_5 x_{i, 5}.\n\\end{align*}\n\\tag{1.4}\\]\n\n\nRandom Component: For the \\(i\\)th observation, this is denoted by the random variable \\(\\varepsilon_i\\). This component measures how much the observed value of the response may deviate from its conditioned mean, and it is considered random noise. Since \\(\\varepsilon_i\\) is assumed to be a random variable and is added to a fixed systematic component on the right-hand side of Equation 1.3, this aligns with the notion that \\(Y_i\\) is treated as a random variable on the left-hand side.\n\nWe also need to state the modelling assumptions for this OLS case:\n\nEach observed regressor on the right-hand side of the Equation 1.3 has an associated regression coefficient \\(\\beta_j\\) for \\(j = 1, 2, \\ldots, 5\\) (these were already indicated as part of the regression parameters). These coefficients represent the expected change in the response variable when a specific regressor \\(x_{i,j}\\) changes by one unit. Additionally, the regression parameter \\(\\beta_0\\) serves as the intercept of this linear model, representing the mean of the response when all five regressors are equal to zero. This entire arrangement allows for a more interpretable model and aids in addressing our inferential inquiry.\nTo pave the way for the corresponding inferential test in OLS, the error term \\(\\varepsilon_i\\) is typically assumed to be normally distributed with a mean of zero (this mean is consistent with the conditioned expected value outlined in Equation 1.4). Additionally, it is assumed that the variance is constant across observations, referred to as the so-called homoscedasticity, and denoted as \\(\\sigma^2\\) (another regression parameter fixed and unknown to estimate via the training set). Furthermore, all error terms \\(\\varepsilon_i\\) are assumed to be statistically independent. These assumptions can be mathematically represented as follows:\n\n\\[\n\\begin{gather*}\n\\mathbb{E}(\\varepsilon_i) = 0 \\\\\n\\text{Var}(\\varepsilon_i) = \\sigma^2 \\\\\n\\varepsilon_i \\sim \\text{Normal}(0, \\sigma^2) \\\\\n\\varepsilon_i \\perp \\!\\!\\! \\perp \\varepsilon_k \\; \\; \\; \\; \\text{for} \\; i \\neq k  \\; \\; \\; \\; \\text{(independence)}.\n\\end{gather*}\n\\]\n\n\nHeads-up on the use of an alternative systematic component!\n\n\nThe systematic component in Equation 1.3 is considered linear with respect to the regression parameters \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), and \\(\\beta_5\\). Therefore, we can model the regressors using mathematical transformations, such as the following polynomial:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i, 1} + \\beta_2 x_{i, 2}^2 + \\beta_3 x_{i, 3}^3 + \\beta_4 x_{i, 4} + \\beta_5 x_{i, 5} + \\varepsilon_i.\n\\]\nThis linearity condition on the parameters makes our OLS model flexible enough to improve accuracy in predictive inquiries. However, we would sacrifice some interpretability for inferential inquiries.\n\n\nImage by Manfred Stege via Pixabay.\n\n\n\nBefore we conclude this stage, note that Chapter 2 will explore the fundamentals of probability and statistical inference in greater depth. This exploration will enhance our understanding of the modelling assumptions underlying the regression models discussed throughout this book. Additionally, we will broaden our perspective on regression to consider more appropriate models for nonnegative responses, instead of relying on OLS with the assumption of an unbounded, normally distributed response which might be unrealistic for nonnegative housing prices (and still a mild violation on our response assumptions, given that the housing prices appear to have a bell-shaped distribution as shown in Figure 1.4).\n\n1.2.5 Estimation\nBased on the data we have and our EDA, defining a suitable regression model (along with the equations that relate the response variable \\(Y\\) to the regressors \\(x\\) and the corresponding regression parameters) is an essential step in our data science workflow. This leads us to the next stage: estimation. In this stage, we aim to obtain what we refer to as modelling estimates using our training dataset. The method we choose for estimation largely depends on the specific regression model we adopt to address our inquiries.\n\n\nImage by Manfred Steger via Pixabay.\n\nIn all core chapters of this book, except for Chapter 3, the default method we will use is maximum likelihood estimation (MLE) (the fundamental insights are provided in Section 2.2). Regardless of the chosen estimation method, these estimates (denoted with a hat notation) will allow us to quantify the association (or causation, if applicable) between the outcome variable \\(Y\\) and the \\(x\\) regressors. This is particularly relevant in inferential inquiries, provided that the results are statistically significant, as discussed in Section 1.2.7.\nAs illustrated in Figure 1.11, the data modelling stage will yield the necessary components for this phase in the form of a suitable model, modelling equation, and regression parameters. We will then use the corresponding R or Python fitting function, where the inputs will include the coded modelling equation (which contains the variables of interest: the outcome and the regressors) along with the training set. These fitting functions serve the following purposes:\n\nIn most regression models, obtaining analytical (i.e., exact) solutions for our parameter estimates is not feasible. Specifically, MLE can employ an optimization method such as Newton-Raphson or iteratively reweighted least squares (IRLS), as we aim to maximize the log-likelihood function that involves our observed data and unknown parameters. This function is numerically optimized to estimate these parameters. More information regarding numerical optimization in MLE, including a brief discussion of the Newton-Raphson method, can be found in Section 2.2.3. Throughout the core chapters of the book, we will delve deeper into the fundamentals of IRLS.\nOnce the estimation process has been completed using the appropriate log-likelihood function and numerical optimization method (i.e., when the method has converged to an optimal solution), we will obtain outputs that include parameter estimates. These parameter estimates will be used in the subsequent workflow stage, called goodness of fit, to statistically assess whether our fitted model satisfies the assumptions we made about our data in the previous modelling stage.\n\n\n\n\n\n\nFigure 1.11: Estimation stage from the data science workflow in Figure 1.1. This stage is directly preceded by data modelling and followed by goodness of fit. If necessary, the goodness-of-fit stage could retake the process to data modelling and then to estimation.\n\n\nExample: Fitting the OLS Regression Model for Housing Data\n\n\nImage by Gustavo Rezende via Pixabay.\n\nLet us examine the training_data for this housing case, which consists of 400 observations. As shown in Table 1.1, for the \\(i\\)th house, we have different regressors: the number of bedrooms (bedrooms, denoted as \\(x_{i, 1}\\)), the continuous square footage (sqft, denoted as \\(x_{i, 2}\\)), the continuous proximity to schools in km (school_distance, denoted as \\(x_{i, 3}\\)), and neighborhood type (represented by dummy variables \\(x_{i, 4}\\) as in Equation 1.1 and \\(x_{i, 5}\\) as in Equation 1.2, where rural is the baseline). The response variable we are interested in is the continuous housing sale price in CAD (sale price, denoted as \\(Y_i\\)). Additionally, we will revisit our modeling approach as outlined in Equation 1.3:\n\\[\nY_i = \\beta_0 + \\beta_1 x_{i, 1} + \\beta_2 x_{i, 2} + \\beta_3 x_{i, 3} + \\beta_4 x_{i, 4} + \\beta_5 x_{i, 5} + \\varepsilon_i,\n\\]\nwhere \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), \\(\\beta_4\\), and \\(\\beta_5\\) represent the unknown regression parameters to be estimated to address our inferential and predictive inquiries. Additionally, we have another parameter to estimate, that is the common variance between the random components of each observation (\\(i = 1, 2, \\ldots, n\\)):\n\\[\n\\text{Var}(\\varepsilon_i) = \\sigma^2.\n\\]\nHaving set up the coding starting point for this estimation, we need to use the corresponding fitting functions to find \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), \\(\\hat{\\beta}_2\\), \\(\\hat{\\beta}_3\\), \\(\\hat{\\beta}_4\\), \\(\\hat{\\beta}_5\\), \\(\\hat{\\sigma}^2\\) via OLS regression (as we already decided in the data modelling stage). Therefore, let us the following function and libraries:\n\n\nR: We fit and summarize an OLS model using the {tidyverse} and {broom} (Robinson, Hayes, and Couch 2025) packages. It first loads the two libraries, which provide tools for data manipulation and for converting model outputs into tidy data frames, respectively. The lm() function then fits the OLS regression model for the response sale_price from four explanatory variables: bedrooms, sqft, school_distance, and neighbourhood via the training_data. The resulting model object, stored in training_OLS_model, contains the estimated parameters (column estimate) and related statistics (which will be explained in the results stage via the testing set). The tidy() function from {broom} transforms this model output into a neat, tabular format. Finally, mutate_if() rounds all numeric columns in this tidy summary to two decimal places, producing a clean, readable table of regression results.\n\nPython: This code fits and summarizes the same OLS model using the {statsmodels} (Seabold and Perktold 2010), {pandas}, and {numpy} libraries. It begins by specifying and fitting this OLS model through smf.ols(), which regresses sale_price based on four explanatory variables: bedrooms, sqft, school_distance, and neighbourhood via the training_data. The .fit() method estimates the regression coefficients and computes related statistics. Next, a tidy summary table is created using pd.DataFrame(), which organizes the model output into a clear format. Each numeric value is rounded to two decimal places with np.round() for readability. Finally, the code prints this table, providing a concise, easy-to-read summary of the regression model.\n\n\n\nR\nPython\n\n\n\n\n# Loading libraries\nlibrary(tidyverse)\nlibrary(broom)\n# To import R-generated datasets to Python environment\nlibrary(reticulate)\n\n# Fitting the OLS model\ntraining_OLS_model &lt;- lm(\n  formula = sale_price ~ bedrooms + sqft +\n    school_distance + neighbourhood,\n  data = training_data\n)\n\n# Displaying the tidy table\ntidy(training_OLS_model) |&gt;\n  mutate_if(is.numeric, round, 2)\n\n# A tibble: 6 × 5\n  term                  estimate std.error statistic p.value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)            53948.     8029.       6.72       0\n2 bedrooms               14107.      860.      16.4        0\n3 sqft                      99.0       3.4     29.1        0\n4 school_distance        -6964.     1032.      -6.75       0\n5 neighbourhoodSuburban  26908.     3999.       6.73       0\n6 neighbourhoodUrban     60509.     3970.      15.2        0\n\n\n\n\n\n# Importing libraries\nimport statsmodels.formula.api as smf\n\n# Importing R-generated training set via R library reticulate\ntraining_data = r.training_data\n\n# Fitting the OLS model\ntraining_OLS_model = smf.ols(\n    formula=\"sale_price ~ bedrooms + sqft + school_distance + neighbourhood\",\n    data=training_data\n).fit()\n\n# Creating a tidy summary table\ntraining_OLS_tidy = pd.DataFrame({\n    \"term\": training_OLS_model.params.index,\n    \"estimate\": np.round(training_OLS_model.params.values, 2),\n    \"std_error\": np.round(training_OLS_model.bse.values, 2),\n    \"t_value\": np.round(training_OLS_model.tvalues.values, 2),\n    \"p_value\": np.round(training_OLS_model.pvalues.values, 2)\n})\n\n# Displaying the tidy table\nprint(training_OLS_tidy)\n\n                        term  estimate  std_error  t_value  p_value\n0                  Intercept  53948.21    8029.29     6.72      0.0\n1  neighbourhood[T.Suburban]  26908.04    3998.83     6.73      0.0\n2     neighbourhood[T.Urban]  60509.46    3969.56    15.24      0.0\n3                   bedrooms  14107.19     859.81    16.41      0.0\n4                       sqft     99.01       3.40    29.12      0.0\n5            school_distance  -6963.93    1031.76    -6.75      0.0\n\n\n\n\n\n\n\nHeads-up on the OLS analytical estimates!\n\n\nUnlike GLMs, which will be discussed beginning in Chapter 4, OLS regression provides exact analytical estimates. Therefore, it is not necessary to rely on numerical optimization in this case. You can find further details about this matter in Chapter 3.\n\n\nSince the inferential inquiry in this example aims to understand the statistical associations between housing sale prices and their explanatory variables, while the predictive inquiry seeks to fit a suitable model that allows us to predict housing sale prices based on these same features, we might be tempted to use training_OLS_model to address both inquiries right away. However, according to our data science workflow, these analyses should be conducted during the results stage, after we have completed model diagnostic checks. For now, we will use the training_data and training_OLS_model to perform this corresponding goodness of fit in the next stage.\n\n1.2.6 Goodness of Fit\n\n\n\n\n\nFigure 1.12: Goodness-of-fit stage from the data science workflow in Figure 1.1. This stage is directly preceded by estimation and followed by results. If necessary, the goodness-of-fit stage could retake the process to data modelling and then to estimation.\n\n\n\n1.2.7 Results\n\n\n\n\n\n\nFigure 1.13: Results stage from the data science workflow in Figure 1.1. This stage is directly followed by storytelling and preceded by goodness of fit.\n\n\n\n1.2.8 Storytelling\n\n\n\n\n\nFigure 1.14: Storytelling stage from the data science workflow in Figure 1.1. This stage preceded by results.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-regression-mindmap",
    "href": "book/01-intro.html#sec-regression-mindmap",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "\n1.3 Mind Map of Regression Analysis",
    "text": "1.3 Mind Map of Regression Analysis\n\n\n\nImage by Manfred Steger via Pixabay.\n\n\nHaving defined the necessary statistical aspects to execute a proper supervised learning analysis, either inferential or predictive across its seven sequential phases, we must dig into the different approaches we might encounter in practice as regression models. The nature of our outcome of interest will dictate any given modelling approach to apply, depicted as clouds in Figure 1.15. Note these regression models can be split into two sets depending on whether the outcome of interest is continuous or discrete. Therefore, under a probabilistic view, identifying the nature of a given random variable is crucial in regression analysis.\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n        {{Overdispersed or &lt;br/&gt;Underdispersed &lt;br/&gt;Data}}\n          )Chapter 13: &lt;br/&gt;Generalized &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Generalized &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n        {{Zero Inflated &lt;br/&gt;Data}}\n          )Chapter 12: &lt;br/&gt;Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n      {{Categorical &lt;br/&gt;Outcome Y}}\n        {{Nominal &lt;br/&gt;Outcome Y}}\n          )Chapter 14: &lt;br/&gt;Multinomial &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Multinomial &lt;br/&gt;Outcome Y)\n        {{Ordinal &lt;br/&gt;Outcome Y}}\n          )Chapter 15: &lt;br/&gt;Ordinal &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Logistic &lt;br/&gt;Distributed &lt;br/&gt;Cumulative Outcome &lt;br/&gt;Probability)\n\n\n\n\n\n\n\n\nFigure 1.15: Regression analysis mind map depicting all modelling techniques to be explored in this book. Depending on the type of outcome \\(Y\\), these techniques are split into two large zones: discrete and continuous.\n\n\nThat said, we will go beyond OLS regression and explore further regression techniques. In practice, these techniques have been developed in the statistical literature to address practical cases where the OLS modelling framework and assumptions are not suitable anymore. Thus, throughout this block, we will cover (at least) one new regression model per lecture.\nAs we can see in the clouds of Figure 1.15, there are 13 regression models: 8 belonging to discrete outcomes and 5 to continuous outcomes. Each of these models is contained in a chapter of this book, beginning with the most basic regression tool known as ordinary least-squares in Chapter 3. We must clarify that the current statistical literature is not restricted to these 13 regression models. The field of regression analysis is vast, and one might encounter more complex models to target certain specific inquiries. Nonetheless, I consider these models the fundamental regression approaches that any data scientist must be familiar with in everyday practice.\nEven though this book comprises thirteen core chapters, each depicting a different regression model, we have split these chapters into two major subsets: those with continuous outcomes and those with discrete outcomes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-sup-learning-regression",
    "href": "book/01-intro.html#sec-sup-learning-regression",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "\n1.4 Supervised Learning and Regression Analysis",
    "text": "1.4 Supervised Learning and Regression Analysis",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/01-intro.html#sec-chapter-3-summary",
    "href": "book/01-intro.html#sec-chapter-3-summary",
    "title": "1  Getting Ready for Regression Cooking!",
    "section": "\n1.5 Chapter Summary",
    "text": "1.5 Chapter Summary\n\n\n\n\nAllaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, Christophe Dervieux, and Gordon Woodhull. 2025. “Quarto.” https://doi.org/10.5281/zenodo.5960048.\n\n\nFrick, Hannah, Fanny Chow, Max Kuhn, Michael Mahoney, Julia Silge, and Hadley Wickham. 2025. Rsample: General Resampling Infrastructure. https://doi.org/10.32614/CRAN.package.rsample.\n\n\nGelbart, Michael. 2017. “Data Science Terminology.” UBC MDS. Master of Data Science at the University of British Columbia. https://ubc-mds.github.io/resources_pages/terminology/.\n\n\nHarris, Charles R., K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020. “Array Programming with NumPy.” Nature 585 (7825): 357–62. https://doi.org/10.1038/s41586-020-2649-2.\n\n\nLohr, S. L. 2021. Sampling: Design and Analysis. Chapman; Hall/CRC. https://doi.org/https://doi.org/10.1201/9780429298899.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nReinhart, Alex. 2015. Statistics Done Wrong: The Woefully Complete Guide. 1st ed. San Francisco, CA: No Starch Press. https://www.statisticsdonewrong.com/index.html.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2025. Broom: Convert Statistical Objects into Tidy Tibbles. https://broom.tidymodels.org/.\n\n\nSeabold, Skipper, and Josef Perktold. 2010. “Statsmodels: Econometric and Statistical Modeling with Python.” In 9th Python in Science Conference.\n\n\nUshey, Kevin, JJ Allaire, and Yuan Tang. 2025. Reticulate: Interface to ’Python’. https://doi.org/10.32614/CRAN.package.reticulate.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting Ready for Regression Cooking!</span>"
    ]
  },
  {
    "objectID": "book/02-stats-review.html",
    "href": "book/02-stats-review.html",
    "title": "2  Basic Cuisine: A Review on Probability and Frequentist Statistical Inference",
    "section": "",
    "text": "2.1 Basics of Probability\nIn terms of regression analysis (either on an inferential or predictive framework), probability can be viewed as the solid foundation on which more complex tools, including estimation and hypothesis testing, are built upon. Having said that, let us scaffold across all the necessary probabilistic concepts that will allow us to move forward into these more complex tools.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span>"
    ]
  },
  {
    "objectID": "book/02-stats-review.html#sec-basics-prob",
    "href": "book/02-stats-review.html#sec-basics-prob",
    "title": "2  Basic Cuisine: A Review on Probability and Frequentist Statistical Inference",
    "section": "",
    "text": "2.1.1 First Insights\nTo start building up our solid probabilistic foundation, we assume our data is coming from a given population or system of interest. Moreover, the population or system is assumed to be governed by parameters which, as data scientists or researchers, they are of our best interest to study. That said, the terms population and parameter will pave the way to our first statistical definitions.\n\n\nDefinition of population\n\n\nIt is a whole collection of individuals or items that share distinctive attributes. As data scientists or researchers, we are interested in studying these attributes, which we assume are governed by parameters. In practice, we must be as specific as possible when defining our given population such that we would frame our entire data modelling process since its very early stages. Examples of a population could be the following:\n\nChildren between the ages of 5 and 10 years old in states of the American West Coast.\nCustomers of musical vinyl records in the Canadian provinces of British Columbia and Alberta.\nAvocado trees grown in the Mexican state of Michoacán.\nAdult giant pandas in the Southwestern Chinese province of Sichuan.\nMature açaí palm trees from the Brazilian Amazonian jungle.\n\n\n\nImage by Eak K. via Pixabay.\n\nNote that the term population could be exchanged for the term system, given that certain contexts do not particularly refer to individuals or items. Instead, these contexts could refer to processes whose attributes are also governed by parameters. Examples of a system could be the following:\n\nThe production of cellular phones from a given model in a set of manufacturing facilities.\nThe sale process in the Vancouver franchises of a well-known ice cream parlour.\nThe transit cycle during rush hours on weekdays in the twelve lines of Mexico City’s subway.\n\n\n\n\n\nDefinition of parameter\n\n\nIt is a characteristic (numerical or even non-numerical, such as a distinctive category) that summarizes the state of our population or system of interest. Examples of a population parameter can be described as follows:\n\nThe average weight of children between the ages of 5 and 10 years old in states of the American west coast (numerical).\nThe variability in the height of the mature açaí palm trees from the Brazilian Amazonian jungle (numerical).\nThe proportion of defective items in the production of cellular phones in a set of manufacturing facilities (numerical).\nThe average customer waiting time to get their order in the Vancouver franchises of a well-known ice cream parlour (numerical).\nThe most favourite pizza topping of vegetarian adults between the ages of 30 and 40 years old in Edmonton (non-numerical).\n\n\n\nImage by meineresterampe via Pixabay.\n\nNote the standard mathematical notation for population parameters are Greek letters (for more insights, you can check Appendix B). Moreover, in practice, these population parameter(s) of interest will be unknown to the data scientist or researcher. Instead, they would use formal statistical inference to estimate them.\n\n\nThe parameter definition points out a crucial fact in investigating any given population or system:\n\nOur parameter(s) of interest are usually unknown!\n\nGiven this fact, it would be pretty unfortunate and inconvenient if we eventually wanted to discover any significant insights about the population or system. Therefore, let us proceed to our so-called tasty example so we can dive into the need for statistical inference and why probability is our perfect ally in this parameter quest.\nImagine you are the owner of a large fleet of ice cream carts, around 900 to be exact. These ice cream carts operate across different parks in the following Canadian cities: Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal. In the past, to optimize operational costs, you decided to limit ice cream cones to only two items: vanilla and chocolate flavours, as in Figure 2.1.\n\n\n\n\n\nFigure 2.1: The two flavours of the ice cream cone you sell across all your ice cream carts: vanilla and chocolate. Image by tomekwalecki via Pixabay.\n\n\nNow, let us direct this whole case onto a more statistical and probabilistic field; suppose you have a well-defined overall population of interest for those above eight Canadian cities: children between 4 and 11 years old attending these parks during the Summer weekends. Of course, Summer time is coming this year, and you would like to know which ice cream cone flavour is the favourite one for this population (and by how much!). As a business owner, investigating ice cream flavour preferences would allow you to plan Summer restocks more carefully with your corresponding suppliers. Therefore, it would be essential to start collecting consumer data so the company can tackle this demand query.\nAlso, suppose there is a second query. For the sake of our case, we will call it a time query. As a critical component of demand planning, besides estimating which cone flavour is the most preferred one (and by how much!) for the above population of interest, the operations area is currently requiring a realistic estimation of the average waiting time from one customer to the next one in any given cart during Summer weekends. This average waiting time would allow the operations team to plan carefully how much stock each cart should have so there will not be any waste or shortage.\n\n\nImage by Icons8 Team via Unsplash.\n\nNote that the time query is related to a different population from the previous query. Therefore, we can define it as all our ice cream customers during the Summer weekends and not just all the children between 4 and 11 years old attending the parks during Summer weekends. Consequently, it is crucial to note that the nature of our queries will dictate how we define our population and our subsequent data modelling and statistical inference.\nSummer time represents the most profitable season from a business perspective, thus solving these above two queries is a significant priority for your company. Hence, you decide to organize a meeting with your eight general managers (one per Canadian city). Finally, during the meeting with the general managers, it was decided to do the following:\n\nFor the demand query, a comprehensive market study will be run on the population of interest across the eight Canadian cities right before next Summer; suppose we are currently in Spring.\nFor the time query, since the operations team has not previously recorded any historical data (surprisingly!), all vendor staff from the 900 carts will start collecting data on the waiting time in seconds between each customer this upcoming Summer.\n\nWhen discussing study requirements for the marketing firm who would be in charge of it for the demand query, Vancouver’s general manager dares to state the following:\n\nSince we’re already planning to collect consumer data on these cities, let’s mimic a census-type study to ensure we can have the most precise results on their preferences.\n\nOn the other hand, when agreeing on the specific operations protocol to start recording waiting times for all the 900 vending carts this upcoming Summer, Ottawa’s general manager provides a comment for further statistical food for thought:\n\nThe operations protocol for recording waiting times in the 900 vending carts looks too cumbersome to implement straightforwardly this upcoming Summer. Why don’t we select a smaller set of waiting times between two general customers across the 900 ice cream carts in the eight cities to have a more efficient process implementation that would allow us to optimize operational costs?\n\nBingo! Ottawa’s general manager just nailed the probabilistic way of making inference on our population parameter of interest for the time query. Indeed, their comment was primarily framed from a business perspective of optimizing operational costs. Still, this fact does not take away a crucial insight on which statistical inference is built: a random sample (as in its corresponding definition). As for Vancouver’s general manager, their proposal is not feasible. Mimicking a census-type study might not be the most optimal decision for the demand query given the time constraint and the potential size of its target population.\n\n\nHeads-up on the use random sampling with probabilistic foundations!\n\n\nLet us clarify things from the start, especially from a statistical perspective:\n\nRealistically, there is no cheap and efficient way to conduct a census-type study for either of the two queries.\n\nWe must rely on probabilistic random sampling, selecting two small subsets of individuals from our two populations of interest. This approach allows us to save both financial and operational resources compared to conducting a complete census. However, random sampling requires us to use various probabilistic and inferential tools to manage and report the uncertainty associated with the estimation of the corresponding population parameters, which will help us answer our initial main queries.\n\n\nImage by manfredsteger via Pixabay.\n\nTherefore, having said all this, let us assume that in this ice cream case, the company decided to go ahead with random sampling to answer both queries.\n\n\nMoving on to one of the core topics in this chapter, we can state that probability is viewed as the language to decode random phenomena that occur in any given population or system of interest. In our example, we have two random phenomena:\n\nFor the demand query, a phenomenon can be represented by the preferred ice cream cone flavour of any randomly selected child between 4 and 11 years old attending the parks of the above eight Canadian cities during the Summer weekends.\nRegarding the time query, a phenomenon of this kind can be represented by any randomly recorded waiting time between two customers during a Summer weekend in any of the above eight Canadian cities across the 900 ice cream carts.\n\nNow, let us finally define what we mean by probability along with the inherent concept of sample space.\n\n\nDefinition of probability\n\n\nLet \\(A\\) be an event of interest in a random phenomenon of a population or system of interest, whose all possible outcomes belong to a given sample space \\(S\\). Generally, the probability for this event \\(A\\) happening can be mathematically depicted as \\(P(A)\\). Moreover, suppose we observe the random phenomenon \\(n\\) times such as we were running some class of experiment, then \\(P(A)\\) is defined as the following ratio:\n\\[\nP(A) = \\frac{\\text{Number of times event $A$ is observed}}{n},\n\\tag{2.1}\\]\nas the \\(n\\) times we observe the random phenomenon goes to infinity.\nEquation 2.1 will always put \\(P(A)\\) in the following numerical range:\n\\[\n0 \\leq P(A) \\leq 1.\n\\]\n\n\n\n\nDefinition of sample space\n\n\nLet \\(A\\) be an event of interest in a random phenomenon of a population or system of interest. The sample space \\(S\\) of event \\(A\\) denotes the set of all the possible random outcomes we might encounter every time we randomly observe \\(A\\) such as we were running some class of experiment.\nNote each of these outcomes has a determined probability associated with them. If we add up all these probabilities, the probability of the sample space \\(S\\) will be one, i.e.,\n\\[\nP(S) = 1.\n\\tag{2.2}\\]\n\n\n\n2.1.2 Schools of Statistical Thinking\nNote the above definition for the probability of an event \\(A\\) specifically highlights the following:\n\n… as the \\(n\\) times we observe the random phenomenon goes to infinity.\n\nThe “infinity” term is key when it comes to understanding the philosophy behind the frequentist school of statistical thinking in contrast to its Bayesian counterpart. In general, the frequentist way of practicing statistics in terms of probability and inference is the approach we usually learn in introductory courses, more specifically when it comes to hypothesis testing and confidence intervals which will be explored in Section 2.3. That said, the Bayesian approach is another way of practicing statistical inference. Its philosophy differs in what information is used to infer our population parameters of interest. Below, we briefly define both schools of thinking.\n\n\nDefinition of frequentist statistics\n\n\nThis statistical school of thinking heavily relies on the frequency of events to estimate specific parameters of interest in a population or system. This frequency of events is reflected in the repetition of \\(n\\) experiments involving a random phenomenon within this population or system.\nUnder the umbrella of this approach, we assume that our governing parameters are fixed. Note that, within the philosophy of this school of thinking, we can only make precise and accurate predictions as long as we repeat our \\(n\\) experiments as many times as possible, i.e.,\n\\[\nn \\rightarrow \\infty.\n\\]\n\n\n\n\nDefinition of Bayesian statistics\n\n\nThis statistical school of thinking also relies on the frequency of events to estimate specific parameters of interest in a population or system. Nevertheless, unlike frequentist statistics, Bayesian statisticians use prior knowledge on the population parameters to update their estimations on them along with the current evidence they can gather. This evidence is in the form of the repetition of \\(n\\) experiments involving a random phenomenon. All these ingredients allow Bayesian statisticians to make inference by conducting appropriate hypothesis testings, which are designed differently from their mainstream frequentist counterpart.\n\n\nThe unique known portrait of Reverend Thomas Bayes according to O’Donnell, T. (1936), even though Bellhouse (2004) argues it might not be a Bayes’ portrait.\n\nUnder the umbrella of this approach, we assume that our governing parameters are random; i.e., they have their own sample space and probabilities associated to their corresponding outcomes. The statistical process of inference is heavily backed up by probability theory mostly in the form of the Bayes’ rule (named after Reverend Thomas Bayes, an English statistician from the 18th century). This rule uses our current evidence along with our prior beliefs to deliver a posterior distribution of our random parameter(s) of interest.\n\n\nLet us put the definitions for these two schools of statistical thinking into a more concrete example. We can use the demand query from our ice cream case as a starting point. More concretely, we can dig more into a standalone population parameter such as the probability that a randomly selected child between 4 and 11 years old, attending the parks of the above eight Canadian cities during the Summer weekends, prefers the chocolate-flavoured ice cream cone over the vanilla one. Think about the following two hypothetical questions:\n\nFrom a frequentist point of view, what is the estimated probability of preferring chocolate over vanilla after randomly surveying \\(n = 100\\) children from our population of interest?\nUsing a Bayesian approach, suppose the marketing team has found ten prior market studies on similar children populations on their preferred ice cream flavour (between chocolate and vanilla). Therefore, along with our actual random survey of \\(n = 100\\) children from our population of interest, what is the posterior estimation of the probability of preferring chocolate over vanilla?\n\nBy comparing the above (a) and (b), we can see one characteristic in common when it comes to the estimation of the probability of preferring chocolate over vanilla: both frequentist and Bayesian approaches rely on the gathered evidence coming from the random survey of \\(n = 100\\) children from our population of interest. On the one hand, the frequentist approach solely relies on observed data to estimate this single probability of preferring chocolate over vanilla. On the other hand, the Bayesian approach uses the observed data in conjunction with the prior knowledge provided by the ten estimated probabilities to deliver a whole posterior distribution (i.e., the posterior estimation) of the probability of preferring chocolate over vanilla.\n\n\nHeads-up on the debate between frequentist and Bayesian statistics!\n\n\nEven though most of us began our statistical journey in a frequentist framework, we might be tempted to state that a Bayesian paradigm for parameter estimation and inference is better than a frequentist one since the former only takes into account the observed evidence without the prior knowledge on our parameters of interest.\n\n\nImage by Manfred Steger via Pixabay.\n\nIn the statistical community, there could be a fascinating debate between the pros and cons of each school of thinking. That said, it is crucial to state that no paradigm is considered wrong! Instead, using a pragmatic strategy of performing statistics according to our specific context is more convenient.\n\n\n\n\nTip on further Bayesian and frequentist insights!\n\n\nLet us check the following two examples (aside from our ice cream case) to illustrate the above pragmatic way of doing things:\n\nTake the production of cellular phones from a given model in a set of manufacturing facilities as the context. Hence, one might find a frequentist estimation of the proportion of defective items as a quicker and more efficient way to correct any given manufacturing process. That is, we will sample products from our finalized batches and check their status (defective or non-defective, our observed evidence) to deliver a proportion estimation of defective items.\nNow, take a physician’s context. It would not make a lot of sense to study the probability that a patient develops a certain disease by only using a frequentist approach, i.e., looking at the current symptoms which account for the observed evidence. In lieu, a Bayesian approach would be more suitable to study this probability which uses the observed evidence combined with the patient’s history (i.e., the prior knowledge) to deliver our posterior belief on the disease probability.\n\n\n\nHaving said all this, it is important to reiterate that the focus of this textbook is purely frequentist in regards to data modelling in regression analysis. If you would like to explore the fundamentals of the Bayesian paradigm; Johnson, Ott, and Dogucu (2022) have developed an amazing textbook on the basic probability theory behind this school of statistical thinking along with a whole variety regression techniques including the parameter estimation rationale.\n\n2.1.3 The Random Variables\nAs we continue our frequentist quest to review the probabilistic insights related to parameter estimation and statistical inference, we will focus on our ice cream case while providing a comprehensive array of definitions. Many of these definitions are inspired by the work of Casella and Berger (2024) and Soch et al. (2024).\nEach time we introduce a new probabilistic or statistical concept, we will apply it immediately to this ice cream case, allowing for hands-on practice that meets the learning objectives of this chapter. It is important to pay close attention to the definition and heads-up admonitions, as they are essential for fully understanding how these concepts apply to the ice cream case. On the other hand, the tip admonitions are designed to offer additional theoretical insights that may interest you, but they can be skipped if you prefer.\n\n\nTable 2.1: Table containing the corresponding insights to solve our demand and time queries.\n\n\n\n\n\n\n\n\n\nDemand Query\nTime Query\n\n\n\nStatement\nWe would like to know which ice cream flavour is the favourite one (either chocolate or vanilla) and by how much.\nWe would like to know the average waiting time from one customer to the next one in any given ice cream cart.\n\n\nPopulation of interest\n\nChildren between 4 and 11 years old attending different parks in Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during Summer weekends.\n\nAll our general customer-to-customer waiting times in the different parks of Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during Summer weekends across the 900 ice cream carts.\n\n\nParameter\n\nProportion of individuals from the population of interest who prefer the chocolate flavour versus the vanilla flavour.\n\nAverage waiting time from one customer to the next one.\n\n\n\n\n\n\nTable 2.1 presents the general statements and populations of interest derived from our two queries: demand and time. It is important to note that these general statements are based on the storytelling we initiated in Section 2.1.1. In practice, summarizing the overarching statistical problem is essential. This will enable us to translate the corresponding issue into a specific statement and population, from which we can define the parameters we aim to estimate later in our statistical process.\nNow, recall that in our initial meeting with the general managers, Ottawa’s general manager provided valuable statistical insights regarding the foundation of a random sample. For the time query, they suggested selecting a smaller set of waiting times between two general customers across the 900 ice cream carts. We already addressed this process as sampling, more specifically random sampling in technical language.\nSimilarly, we can apply this concept to the demand query by selecting a subgroup of children aged 4 to 11 who are visiting different parks in these eight cities. Then, we can ask them about their favorite ice cream flavour, specifically whether they prefer chocolate or vanilla. It is important to note that we are not conducting any census-type studies; instead, we are carrying out two studies that heavily rely on sampling to estimate population parameters.\n\n\nImage by Manfred Stege via Pixabay.\n\nFurthermore, we want to ensure that our two groups of observations—both children and waiting times—are representative of their respective populations. So, how can we achieve this? The baseline key is through what we call simple random sampling. This process involves the following per query:\n\nFor the demand query, let us assume there are \\(N_d\\) observations in our population of interest. In a simple random sampling scheme with replacement, our random sample will consist of \\(n_d\\) observations (noting that \\(n_d &lt;&lt; N_d\\)), each having the same probability of being selected for our estimation and inferential purposes, which is given by \\(\\frac{1}{N_d}\\).\nFor the time query, assume there are \\(N_t\\) observations in our population of interest. Again, in a simple random sampling scheme with replacement, our random sample will consist of \\(n_t\\) observations (noting that \\(n_t &lt;&lt; N_t\\)), each having the same probability of selection for estimation and inferential purposes, which is \\(\\frac{1}{N_t}\\).\n\n\n\nHeads-up on sampling with replacement!\n\n\nKeep in mind that sampling with replacement means you return any specific drawn observation back to the corresponding population before the next draw.\n\n\n\n\nTip of further sampling techniques!\n\n\nIf you want to explore additional and more complex sampling techniques besides simple random sampling, Section 1.2.2 provides further details and an external resource.\n\n\nWe can observe the concept of randomness reflected throughout the sampling schemes mentioned above. This aligns with what we referred to as random phenomena in both queries back in Section 2.1.1. Consequently, there should be a way to mathematically represent these phenomena, and the random variable is the starting point in this process.\n\n\nDefinition of random variable\n\n\nA random variable is a function where the input values correspond to real numbers assigned to events belonging to the sample space \\(S\\), and whose outcome is one of these real numbers after executing a given random experiment. For instance, a random variable (and its support, i.e., real numbers) is depicted with an uppercase such that\n\\[Y \\in \\mathbb{R}.\\]\n\n\nTo begin experimenting with random variables in this ice cream case, we need to define them. It is important to be as clear as possible when defining random variables, and we should also remember to use uppercase letters as follows:\n\\[\n\\begin{align*}\nD_i &= \\text{A favourite ice cream flavour of a randomly surveyed $i$th child} \\\\\n& \\qquad \\text{between 4 and 11 years old attending the parks of} \\\\\n& \\qquad \\text{Vancouver, Victoria, Edmonton, Calgary,} \\\\\n& \\qquad \\text{Winnipeg, Ottawa, Toronto, and Montréal} \\\\\n& \\qquad \\text{during the Summer weekends} \\\\\n& \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\text{for $i = 1, \\dots, n_d.$}  \\\\\n\\\\\nT_j &= \\text{A randomly recorded $j$th waiting time in minutes between two} \\\\\n& \\qquad \\text{customers during a Summer weekend in any of the above} \\\\\n& \\qquad \\text{eight Canadian cities across the 900 ice cream carts} \\\\\n& \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\text{for $j = 1, \\dots, n_t.$}  \\\\\n\\end{align*}\n\\]\nNote that the demand query corresponds to the \\(i\\)th random variable \\(D_i\\), where the subindex \\(i\\) ranges from \\(1\\) to \\(n_d\\). The term \\(n_d\\) represents the sample size for this query and theoretically indicates the number of random variables we intend to observe from our population of interest during our sampling. On the other hand, for the time query, we have the \\(j\\)th random variable \\(T_j\\), with the subindex \\(j\\) ranging from \\(1\\) to \\(n_t\\). In the context of this query, \\(n_t\\) denotes the sample size and indicates how many random variables we plan to observe from our population of interest as part of our sampling.\nNow, \\(D_i\\) will require real numbers that correspond to potential outcomes derived from the specific demand sample space of ice cream flavour. It is crucial to note that a given child from our population may prefer a flavour other than chocolate or vanilla—for example, strawberry, salted caramel, or pistachio. However, we are limited by our available flavour menu as a company. Therefore, we will restrict our survey question regarding these potential \\(n_d\\) surveyed children as follows:\n\\[\nd_i =\n\\begin{cases}\n1 \\qquad \\text{The surveyed child prefers chocolate.}\\\\\n0 \\qquad \\text{Otherwise.}\n\\end{cases}\n\\tag{2.3}\\]\nIn the modelling associated with Equation 2.3, an observed random variable \\(d_i\\) (thus, the lowercase) can only yield values of \\(1\\) if the surveyed child prefers chocolate and \\(0\\) otherwise. The term “otherwise” refers to any flavour other than chocolate, which, in our limited menu context, is vanilla.\nTo define the real numbers from a given waiting time sample space, associated with an observed random variable \\(t_j\\) (thus, the lowercase) measured in minutes, we need to establish a possible range for these waiting times. It would not make sense to have observed negative waiting times in this ice cream scenario; therefore, our lower bound for this range of potential values should be \\(0\\) minutes. However, we cannot set an upper limit on these waiting times since any ice cream vendor might need to wait for \\(1, 2, 3, \\ldots, 10, \\ldots, 20, \\ldots, 60, \\ldots\\) minutes for the next customer to arrive. In fact, it is possible to wait for a very long time, especially on a low sales day! Thus, the range of this observed random variable can be expressed as:\n\\[\nt_j \\in [0, \\infty),\n\\]\nwhere the \\(\\infty\\) symbol indicates no upper bound.\nAfter defining the possible values for our two random variables \\(D_i\\) and \\(T_j\\), we will now classify them correctly using further probabilistic definitions as shown below.\n\n\nDefinition of discrete random variable\n\n\nLet \\(Y\\) be a random variable whose support is \\(\\mathcal{Y}\\). If this support \\(\\mathcal{Y}\\) corresponds to a finite set or a countably infinite set of possible values, then \\(Y\\) is considered a discrete random variable.\nFor instance, we can encounter discrete random variables which could be classified as\n\n\nbinary (i.e., a finite set of two possible values),\n\ncategorical (either nominal or ordinal, which have a finite set of three or more possible values), or\n\ncounts (which might have a finite set or a countably infinite set of possible values as integers).\n\n\n\nImage by Pexels via Pixabay.\n\n\n\n\n\nDefinition of continuous random variable\n\n\nLet \\(Y\\) be a random variable whose support is \\(\\mathcal{Y}\\). If this support \\(\\mathcal{Y}\\) corresponds to an uncountably infinite set of possible values, then \\(Y\\) is considered a continuous random variable.\nNote a continuous random variable could be\n\n\ncompletely unbounded (i.e., its set of possible values goes from \\(-\\infty\\) to \\(\\infty\\) as in \\(-\\infty &lt; y &lt; \\infty\\)),\n\npositively unbounded (i.e., its set of possible values goes from \\(0\\) to \\(\\infty\\) as in \\(0 \\leq y &lt; \\infty\\)),\n\nnegatively unbounded (i.e., its set of possible values goes from \\(-\\infty\\) to \\(0\\) as in \\(-\\infty &lt; y \\leq 0\\)), or\n\nbounded between two values \\(a\\) and \\(b\\) (i.e., its set of possible values goes from \\(a\\) to \\(b\\) as in \\(a \\leq y \\leq b\\)).\n\n\n\nImage by arielrobin via Pixabay.\n\n\n\nTherefore, we can classify our two random variables as follows:\n\nFor the demand query, the support of \\(D_i\\) (denoted as \\(\\mathcal{D}\\)) is a countable finite set with two possible values: \\(d_i \\in \\{0, 1\\}\\), as noted by Equation 2.3. Therefore, \\(D_i\\) is categorized as a binary discrete random variable.\nFor the time query, the support of \\(T_j\\) (denoted as \\(\\mathcal{T}\\)) is positively unbounded. This results in an uncountably infinite set of values that \\(T_j\\) can take, including (but not limited to) \\(0, \\dots, 0.01, \\ldots, 0.02, \\ldots, 0.00234, \\ldots, 1, \\ldots, 1.5576, \\ldots\\) minutes. Therefore, \\(T_j\\) is classified as a positively unbounded continuous random variable.\n\nSo far, we have successfully translated our two statistical queries into proper random variables, along with clear definitions and classifications derived from our problem statements, as well as the populations of interest, as noted in Table 2.1. However, we still need to find a way to include our parameters. The upcoming section will allow us to do that.\n\n2.1.4 The Wonders of Generative Modelling and Probability Distributions\nBefore exploring the wonders of generative models, let us introduce Table 2.2, an extension of Table 2.1 that now includes the elements discussed in Section 2.1.3.\n\n\nTable 2.2: Table containing the corresponding insights to solve our demand and time queries.\n\n\n\n\n\n\n\n\n\nDemand Query\nTime Query\n\n\n\nStatement\nWe would like to know which ice cream flavour is the favourite one (either chocolate or vanilla) and by how much.\nWe would like to know the average waiting time from one customer to the next one in any given ice cream cart.\n\n\nPopulation of interest\n\nChildren between 4 and 11 years old attending different parks in Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during Summer weekends.\n\nAll our general customer-to-customer waiting times in the different parks of Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during Summer weekends across the 900 ice cream carts.\n\n\nParameter\n\nProportion of individuals from the population of interest who prefer the chocolate flavour versus the vanilla flavour.\n\nAverage waiting time from one customer to the next one.\n\n\nRandom variable\n\n\\(D_i\\) for \\(i = 1, \\dots, n_d\\).\n\n\\(T_j\\) for \\(j = 1, \\dots, n_t\\).\n\n\nRandom variable definition\nA favourite ice cream flavour of a randomly surveyed \\(i\\)th child between 4 and 11 years old attending the parks of Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during the Summer weekends.\nA randomly recorded \\(j\\)th waiting time in minutes between two customers during a Summer weekend across the 900 ice cream carts found in Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal.\n\n\nRandom variable type\nDiscrete and binary.\nContinuous and positively unbounded.\n\n\nRandom variable support\n\n\\(d_i \\in \\{ 0, 1\\}\\) as in Equation 2.3.\n\\(t_j \\in [0, \\infty).\\)\n\n\n\n\n\n\nHaving summarized all our probabilistic elements in Table 2.2, the parameters of interest must come into play for our data modelling game. Hence, the question is:\n\nIs there any feasible way to do so via the foundations of random variables?\n\nThe answer lies in what we call a generative model, for which we have a whole toolbox corresponding to another important concept called probability distributions, as shown below.\n\n\nDefinition of generative model\n\n\nSuppose you observe some data \\(y\\) from a population or system of interest. Moreover, let us assume this population or system is governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nIf we state that the random variable \\(Y\\) follows certain probability distribution \\(\\mathcal{D}(\\cdot),\\) then we will have a generative model \\(m\\) such that\n\\[\n\\text{$m$: } Y \\sim \\mathcal{D}(\\boldsymbol{\\theta}).\n\\]\n\n\nImage by Manfred Stege via Pixabay.\n\n\n\n\n\nDefinition of probability distribution\n\n\nWhen we set a random variable \\(Y\\), we also set a new set of \\(v\\) possible outcomes \\(\\mathcal{Y} = \\{ y_1, \\dots, y_v\\}\\) coming from the sample space \\(S\\). This new set of possible outcomes \\(\\mathcal{Y}\\) corresponds to the support of the random variable \\(Y\\) (i.e., all the possible values that could be taken on once we execute a given random experiment involving \\(Y\\)).\nThat said, let us suppose we have a sample space of \\(u\\) elements defined as\n\\[\nS = \\{ s_1, \\dots, s_u \\},\n\\]\nwhere each one of these elements has a probability assigned via a function \\(P_S(\\cdot)\\) such that\n\\[\nP(S) = \\sum_{i = 1}^u P_S(s_i) = 1.\n\\]\nwhich has to satisfy Equation 2.2.\nThen, the probability distribution of \\(Y\\), i.e., \\(P_Y(\\cdot)\\) assigns a probability to each observed value \\(Y = y_j\\) (with \\(j = 1, \\dots, v\\)) if and only if the outcome of the random experiment belongs to the sample space, i.e., \\(s_i \\in S\\) (for \\(i = 1, \\dots, u\\)) such that \\(Y(s_i) = y_j\\):\n\\[\nP_Y(Y = y_j) = P \\left( \\left\\{ s_i \\in S : Y(s_i) = y_j \\right\\} \\right).\n\\]\n\n\nSince we have two different queries, we will use two instances of generative models. It is worth noting that more complex modelling could refer to a single generative model. However, for the purposes of this review chapter, we will keep it simple with via two separate generative models.\nNow, let us introduce a specific notation for our discussion: the Greek alphabet. Greek letters are frequently used to statistically represent population parameters in modelling setups, estimation, and statistical inference. These letters will be quite useful for our parameters in this ice cream case.\n\n\nTip on the Greek alphabet in statistics!\n\n\nIn the early stages of learning statistical modelling, including concepts such as regression analysis, it is common to feel overwhelmed by unfamiliar letters and terminology. Whenever confusion arises in any of the main chapters of this book regarding these letters, we recommend referring to the Greek alphabet found in Appendix B. It is important to note that frequentist statistical inference primarily uses lowercase letters. With consistent practice over time, you will likely memorize most of this alphabet.\n\n\nImage by meineresterampe via Pixabay.\n\n\n\nLet us retake the row corresponding to parameters in Table 2.2 and assign their Greek letters:\n\nFor the demand query, we are interested in the parameter \\(\\pi\\), which represents the proportion of individuals from the children population who prefer the chocolate flavour over the vanilla flavour. It is crucial to note that a proportion is always bounded between \\(0\\) and \\(1\\), similar to how probabilities function. For instance, a proportion of \\(0.2\\) would mean that \\(20\\%\\) of the children in our population prefer chocolate flavour over vanilla. This definition establishes our demand query parameter as follows:\n\n\\[\n\\pi \\in [0, 1].\n\\]\n\n\nHeads-up on the use of \\(\\pi\\)!\n\n\nIn this textbook, unless stated otherwise, the letter \\(\\pi\\) will denote a population parameter and not the mathematical constant \\(3.141592...\\)\n\n\n\nFor the time query, we are interested in the parameter \\(\\beta\\), which represents the average waiting time in minutes from one customer to the next one in our population of interest. Unlike the above \\(\\pi\\) parameter, \\(\\beta\\) is only positively unbounded given the definition of our random variable \\(T_j\\). Therefore, this definition establishes our time query parameter as follows:\n\n\\[\n\\beta \\in (0, \\infty).\n\\]\nHaving defined our parameters of interest with proper lowercase Greek letters, it is time to declare our corresponding generative models on a general basis. For the demand query, there will be a single parameter called \\(\\pi\\), where the randomly surveyed child \\(D_i\\) will follow the model \\(m_D\\) such that\n\\[\n\\begin{gather*}\nm_D : D_i \\sim \\mathcal{D}_D(\\pi) \\\\\n\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\text{for $i = 1, \\dots, n_d.$}\n\\end{gather*}\n\\tag{2.4}\\]\nNow, for the time query, there will also be a single parameter called \\(\\beta\\). Thus, the randomly recorded waiting time \\(T_j\\) will follow the model \\(m_T\\) such that\n\\[\n\\begin{gather*}\nm_T : T_j \\sim \\mathcal{D}_T(\\beta) \\\\\n\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\text{for $j = 1, \\dots, n_t.$}\n\\end{gather*}\n\\tag{2.5}\\]\nNonetheless, we might wonder the following:\n\nHow can we determine the corresponding distributions \\(\\mathcal{D}_D(\\pi)\\) and \\(\\mathcal{D}_T(\\beta)\\)?\n\nOf course the above definition of a probability distribution will come in handy to resolve this question. That said, given that we have two types of random variables (discrete and continuous), it is necessary to introduce two specific types of probability functions: probability mass function (PMF) and probability density function (PDF).\n\n\nDefinition of probability mass function (PMF)\n\n\nLet \\(Y\\) be a discrete random variable whose support is \\(\\mathcal{Y}\\). Moreover, suppose that \\(Y\\) has a probability distribution such that\n\\[\nP_Y(Y = y) : \\mathbb{R} \\rightarrow [0, 1]\n\\]\nwhere, for all \\(y \\notin \\mathcal{Y}\\), we have\n\\[\nP_Y(Y = y) = 0\n\\]\nand\n\\[\n\\sum_{y \\in \\mathcal{Y}} P_Y(Y = y) = 1.\n\\tag{2.6}\\]\nThen, \\(P_Y(Y = y)\\) is considered a PMF.\n\n\nAs we have discussed throughout this ice cream case, let us begin with the demand query. We have already defined the \\(i\\)th random variable \\(D_i\\) as discrete and binary. In statistical literature, certain random variables in common random processes can be modelled using what we call parametric families. We refer to these tools as parametric families because they are characterized by a specific set of parameters (in our case, each query has a single-element set, such as \\(\\pi\\) or \\(\\beta\\)).\nMoreover, we call them families since each member corresponds to a particular value of our parameter(s). For instance, in our demand query, a chosen member could be where \\(\\pi = 0.8\\) within the respective chosen parametric family to model our surveyed children. Other possible members could correspond to \\(\\pi = 0.2\\), \\(\\pi = 0.4\\) or \\(\\pi = 0.6\\). In fact, the number of members in our chosen parametric family is infinite in this demand query!\n\nTherefore, what parametric family can we choose for our demand query?\n\nThe question above introduces a new, valuable resource that is further elaborated upon in Appendix C. This resource outlines the various distributions that will be utilized in this textbook. In reality, the realm of parametric families—specifically, distributions—is quite extensive, and appendix material serves as only a brief overview of the many parametric families documented in statistical literature.\n\n\nTip on data modelling alternatives via different parametric families!\n\n\nAny data model is simply an abstraction of reality, and different parametric families can provide various alternatives for modelling. In practice, we often need to select a specific family based on our particular inquiries and the conditions of our data. This process requires time and experience to master. Furthermore, it is important to note that different families are often interconnected!\n\n\nImage by Manfred Stege via Pixabay.\n\nIf you wish to explore the world of univariate distribution families—which are used to model a single random variable—Leemis (n.d.) has created a comprehensive relational chart that covers 76 distinct probability distributions: 19 are discrete, and 57 are continuous. However, this chart does not encompass all the possible families that one might encounter in statistical literature (you can check another list at the end of this section).\n\n\nReferring back to our discussion about Appendix C, it is time to choose the most suitable parametric family for a discrete and a binary random variable, such as the \\(i\\)th random variable \\(D_i\\). A particular case we can examine is the Bernoulli distribution (also, commonly known as a Bernoulli trial). The Bernoulli distribution applies to a discrete random variable that can take one of two values: \\(0\\), which we refer to as a failure, and \\(1\\), identified as a success. This aligns with our previous definition from Equation 2.3:\n\\[\nd_i =\n\\begin{cases}\n1 \\qquad \\text{The surveyed child prefers chocolate.}\\\\\n0 \\qquad \\text{Otherwise.}\n\\end{cases}\n\\]\nThe equation above defines the chocolate preference of the \\(i\\)th surveyed child as a success, while another flavour—specifically vanilla in the context of our limited menu—is categorized as a failure. Thus, we can denote the support as \\(d_i \\in \\{0, 1\\}\\).\nWe need to define our population parameter for this demand query in the context of a Bernoulli trial, which is denoted by \\(\\pi \\in [0, 1]\\). This represents the proportion of children who prefer the chocolate flavour over the vanilla flavour. In a Bernoulli trial, this parameter refers to the probability of success. Lastly, we can specify our generative model accordingly:\n\\[\n\\begin{gather*}\nm_D : D_i \\sim \\text{Bern}(\\pi) \\\\\n\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\text{for $i = 1, \\dots, n_d.$}\n\\end{gather*}\n\\]\nWe must define the PMF corresponding to the above generative model. The statistical literature assigns the following PMF for the a Bernoulli trial \\(D_i\\):\n\\[\nP_{D_i} \\left( D_i = d_i \\mid \\pi \\right) = \\pi^{d_i} (1 - \\pi)^{1 - d_i} \\quad \\text{for $d_i \\in \\{ 0, 1 \\}$.}\n\\tag{2.7}\\]\nA further question arises regarding whether Equation 2.7 satisfies the condition of the total probability of the sample space defined in the Equation 2.6 under the definition of a PMF. This condition states that a valid PMF should result in a total probability equal to one when we sum all the probabilities produced by this function over every possible value that the random variable can take.\nHence, we can state Equation 2.7 is a proper probability distribution (i.e., all the standalone probabilities over the support of \\(D_i\\) add up to one) given that:\n\nProof. \\[\n\\begin{align*}\n\\sum_{d_i = 0}^1 P_{D_i} \\left( D_i = d_i \\mid \\pi \\right) &=  \\sum_{d_i = 0}^1 \\pi^{d_i} (1 - \\pi)^{1 - d_i}  \\\\\n&= \\underbrace{\\pi^0}_{1} (1 - \\pi) + \\pi \\underbrace{(1 - \\pi)^{0}}_{1} \\\\\n&= (1 - \\pi) + \\pi \\\\\n&= 1. \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\tag{2.8}\\]\n\nIndeed, this Bernoulli PMF is a proper probability distribution!\n\n\nThe probability distribution, obtained from Equation 2.8, is summarized in Table 2.3. Note that the chocolate preference has a probability equal to \\(\\pi\\), whereas the vanilla preference corresponds to the complement \\(1 - \\pi\\). This probability arrangement completely fulfils the corresponding probability condition of the sample space seen in Equation 2.6.\n\n\nTable 2.3: Probability distribution for the \\(i\\)th Bernoulli trial \\(D_i\\).\n\n\n\n\\(d_i\\)\n\\(P_{D_i} \\left( D_i = d_i \\mid \\pi \\right)\\)\n\n\n\n\\(0\\)\n\\(1 - \\pi\\)\n\n\n\\(1\\)\n\\(\\pi\\)\n\n\n\n\n\n\nTo proceed with the time query, we need to analyze the \\(j\\)th continuous random variable \\(T_j\\) and subsequently work with a PDF.\n\n\nDefinition of probability density function (PDF)\n\n\nLet \\(Y\\) be a continuous random variable whose support is \\(\\mathcal{Y}\\). Furthermore, consider a function \\(f_Y(y)\\) such that\n\\[\nf_Y(y) : \\mathbb{R} \\rightarrow \\mathbb{R}\n\\]\nwith\n\\[\nf_Y(y) \\geq 0.\n\\tag{2.9}\\]\nThen, \\(f_Y(y)\\) is considered a PDF if the probability of \\(Y\\) taking on a value within the range represented by the subset \\(A \\subset \\mathcal{Y}\\) is equal to\n\\[\nP_Y(Y \\in A) = \\int_A f_Y(y) \\mathrm{d}y\n\\]\nwith\n\\[\n\\int_{\\mathcal{Y}} f_Y(y) \\mathrm{d}y = 1.\n\\tag{2.10}\\]\n\n\nTo begin our second analysis, let us examine the nature of the variable \\(T_j\\) represented as a continuous random variable. This variable is nonnegative, meaning it is positively unbounded, as it models a waiting time. We can interpret \\(T_j\\) as the waiting time until a specific event of interest occurs, such as when the next customer arrives at the ice cream cart. In statistical literature, this is commonly referred to as a survival time. Hence, we might wonder:\n\nWhat is the most suitable parametric family to model a survival time?\n\n\n\nImage by Manfred Stege via Pixabay.\n\nWell, in this case within our textbook and in general in statistical literature, there is more than one alternative to model a continuous and nonnegative survival time. Appendix C offers four possible ways:\n\n\nExponential. A random variable with a single parameter that can come in either of the following forms:\n\nAs a rate \\(\\lambda \\in (0, \\infty)\\), which generally defines the mean number of events of interest per time interval or space unit.\nAs a scale \\(\\beta \\in (0, \\infty)\\), which generally defines the mean time until the next event of interest occurs.\n\n\n\nWeibull. A random variable that is a generalization of the Exponential distribution. Note its distributional parameters are the scale continuous parameter \\(\\beta \\in (0, \\infty)\\) and shape continuous parameter \\(\\gamma \\in (0, \\infty)\\).\n\nGamma A random variable whose distributional parameters are the shape continuous parameter \\(\\eta \\in (0, \\infty)\\) and scale continuous parameter \\(\\theta \\in (0, \\infty)\\).\n\nLognormal. A random variable whose logarithmic transformation yields a Normal distribution. Its distributional parameters are the Normal location continuous parameter \\(\\mu \\in (-\\infty, \\infty)\\) and Normal scale continuous parameter \\(\\sigma^2 \\in (0, \\infty)\\).\n\nIn our context, as summarized in the corresponding generative model, it is in our best interest to select a probability distribution characterized by a single parameter. Therefore, the Exponential distribution is the most suitable choice for our current time query, particularly under the scale parametrization, since we aim to estimate the waiting time between two customers.\n\n\nHeads-up on survival analysis!\n\n\nAlthough our ice cream case can be straightforwardly modelled using an Exponential distribution for our time query, by using a single population parameter which indicates a mean waiting time between two customers, it is important to stress that other distributions, such as the Weibull, Gamma, or Lognormal, are also entirely valid options. In fact, utilizing these distributions, that involve more than just a standalone parameter, can enhance the flexibility of our data modelling!\n\n\nImage by toushirou_px via Pixabay.\n\nAdditionally, there is a specialized statistical field focused on modelling waiting times—specifically, the time until an event of interest occurs. These types of times are formally referred to as survival times, and the associated field is known as survival analysis. It is worth noting that regression analysis can be extended to this area, and Chapter 6 will provide a more in-depth exploration of various parametric models that involve the Exponential, Weibull, and Lognormal distributions.\n\n\nSince we are using an Exponential distribution, we need to establish our population parameter for this time query. As mentioned in Table 2.2, this parameter refers to the average (or mean) waiting time from one customer to the next. This corresponds to a scale parametrization, where the parameter \\(\\beta \\in (0, \\infty)\\) defines the mean time until the next event of interest occurs (in this case, the next customer). Therefore, we can specify our generative model as follows:\n\\[\n\\begin{gather*}\nm_T : T_j \\sim \\text{Exponential}(\\beta) \\\\\n\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\text{for $j = 1, \\dots, n_t.$}\n\\end{gather*}\n\\]\nSince \\(T_j\\) is a continuous random variable, we must define the PMF corresponding to the above generative model. The statistical literature assigns the following PDF for \\(T_j\\):\n\\[\nf_{T_j} \\left(t_j \\mid \\beta \\right) = \\frac{1}{\\beta} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\quad \\text{for $t_j \\in [0, \\infty )$.}\n\\tag{2.11}\\]\nNow, we might wonder whether Equation 2.11 satisfies the condition of the total probability of the sample space defined in the Equation 2.10 under the definition of a PDF. This condition states that a valid PDF should result in a total probability equal to one when we integrate this function over all the support of \\(T_j\\).\nThus, we can state that Equation 2.11 is a proper probability distribution (i.e., Equation 2.11 integrates to one over the support of \\(T_j\\)) given that:\n\nProof. \\[\n\\begin{align*}\n\\int_{t_j = 0}^{t_j = \\infty} f_{T_j} \\left(t_j \\mid \\beta \\right) \\mathrm{d}y &= \\int_{t_j = 0}^{t_j = \\infty} \\frac{1}{\\beta} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j \\\\\n&= \\frac{1}{\\beta} \\int_{t_j = 0}^{t_j = \\infty} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j \\\\\n&= - \\frac{\\beta}{\\beta} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\Bigg|_{t_j = 0}^{t_j = \\infty} \\\\\n&= - \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\Bigg|_{t_j = 0}^{t_j = \\infty} \\\\\n&= - \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\\\\n&= - \\left( 0 - 1 \\right) \\\\\n&= 1. \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\tag{2.12}\\]\n\nIndeed, the Exponential PDF, under a scale parametrization, is a proper probability distribution!\n\n\nUnlike our demand query, which features a table illustrating the PMF for \\(D_i \\in \\{ 0, 1 \\}\\) (see Table 2.3), it is not feasible to create a table for the PDF of \\(T_j \\in [0, \\infty)\\) because it represents an uncountably infinite set of possible values. However, we can plot the corresponding PDF using three specific members of the Exponential parametric family as examples. Figure 2.2 presents these three example members, with scale parameters values of \\(\\beta = 0.25, 0.5, 1\\) minutes, representing waiting times through their corresponding PDFs. Based on our findings in Equation 2.12, we know that the area under these three density plots equals one, indicating the total probability of the sample space. Additionally, it is important to note that as we increase the scale parameter, larger observed values \\(t_j\\) become more probable.\n\n\n\n\n\n\n\nFigure 2.2: Some members of the Exponential family with scale parametrization.\n\n\n\n\n\n2.1.5 Characterizing Probability Distributions\nBefore moving on into our distributional journey, let us update Table 2.2 with the specific probability distributions, and mathematical definitions of the parameters to estimate per query.\n\n\nTable 2.4: Table containing the corresponding insights to solve our demand and time queries.\n\n\n\n\n\n\n\n\n\nDemand Query\nTime Query\n\n\n\nStatement\nWe would like to know which ice cream flavour is the favourite one (either chocolate or vanilla) and by how much.\nWe would like to know the average waiting time from one customer to the next one in any given ice cream cart.\n\n\nPopulation of interest\n\nChildren between 4 and 11 years old attending different parks in Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during Summer weekends.\n\nAll our general customer-to-customer waiting times in the different parks of Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during Summer weekends across the 900 ice cream carts.\n\n\nParameter\n\nProportion of individuals from the population of interest who prefer the chocolate flavour versus the vanilla flavour.\n\nAverage waiting time in minutes from one customer to the next one.\n\n\nRandom variable\n\n\\(D_i\\) for \\(i = 1, \\dots, n_d\\).\n\n\\(T_j\\) for \\(j = 1, \\dots, n_t\\).\n\n\nRandom variable definition\nA favourite ice cream flavour of a randomly surveyed \\(i\\)th child between 4 and 11 years old attending the parks of Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during the Summer weekends.\nA randomly recorded \\(j\\)th waiting time in minutes between two customers during a Summer weekend across the 900 ice cream carts found in Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal.\n\n\nRandom variable type\nDiscrete and binary.\nContinuous and positively unbounded.\n\n\nRandom variable support\n\n\\(d_i \\in \\{ 0, 1\\}\\) as in Equation 2.3.\n\\(t_j \\in [0, \\infty).\\)\n\n\nProbability distribution\n\\(D_i \\sim \\text{Bern}(\\pi)\\)\n\\(T_j \\sim \\text{Exponential}(\\beta)\\)\n\n\nMathematical definition of the parameter\n\\[\\pi\\]\n\\[\\beta\\]\n\n\n\n\n\n\nLet us proceed, then. We have been exploring the basics of random variables, as well as the importance of generative modelling and probability distributions in addressing different data inquiries. These concepts are fundamental to understanding the population parameter setup before we actually collect data and solve these inquiries to create effective storytelling. Therefore, before we delve into those stages, however, we need to identify and explain efficient ways to summarize probability distributions. This will help us make our storytelling compelling for a general audience, as we will discuss further.\nTo continue with this example, we need to use R and Python code. Thus, we will work with some simulated populations to create the corresponding proofs of concept in this section and the subsequent ones. Let us start with our demand query. We will consider a population size of \\(N_d = 2,000,000\\) children (whose characteristics are defined in Table 2.4). The code (in either R or Python) below assigns this value as N_d, along with a simulation seed to ensure our results are reproducible. Additionally, for the simulation purposes related to our generative modelling, we will assume that \\(65\\%\\) of these children in this population prefer chocolate over vanilla (i.e., \\(\\pi = 0.65\\)).\n\n\nHeads-up on real and unknown parameters!\n\n\nAlthough we are assigning a value of \\(\\pi = 0.65\\) as our true population parameter in this query, we can never know the exact value in practice unless we conduct a full census. This is why we rely on probabilistic tools, via random sampling and statistical inference, to estimate this \\(\\pi\\) in frequentist statistics.\n\n\nLet us recall that we are assuming each child as a Bernoulli trial, where a success (denoted as 1) indicates that the child “prefers chocolate.” This also reflects the flavour mapping in the code. Furthermore, instead of using a Bernoulli random number generator, we are utilizing a Binomial random number generator. This is because the Binomial case with parameters \\(n = 1\\) and \\(\\pi\\) is equivalent to a Bernoulli trial with parameter \\(\\pi\\). Hence, consider the following Binomial case:\n\\[\nY \\sim \\text{Bin}(n = 1, \\pi),\n\\]\nwhose PMF is simplified as a Bernoulli given that\n\\[\n\\begin{align*}\nP_Y \\left( Y = y \\mid n = 1, \\pi \\right) &= {1 \\choose y} \\pi^y (1 - \\pi)^{1 - y} \\\\\n&= \\underbrace{\\frac{1!}{y!(1 - y)!}}_{\\text{$1$ for $y \\in \\{ 0, 1 \\}$}} \\pi^y (1 - \\pi)^{1 - y} \\\\\n&= \\pi^y (1 - \\pi)^{1 - y} \\\\\n& \\qquad \\qquad \\qquad \\qquad \\qquad \\text{for $y \\in \\{ 0, 1 \\}$.}\n\\end{align*}\n\\tag{2.13}\\]\nThe final output of this quick simulation, which models a population of \\(N_d\\) children as Bernoulli trials with a probability of success \\(\\pi = 0.65\\), consists of a data frame containing \\(N_d = 2,000,000\\) rows, with each row representing a child and their preferred ice cream flavour: either chocolate or vanilla. It is worth noting that the outputs from both R and Python differ due to the fact that each language employs different pseudo-random number generators (even though we use the same seed). Note that Python additionally uses the {numpy} and {pandas} libraries.\n\n\nR Code\nPython Code\n\n\n\nset.seed(123)  # Seed for reproducibility\n\n# Population size\nN_d &lt;- 2000000\n\n# Simulate binary outcomes: 1 = chocolate, 0 = vanilla\nflavour_bin &lt;- rbinom(N_d, size = 1, prob = 0.65)\n\n# Map binary to flavour names\nflavours &lt;- ifelse(flavour_bin == 1, \"chocolate\", \"vanilla\")\n\n# Create data frame\nchildren_pop &lt;- data.frame(\n  children_ID = 1:N_d,\n  fav_flavour = flavours\n)\n\n# Showing the first 100 children of the population\nhead(children_pop, n = 100) \n\n\n# Importing libraries\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(123)   # Seed for reproducibility\n\n# Population size\nN_d = 2000000\n\n# Simulate binary outcomes: 1 = chocolate, 0 = vanilla\nflavour_bin = np.random.binomial(n = 1, p = 0.65, size = N_d)\n\n# Map binary to flavour names\nflavours = np.where(flavour_bin == 1, \"chocolate\", \"vanilla\")\n\n# Create data frame\nchildren_pop = pd.DataFrame({\n    \"children_ID\": np.arange(1, N_d + 1),\n    \"fav_flavour\": flavours\n})\n\n# Showing the first 100 children of the population\nprint(children_pop.head(100))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn our time query, we will simulate another population consisting of \\(N_t = 500,000\\) general customer-to-customer waiting times (as defined in Table 2.4). The code below assigns this population size to the variable N_t. We have already established that this class of data will be modelled using an Exponential distribution under a scale parameterization, where the parameter \\(\\beta\\) defines the mean waiting time between customers. For this query, we will assume that the population of waiting times has a true parameter value of \\(\\beta = 10\\) minutes. Therefore, the code below illustrates this generative modelling mechanism, which produces a data frame containing \\(N_t = 500,000\\) rows, with each row representing a specific waiting time in minutes.\n\n\nR Code\nPython Code\n\n\n\nset.seed(123)  # Seed for reproducibility\n\n# Population size\nN_t &lt;- 500000\n\n# In R, 'rate' is 1 / scale and rounding to two decimal places\nwaiting_times &lt;- round(rexp(N_t, rate = 1 / 10), 2)\n\n# Create data frame\nwaiting_pop &lt;- data.frame(\n  time_ID = 1:N_t,\n  waiting_time = waiting_times\n)\n\n# Showing the first 100 waiting times of the population\nhead(waiting_pop, n = 100)\n\n\nnp.random.seed(123)  # Seed for reproducibility\n\n# Population size\nN_t = 500000\n\n# Simulate waiting times\nwaiting_times = np.round(np.random.exponential(scale = 10, size = N_t), 2)\n\n# Create DataFrame\nwaiting_pop = pd.DataFrame({\n    \"time_ID\": np.arange(1, N_t + 1),\n    \"waiting_time\": waiting_times\n})\n\n# Showing the first 100 waiting times of the population\nprint(waiting_pop.head((100))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImagine that the data collection and analysis for the ice cream case have progressed into the future. You have a follow-up meeting with the eight general managers, one from each Canadian city, to discuss the statements related to both demand and time queries, in relation to our populations of interest. Additionally, you have collected data from a sample of \\(n_d = 500\\) randomly surveyed children across these eight Canadian cities. Note that the below R and Python sampling functions perform simple random sampling with replacement.\n\n\nR Code\nPython Code\n\n\n\nset.seed(678)  # Seed for reproducibility\n\n# Simple random sample of 500 children with replacement\nn_d &lt;- 500\nchildren_sample &lt;- children_pop[sample(1:nrow(children_pop), n_d, replace = TRUE), ]\n\n# Showing the first 100 sampled children\nhead(children_sample, n = 100)\n\n\nnp.random.seed(678)  # Seed for reproducibility\n\n# Simple random sample of 500 children with replacement\nn_d = 500\nchildren_sample = children_pop.sample(n = n_d, replace = True)\n\n# Showing the first 100 sampled children\nprint(children_sample.head(100))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlso, you have sampled data on \\(n_t = 200\\) randomly recorded waiting times between customers across our 900 ice cream carts in the same cities.\n\n\nR Code\nPython Code\n\n\n\nset.seed(345)  # Seed for reproducibility\n\n# Simple random sample of 200 waiting times with replacement\nn_t &lt;- 200\nwaiting_sample &lt;- waiting_pop[sample(1:nrow(waiting_pop), n_t, replace = TRUE), ]\n\n# Showing the first 100 sampled waiting times\nhead(waiting_sample, n = 100)\n\n\nnp.random.seed(345)  # Seed for reproducibility\n\n# Simple random sample of 200 waiting times with replacement\nn_t = 200\nwaiting_sample = waiting_pop.sample(n = n_t, replace = True)\n\n# Showing the first 100 sampled waiting times\nprint(waiting_sample.head(100))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.6685\n\n\n\n\n\n\n\n\n\n\n\nIn terms of the executive meeting with the eight general managers, it would not be an efficient use of time to go individually over these \\(n_d = 500\\) and \\(n_t = 200\\) data points along with abstract mathematical concepts such as PMFs or PDFs, as well as probabilistic definitions of random variables and parameters represented by Greek letters. Instead, there should be a more straightforward and simple way to explain how these \\(n_d\\) and \\(n_t\\) observed random variables behaved during our data collection process. The key to addressing this complexity lies in understanding measures of central tendency and uncertainty.\n\n\nHeads-up on population and sample-based measures of central tendency and uncertainty!\n\n\nWhen learning about measures of central tendency and uncertainty, it is best to begin with those directly related to our population(s) of interest. These concepts provide valuable insights into how any given population behaves concerning typical values and spread. Subsequently, through sampled data, we can derive estimates for these population-based measures.\nIn this section, we will focus on the population measures, while Section 2.2 and Section 2.3 will examine the sample-based measures, which are simply the corresponding estimates. Of course, the latter measures will eventually be used in the upcoming executive meeting within the ice cream case. But, for now, let us immerse ourselves a bit into the fundamentals behind the population side of things.\n\n\n\n\nImage by Manfred Stege via Pixabay.\n\nOur first measure to explore is related to typical values found within any given population, specifically a measure of central tendency. We can think of this measure as representing “the most common value” we can expect when observing a certain number of random variables that are drawn from this particular population. Let us start with its formal definition.\n\n\nDefinition of measure of central tendency\n\n\nProbabilistically, a measure of central tendency is defined as a metric that identifies a central or typical value of a given probability distribution. In other words, a measure of central tendency refers to a central or typical value that a given random variable might take when we observe various realizations of this variable over a long period.\n\n\nImage by Manfred Stege via Pixabay.\n\n\n\nThere is more than one measure of central tendency in the statistical literature. However, for the regression models discussed in this book, we will focus on the expected value (see the definition below), which is commonly referred to as the average or mean. You will notice that this measure is closely related to the mainstream average we use in our everyday life (to be discussed later on in this section).\n\n\nDefinition of expected value\n\n\nLet \\(Y\\) be a random variable whose support is \\(\\mathcal{Y}\\). In general, the expected value or mean \\(\\mathbb{E}(Y)\\) of this random variable is defined as a weighted average according to its corresponding probability distribution. In other words, this measure of central tendency \\(\\mathbb{E}(Y)\\) aims to find the middle value of this random variable by weighting all its possible values in its support \\(\\mathcal{Y}\\) as dictated by its probability distribution.\nGiven the above definition, when \\(Y\\) is a discrete random variable whose PMF is \\(P_Y(Y = y)\\), then its expected value is mathematically defined as\n\\[\n\\mathbb{E}(Y) = \\sum_{y \\in \\mathcal{Y}} y \\cdot P_Y(Y = y).\n\\tag{2.14}\\]\nWhen \\(Y\\) is a continuous random variable whose PDF is \\(f_Y(y)\\), its expected value is mathematically defined as\n\\[\n\\mathbb{E}(Y) = \\int_{\\mathcal{Y}} y \\cdot f_Y(y) \\mathrm{d}y.\n\\tag{2.15}\\]\n\n\nImage by Manfred Stege via Pixabay.\n\n\n\n\n\nTip on further measures of central tendency!\n\n\nIn addition to the expected value, there are other measures that will not be explored in this book such as:\n\n\nMode: For a discrete random variable, the mode is the outcome that corresponds to the highest probability in the PMF. In the case of a continuous random variable, the mode refers to the outcome at which the maximum value occurs in the corresponding PDF.\n\nMedian: This measure primarily relates to continuous random variables. The median is the outcome for which there is a probability of \\(0.5\\) for observing a value either greater or lesser than it.\n\n\n\nNote that the discrete random variable case in Equation 2.14 somehow resembles the mainstream average, which actually would assign an equal weight to each possible outcome of the random variable. On the other hand, for the above statistical definition, we use the corresponding PMF to assign these weights by possible outcome of the discrete random variable.\nLet us exemplify this by using our demand query. Recall that the \\(i\\)th discrete random variable (as in Table 2.4) \\(D_i\\) is distributed as follows:\n\\[\nD_i \\sim \\text{Bern}(\\pi),\n\\]\nwhose PMF is defined as\n\\[\nP_{D_i} \\left( D_i = d_i \\mid \\pi \\right) = \\pi^{d_i} (1 - \\pi)^{1 - d_i} \\quad \\text{for $d_i \\in \\{ 0, 1 \\}$.}\n\\]\nNow, by applying Equation 2.14, note we have the following result (which is in fact the formal proof of the expected value of Bernoulli trial):\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(D_i) &= \\sum_{d_i = 0}^1 d_i P_{D_i} \\left( D_i = d_i \\mid \\pi \\right) \\\\\n&= \\sum_{d_i = 0}^1 d_i \\left[ \\pi^{d_i} (1 - \\pi)^{1 - d_i} \\right] \\\\\n&= \\underbrace{(0) \\left[ \\pi^0 (1 - \\pi) \\right]}_{0} + (1) \\left[ \\pi (1 - \\pi)^{0} \\right] \\\\\n&= 0 + \\pi \\\\\n&= \\pi. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\tag{2.16}\\]\n\nFor the specific case of a Bernoulli-type population, we have just found that its expected value (from Equation 2.16) is equal to the corresponding parameter we aim to estimate, which is \\(\\pi\\). Then, you might wonder:\n\nIn practical terms for our ice cream company, how can I explain \\(\\mathbb{E}(D_i) = \\pi\\)?\n\nSuppose you sample a sufficiently large number of children (i.e., your sample size \\(n_d \\rightarrow \\infty\\)) from a Bernoulli-type population where \\(65\\%\\) of these children prefer chocolate over vanilla (i.e., \\(\\pi = 0.65\\)). Then, you will obtain the proportion of observed random variables \\(d_i\\) (for \\(i = 1, \\dots, n_d\\)) that correspond to \\(1\\) (as in Equation 2.3) which is merely a mainstream average. Theoretically speaking, according to our proof in Equation 2.16, this observed proportion should converge to the true population parameter \\(\\pi = 0.65\\).\nLet us now explore the expected value for a continuous random variable using our time query. Unlike the previous case depicted in Equation 2.14, the continuous counterpart in Equation 2.15 cannot utilize a summation. Instead, since we have an uncountably infinite set of possible values for a continuous random variable, we must use an integral. This integral involves the corresponding PDF, which weights all possible observed outcomes of the continuous random variable in conjunction with the differential.\nMoving along in this query, recall the \\(j\\)th continuous random variable (as in Table 2.4) \\(T_j\\) is distributed as follows:\n\\[\nT_j \\sim \\text{Exponential}(\\beta),\n\\]\nwhose PDF is defined as\n\\[\nf_{T_j} \\left(t_j \\mid \\beta \\right) = \\frac{1}{\\beta} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\quad \\text{for $t_j \\in [0, \\infty )$.}\n\\]\nNow, by applying Equation 2.15, note we have the following result (which is the formal proof of the expected value of an Exponential-distributed random variable under a scale parametrization):\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(T_j) &= \\int_{t_j = 0}^{t_j = \\infty} t_j f_{T_j} \\left(t_j \\mid \\beta \\right) \\mathrm{d}t_j \\\\\n&= \\int_{t_j = 0}^{t_j = \\infty} \\frac{t_j}{\\beta} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j \\\\\n&= \\frac{1}{\\beta} \\int_{t_j = 0}^{t_j = \\infty} t_j \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j. \\\\\n\\end{align*}\n\\tag{2.17}\\]\nEquation 2.17 cannot be solved straightforwardly, we need to use integration by parts as follows:\n\\[\n\\begin{align*}\nu &= t_j & &\\Rightarrow & \\mathrm{d}u &= \\mathrm{d}t_j \\\\\n\\mathrm{d}v &= \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j & &\\Rightarrow & v &= -\\beta \\exp \\left( -\\frac{t_j}{\\beta} \\right),\n\\end{align*}\n\\]\nwhich yields\n\\[\n\\begin{align*}\n\\mathbb{E}(T_j) &= \\frac{1}{\\beta} \\left[ u v \\Bigg|_{t_j = 0}^{t_j = \\infty} - \\int_{t_j = 0}^{t_j = \\infty} v \\mathrm{d}u \\right] \\\\\n&= \\frac{1}{\\beta} \\Bigg\\{ \\left[ -\\beta t_j \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\right] \\Bigg|_{t_j = 0}^{t_j = \\infty} + \\\\\n& \\qquad \\beta \\int_{t_j = 0}^{t_j = \\infty} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j \\Bigg\\} \\\\\n&= \\frac{1}{\\beta} \\Bigg\\{ -\\beta \\Bigg[ \\underbrace{\\infty \\times \\exp(-\\infty)}_{0} - \\underbrace{0 \\times \\exp(0)}_{0} \\Bigg] - \\\\\n& \\qquad \\beta^2 \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\Bigg|_{t_j = 0}^{t_j = \\infty} \\Bigg\\} \\\\\n&= \\frac{1}{\\beta} \\left\\{ -\\beta (0) - \\beta^2 \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\right\\} \\\\\n&= \\frac{1}{\\beta} \\left[ 0 - \\beta^2 (0 - 1) \\right] \\\\\n&= \\frac{\\beta^2}{\\beta} \\\\\n&= \\beta. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\qquad \\qquad \\square\n\\end{align*}\n\\tag{2.18}\\]\n\nAgain, for the specific case of an Exponential-type population, we have just found that the its expected value (from Equation 2.18) is equal to the corresponding parameter we aim to estimate, which is \\(\\beta\\). This yields the following question:\n\nIn practical terms for our ice cream company, how can I explain \\(\\mathbb{E}(T_j) = \\beta\\)?\n\nImagine you sample a sufficiently large number of customer-to-customer waiting times (i.e., your sample size \\(n_t \\rightarrow \\infty\\)) from an Exponential-type population where the true average waiting time is \\(\\beta = 10\\) minutes. Then, you will obtain the mainstream average coming from these observed random variables \\(t_j\\) (for \\(j = 1, \\dots, n_t\\)). Theoretically speaking, according to our proof in Equation 2.18, this observed mainstream average should converge to the true population parameter \\(\\beta = 10\\).\n\n\nHeads-up on the so-called mainstream average!\n\n\nIn general, suppose that you obtain \\(n\\) realizations \\(y_k\\) (for \\(k = 1, \\dots, n\\)) of a given random variable \\(Y\\). The observed mainstream average is given by\n\\[\n\\bar{y} = \\frac{\\sum_{k = 1}^n y_k}{n}.\n\\tag{2.19}\\]\nFor the respective classes of observed random variables in the demand and time queries, Equation 2.19 can be applied to obtain estimates of the corresponding population parameters \\(\\pi\\) and \\(\\beta\\). The statistical rationale for using Equation 2.19 will be expanded in Section 2.2.\n\n\nWe have discussed measures of central tendency in detail, more specifically the mean, which represents typical values derived from observing a standalone random variable over a sufficiently large number of trials, denoted as \\(n\\). Interestingly, there is a useful probabilistic theorem that allows us to calculate expected values for general mathematical functions involving a standalone random variable. This theorem is crucial for obtaining another type of measure related to the spread of a random variable.\nAs we previously highlighted, you might choose to skip the tip admonition below; however, recall the theorem’s mathematical expressions for discrete random variables found in Equation 2.20 and for continuous random variables found in Equation 2.26, as they will be important for the upcoming discussions about our ice cream case.\n\n\nTip on the Law of the Unconscious Statistician!\n\n\nThe law of the unconscious statistician (LOTUS) is a particular theorem in probability theory that allows us to compute a wide variety of expected values. Let us properly define it for both discrete and continuous random variables.\n\nTheorem 2.1 Let \\(Y\\) be a discrete random variable whose support is \\(\\mathcal{Y}\\). The LOTUS indicates that the expected value of a general function \\(g(Y)\\) of this random variable \\(Y\\) can be obtained via \\(g(Y)\\) along with the corresponding PMF \\(P_Y(Y = y)\\). Hence, the expected value of \\(g(Y)\\) can be obtained as\n\\[\n\\mathbb{E}\\left[ g(Y) \\right] = \\sum_{y \\in \\mathcal{Y}} g(y) \\cdot P_Y(Y = y).\n\\tag{2.20}\\]\n\nProof. Let us explore the rationale provided by Soch et al. (2024). Thus, we will rename the general function \\(g(Y)\\) as another random variable called \\(Z\\) such that:\n\\[\nZ = g(Y).\n\\tag{2.21}\\]\nNote this function \\(g(Y)\\) can take on equal values \\(g(y_1), g(y_2), \\dots\\) coming from different observed values \\(y_1, y_2, \\dots\\); for example, if\n\\[\ng(y) = y^2\n\\]\nboth\n\\[\ny_1 = 2 \\quad \\text{and} \\quad y_2 = -2\n\\]\nyield\n\\[\ng(y_1) = g(y_2) = 4.\n\\]\nThe above Equation 2.21 is formally called a random variable transformation from the general function of random variable \\(Y\\), \\(g(Y)\\), to a new random variable \\(Z\\). Having said that, when we set up a transformation of this class, there will be a support mapping from this general function \\(g(Y)\\) to \\(Z\\). This will also yield a proper PMF,\n\\[\nP_Z(Z = z) : \\mathbb{R} \\rightarrow [0, 1] \\quad \\forall z \\in \\mathcal{Z},\n\\]\ngiven that \\(g(Y)\\) is a random variable-based function.\nTherefore, using the expected value definition for a discrete random variable as in Equation 2.14, we have the following for \\(Z\\):\n\\[\n\\mathbb{E}(Z) = \\sum_{z \\in \\mathcal{Z}} z \\cdot P_Z(Z = z).\n\\tag{2.22}\\]\nWithin the support \\(\\mathcal{Z}\\), suppose that \\(z_1, z_2, \\dots\\) are the possible different values of \\(Z\\) corresponding to function \\(g(Y)\\). Then, for the \\(i\\)th value \\(z_i\\) in this correspondence, let \\(I_i\\) be the collection of all \\(y_j\\) such that\n\\[\ng(y_j) = z_i.\n\\tag{2.23}\\]\nNow, let us tweak a bit the above expression from Equation 2.22 to include this setting:\n\\[\n\\begin{align*}\n\\mathbb{E}(Z) &= \\sum_{z \\in \\mathcal{Z}} z \\cdot P_Z(Z = z) \\\\\n&= \\sum_{i} z_i \\cdot P_{g(Y)}(Z = z_i) \\\\\n& \\qquad \\text{we subset the summation to all $z_i$ with $Z = g(Y)$}\\\\\n&= \\sum_{i} z_i \\sum_{j \\in I_i} P_Y(Y = y_j). \\\\\n\\end{align*}\n\\tag{2.24}\\]\nThe last line of Equation 2.24 maps the probabilities associated to all \\(z_i\\) in the corresponding PMF of \\(Z\\), \\(P_Z(\\cdot)\\) via the function \\(g(Y)\\), to the original PMF of \\(Y\\), \\(P_Y(\\cdot)\\), for all those \\(y_j\\) contained in the collection \\(I_i\\). Given that certain values \\(z_i\\) can be obtained with more than one value \\(y_j\\), such as in the above example when \\(g(y) = y^2\\) for \\(y_1 = 2\\) and \\(y_2 = -2\\), note we have a second summation of probabilities applied to the PMF of \\(Y\\).\nMoving along with Equation 2.24 in conjunction with Equation 2.23, we have that:\n\\[\n\\begin{align*}\n\\mathbb{E}(Z) &= \\sum_{i} z_i \\sum_{j \\in I_i} P_Y(Y = y_j) \\\\\n&= \\sum_{i} \\sum_{j \\in I_i} z_i \\cdot P_Y(Y = y_j) \\\\\n&= \\sum_{i} \\sum_{j \\in I_i} g(y_j) \\cdot P_Y(Y = y_j).\n\\end{align*}\n\\tag{2.25}\\]\nThe double summation in Equation 2.25 can be summarized into a single one, given neither of the factors on the right-hand side is subindexed by \\(i\\). Furthermore, this standalone summation can be applied to all \\(y \\in \\mathcal{Y}\\) while getting rid of the subindex \\(j\\) in the factors on the right-hand side:\n\\[\n\\begin{align*}\n\\mathbb{E}(Z) &= \\sum_{i} \\sum_{j \\in I_i} g(y_j) \\cdot P_Y(Y = y_j) \\\\\n&= \\sum_{y \\in \\mathcal{Y}} g(y) \\cdot P_Y(Y = y) \\\\\n&= \\mathbb{E}\\left[ g(Y) \\right].\n\\end{align*}\n\\]\nTherefore, we have:\n\\[\n\\mathbb{E}\\left[ g(Y) \\right] = \\sum_{y \\in \\mathcal{Y}} g(y) \\cdot P_Y(Y = y). \\quad \\square\n\\]\n\n\n\nTheorem 2.2 Let \\(Y\\) be a continuous random variable whose support is \\(\\mathcal{Y}\\). The LOTUS indicates that the expected value of a general function \\(g(Y)\\) of this random variable \\(Y\\) can be obtained via \\(g(Y)\\) along with the corresponding PDF \\(f_Y(y)\\). Thus, the expected value of \\(g(Y)\\) can be obtained as\n\\[\n\\mathbb{E}\\left[ g(Y) \\right] = \\int_{\\mathcal{Y}} g(y) \\cdot f_Y(y).\n\\tag{2.26}\\]\n\nProof. Let us explore the rationale provided by Soch et al. (2024). Hence, we will rename the general function \\(g(Y)\\) as another random variable called \\(Z\\) such that:\n\\[\nZ = g(Y).\n\\tag{2.27}\\]\nAs in the discrete LOTUS proof, the above Equation 2.27 is formally called a random variable transformation from the general function of random variable \\(Y,\\) \\(g(Y)\\), to a new random variable \\(Z\\). Therefore, when we set up a transformation of this class, there will be a support mapping from this general function \\(g(Y)\\) to \\(Z\\). This will also yield a proper PDF:\n\\[\nf_Z(z) : \\mathbb{R} \\rightarrow [0, 1] \\quad \\forall z \\in \\mathcal{Z},\n\\]\ngiven that \\(g(Y)\\) is a random variable-based function.\nNow, we will use the concept of the cumulative distribution function (CDF) for a continuous random variable \\(Z\\):\n\\[\n\\begin{align*}\nF_Z(z) &= P(Z \\leq z) \\\\\n&= P\\left[g(Y) \\leq z \\right] \\\\\n&= P\\left[Y \\leq g^{-1}(z) \\right] \\\\\n&= F_Y\\left[ g^{-1}(z) \\right].\n\\end{align*}\n\\tag{2.28}\\]\nA well-known Calculus result is the inverse function theorem. Assuming that\n\\[\nz = g(y)\n\\]\nis an invertible and differentiable function, then the inverse\n\\[\ny = g^{-1}(z)\n\\tag{2.29}\\]\nmust be differentiable as in:\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d}z} \\left[ g^{-1}(z) \\right] = \\frac{1}{g' \\left[ g^{-1}(z) \\right]}.\n\\tag{2.30}\\]\nNote that we differentiate Equation 2.29 as follows:\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d}z} y = \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left[ g^{-1}(z) \\right].\n\\tag{2.31}\\]\nThen, plugging Equation 2.31 into Equation 2.30, we obtain:\n\\[\n\\begin{gather*}\n\\frac{\\mathrm{d}}{\\mathrm{d}z} y = \\frac{1}{g' \\left[ g^{-1}(z) \\right]} \\\\\n\\mathrm{d}y = \\frac{1}{g' \\left[ g^{-1}(z) \\right]} \\mathrm{d}z.\n\\end{gather*}\n\\tag{2.32}\\]\nThen, we use the property that relates the CDF \\(F_Z(z)\\) to the PDF \\(f_Z(z)\\):\n\\[\nf_Z(z) = \\frac{\\mathrm{d}}{\\mathrm{d}z} F_Z(z).\n\\]\nUsing Equation 2.28, we have:\n\\[\n\\begin{align*}\nf_Z(z) &= \\frac{\\mathrm{d}}{\\mathrm{d}z} F_Z(z) \\\\\n&= \\frac{\\mathrm{d}}{\\mathrm{d}z} F_Y\\left[ g^{-1}(z) \\right] \\\\\n&= f_Y\\left[ g^{-1}(z) \\right] \\frac{\\mathrm{d}}{\\mathrm{d}z} \\left[ g^{-1}(z) \\right].\n\\end{align*}\n\\]\nThen, via Equation 2.30, it follows that:\n\\[\nf_Z(z) = f_Y\\left[ g^{-1}(z) \\right] \\frac{1}{g' \\left[ g^{-1}(z) \\right]}.\n\\tag{2.33}\\]\nTherefore, using the expected value definition for a continuous random variable as in Equation 2.15, we have for \\(Z\\) that\n\\[\n\\mathbb{E}(Z) = \\int_{\\mathcal{Z}} z \\cdot f_Z(z) \\mathrm{d}z,\n\\]\nwhich yields via Equation 2.33:\n\\[\n\\mathbb{E}(Z) = \\int_{\\mathcal{Z}} z \\cdot f_Y \\left[ g^{-1}(z) \\right] \\frac{1}{g' \\left[ g^{-1}(z) \\right]} \\mathrm{d}z.\n\\]\nUsing Equation 2.29 and Equation 2.32, it follows that:\n\\[\n\\begin{align*}\n\\mathbb{E}(Z) &= \\int_{\\mathcal{Z}} z \\cdot f_Y(y) \\frac{1}{g' \\left[ g^{-1}(z) \\right]} \\mathrm{d}z \\\\\n&= \\int_{\\mathcal{Y}} g(y) \\cdot f_Y(y) \\mathrm{d}y.\n\\end{align*}\n\\]\nNote the last line in the above equation changes the integration limits to the support of \\(Y\\), given all terms end up depending on \\(y\\) on the right-hand side.\nFinally, given the random variable transformation from Equation 2.27, we have:\n\\[\n\\mathbb{E}\\left[ g(X) \\right] = \\int_{\\mathcal{Y}} g(y) \\cdot f_Y(y) \\mathrm{d}y. \\quad \\square\n\\]\n\n\n\n\nThe previous tip was quite theoretical! With that in mind, let us move on to the next stage where we will measure the spread of a random variable. This type of measure is formally known as a measure of uncertainty. We can think of this measure as a way to quantify the dispersion of any random variable from a specific population as we continue to observe it over a given number of trials. Let us begin with its formal definition.\n\n\nDefinition of measure of uncertainty\n\n\nProbabilistically, a measure of uncertainty refers to the spread of a given random variable when we observe its different realizations in the long term. Note a larger spread indicates more variability in these realizations. On the other hand, a smaller spread denotes less variability in these realizations.\n\n\nImage by Manfred Stege via Pixabay.\n\n\n\nSpecifically for this chapter, we will elaborate on the variance (and its derived measure called standard deviation) as a measure of uncertainty, given that it is commonly used across the different regression models in subsequent chapters. When it comes to exploratory data analysis, Chapter 3 will introduce an additional measure of this class called the interquartile range (IQR).\n\n\nDefinition of variance\n\n\nLet \\(Y\\) be a discrete or continuous random variable whose support is \\(\\mathcal{Y}\\) with a mean represented by \\(\\mathbb{E}(Y)\\). Then, the variance of \\(Y\\) is the mean of the squared deviation from the corresponding mean as follows:\n\\[\n\\text{Var}(Y) = \\mathbb{E}\\left\\{[ Y - \\mathbb{E}(Y)]^2 \\right\\}. \\\\\n\\tag{2.34}\\]\nNote the expression above is equivalent to:\n\\[\n\\text{Var}(Y) = \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y) \\right]^2.\n\\tag{2.35}\\]\nFinally, to put the spread measurement on the same units of random variable \\(Y\\), the standard devation of \\(Y\\) is merely the square root of \\(\\text{Var}(Y)\\):\n\\[\n\\text{sd}(Y) = \\sqrt{\\text{Var}(Y)}.\n\\tag{2.36}\\]\n\n\nImage by Manfred Stege via Pixabay.\n\n\n\nSome of us might wonder why Equation 2.34 and Equation 2.35 are equivalent. Therefore, we provide the below tip as an optional clarification on this matter. Note the work of Casella and Berger (2024) inspires all these insights.\n\n\nTip on the two mathematical expressions of the variance!\n\n\nProving the equivalence of Equation 2.34 and Equation 2.35, requires the introduction of some further properties of the expected value of a random variable while using the LOTUS.\n\nTheorem 2.3 Let \\(Y\\) be a discrete or continuous random variable. Furthermore, let \\(a\\), \\(b\\), and \\(c\\) be constants. Thus, for any functions \\(g_1(y)\\) and \\(g_2(x)\\) whose means exist, we have that:\n\\[\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] = a \\mathbb{E}\\left[ g_1(Y) \\right] + b \\mathbb{E}\\left[ g_2(Y) \\right] + c.\n\\tag{2.37}\\]\nFirstly, let us prove Equation 2.37 for the discrete case.\n\nProof. Let \\(Y\\) be a discrete random variable whose support is \\(\\mathcal{Y}\\) and PMF is \\(P_Y(Y = y)\\). Let us apply the LOTUS as in Equation 2.20:\n\\[\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] = \\sum_{y \\in \\mathcal{Y}} \\left[ a g_1(y) + b g_2(y) + c \\right] \\cdot P_Y(Y = y).\n\\] We can distribute the summation across each addend as follows:\n\\[\n\\begin{align*}\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] &= \\sum_{y \\in \\mathcal{Y}} \\left[ a g_1(y) \\right] \\cdot P_Y(Y = y) + \\\\\n& \\qquad \\sum_{y \\in \\mathcal{Y}} \\left[ b g_2(y) \\right] \\cdot P_Y(Y = y) + \\\\\n& \\qquad \\sum_{y \\in \\mathcal{Y}} c \\cdot P_Y(Y = y).\n\\end{align*}\n\\]\nLet us take the constants out of the corresponding summations:\n\\[\n\\begin{align*}\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] &= a \\sum_{y \\in \\mathcal{Y}} g_1(y) \\cdot P_Y(Y = y) + \\\\\n& \\qquad b \\sum_{y \\in \\mathcal{Y}} g_2(y) \\cdot P_Y(Y = y) + \\\\\n& \\qquad c \\underbrace{\\sum_{y \\in \\mathcal{Y}} P_Y(Y = y)}_1 \\\\\n&= a \\underbrace{\\sum_{y \\in \\mathcal{Y}} g_1(y) \\cdot P_Y(Y = y)}_{\\mathbb{E} \\left[ g_1(Y) \\right]} + \\\\\n& \\qquad b \\underbrace{\\sum_{y \\in \\mathcal{Y}} g_2(y) \\cdot P_Y(Y = y)}_{\\mathbb{E} \\left[ g_2(Y) \\right]} + c.\n\\end{align*}\n\\]\nFor the first and second addends on the right-hand side in the above equation, let us apply the LOTUS again:\n\\[\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] = a \\mathbb{E} \\left[ g_1(Y) \\right] + b \\mathbb{E} \\left[ g_2(Y) \\right] + c. \\quad \\square\n\\]\n\nSecondly, let us prove Equation 2.37 for the continuous case.\n\nProof. Let \\(Y\\) be a continuous random variable whose support is \\(\\mathcal{Y}\\) and PDF is \\(f_Y(y)\\). Let us apply the LOTUS as in Equation 2.26:\n\\[\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] = \\int_{\\mathcal{Y}} \\left[ a g_1 (y) + b g_2(y) + c \\right] \\cdot f_Y(y) \\mathrm{d}y.\n\\]\nWe distribute the integral on the right-hand side of the above equation:\n\\[\n\\begin{align*}\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] &= \\int_{\\mathcal{Y}} \\left[ a g_1 (y) \\right] \\cdot f_Y(y) \\mathrm{d}y + \\\\\n& \\qquad \\int_{\\mathcal{Y}} \\left[ b g_2(y) \\right] \\cdot f_Y(y) \\mathrm{d}y + \\\\\n& \\qquad \\int_{\\mathcal{Y}} c \\cdot f_Y(y) \\mathrm{d}y.\n\\end{align*}\n\\]\nLet us take the constants out of the corresponding integrals:\n\\[\n\\begin{align*}\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] &= a \\int_{\\mathcal{Y}} g_1 (y) \\cdot f_Y(y) \\mathrm{d}y + \\\\\n& \\qquad b \\int_{\\mathcal{Y}} g_2(y) \\cdot f_Y(y) \\mathrm{d}y + \\\\\n& \\qquad c \\underbrace{\\int_{\\mathcal{Y}} f_Y(y) \\mathrm{d}y}_{1} \\\\\n&= a \\underbrace{\\int_{\\mathcal{Y}} g_1 (y) \\cdot f_Y(y) \\mathrm{d}y}_{\\mathbb{E} \\left[ g_1(Y) \\right]} + \\\\\n& \\qquad b \\underbrace{\\int_{\\mathcal{Y}} g_2(y) \\cdot f_Y(y) \\mathrm{d}y}_{\\mathbb{E} \\left[ g_2(Y) \\right]} + c.\n\\end{align*}\n\\]\nFor the first and second addends on the right-hand side in the above equation, let us apply the LOTUS again:\n\\[\n\\mathbb{E}\\left[ a g_1(Y) + b g_2(Y) + c \\right] = a \\mathbb{E} \\left[ g_1(Y) \\right] + b \\mathbb{E} \\left[ g_2(Y) \\right] + c. \\quad \\square\n\\]\n\n\nFinally, after applying some algebraic rearrangements and the expected value properties shown in Equation 2.37, Equation 2.34 and Equation 2.35 are equivalent as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var}(Y) &= \\mathbb{E}\\left\\{[ Y - \\mathbb{E}(Y)]^2 \\right\\} \\\\\n&= \\mathbb{E} \\left\\{ Y^2 - 2Y \\mathbb{E}(Y) + \\left[ \\mathbb{E}(Y) \\right]^2 \\right\\} \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - \\mathbb{E} \\left[ 2Y \\mathbb{E}(Y) \\right] + \\mathbb{E} \\left[ \\mathbb{E}(Y) \\right]^2 \\\\\n& \\qquad \\text{distributing the expected value operator} \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - 2 \\mathbb{E} \\left[ Y \\mathbb{E}(Y) \\right] + \\mathbb{E} \\left[ \\mathbb{E}(Y) \\right]^2 \\\\\n& \\qquad \\text{since $2$ is a constant} \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - 2 \\mathbb{E}(Y) \\mathbb{E} \\left( Y \\right) + \\left[ \\mathbb{E}(Y) \\right]^2 \\\\\n& \\qquad \\text{since $\\mathbb{E}(Y)$ is a constant} \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - 2 \\left[ \\mathbb{E}(Y) \\right]^2 + \\left[ \\mathbb{E}(Y) \\right]^2 \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y) \\right]^2.  \\qquad \\qquad \\qquad \\qquad \\qquad \\square\n\\end{align*}\n\\]\n\n\n\nIt is time to dig into the application of the variance as a measure of uncertainty for our ice cream case. We will start with the demand query. Let us remember that the \\(i\\)th discrete random variable (as in Table 2.4) \\(D_i\\) is distributed as follows:\n\\[\nD_i \\sim \\text{Bern}(\\pi),\n\\]\nwhose PMF is defined as\n\\[\nP_{D_i} \\left( D_i = d_i \\mid \\pi \\right) = \\pi^{d_i} (1 - \\pi)^{1 - d_i} \\quad \\text{for $d_i \\in \\{ 0, 1 \\}$.}\n\\]\nVia Equation 2.14, the LOTUS in Equation 2.20, and Equation 2.35, the variance of this Bernoulli-distributed random variable \\(D_i\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (D_i) &= \\mathbb{E} \\left( D_i^2 \\right) - \\left[ \\mathbb{E}(D_i)\\right]^2 \\\\\n&= \\mathbb{E} \\left( D_i^2 \\right) - \\pi^2 \\qquad \\text{since $\\mathbb{E}(D_i) = \\pi$} \\\\\n&= \\sum_{d_i = 0}^1 d_i^2 P_{D_i} \\left( D_i = d_i \\mid \\pi \\right) - \\pi^2 \\qquad \\text{by LOTUS} \\\\\n&= \\left\\{ \\underbrace{(0^2) \\left[ \\pi^0 (1 - \\pi) \\right]}_{0} + \\underbrace{(1^2) \\left[ \\pi (1 - \\pi)^{0} \\right]}_{\\pi} \\right\\} - \\pi^2 \\\\\n&= (0 + \\pi) - \\pi^2 \\\\\n&= \\pi - \\pi^2 \\\\\n&= \\pi (1 - \\pi). \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\nTherefore, the standard deviation is given by:\n\\[\n\\text{sd}(D_i) = \\sqrt{\\pi (1 - \\pi)}.\n\\]\nSince the above standard deviation puts the variance on the same units of random variable \\(D_i\\), you might wonder:\n\nIn practical terms for our ice cream company, how can I explain \\(\\text{sd}(D_i) = \\sqrt{\\pi (1 - \\pi)}\\)?\n\nBefore discussing the specific case of a Bernoulli-type population where \\(65\\%\\) of children prefer chocolate over vanilla (i.e., a parameter \\(\\pi = 0.65\\)), let us first examine the general case for \\(\\pi \\in [0, 1]\\), as shown in Figure 2.3. In this plot, the range of \\(\\pi\\) is represented on the \\(x\\)-axis, while \\(\\text{sd}(D_i) = \\sqrt{\\pi (1 - \\pi)}\\) is shown on the \\(y\\)-axis. We will initially focus on three specific and increasing values of \\(\\pi\\), which are indicated by orange vertical dashed lines:\n\nWhen \\(\\pi = 0.5\\), it means that there is an equal chance of randomly obtaining either a success (i.e., a \\(1\\)) or a failure (i.e., a \\(0\\)). Note the standard deviation is given by \\(\\text{sd}(D_i) = \\sqrt{(0.5)(1 - 0.5)} = 0.5\\). Therefore, we state that the square root of the average squared distance from the mean is \\(0.5\\). Moreover, in Figure 2.3, this results in the largest spread of observed values (either \\(0\\) or \\(1\\)).\nWhen \\(\\pi = 0.7\\), the standard deviation is \\(\\text{sd}(D_i) = \\sqrt{(0.7)(1 - 0.7)} \\approx 0.46\\). Therefore, we state that the square root of the average squared distance from the mean is \\(0.46\\). In this case, there is a higher chance of randomly getting a success (i.e., a \\(1\\)) compared to a failure (i.e., a \\(0\\)). As a result, there is a smaller spread of observed values around the mean of \\(\\pi = 0.7\\), as most values in this population tend to be \\(1\\).\nWhen \\(\\pi = 0.9\\), the standard deviation is \\(\\text{sd}(D_i) = \\sqrt{(0.9)(1 - 0.9)} = 0.3\\). Here, the chance of obtaining a success (i.e., a \\(1\\)) is even higher than the chance of a failure (i.e., a \\(0\\)). Therefore, we state that the square root of the average squared distance from the mean is \\(0.3\\). This leads to an even smaller spread of observed values around the mean of \\(\\pi = 0.9\\), with most values in this population also being \\(1\\).\n\n\n\n\n\n\n\n\nFigure 2.3: Behaviour of the theoretical Bernoulli’s standard deviation over the range of its distributional parameter.\n\n\n\n\nLet us focus on our specific case where \\(\\pi = 0.65\\) in terms of the standard deviation, represented by the solid purple vertical line in Figure 2.3. Suppose you sample a sufficiently large number of children (i.e., your sample size \\(n_d \\rightarrow \\infty\\)) from a Bernoulli-type population where \\(65\\%\\) of these children prefer chocolate over vanilla. In this scenario, the standard deviation \\(\\text{sd}(D_i) = \\sqrt{(0.65)(1 - 0.65)} \\approx 0.48\\) indicates a slightly smaller spread compared to a population where there is an equal 50-50 chance of preferring chocolate over vanilla. Thus, when \\(\\pi = 0.65\\), the spread of values around the corresponding mean is slightly narrower since a slight majority leans towards preference for chocolate, which is represented by the value \\(1\\).\nTo address our time query, we can use a similar approach to obtain variance, but this time for a continuous random variable. Recall that the \\(j\\)th continuous random variable, denoted as \\(T_j\\), is distributed as follows:\n\\[\nT_j \\sim \\text{Exponential}(\\beta),\n\\]\nwhose PDF is defined as\n\\[\nf_{T_j} \\left(t_j \\mid \\beta \\right) = \\frac{1}{\\beta} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\quad \\text{for $t_j \\in [0, \\infty )$.}\n\\]\nVia Equation 2.35 and the Equation 2.15 of a continuous expected value, the variance of this Exponential-distributed random variable \\(T_j\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (T_j) &= \\mathbb{E} \\left( T_j^2 \\right) - \\left[ \\mathbb{E}(T_j)\\right]^2 \\\\\n&= \\mathbb{E} \\left( T_j^2 \\right) - \\beta^2 \\qquad \\text{since $\\mathbb{E}(T_j) = \\beta$}.\n\\end{align*}\n\\tag{2.38}\\]\nWe need to find \\(\\mathbb{E} \\left( T_j^2 \\right)\\) from Equation 2.38. Therefore, we make the following derivation via the LOTUS from Equation 2.26 when \\(g(T_j) = t_j^2\\):\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( T_j^2 \\right) &= \\int_{t_j = 0}^{t_j = \\infty} t_j^2 f_{T_j} \\left(t_j \\mid \\beta \\right) \\mathrm{d}t_j \\\\\n&= \\int_{t_j = 0}^{t_j = \\infty} t_j^2 \\frac{1}{\\beta} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j \\\\\n&= \\frac{1}{\\beta} \\int_{t_j = 0}^{t_j = \\infty} t_j^2 \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j. \\\\\n\\end{align*}\n\\tag{2.39}\\]\nEquation 2.39 cannot be solved straightforwardly, we need to use integration by parts as follows:\n\\[\n\\begin{align*}\nu &= t_j^2 & &\\Rightarrow & \\mathrm{d}u &= 2t_j \\mathrm{d}t_j \\\\\n\\mathrm{d}v &= \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j & &\\Rightarrow & v &= -\\beta \\exp \\left( -\\frac{t_j}{\\beta} \\right),\n\\end{align*}\n\\]\nwhich yields\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( T_j^2 \\right) &= \\frac{1}{\\beta} \\left[ u v \\Bigg|_{t_j = 0}^{t_j = \\infty} - \\int_{t_j = 0}^{t_j = \\infty} v \\mathrm{d}u \\right] \\\\\n&= \\frac{1}{\\beta} \\Bigg\\{ \\left[ -\\beta t_j^2 \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\right] \\Bigg|_{t_j = 0}^{t_j = \\infty} + \\\\\n& \\qquad 2 \\beta \\int_{t_j = 0}^{t_j = \\infty} t_j \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j \\Bigg\\} \\\\\n&= \\frac{1}{\\beta} \\Bigg\\{ -\\beta \\Bigg[ \\underbrace{\\infty \\times \\exp(-\\infty)}_{0} - \\underbrace{0 \\times \\exp(0)}_{0} \\Bigg] + \\\\\n& \\qquad 2 \\beta \\int_{t_j = 0}^{t_j = \\infty} t_j \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j \\Bigg\\} \\\\\n&= \\frac{1}{\\beta} \\left\\{ -\\beta (0) + 2 \\beta \\int_{t_j = 0}^{t_j = \\infty} t_j \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j \\right\\} \\\\\n&= \\frac{1}{\\beta} \\left\\{ 0 + 2 \\beta \\int_{t_j = 0}^{t_j = \\infty} t_j \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j \\right\\} \\\\\n&= 2 \\int_{t_j = 0}^{t_j = \\infty} t_j \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j. \\\\\n\\end{align*}\n\\tag{2.40}\\]\nAgain, we need to apply integration by parts to solve Equation 2.40:\n\\[\n\\begin{align*}\nu &= t_j & &\\Rightarrow & \\mathrm{d}u &= \\mathrm{d}t_j \\\\\n\\mathrm{d}v &= \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j & &\\Rightarrow & v &= -\\beta \\exp \\left( -\\frac{t_j}{\\beta} \\right),\n\\end{align*}\n\\]\nwhich yields\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( T_j^2 \\right) &= 2 \\left[ u v \\Bigg|_{t_j = 0}^{t_j = \\infty} - \\int_{t_j = 0}^{t_j = \\infty} v \\mathrm{d}u \\right] \\\\\n&= 2 \\Bigg\\{ \\left[ -\\beta t_j \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\right] \\Bigg|_{t_j = 0}^{t_j = \\infty} + \\\\\n& \\qquad \\beta \\int_{t_j = 0}^{t_j = \\infty} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\mathrm{d}t_j \\Bigg\\} \\\\\n&= 2 \\Bigg\\{ -\\beta \\Bigg[ \\underbrace{\\infty \\times \\exp(-\\infty)}_{0} - \\underbrace{0 \\times \\exp(0)}_{0} \\Bigg] - \\\\\n& \\qquad \\beta^2 \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\Bigg|_{t_j = 0}^{t_j = \\infty} \\Bigg\\} \\\\\n&= 2 \\left\\{ -\\beta (0) - \\beta^2 \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\right\\} \\\\\n&= 2 \\left[ 0 - \\beta^2 (0 - 1) \\right] \\\\\n&= 2 \\beta^2.\n\\end{align*}\n\\tag{2.41}\\]\nFinally, we plug Equation 2.41 into Equation 2.38:\n\\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\beta^2 \\\\\n&= 2 \\beta^2 - \\beta^2 \\\\\n&= \\beta^2. \\qquad \\qquad \\square\n\\end{align*}\n\\]\n\nHence, the standard deviation is given by:\n\\[\n\\text{sd}(T_j) = \\sqrt{\\beta^2} = \\beta.\n\\]\nSince the above standard deviation puts the variance on the same units of random variable \\(T_j\\), you might wonder:\n\nIn practical terms for our ice cream company, how can I explain \\(\\text{sd}(T_j) = \\beta\\)?\n\nLet us illustrate the behavior of the standard deviation within the context of the Exponential distribution, using a scale parameterization, through Figure 2.4. On the \\(x\\)-axis, we have the scale parameter \\(\\beta\\), which has a lower bound of \\(0\\) and an upper bound truncated at \\(50\\). Nonetheless, according to the definition of the Exponential distribution, recall that \\(\\beta\\) can range from \\(0\\) to \\(\\infty\\). Interestingly, both axes are measured in minutes, as the parameter definition of \\(\\beta\\) is expressed in the units of the Exponential random variable under this scale parametrization. Additionally, \\(\\text{sd}(T_j)\\) indicates the spread of the \\(T_j\\) in the same units. You can confirm these facts in Table 2.4.\n\n\n\n\n\n\n\nFigure 2.4: Behaviour of the theoretical Exponential’s standard deviation, under a scale parametrization, over a truncated range of its distributional parameter.\n\n\n\n\nFirstly, in Figure 2.4, it is important to emphasize that the relationship between the Exponential parameter \\(\\beta\\) and the standard deviation \\(\\text{sd}(T_j)\\) is linear. We have highlighted three specific cases as vertical lines:\n\nThe orange dashed line on the left indicates a small spread with \\(\\text{sd}(T_j) = 5\\) when \\(\\beta = 5\\). This signifies that the square root of the average squared distance from the mean is 5 minutes for the waiting times corresponding to this specific population.\nThe solid purple line represents our ice cream case, where \\(\\beta = 10\\), resulting in a larger spread with \\(\\text{sd}(T_j) = 10\\) compared to the case above. This means that the square root of the average squared distance from the mean is 10 minutes for the waiting times related to this specific population.\nThe orange dashed line on the right indicates the largest spread, compared to the two cases above, with \\(\\text{sd}(T_j) = 45\\) when \\(\\beta = 45\\). This means that the square root of the average squared distance from the mean is 45 minutes for the waiting times corresponding to this specific population.\n\n\n\nHeads-up on a further estimation technique for the variance!\n\n\nBefore concluding this section, it is important to note that, unlike the expected value case, we will not directly address how a sample-based tool similar to the mainstream average (i.e., Equation 2.19) can be used to estimate the variances for our demand and time queries. Instead, we will introduce a useful property related to the estimation parameter approach discussed in Section 2.2.\n\n\nImage by Manfred Stege via Pixabay.\n\n\n\n\n2.1.6 The Rationale in Random Sampling\nIn Section 2.1.1, we explained that random sampling allows us to save both financial and operational resources compared to conducting an entire census. This approach is particularly beneficial for estimating our population parameters for both demand and time queries. Furthermore, we highlighted that random sampling utilizes probabilistic and inferential tools to manage and report the uncertainty associated with these estimations. In this section, we will begin by examining those probabilistic tools.\nIt is important to note that random sampling is heavily based on the concept of random variables. However, mathematically expressing a set of random variables in a single expression requires the application of certain probabilistic concepts, specifically conditional probability and independence. While we will certainly need the Bayes’ rule to connect these two concepts, we will not delve deeper into this rule in the subsequent chapters of this book. Therefore, let us begin with conditional probability.\n\n\nDefinition of conditional probability\n\n\nSuppose you have two events of interest, \\(A\\) and \\(B\\), in a random phenomenon of a population or system of interest. These two events belong to the sample space \\(S\\). Moreover, assume that the probability of event \\(B\\) is such that\n\\[\nP(B) &gt; 0,\n\\]\nwhich is considered the conditioning event.\nHence, the conditional probability of event \\(A\\) given event \\(B\\) is defined as\n\\[\nP(A | B) = \\frac{P(A \\cap B)}{P(B)},\n\\tag{2.42}\\]\nwhere \\(P(A \\cap B)\\) is read as the probability of the intersection of events \\(A\\) and \\(B\\).\n\n\nImage by Manfred Steger via Pixabay.\n\n\n\nHow can we think about the above concept in terms of our ice cream case? Let us consider the following in terms of our demand query. For the purpose of this explanation, imagine that we have a small population of \\(N_d = 20\\) children to sample from. We can define an event \\(B\\) as the selection of a specific child, referred to as child B, from this group of \\(N_d = 20\\) children. The probability of selecting child B can be calculated as follows:\n\\[\nP(B) = \\frac{1}{20}.\n\\]\nIf you choose to conduct random sampling without replacement, you will not return any sampled child back to the small population before the next draw. In this context, let us define event \\(A\\) as the selection of a second specific child, referred to as child A. When sampling without replacement, the probability of selecting child A (given you already sampled child B) is determined by calculating the conditional probability\n\\[\nP(A | B) = \\frac{1}{19}.\n\\]\nIn the above ratio, note we have updated the sample space to just 19 children given event \\(A\\) is conditioned on \\(B\\). Finally, how can we connect these two probabilities with the intersection \\(P(A \\cap B)\\) found in Equation 2.42? We can view this intersection as the probability of a sequence of two events:\n\nWe sample child B from our initial pool of \\(N_d = 20\\) children.\nThen, from the updated pool of \\(N_d - 1 = 19\\) children since we are sampling without replacement, we sample child A.\n\nProbabilistically, this sequence is expressed as follows:\n\\[\n\\begin{align*}\nP(A \\cap B) &= P(B \\cap A) \\\\\n&= P(B) \\times P(A | B) \\\\\n&= \\frac{1}{20} \\times \\frac{1}{19} = \\frac{1}{380}.\n\\end{align*}\n\\]\n\n\nTip on the rationale behind conditional probability!\n\n\nWe can delve into the rationale of Equation 2.42 by using a handy mathematical concept called cardinality, which refers to the corresponding total number of possible outcomes in a random phenomenon belonging to any given event or sample space.\n\nProof. Let \\(|S|\\) be the cardinality corresponding to the sample space in a random phenomenon. Hence, as in Equation 2.2, we have that:\n\\[\nP(S) = \\frac{|S|}{|S|} = 1.\n\\]\nMoreover, suppose that \\(A\\) is the primary event of interest whose cardinality is represented by \\(|A|\\). Alternatively to Equation 2.1, the probability of \\(A\\) can be represented as\n\\[\nP(A) = \\frac{|A|}{|S|}.\n\\]\nOn the other hand, the cardinality of the conditioning event is\n\\[\nP(B) = \\frac{|B|}{|S|}.\n\\tag{2.43}\\]\nNow, let \\(|A \\cap B|\\) be the cardinality of the intersection between events \\(A\\) and \\(B\\). Its probability can be represented as:\n\\[\nP(A \\cap B) = \\frac{|A \\cap B|}{|B|}.\n\\tag{2.44}\\]\nAnalogous to Equation 2.43 and Equation 2.44, we can view the conditional probability \\(P(A | B)\\) as an updated probability of the primary event \\(A\\) restricted to the cardinality of the conditioning event \\(|B|\\). This places \\(|A \\cap B|\\) in the numerator and \\(|B|\\) in the denominator as follows:\n\\[\nP(A | B) = \\frac{|A \\cap B|}{|B|}.\n\\tag{2.45}\\]\nTherefore, we can play around with Equation 2.45 along with Equation 2.43 and Equation 2.44 as follows:\n\\[\n\\begin{align*}\nP(A | B) &= \\frac{|A \\cap B|}{|B|} \\\\\n&= \\frac{\\frac{|A \\cap B}{|S|}}{\\frac{|B|}{|S|}} \\qquad \\text{dividing numerator and denominator over $|S|$} \\\\\n&= \\frac{P(A \\cap B)}{P(B)}. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\n\n\nTo connect the previous result regarding conditional probability with the concept of statistical independence, we need to utilize Bayes’ rule, which is another important result in probability theory. It is worth noting that this principle is a fundamental aspect of Bayesian statistics.\n\n\nDefinition of the Bayes’ rule\n\n\nSuppose you have two events of interest, \\(A\\) and \\(B\\), in a random phenomenon of a population or system of interest. From Equation 2.42, we can state the following expression for the conditional probability of \\(A\\) given \\(B\\):\n\\[\nP(A | B) = \\frac{P(A \\cap B)}{P(B)} \\quad \\text{if $P(B) &gt; 0$.}\n\\tag{2.46}\\]\nNote the conditional probability of \\(B\\) given \\(A\\) can be stated as:\n\\[\n\\begin{align*}\nP(B | A) &= \\frac{P(B \\cap A)}{P(A)} \\quad \\text{if $P(A) &gt; 0$} \\\\\n&= \\frac{P(A \\cap B)}{P(A)} \\quad \\text{since $P(B \\cap A) = P(A \\cap B)$.}\n\\end{align*}\n\\tag{2.47}\\]\nThen, we can manipulate Equation 2.47 as follows:\n\\[\nP(A \\cap B) = P(B | A) \\times P(A).\n\\]\nThe above result can be plugged into Equation 2.46:\n\\[\n\\begin{align*}\nP(A | B) &= \\frac{P(A \\cap B)}{P(B)} \\\\\n&= \\frac{P(B | A) \\times P(A)}{P(B)}.\n\\end{align*}\n\\tag{2.48}\\]\nEquation 2.48 is called the Bayes’ rule. We are basically flipping around conditional probabilities.\n\n\nEven though this textbook has a frequentist tone, let us quickly connect Equation 2.48 with the elements mentioned in the definition of Bayesian statistics:\n\\[\nP(A | B) = \\frac{P(B | A) \\times P(A)}{P(B)}.\n\\]\n\n\n\\(P(A | B)\\) is known as the posterior probability of observing a primary event \\(A\\) given the conditioning event \\(B\\), which is referred to as the current evidence.\n\n\\(P(B | A)\\) is the conditional probability of observing the current evidence represented by event \\(B\\) given that the primary event \\(A\\) has occurred.\n\n\\(P(A)\\) is called the prior probability of observing the primary event \\(A\\).\n\n\\(P(B)\\) represents the overall probability of observing the current evidence represented by event \\(B\\), without considering event \\(A\\).\n\nFinally, let us use all the above results to elaborate on what statistical independence is.\n\n\nDefinition of independence\n\n\nSuppose you have two events of interest, \\(A\\) and \\(B\\), in a random phenomenon of a population or system of interest. These two events are statistically independent if event \\(B\\) does not affect event \\(A\\) and vice versa. Therefore, the probability of their corresponding intersection is given by:\n\\[\nP(A \\cap B) = P(A) \\times P(B).\n\\tag{2.49}\\]\nLet us expand the above definition to a random variable framework:\n\nSuppose you have a set of \\(n\\) discrete random variables \\(Y_1, \\dots, Y_n\\) whose supports are \\(\\mathcal{Y_1}, \\dots, \\mathcal{Y_n}\\) with PMFs \\(P_{Y_1}(Y_1 = y_1), \\dots, P_{Y_n}(Y_n = y_n)\\) respectively. That said, the joint PMF of these \\(n\\) random variables is the multiplication of their corresponding standalone PMFs:\n\n\\[\n\\begin{align*}\nP_{Y_1, \\dots, Y_n}(Y_1 = y_1, \\dots, Y_n = y_n) &= \\prod_{i = 1}^n P_{Y_i}(Y_i = y_i) \\\\\n& \\qquad \\text{for all} \\\\\n& \\qquad \\quad y_i \\in \\mathcal{Y}_i, i = 1, \\dots, n.\n\\end{align*}\n\\tag{2.50}\\]\n\nSuppose you have a set of \\(n\\) continuous random variables \\(Y_1, \\dots, Y_n\\) whose supports are \\(\\mathcal{Y_1}, \\dots, \\mathcal{Y_n}\\) with PDFs \\(f_{Y_1}(y_1), \\dots, f_{Y_n}(y_n)\\) respectively. That said, the joint PDF of these \\(n\\) random variables is the multiplication of their corresponding standalone PDFs:\n\n\\[\n\\begin{align*}\nf_{Y_1, \\dots, Y_n}(y_1, \\dots, y_n) &= \\prod_{i = 1}^n f_{Y_i}(y_i) \\\\\n& \\qquad \\text{for all} \\\\\n& \\qquad \\quad y_i \\in \\mathcal{Y}_i, i = 1, \\dots, n.\n\\end{align*}\n\\tag{2.51}\\]\n\n\nLet us clarify Equation 2.49 using our ice cream case in relation to our demand query. We will revisit the scenario involving a small population consisting of \\(N_d = 20\\) children. We can define an event \\(B\\) as the selection of a specific child, referred to as child B, from these \\(N_d = 20\\) children. The probability of selecting child B can be calculated as follows:\n\\[\nP(B) = \\frac{1}{20}.\n\\]\nWhen conducting random sampling with replacement, each sampled child is returned to the population before the next draw. Let us define event \\(A\\) as the selection of a specific child, referred to as child A. In this type of sampling, event \\(B\\) does not influence event \\(A\\); in other words, \\(A\\) and \\(B\\) are statistically independent. Therefore, when sampling with replacement, the probability of selecting child A, given that you have already sampled child B, can be determined by calculating the conditional probability\n\\[\nP(A | B) = \\frac{1}{20}.\n\\]\nIn the above ratio, we do not need to update the sample space. We will still have 20 children available for sampling for event \\(A\\):\n\\[\nP(A) = P(A | B) = \\frac{1}{20}.\n\\]\nTherefore, we can connect the probabilities \\(P(A)\\) and \\(P(B)\\) with the intersection \\(P(A \\cap B)\\). Again, this intersection is the probability of a sequence of two events:\n\nWe sample child B from our initial pool of \\(N_d = 20\\) children.\nThen, using the same pool of \\(N_d = 20\\) children since we are sampling with replacement, we sample child A.\n\nProbabilistically, this sequence is expressed as follows:\n\\[\n\\begin{align*}\nP(A \\cap B) &= P(B \\cap A) \\\\\n&= P(B) \\times P(A | B) \\\\\n&= P(B) \\times P(A) \\\\\n&= \\frac{1}{20} \\times \\frac{1}{20} = \\frac{1}{400}.\n\\end{align*}\n\\]\nThis is Equation 2.49! The tip below theoretically delves further into this equation, but you can skip it if you prefer.\n\n\nTip on the rationale behind the rule of independent events!\n\n\nWe can delve into the rationale of Equation 2.49 by using the Bayes’ rule from Equation 2.48 along with the basic conditional probability formula from Equation 2.42.\n\nProof. Firstly, let us assume that a given event \\(B\\) does not affect event \\(A\\) which can be probabilistically represented as\n\\[\nP(A | B) = P(A).\n\\tag{2.52}\\]\nIf the statement in Equation 2.52 holds, by using the Bayes’ rule from Equation 2.48, we have the following manipulation for the below conditional probability formula:\n\\[\n\\begin{align*}\nP(B | A) &= \\frac{P(B \\cap A)}{P(A)} \\\\\n&= \\frac{P(A \\cap B)}{P(A)} \\qquad \\text{since $P(B \\cap A) = P(A \\cap B$)} \\\\\n&= \\frac{P(A | B) \\times P(B)}{P(A)} \\qquad \\text{by the Bayes' rule} \\\\\n&= \\frac{P(A) \\times P(B)}{P(A)} \\qquad \\text{since $P(A | B) = P(A)$} \\\\\n&= P(B).\n\\end{align*}\n\\]\nThen, again by using the Bayes’ rule, we obtain \\(P(B \\cap A)\\) as follows:\n\\[\n\\begin{align*}\nP(B \\cap A) &= P(B | A) \\times P(A) \\\\\n&= P(B) \\times P(A) \\qquad \\text{since $P(B | A) = P(B)$.}\n\\end{align*}\n\\]\nFinally, we have that:\n\\[\n\\begin{align*}\nP(A \\cap B) &= P(B \\cap A) \\\\\n&= P(B) \\times P(A) \\\\\n&= P(A) \\times P(B). \\qquad \\square\n\\end{align*}\n\\]\n\n\n\nIt is time to outline how our random sampling with replacement (as mentioned in Section 2.1.3) for both demand and time queries can be mathematically represented using random variables. This representation should also involve a crucial statistical concept such as the random sample. Ultimately, we aim to construct what is known as joint probability distributions, which can take the form of either PMF or PDF depending on the type of random variables we are dealing with.\nThe previously introduced ideas by Equation 2.50 and Equation 2.51 serve as the primary steps for our discrete (Bernoulli trials as discussed by Equation 2.7) and continuous (waiting times as demonstrated by Equation 2.11) random variables respectively. However, these probability distributions will require an extra tweak, as discussed below.\n\n\nDefinition of random sample\n\n\nA random sample is a collection of random variables \\(Y_1, \\dots, Y_n\\) of size \\(n\\) coming from a given population or system of interest. Note that the most elementary definition of a random sample assumes that these \\(n\\) random variables are mutually independent and identically distributed (which is abbreviated as iid).\nThe fact that these \\(n\\) random variables are identically distributed indicates that they have the same mathematical form for their corresponding PMFs or PDFs, depending on whether they are discrete or continuous respectively. Hence, under a generative modelling approach in a population or system of interest governed by \\(k\\) parameters contained in the vector\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T,\n\\]\nwe can apply the iid property in an elementary random sample to obtain the following joint probability distributions:\n\nIn the case of \\(n\\) iid discrete random variables \\(Y_1, \\dots, Y_n\\) whose common standalone PMF is \\(P_Y(Y = y | \\boldsymbol{\\theta})\\) with support \\(\\mathcal{Y}\\), the joint PMF is mathematically expressed as\n\n\\[\n\\begin{align*}\nP_{Y_1, \\dots, Y_n}(Y_1 = y_1, \\dots, Y_n = y_n | \\boldsymbol{\\theta}) &= \\prod_{i = 1}^n P_Y(Y = y_i | \\boldsymbol{\\theta}) \\\\\n& \\quad \\text{for all} \\\\\n& \\quad \\quad y_i \\in \\mathcal{Y}, i = 1, \\dots, n.\n\\end{align*}\n\\tag{2.53}\\]\n\nIn the case of \\(n\\) iid continuous random variables \\(Y_1, \\dots, Y_n\\) whose common standalone PDF is \\(f_Y(y | \\boldsymbol{\\theta})\\) with support \\(\\mathcal{Y}\\), the joint PDF is mathematically expressed as\n\n\\[\n\\begin{align*}\nf_{Y_1, \\dots, Y_n}(y_1, \\dots, y_n | \\boldsymbol{\\theta}) &= \\prod_{i = 1}^n f_Y(y_i | \\boldsymbol{\\theta}) \\\\\n& \\quad \\text{for all} \\\\\n& \\quad \\quad y_i \\in \\mathcal{Y}, i = 1, \\dots, n.\n\\end{align*}\n\\tag{2.54}\\]\nUnlike Equation 2.50 and Equation 2.51, note that Equation 2.53 and Equation 2.54 do not indicate a subscript for \\(Y\\) in the corresponding probability distributions since we have identically distributed random variables. Furthermore, the joint distributions are conditioned on the population parameter vector \\(\\boldsymbol{\\theta}\\) which reflects our generative modelling approach.\n\n\nImage by Manfred Steger via Pixabay.\n\n\n\nAlright! We are ready to deliver our joint probability distributions for our ice cream case assuming sampling with replacement. Thus, in the demand query via a random sample under the iid property along with the standalone PMF for \\(D_i\\)\n\\[\nP_{D_i} \\left( D_i = d_i \\mid \\pi \\right) = \\pi^{d_i} (1 - \\pi)^{1 - d_i} \\quad \\text{for $d_i \\in \\{ 0, 1 \\}$},\n\\]\nour joint PMF is\n\\[\n\\begin{align*}\nP_{D_1, \\dots, D_{n_d}} \\left( D_1 = d_1, \\dots, D_{n_d} = d_{n_d} | \\pi \\right) &= \\prod_{i = 1}^{n_d} P_{D_i} \\left( D_i = d_i \\mid \\pi \\right) \\\\\n&= \\prod_{i = 1}^{n_d} P_D \\left( D = d_i \\mid \\pi \\right) \\\\\n& \\qquad \\qquad \\qquad \\quad \\quad \\text{iid} \\quad \\\\\n&= \\prod_{i = 1}^{n_d} \\pi^{d_i} (1 - \\pi)^{1 - d_i} \\\\\n& \\quad \\text{for all} \\\\\n& \\quad \\quad d_i \\in \\{ 0, 1 \\}, \\\\\n& \\qquad \\quad i = 1, \\dots, n_d.\n\\end{align*}\n\\tag{2.55}\\]\nIt is important to clarify that \\(n_d\\) refers to our sample size for this query. In contrast, our vector of population parameters consists of a single element: the probability of success \\(\\pi\\) (i.e., the proportion of children who prefer chocolate over vanilla from our population). Recall that, in practice, this parameter is unknown and we aim to estimate it via our \\(n_d\\) sampled data points. In the following section, we will see that these data points are contained in the data frame children_sample from Section 2.1.5, which was drawn with replacement.\nOn the other hand, our time query will demand a joint PDF. Again, this probability distribution will represent our random sample assuming sampling with replacement. Therefore, under the iid property along with the standalone PMF for \\(T_j\\)\n\\[\nf_{T_j} \\left(t_j \\mid \\beta \\right) = \\frac{1}{\\beta} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\quad \\text{for $t_j \\in [0, \\infty )$},\n\\]\nour joint PDF is\n\\[\n\\begin{align*}\nf_{T_1, \\dots, T_{n_t}}(t_1, \\dots, t_{n_t} | \\beta) &= \\prod_{j = 1}^{n_t} f_{T_j}(t_j | \\beta) \\\\\n&= \\prod_{j = 1}^{n_t} f_T(t_j | \\beta) \\qquad \\text{iid} \\\\\n&= \\prod_{j = 1}^{n_t} \\frac{1}{\\beta} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\\\\n& \\quad \\text{for all} \\\\\n& \\quad \\quad t_j \\in [0, \\infty), \\\\\n& \\qquad \\quad j = 1, \\dots, n_t.\n\\end{align*}\n\\tag{2.56}\\]\nIn this query, \\(n_t\\) refers to our sample size of waiting times. Our vector of population parameters also consists of a single element: the average waiting time in minutes from one customer to the next, denoted as \\(\\beta\\). Again, in practice, this parameter is unknown, and our goal is to estimate it using the \\(n_t\\) sampled data points. As we will see in the upcoming section, these data points are contained in data frame waiting_sample from Section 2.1.5, which was drawn with replacement.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span>"
    ]
  },
  {
    "objectID": "book/02-stats-review.html#sec-mle",
    "href": "book/02-stats-review.html#sec-mle",
    "title": "2  Basic Cuisine: A Review on Probability and Frequentist Statistical Inference",
    "section": "\n2.2 What is Maximum Likelihood Estimation?",
    "text": "2.2 What is Maximum Likelihood Estimation?\nOnce we have introduced all the necessary probabilistic concepts to address our demand and time queries from Table 2.4, it is time to explore an intriguing statistical concept: estimating our true population parameters using the sample data in children_sample and waiting_sample. Therefore, we will focus on maximum likelihood estimation (MLE) as a fundamental frequentist tool in this process, which will also be invoked in our subsequent regression-related chapters. MLE is closely connected to the probabilistic concepts we discussed in the previous section. As a result, we will partially transition from the probabilistic realm to the realm of inference since we are merely targeting estimation in this section.\n\n\nImage by Manfred Steger via Pixabay.\n\n\n2.2.1 Key Concepts\nMLE primarily relies on a random sample of \\(n\\) observations from the population or system of interest. In some cases, such as the ice cream example we have been discussing per query, we may only have one parameter to estimate. In this scenario, we will apply univariate MLE. Conversely, we may encounter situations, such as with regression models, that involve more than one parameter to estimate; this is referred to as multivariate MLE. Regardless of the case, MLE offers an effective method for finding estimators, which tend to behave well (asymptotically speaking, meaning when we gather a sufficiently large number of \\(n\\) data points).\n\n\nDefinition of estimator\n\n\nAn estimator is a mathematical rule involving the random variables \\(Y_1, \\dots, Y_n\\) from our random sample of size \\(n\\). As its name says, this rule allows us to estimate our population parameter of interest.\n\n\nIn statistical practice, finding reliable estimators can be quite challenging. However, MLE offers decent performance and is not a black box approach. The entire estimation process is transparent and well-structured, as we have elaborated throughout this chapter. For example, the sample mean\n\\[\n\\bar{Y} = \\frac{\\sum_{i = 1}^n Y_i}{n}\n\\tag{2.57}\\]\nis a primary case with a very intuitive answer to estimate any given parameter (more specifically, one related to a measure of central tendency). However, this primary case is supported by statistical modelling procedures in MLE, as we will explore further in this section. Note that Equation 2.57 is considered an estimator since its notation only involves random variables (i.e., uppercases). That said, let us explore an additional concept called the estimate.\n\n\nDefinition of estimate\n\n\nSuppose we have an observed random sample of size \\(n\\) with values \\(y_1, \\dots , y_n\\). Then, we apply a given estimator mathematical rule to these \\(n\\) observed values. Hence, this numerical computation is called an estimate of our population parameter of interest.\n\n\nIn simple terms, an estimate is the version of the estimator that is based on observed data. Therefore, an observed sample mean serves as an estimate, represented in this way using lowercase letters:\n\\[\n\\bar{y} = \\frac{\\sum_{i = 1}^n y_i}{n}.\n\\tag{2.58}\\]\nNote that the mainstream average from Equation 2.19 is considered an estimate. Now, moving along with our key concepts, it is time to have a crucial conversation on the difference between likelihood and probability.\n\n\nHeads-up on the statistical difference between probability and likelihood!\n\n\nUnlike everyday language, in statistics, probability and likelihood are not the same. In general, probability refers to the chance that some outcome of interest will happen for a particular random variable. Note a probability is always bounded between \\(0\\) and \\(1\\).\n\n\nImage by Manfred Steger via Pixabay.\n\nOn the other hand, given some observed data, a likelihood refers to how plausible a given distributional parameter is. Furthermore, a likelihood is not bounded between \\(0\\) and \\(1\\).\n\n\nLet us set aside our ice cream case and explore the difference between these two concepts using the Binomial distribution (whose specific theoretical insights can be found in Section D.2). This distribution models the number of successes in \\(n\\) independent Bernoulli trials, each sharing a common probability of success \\(\\pi\\) where \\(\\pi \\in [0, 1]\\). Let the random variable\n\\[\nY = \\text{Number of successes out of $n$ independent Bernoulli trials,}\n\\]\nwhich follows Binomial distribution:\n\\[\nY \\sim \\text{Bin}(n, \\pi).\n\\]\nIts PMF is\n\\[\n\\begin{align*}\nP_Y \\left( Y = y \\mid n, \\pi \\right) &= {n \\choose y} \\pi^y (1 - \\pi)^{n - y} \\\\\n& \\qquad \\qquad \\qquad \\text{for $y \\in \\{ 0, 1, \\dots, n \\}$.}\n\\end{align*}\n\\tag{2.59}\\]\nTerm \\({n \\choose y}\\) represents the total number of combinations for \\(y\\) successes out of \\(n\\) trials:\n\\[\n{n \\choose y} = \\frac{n!}{y!(n - y)!}.\n\\]\n\n\nHeads-up on the \\(n\\) trials in the Binomial distribution!\n\n\nIn our estimation framework, we use random samples consisting of \\(n\\) observed random variables. Hence, it is important to avoid confusing this sample size with the above distributional parameter \\(n\\), which refers to the number of independent Bernoulli trials in a Binomial random variable.\n\n\nTo provide a practical explanation on the difference between a probability and a likelihood in statistics, we will plot the PMFs of six Binomial random variables, as shown in Figure 2.5. The following key points can be highlighted in these plots:\n\nEach plot illustrates a Binomial PMF with the same number of trials \\(n = 10\\), but the probabilities of success vary as in \\(\\pi = 0.3, 0.4, 0.5, 0.6, 0.7, 0.8\\). This variation results in six distinct PMFs.\nThe \\(x\\)-axis depicts the possible values that the corresponding random variable can take, \\(y \\in \\{ 0, 1, \\dots, 10 \\}\\).\nThe \\(y\\)-axis displays the probability \\(P_Y \\left( Y = y \\mid n = 10, \\pi \\right)\\). Above each bar, we indicate the probability \\(P_Y \\left( Y = y \\mid n = 10, \\pi \\right)\\) for each observed value of \\(y\\). When we sum all these probabilities, we arrive at a total of \\(1\\).\nAs the value of \\(\\pi\\) increases, the PMFs become more skewed to the left, indicating that larger probabilities are assigned to a greater number of successes.\nLastly, the red bar highlights the specific probability \\(P(Y = 7 | n = 10, \\pi)\\). This value will be particularly important later in our proof of concept for MLE.\n\n\n\n\n\n\n\n\nFigure 2.5: Probability mass functions of six Binomial distributions with the same number of trials and six different probabilities of success.\n\n\n\n\nImagine you are interested in estimating the unknown probability of success \\(\\pi\\), from a population that you have chosen to model as Binomial. Therefore, you draw a random sample of a standalone Binomial observation and you obtain an observed value of \\(y = 7\\) successes out of \\(n = 10\\) trials. Within a frequentist framework, our main objective is to estimate the value of \\(\\pi\\) that is most likely given these specific \\(n = 10\\) trials and \\(y = 7\\) observed successes.\nTo achieve the above, we will introduce the concept of the likelihood function.\n\n\nDefinition of likelihood function\n\n\nSuppose you observe some data \\(y\\) from a population or system of interest which is governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nThe corresponding random variable \\(Y\\) has a given probability function \\(P_Y(Y = y | \\boldsymbol{\\theta})\\), which can be either a PMF in the discrete case or a PDF in the continuous case. Then, the likelihood function for the parameter vector \\(\\boldsymbol{\\theta}\\) given the observed data \\(y\\) is mathematically equivalent to the aforementioned probability function such that:\n\\[\nL(\\boldsymbol{\\theta} | y) = P_Y(Y = y | \\boldsymbol{\\theta}).\n\\tag{2.60}\\]\nIt is important to note that the above likelihood is in function of the parameter vector \\(\\boldsymbol{\\theta}\\) and conditioned on the observed data \\(y\\). Additionally, in many continuous cases, the likelihood function may exceed \\(1\\) given the definition of bounds we have already established for a PDF (see Equation 2.9).\n\n\nUsing the above definition, let us retake the PMF from Equation 2.59 and turn into a likelihood function given \\(n = 10\\) trials and \\(y = 7\\) observed successes:\n\\[\n\\begin{align*}\nL \\left( \\pi | y = 7, n = 10 \\right) &= P_Y \\left( Y = 7 \\mid n = 10, \\pi \\right) \\\\\n&= {10 \\choose 7} \\pi^7 (1 - \\pi)^{10 - 7}.\n\\end{align*}\n\\tag{2.61}\\]\nTo estimate the value of \\(\\pi\\) that is most likely given \\(n = 10\\) trials and \\(y = 7\\) observed successes, we need to find the value of \\(\\pi\\) that maximizes the above Binomial likelihood function. Since we know that the probability of success \\(\\pi\\) falls within the range \\([0, 1]\\), we can determine the value of \\(\\pi\\) that maximizes this function through trial and error by testing different values within this range. To illustrate this process more graphically, let us take a look at Figure 2.6:\n\nThis plot displays the likelihood function from Equation 2.61 on the \\(y\\)-axis, compared to the plausible range of \\(\\pi\\) on the \\(x\\)-axis. The function is left-skewed.\nWe have highlighted the values corresponding to the red bars in five of the previous Binomial PMFs from Figure 2.5 with vertical red dashed lines at \\(\\pi = 0.3, 0.4, 0.5, 0.6, 0.8\\); since a probability function is mathematically equivalent to a likelihood function.\nThe likelihood value for \\(\\pi = 0.7\\) is indicated by a solid purple vertical line. This value corresponds to the red bar in the Binomial PMF from Figure 2.5 when \\(\\pi = 0.7\\). It is important to note that this value of \\(\\pi\\) maximizes this likelihood function.\n\n\n\nHeads-up on the intuition behind MLE!\n\n\nRetaking (3), given our observed data of \\(y = 7\\) successes out of \\(n = 10\\) trials, and assuming a Binomial-distributed population, we can say that the most plausible value of \\(\\pi\\) that is generating this collected evidence is \\(0.7\\).\n\n\n\n\n\n\n\n\n\nFigure 2.6: Binomial likelihood function of the probability of success given 10 trials and 7 observed successes.\n\n\n\n\nHow can we translate this MLE principle to our demand and time queries in the ice cream case? Firstly, we need to make the corresponding jump from the joint probability to a likelihood function in each query. For the demand query, via Equation 2.55, the likelihood function is mathematically defined as:\n\\[\n\\begin{align*}\nL \\left( \\pi | d_1, \\dots, d_{n_d} \\right) &= P_{D_1, \\dots, D_{n_d}} \\left( D_1 = d_1, \\dots, D_{n_d} = d_{n_d} | \\pi \\right) \\\\\n&= \\prod_{i = 1}^{n_d} \\pi^{d_i} (1 - \\pi)^{1 - d_i}.\n\\end{align*}\n\\tag{2.62}\\]\nFor the time query, via Equation 2.56, the likelihood function is mathematically defined as:\n\\[\n\\begin{align*}\nL \\left( \\beta | t_1, \\dots, t_{n_t} \\right) &= f_{T_1, \\dots, T_{n_t}}(t_1, \\dots, t_{n_t} | \\beta) \\\\\n&= \\prod_{j = 1}^{n_t} \\frac{1}{\\beta} \\exp \\left( -\\frac{t_j}{\\beta} \\right).\n\\end{align*}\n\\tag{2.63}\\]\nTo estimate the parameters \\(\\pi\\) and \\(\\beta\\) using MLE, we can pursue either an analytical or numerical optimization-based approach via our observed data, which is stored in children_sample and waiting_sample. The following two sections will elaborate on these approaches.\n\n2.2.2 Analytical Estimates\nThis section will address our inquiries regarding the ice cream case, as summarized in Table 2.4, through what we refer to as an analytical MLE approach. At the end of this section, we will revisit the scenario of the follow-up meeting with the eight general managers. We will discuss how the sampled data stored in children_sample and waiting_sample can be utilized to resolve the demand and time queries, while incorporating estimates of the measures of central tendency and uncertainty introduced in Section 2.1.5. Ultimately, this process will provide the statistical rationale for the previously mentioned mainstream average from Equation 2.19 regarding the estimation of the Bernoulli and Exponential parameters.\n\n\nImage by Manfred Stege via Pixabay.\n\nUnlike our previous toy example involving a standalone observed Binomial random variable to estimate a probability of success \\(\\pi\\) using MLE, it is impractical to use a trial-and-error method to find the estimates of our population parameters based on sampled data from real-life cases. Additionally, the likelihood functions we might encounter in these scenarios will be even more complex than those corresponding to our previous examples with Equation 2.62 and Equation 2.63. Therefore, what is the primary mathematical approach to applying MLE in complex situations? The first step is to consider a fundamental method for finding the maximum value of any given function. This tool, which comes from calculus, is known as the derivative.\nIn general, we can use the first derivative \\(f'(y)\\) to find critical points in the function \\(f(y)\\), which can either be a maximum or a minimum. In the context of MLE, our goal is to identify the critical point within the range of our population parameter that corresponds to a maximum. Essentially, we aim to find the estimate that makes our observed data, as indicated by the likelihood function, the most plausible (or likely!). Additionally, we need to confirm whether this critical point is indeed a maximum; further discussion in this section will address this by applying the second derivative test.\nFor the time being, let us retake the likelihood function for our demand and time queries:\n\\[\nL \\left( \\pi | d_1, \\dots, d_{n_d} \\right) = \\prod_{i = 1}^{n_d} \\pi^{d_i} (1 - \\pi)^{1 - d_i},\n\\tag{2.64}\\]\nand\n\\[\nL \\left( \\beta | t_1, \\dots, t_{n_t} \\right) = \\prod_{j = 1}^{n_t} \\frac{1}{\\beta} \\exp \\left( -\\frac{t_j}{\\beta} \\right).\n\\tag{2.65}\\]\nWe can initiate the calculus-based process described above, which will involve the following steps:\n\nCalculate the first partial derivative of either Equation 2.64 or Equation 2.65 with respect to the population parameter of interest (\\(\\pi\\) or \\(\\beta\\)). This will be a partial derivative, as the right-hand side of both equations includes additional variables, such as the observed values from the corresponding random samples.\nSet the partial derivative obtained in step (1) equal to zero and solve for the parameter of interest. This involves isolating the parameter on the left-hand side while moving the other variables to the right-hand side. This is referred to as a closed-form solution.\nVerify that the closed-form solution is indeed a maximum by performing the second derivative test.\n\nNevertheless, there is a distinctive characteristic in both likelihood functions: they involve a multiplication of either \\(n_d\\) or \\(n_t\\) factors. Consequently, performing step (1) would be quite complicated, as we would need to apply the product rule to derive these functions. To address this issue, we can turn to what is known in MLE as the log-likelihood function.\n\n\nDefinition of log-likelihood function\n\n\nSuppose you observe some data \\(y\\) from a population or system of interest which is governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nThe corresponding random variable \\(Y\\) has a given probability function \\(P_Y(Y = y | \\boldsymbol{\\theta})\\), which can be either a PMF in the discrete case or a PDF in the continuous case. Moreover, the likelihood function, as described in Equation 2.60, is defined as follows:\n\\[\nL(\\boldsymbol{\\theta} | y) = P_Y(Y = y | \\boldsymbol{\\theta}).\n\\]\nThen, the log-likelihood function is merely the logarithm of the above function:\n\\[\n\\log L(\\boldsymbol{\\theta} | y) = \\log \\left[ P_Y(Y = y | \\boldsymbol{\\theta}) \\right].\n\\tag{2.66}\\]\nUsing a log-likelihood function, which is a monotonic transformation of the likelihood function, offers the following practical advantages:\n\nThe logarithmic properties convert products into sums. This is particularly useful in likelihood functions for random samples that involve multiplying probability functions (and its corresponding factors).\nWhen estimating parameters, calculating the derivative of a sum is easier than that of a product.\nIn many cases, the likelihood functions for observed random samples can yield very small values, which may lead to computational instability. By working on a logarithmic scale, these computations become more stable. This stability is crucial for numerical optimization methods applied to a given log-likelihood function, in cases where a closed-form solution for an estimate is not mathematically feasible.\n\n\n\n\n\nHeads-up on the \\(\\log(\\cdot)\\) notation!\n\n\nStatistically, unless indicated otherwise, the \\(\\log(\\cdot)\\) notation implicates the natural logarithm with a base of \\(e = 2.71828...\\)\n\n\nTherefore, let us modify the above three MLE steps and expand them into four:\n\nTransform the likelihood function (using either Equation 2.64 or Equation 2.65) into a log-likelihood function, applying the necessary logarithmic and exponent properties to simplify the resulting expression.\nObtain the first partial derivative of the simplified log-likelihood function with respect to the population parameter of interest (either \\(\\pi\\) or \\(\\beta\\)).\nSet the partial derivative obtained in step (2) equal to zero and solve for the parameter of interest to find the closed-form solution.\nVerify that the closed-form solution is indeed a maximum by conducting the second derivative test.\n\nWith these steps in mind, let us formally define what MLE is before moving on to the derivations of our queries.\n\n\nDefinition of maximum likelihood estimation (MLE)\n\n\nSuppose you observe some data \\(y\\) from a population or system of interest which is governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nThe corresponding random variable \\(Y\\) has a given probability function \\(P_Y(Y = y | \\boldsymbol{\\theta})\\), which can be either a PMF in the discrete case or a PDF in the continuous case. Furthermore, the log-likelihood function is defined as in Equation 2.66:\n\\[\n\\log L(\\boldsymbol{\\theta} | y) = \\log \\left[ P_Y(Y = y | \\boldsymbol{\\theta}) \\right].\n\\]\nMLE aims to find the estimate of \\(\\boldsymbol{\\theta}\\) that maximizes the above log-likelihood function as in:\n\\[\n\\hat{\\boldsymbol{\\theta}} = \\underset{\\boldsymbol{\\theta}}{\\operatorname{arg max}} \\log L(\\boldsymbol{\\theta} | y).\n\\]\nIn supervised learning, MLE is analogous to minimizing any given loss function during model training.\n\n\nNow, let us apply the above four steps for both queries. For the demand query, we go as follows:\n\nWe will transform the likelihood function into a log-likelihood function, and apply the necessary logarithmic and exponent properties to simplify this mathematical expression. The log-likelihood function, via Equation 2.64, is given by:\n\n\\[\n\\begin{align*}\n\\log L \\left( \\pi | d_1, \\dots, d_{n_d} \\right) &= \\log \\left[ \\prod_{i = 1}^{n_d} \\pi^{d_i} (1 - \\pi)^{1 - d_i} \\right] \\\\\n&= \\log \\left[ \\pi^{\\sum_{i = 1}^{n_d} d_i} (1 - \\pi)^{\\sum_{i = 1}^{n_d} (1 - d_i)} \\right] \\\\\n&= \\log \\left[ \\pi^{\\sum_{i = 1}^{n_d} d_i} (1 - \\pi)^{n_d - \\sum_{i = 1}^{n_d} d_i} \\right] \\\\\n&= \\left( \\sum_{i = 1}^{n_d} d_i \\right) \\log(\\pi) \\\\\n& \\qquad + \\left( n_d - \\sum_{i = 1}^{n_d} d_i \\right) \\log(1 - \\pi). \\\\\n\\end{align*}\n\\tag{2.67}\\]\n\nWe obtain the first partial derivative of the simplified log-likelihood function, from Equation 2.67, with respect to the population parameter \\(\\pi\\):\n\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\pi} \\log L \\left( \\pi | d_1, \\dots, d_{n_d} \\right) &= \\frac{\\sum_{i = 1}^{n_d} d_i}{\\pi} - \\frac{n_d - \\sum_{i = 1}^{n_d} d_i}{1 - \\pi}.\n\\end{align*}\n\\tag{2.68}\\]\n\nWe set Equation 2.68 to zero and solve for \\(\\pi\\). This will yield the close-form solution, and thus the estimate \\(\\hat{\\pi}\\):\n\n\\[\n\\begin{gather*}\n\\frac{\\sum_{i = 1}^{n_d} d_i}{\\pi} - \\frac{n_d - \\sum_{i = 1}^{n_d} d_i}{1 - \\pi} = 0 \\\\\n\\frac{\\sum_{i = 1}^{n_d} d_i}{\\pi} = \\frac{n_d - \\sum_{i = 1}^{n_d} d_i}{1 - \\pi} \\\\\n(1 - \\pi) \\sum_{i = 1}^{n_d} d_i = \\pi \\left( n_d - \\sum_{i = 1}^{n_d} d_i \\right) \\\\\n\\sum_{i = 1}^{n_d} d_i - \\pi \\sum_{i = 1}^{n_d} d_i = \\pi n_d - \\pi \\sum_{i = 1}^{n_d} d_i \\\\\n\\pi n_d - \\pi \\sum_{i = 1}^{n_d} +  \\pi \\sum_{i = 1}^{n_d} = \\sum_{i = 1}^{n_d} d_i \\\\\n\\pi n_d = \\sum_{i = 1}^{n_d} d_i \\qquad \\Rightarrow \\qquad \\hat{\\pi} = \\frac{\\sum_{i = 1}^{n_d} d_i}{n_d}.\n\\end{gather*}\n\\]\n\nIs \\(\\hat{\\pi}\\) truly a maximum? Let us apply second derivative test. If \\[\\frac{\\partial^2}{\\partial \\pi^2} L \\left( \\pi | d_1, \\dots, d_{n_d} \\right) \\Bigg|_{\\pi = \\hat{\\pi}} &lt; 0,\\] then \\(\\hat{\\pi}\\) is indeed a maximum point:\n\n\\[\n\\begin{gather*}\n\\frac{\\partial^2}{\\partial \\pi^2} L \\left( \\pi | d_1, \\dots, d_{n_d} \\right) = - \\frac{\\sum_{i = 1}^{n_d} d_i}{\\pi^2} - \\frac{n_d - \\sum_{i = 1}^{n_d} d_i}{(1 - \\pi)^2}\n\\end{gather*}\n\\]\n\\[\n\\begin{align*}\n\\frac{\\partial^2}{\\partial \\pi^2} L \\left( \\pi | d_1, \\dots, d_{n_d} \\right) \\Bigg|_{\\pi = \\hat{\\pi}} &= - \\frac{\\sum_{i = 1}^{n_d} d_i}{\\hat{\\pi}^2} - \\frac{n_d - \\sum_{i = 1}^{n_d} d_i}{\\left( 1 - \\hat{\\pi} \\right)^2} \\\\\n&= - \\frac{n_d \\hat{\\pi}}{\\hat{\\pi}^2} - \\frac{n_d - n_d \\hat{\\pi}}{\\left( 1 - \\hat{\\pi} \\right)^2} \\\\\n&= - \\frac{n_d}{\\hat{\\pi}} - \\frac{n_d \\left( 1 - \\hat{\\pi} \\right)}{\\left( 1 - \\hat{\\pi} \\right)^2} \\\\\n&= - \\frac{n_d}{\\hat{\\pi}} - \\frac{n_d}{1 - \\hat{\\pi}} &lt; 0.\n\\end{align*}\n\\]\nWe pass the second derivative test given that the sample size \\(n_d\\) and the estimate \\(\\hat{\\pi}\\) will always be nonnegative. Therefore, the estimate\n\\[\n\\hat{\\pi} = \\frac{\\sum_{i = 1}^{n_d} d_i}{n_d},\n\\tag{2.69}\\]\nis indeed a truly maximum for the demand query.\nThen, we will follow the four steps for the time query:\n\nWe will convert the likelihood function to a log-likelihood function and use logarithmic and exponent properties to simplify this mathematical expression. According to Equation 2.65, the log-likelihood function is given as:\n\n\\[\n\\begin{align*}\n\\log L \\left( \\beta | t_1, \\dots, t_{n_t} \\right) &= \\log \\left[ \\prod_{j = 1}^{n_t} \\frac{1}{\\beta} \\exp \\left( -\\frac{t_j}{\\beta} \\right) \\right] \\\\\n&= \\log \\left[ \\frac{1}{\\beta^{n_t}} \\exp \\left( -\\frac{1}{\\beta} \\sum_{j = 1}^{n_t} t_j \\right) \\right] \\\\\n&= -n_t \\log \\left( \\beta \\right) + \\log \\left[ \\exp \\left( -\\frac{1}{\\beta} \\sum_{j = 1}^{n_t} t_j \\right) \\right] \\\\\n&= -n_t \\log \\left( \\beta \\right) - \\frac{\\sum_{j = 1}^{n_t} t_j}{\\beta}.\n\\end{align*}\n\\tag{2.70}\\]\n\nWe obtain the first partial derivative of the simplified log-likelihood function, from Equation 2.70, with respect to the population parameter \\(\\beta\\):\n\n\\[\n\\frac{\\partial}{\\partial \\beta} \\log L \\left( \\beta | t_1, \\dots, t_{n_t} \\right) = -\\frac{n_t}{\\beta} + \\frac{\\sum_{j = 1}^{n_t} t_j}{\\beta^2}.\n\\tag{2.71}\\]\n\nWe set Equation 2.71 to zero and solve for \\(\\beta\\). This will yield the close-form solution, and thus the estimate \\(\\hat{\\beta}\\):\n\n\\[\n\\begin{gather*}\n-\\frac{n_t}{\\beta} + \\frac{\\sum_{j = 1}^{n_t} t_j}{\\beta^2} = 0 \\\\\n\\frac{1}{\\beta} \\left( -n_t + \\frac{\\sum_{j = 1}^{n_t} t_j}{\\beta} \\right) = 0 \\\\\n-n_t + \\frac{\\sum_{j = 1}^{n_t} t_j}{\\beta} = 0 \\\\\nn_t = \\frac{\\sum_{j = 1}^{n_t} t_j}{\\beta} \\qquad \\Rightarrow \\qquad \\hat{\\beta} = \\frac{\\sum_{j = 1}^{n_t} t_j}{n_t}.\n\\end{gather*}\n\\]\n\nIs \\(\\hat{\\beta}\\) truly a maximum? Let us apply second derivative test. If \\[\\frac{\\partial^2}{\\partial \\beta^2} L \\left( \\beta | t_1, \\dots, t_{n_t} \\right) \\Bigg|_{\\beta = \\hat{\\beta}} &lt; 0,\\] then \\(\\hat{\\pi}\\) is indeed a maximum point:\n\n\\[\n\\begin{gather*}\n\\frac{\\partial^2}{\\partial \\beta^2} L \\left( \\beta | t_1, \\dots, t_{n_t} \\right) = \\frac{n_t}{\\beta^2} - \\frac{2 \\sum_{j = 1}^{n_t} t_j}{\\beta^3}\n\\end{gather*}\n\\]\n\\[\n\\begin{align*}\n\\frac{\\partial^2}{\\partial \\beta^2} L \\left( \\beta | t_1, \\dots, t_{n_t} \\right) \\Bigg|_{\\beta = \\hat{\\beta}} &= \\frac{n_t}{\\hat{\\beta}^2} - \\frac{2 \\sum_{j = 1}^{n_t} t_j}{\\hat{\\beta}^3} ]\\\\\n&= \\frac{n_t}{\\hat{\\beta}^2} - \\frac{2 n_t \\hat{\\beta}}{\\hat{\\beta}^3} \\\\\n&= \\frac{n_t}{\\hat{\\beta}^2} - \\frac{2 n_t}{\\hat{\\beta}^2} \\\\\n&= -\\frac{n_t}{\\hat{\\beta}^2} &lt; 0.\n\\end{align*}\n\\]\nWe pass the second derivative test given that the sample size \\(n_t\\) and the estimate \\(\\hat{\\pi}\\) will always be nonnegative. Therefore, the estimate\n\\[\n\\hat{\\beta} = \\frac{\\sum_{j = 1}^{n_t} t_j}{n_t},\n\\tag{2.72}\\]\nis indeed a truly maximum for the demand query.\nGreat! We have derived closed-form estimates using MLE for our demand and time queries, as provided by Equation 2.69 and Equation 2.72. You may be wondering:\n\nHow can I obtain the estimators based on the corresponding definition?\n\nTo obtain the estimators for these queries, all it takes is a change in notation. Specifically, we replace our lowercase letters, which represent observations, with uppercase letters that denote random variables. This approach will yield the following estimators (thus, the tilde notation on the left-hand side instead of the hat notation):\n\\[\n\\begin{gather*}\n\\tilde{\\pi} = \\frac{\\sum_{i = 1}^{n_d} D_i}{n_d}\n\\end{gather*}\n\\tag{2.73}\\]\n\\[\n\\begin{gather*}\n\\tilde{\\beta} = \\frac{\\sum_{j = 1}^{n_t} T_j}{n_t}.\n\\end{gather*}\n\\tag{2.74}\\]\n\n\nHeads-up on the use of estimators!\n\n\nThe above expressions (Equation 2.73 and Equation 2.74) establish a rule for estimating the corresponding parameter of interest, which can be applied to a given random sample obtained from these populations. We will retake them in Section 2.3 to elaborate on the last inferential topics of this chapter.\n\n\nBefore we proceed with our sample data found in children_sample and waiting_sample, let us revisit the concept introduced by Section 2.1.5 regarding the observed mainstream average from Equation 2.19. This average is calculated by summing all \\(n\\) realizations \\(y_k\\) (where \\(k = 1, \\dots, n\\)) of a specific random variable and then dividing this total by \\(n\\):\n\\[\n\\bar{y} = \\frac{\\sum_{k = 1}^n y_k}{n}.\n\\]\nFor the demand query, recall the obtained estimate by MLE is given by\n\\[\n\\hat{\\pi} = \\frac{\\sum_{i = 1}^{n_d} d_i}{n_d},\n\\]\nwith a sample size of \\(n_d\\) children and\n\\[\nd_i =\n\\begin{cases}\n1 \\qquad \\text{The surveyed child prefers chocolate.}\\\\\n0 \\qquad \\text{Otherwise.}\n\\end{cases}\n\\]\nBased on the possible numerical values that \\(d_i\\) can take, we can see that \\(\\hat{\\pi}\\) is analogous to the mainstream average! Specifically, we are estimating the proportion of children in our population who prefer chocolate over vanilla. This directly addresses both the statement and the parameter we are interested in, which can be referenced in Table 2.4. Therefore, we are ready to numerically compute \\(\\hat{\\pi}\\) by using the corresponding mean function from R and Python via children_sample.\n\n\nR\nPython\n\n\n\n\npi_hat_MLE &lt;- round(mean(children_sample$fav_flavour == \"chocolate\"), 2)\npi_hat_MLE\n\n[1] 0.67\n\n\n\n\n\npi_hat_MLE = round((children_sample[\"fav_flavour\"] == \"chocolate\").mean(), 2)\nprint(pi_hat_MLE)\n\n0.61\n\n\n\n\n\nTo address the time query, our estimate by MLE is given by\n\\[\n\\hat{\\beta} = \\frac{\\sum_{j = 1}^{n_t} t_j}{n_t},\n\\]\nwhere \\(n_t\\) denotes the sample size of waiting times, and \\(t_j\\) in minutes is a continuous observation that falls within the range \\([0, \\infty)\\). This estimate is analogous to the calculation of the mainstream average. Additionally, \\(\\hat{\\beta}\\) enables us to estimate the average waiting time between customers at any given ice cream cart within our population of interest, as indicated by the statement and parameter discussed by Table 2.4. That said, we can now proceed to calculate \\(\\hat{\\beta}\\) numerically using the appropriate mean function in both R and Python using waiting_sample.\n\n\nR\nPython\n\n\n\n\nbeta_hat_MLE &lt;- round(mean(waiting_sample$waiting_time), 2)\nbeta_hat_MLE\n\n[1] 10.23\n\n\n\n\n\nbeta_hat_MLE = round(waiting_sample[\"waiting_time\"].mean(), 2)\nprint(beta_hat_MLE)\n\n9.67\n\n\n\n\n\n\n\nHeads-up on different sample estimates!\n\n\nNote that the estimates pi_hat_MLE and beta_hat_MLE differ between R and Python. This discrepancy arises because, despite using the same simulation seed to generate the children_sample or waiting_sample, each programming language employs its own pseudo-random number generator.\n\n\nImage by Manfred Stege via Pixabay.\n\nThe same applies to the simulation seeds used for generating the corresponding populations of \\(N_d = 2,000,000\\) children and \\(N_t = 500,000\\) general customer-to-customer waiting times.\n\n\nRetaking the measures of central tendency and uncertain from Section 2.1.5, and applied specifically to our demand query, we have already derived the mean (or expected value) and variance (and its derived standard deviation) for the \\(i\\)th Bernoulli-distributed random variable \\(D_i\\):\n\\[\n\\mathbb{E}(D_i) = \\pi\n\\]\nand\n\\[\n\\text{Var}(D_i) = \\pi (1 - \\pi) \\qquad \\Rightarrow \\qquad \\text{sd}(D_i) = \\sqrt{\\pi (1 - \\pi)}.\n\\]\nSince the parameter \\(\\pi\\) that governs this Bernoulli-distributed population corresponds to the theoretical mean (or expected value), we have also found its corresponding estimate, denoted as \\(\\hat{\\mathbb{E}}(D_i)\\), which is 0.67 in R (or 0.61 in Python). Now, how can we obtain the estimate for the variance, along with the derived standard deviation? We can use an important MLE result known as the invariance property to compute these measures of spread.\n\n\nHeads-up on the MLE invariance property in a Bernoulli-distributed population!\n\n\nThe MLE invariance property states that if we already an estimate of a parameter, such as \\(\\hat{\\pi}\\) in a Bernoulli-distributed population, then any monotonic transformation of parameter \\(\\pi\\) will yield a corresponding estimate. For example, consider the transformation:\n\\[\n\\begin{align*}\ng(\\pi) &= \\pi (1 - \\pi) \\\\\n& = \\text{Var}(D_i).\n\\end{align*}\n\\]\nUsing this transformation, the estimate will be:\n\\[\n\\begin{align*}\ng(\\hat{\\pi}) &= \\hat{\\pi} (1 - \\hat{\\pi}) \\\\\n&= \\hat{\\text{Var}}(D_i).\n\\end{align*}\n\\tag{2.75}\\]\n\n\nTherefore, by using Equation 2.75 and our children_sample, the estimate \\(\\hat{\\text{Var}}(D_i)\\) for the variance in the Bernoulli population corresponding to the demand query is 0.22 in R (or 0.24 in Python). Consequently, the estimate of the standard deviation, namely \\(\\hat{\\text{sd}}(D_i)\\), is 0.47 in R (or 0.49 in Python).\nFor the time query, we can easily obtain the measures of central tendency and uncertainty since we have already estimated \\(\\beta\\). As mentioned by Section 2.1.5, the mean and variance (along with the derived standard deviation) for the \\(j\\)th Exponential-distributed random variable \\(T_j\\) are as follows:\n\\[\n\\mathbb{E}(T_j) = \\beta\n\\]\nand\n\\[\n\\text{Var}(T_j) = \\beta \\qquad \\Rightarrow \\qquad \\text{sd}(T_j) = \\sqrt{\\beta}.\n\\]\nTherefore, via waiting_sample, the estimates \\(\\hat{\\mathbb{E}}(T_j)\\) and \\(\\hat{\\text{Var}}(D_i)\\) are also 10.23 in R (or 9.67 in Python). Then, the estimate of the standard deviation, namely \\(\\hat{\\text{sd}}(T_j)\\), is 3.2 in R (or 3.11 in Python).\nWe have already completed our queries depicted in Table 2.4! Well, at least when it comes to providing point estimates, as defined below.\n\n\nDefinition of point estimate\n\n\nLet \\(\\theta\\) denote a population parameter of interest. Suppose you have observed a random sample of size \\(n\\), represented as the vector:\n\\[\n\\boldsymbol{y} = (y_1, y_2, \\ldots, y_n)^T.\n\\]\nThe point estimate \\(\\hat{\\theta}\\) serves as a possible value for \\(\\theta\\) and is expressed as a function of the observed random sample contained in \\(\\boldsymbol{y}\\):\n\\[\n\\hat{\\theta} = h(\\boldsymbol{y}).\n\\]\n\n\nTo expand on our inquiries regarding the ice cream case, we will discuss what is known as interval estimates in Section 2.3, for which the above estimated variances will play a central role. Since we are working with observed random variables in both samples, these interval estimates will help us report and control the uncertainty in our parameter estimations to our stakeholders. For now, let us clarify why MLE is essential in parameter estimation for our upcoming regression chapters.\n\n\nHeads-up on why MLE is fundamental in regression modelling!\n\n\nWe might initially think that discussing the statistical rationale for estimating \\(\\pi\\) and \\(\\beta\\) in Bernoulli and Exponential populations, respectively, is trivial since we concluded that the sample mean (i.e., the so-called mainstream average) suffices to address the demand and time queries. However, this conclusion is far from trivial. Theoretically, for these specific populations, the sample mean provides an estimate of the corresponding distributional parameter that maximizes the likelihood of our observed data through MLE. This assertion is supported by the probabilistic tools we previously discussed in Section 2.1.\n\n\nImage by Manfred Stege via Pixabay.\n\nThat said, the same above MLE paradigm will significantly influence our upcoming chapters on regression modelling. MLE is a key aspect that makes parameter estimation in many regression models, based on sample data, a transparent process rather than a black box. This paradigm allows data scientists to perform analyses on data derived from specific populations by employing a generative modelling approach. We assume that this data comes from particular distributions, which have their corresponding parameters to estimate.\nFurthermore, as we will see later in this book, these parameters will not only pertain to measures of central tendency and uncertainty but will also include regression parameters. Of course, the MLE approaches in these models will be more complex, as we will need to estimate multiple parameters simultaneously. Still, the frequentist concept of producing the estimates that make our observed data the “most plausible (or likely!)” will remain crucial to our analyses. Essentially, this framework underpins the model fitting functions in both R and Python, which we will explore later in most chapters.\n\n\nTo conclude this section on analytical MLE via point estimates, let us revisit the follow-up meeting with the eight general managers regarding our ice cream case. It would be beneficial to present a clear storytelling about the sampling and estimation process for both queries. This will ensure that all the managers in the room can easily understand the process and effectively communicate the results to others within the company.\nDemand Query\n\nTo understand the relative demand for chocolate versus vanilla flavour among children aged 4 to 11 years old attending various parks in Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during Summer weekends, we estimated the probability that a randomly selected child prefers chocolate. This was done using a maximum likelihood estimation framework based on a sample of 500 children from this population. It is important to note that each of these 500 children was independently selected, meaning the selection of one child did not influence the selection of the next.\nIn this two-flavour choice context, each surveyed child was modelled as a Bernoulli trial, where we asked whether chocolate or vanilla was their preferred flavour. The maximum likelihood approach was particularly suitable in this case because it determines the value that makes the observed set of 500 children preferences most plausible for this population. Using this random sample of children’s flavour choices, we found that the estimated population proportion preferring chocolate was 0.67 (or 0.61 via Python). This indicates that, based on our survey data, approximately 67% (or 61% via Python) of children in the sampled population chose chocolate over vanilla.\nRandom sampling was crucial in this study because it ensured that every child in the population had an equal chance of being included, thereby minimizing bias in our estimation and allowing us to generalize the results to the entire population. However, our estimation approach does have limitations. Relying solely on point estimates, like the one mentioned above, does not account for variability in preferences across different age groups, cities, or times of day. Additionally, the MLE method assumes that the observed ice cream flavor choices are independent and identically distributed, which may not hold true in real-world settings where peer influence or marketing exposure could impact preferences. Therefore, we might be interested in exploring more complex data modelling techniques, such as regression analysis, that take these variables into consideration.\n\n\n\nImage by Manfred Stege via Pixabay.\n\nTime Query\n\nIn a parallel analysis, we examined the frequency of customer arrivals, focusing on the waiting time between purchases for all general customers across the 900 ice cream carts located in various parks across Vancouver, Victoria, Edmonton, Calgary, Winnipeg, Ottawa, Toronto, and Montréal during Summer weekends. Maximum likelihood estimation was essential in this study, enabling us to estimate the parameter associated with the average waiting time across this population, which makes our observed waiting times the most plausible. Note that we modelled the waiting times as being Exponential-distributed, which is commonly used for interarrival times in statistical analysis.\nUsing a random sample of 200 waiting times between two general customers across the 900 ice cream carts in the eight cities, we estimated the average waiting time to be approximately 10.23 minutes (or 9.67 minutes via Python). The assumption of random sampling was crucial to prevent bias; without it, the estimates could reflect peak-hour congestion or quiet periods instead of the flow typical of an average day during Summer weekends. This random sampling ensured that every waiting time in the population had an equal opportunity to be included.\nThe interpretation of the estimated value suggests that, on average, there is an 10.23-minute gap (or 9.67-minute gap via Python) between one customer and the next. This information is valuable for planning service capacity and scheduling employees. However, we must acknowledge the simplifying assumptions behind this model—specifically, that arrivals occur at a constant rate over time. If real-world arrivals are affected by external factors such as the time of day, weather, or marketing campaigns, this basic Exponential model may not fully capture that complexity.\nAs with the demand query, relying on point estimates means we must be cautious: while informative, they provide only a snapshot rather than the full probabilistic picture of customer behaviour. Finally, we might be interested in exploring more complex modelling techniques, such as regression analysis, that take additional external factors into account.\n\n\n2.2.3 Numerical Optimization\nBefore wrapping up this entire MLE delivery so that we can move on to a review of statistical inference via hypothesis testing and confidence intervals, we will briefly elaborate on MLE with numerical optimization in this section. For the univariate example in the ice cream case per query, we can obtain a straightforward analytical solution, as demonstrated in Section 2.2.2. However, in real-life scenarios involving regression analysis, the data modelling becomes more complex as we delve into a multivariate MLE framework. Therefore, deriving analytical expressions for the estimates is not feasible.\n\n\nImage by Manfred Stege via Pixabay.\n\nAn optimization method is a mathematical algorithm designed to find the maximum or minimum values of a function, subject to specific constraints or within a certain range. In statistics, these optimization methods are essential for obtaining parameter estimates. This is often achieved by maximizing likelihood functions, minimizing error terms, or finding the best fit for data. Specifically for MLE, the goal is to find the value of a parameter (or a set of parameters) that maximizes the log-likelihood function. It is important to note that working on a logarithmic scale ensures computational stability and helps avoid issues like numerical underflow, since we typically deal with very small likelihood values. The logarithmic scale transforms these small values into more manageable negative numbers.\nLog-likelihood functions can be nonlinear and complex, so we often use various optimization methods, including:\n\n\nGradient Descent: This method iteratively moves in the direction of the steepest descent, guided by the gradient. Its origin traces back to Cauchy (1847).\n\nNewton-Raphson Method: This approach utilizes both the gradient and the second derivative (known as the Hessian) to accelerate the convergence of the algorithm. It goes back to Newton and Colson (1736) and Raphson (1697).\n\nBrent’s Method: A derivative-free technique that is ideal for scalar functions defined on a bounded interval. It is particularly useful for univariate MLE cases. You can find more information in Brent (1973).\n\nBroyden–Fletcher–Goldfarb–Shanno (BFGS) Method: This is a quasi-Newton method employed for multivariable optimization that uses the gradient and Hessian matrix (Fletcher 1987). As a side note, there is an alternative version of this method called Limited-memory BFGS (L-BFGS), which is less computationally intensive compared to BFGS (Liu and Nocedal 1989).\n\nAlthough it is possible to obtain analytical estimates for our demand and time queries, we will use numerical optimization to provide a proof of concept for the MLE ideas discussed above. For the demand query, the code provided (for either R or Python) estimates the proportion of children who prefer chocolate over vanilla (denoted as \\(\\hat{\\pi}\\)) under a Binomial model. Specifically, this scenario can be represented as:\n\\[\nY \\sim \\text{Bin}(n = 1, \\pi),\n\\]\nwhich is equivalent to a Bernoulli random variable with parameter \\(\\pi\\) (see Equation 2.13).\nFirst, the code below transforms the categorical variable fav_flavour into a binary factor, where \"vanilla\" is coded as 0 and \"chocolate\" is coded as 1, with \"chocolate\" representing the success outcome in Equation 2.3. It then defines a function called neg_log_lik_binomial that calculates the negative log-likelihood for the Binomial distribution based on a candidate probability, \\(\\pi\\), and the observed binary responses. This function penalizes invalid \\(\\pi\\) values that fall outside the interval \\([0, 1]\\) by returning Inf.\nThen, using the corresponding optimization function through Brent’s method, the code minimizes the negative log-likelihood (which is equivalent to maximizing the positive log-likelihood) to obtain the estimate \\(\\hat{\\pi}\\) that best fits the observed data. The resulting estimate is stored in numerical_pi_hat_MLE. Notably, this output aligns with the analytical result obtained by calculating the sample mean.\n\n\nHeads-up on additional Python packages!\n\n\nThe R version of the optimization code below does not require any additional packages beyond the base set of functions. However, the Python version will need the {scipy} (Virtanen et al. 2020) package.\n\n\n\n\nR\nPython\n\n\n\n\nchildren_sample &lt;- children_sample |&gt;\n  mutate(\n    fav_flavour = factor(fav_flavour,\n      levels = c(\"vanilla\", \"chocolate\")\n    ), # Sets vanilla = 0, chocolate = 1\n    flavour_binary = as.integer(fav_flavour) - 1\n  )\n\nneg_log_lik_binomial &lt;- function(pi, y) {\n  if (pi &lt;= 0 || pi &gt;= 1) {\n    return(Inf)\n  }                                  # pi must be in [0, 1]\n  k &lt;- sum(y)                        # Total successes\n  n &lt;- length(y)                     # Number of trials\n  -dbinom(k, size = n, prob = pi, log = TRUE)\n}\n\nMLE_optimizer_pi &lt;- optim(\n  par = 0.5,                         # Initial guess\n  fn = neg_log_lik_binomial,         # Negative log-likelihood function\n  y = children_sample$flavour_binary,              # Observed data\n  method = \"Brent\",                  # Method for scalar optimization\n  lower = 0.0001, upper = 0.9999     # Ensure pi stays within [0.0001, 0.9999] in the search\n)\n\nnumerical_pi_hat_MLE &lt;- round(MLE_optimizer_pi$par, 2)\nnumerical_pi_hat_MLE\n\n[1] 0.67\n\n\n\n\n\nfrom scipy.stats import binom\nfrom scipy.optimize import minimize_scalar\n\nchildren_sample['fav_flavour'] = pd.Categorical(\n    children_sample['fav_flavour'], categories = ['vanilla', 'chocolate'], ordered = True\n)\nchildren_sample['flavour_binary'] = children_sample['fav_flavour'].cat.codes  # Sets vanilla = 0, chocolate = 1\n\ndef neg_log_lik_binomial(pi, y):\n    if pi &lt;= 0 or pi &gt;= 1:\n        return np.inf                # pi must be in [0, 1]\n    k = y.sum()                      # Total successes\n    n = len(y)                       # Number of trials\n    return -binom.logpmf(k, n, pi)\n\nMLE_optimizer_pi = minimize_scalar(\n    neg_log_lik_binomial,            # Negative log-likelihood function\n    bounds = (0.0001, 0.9999),       # Ensure pi stays within [0.0001, 0.9999] in the search\n    method = 'bounded',              # Method for scalar optimization\n    args = (children_sample['flavour_binary'],)   # Observed data\n)\n\nnumerical_pi_hat_MLE = round(MLE_optimizer_pi.x, 2)\nprint(numerical_pi_hat_MLE)\n\n0.61\n\n\n\n\n\nFor the time query, the provided code (for either R or Python) estimates the scale parameter, denoted as \\(\\beta\\), in an Exponential distribution based on a sample of waiting times stored in waiting_sample. It defines a function called neg_log_lik_exponential, which computes the negative log-likelihood of the Exponential distribution for a given \\(\\beta\\). This function ensures that \\(\\beta\\) is always positive; if not, it returns Inf to penalize invalid parameter values. The log-likelihood is calculated using the appropriate PDF.\nThe corresponding optimization function utilized minimizes this negative log-likelihood using Brent’s method. The initial guess for \\(\\beta\\) is set at 5, and the search is confined between 0.0001 and 1000 to guarantee a valid value for \\(\\beta\\). The final result, which is the numerical estimate of \\(\\hat{\\beta}\\), is stored in numerical_beta_hat_MLE. This output matches the analytical result derived from calculating the sample mean.\n\n\nR\nPython\n\n\n\n\nneg_log_lik_exponential &lt;- function(beta, y) {\n  if (beta &lt;= 0) {\n    return(Inf)\n  }                                  # beta must be positive\n  -sum(dexp(y, rate = 1 / beta, log = TRUE))\n}\n\nMLE_optimizer_beta &lt;- optim(\n  par = 5,                           # Initial guess\n  fn = neg_log_lik_exponential,      # Negative log-likelihood function\n  y = waiting_sample$waiting_time,   # Observed data\n  method = \"Brent\",                  # Method for scalar optimization\n  lower = 0.0001, upper = 1000       # Ensure beta stays within [0.0001, 1000] in the search\n)\n\nnumerical_beta_hat_MLE &lt;- round(MLE_optimizer_beta$par, 2)\nnumerical_beta_hat_MLE\n\n[1] 10.23\n\n\n\n\n\nfrom scipy.stats import expon\n\ndef neg_log_lik_exponential(beta, y):\n    if beta &lt;= 0:\n      return np.inf                  # beta must be positive\n    return -np.sum(expon.logpdf(y, scale = beta))\n\nMLE_optimizer_beta = minimize_scalar(\n    neg_log_lik_exponential,         # Negative log-likelihood function\n    bounds = (0.0001, 1000),         # Ensure beta stays within [0.0001, 1000] in the search\n    method = 'bounded',\n    args = (waiting_sample['waiting_time'],)   # Pass the data as a tuple\n)\n\nnumerical_beta_hat_MLE = round(MLE_optimizer_beta.x, 2)\nprint(numerical_beta_hat_MLE)\n\n9.67\n\n\n\n\n\nAs we can see, a proper numerical optimization method is essential in MLE cases where analytical solutions for the log-likelihood function cannot be obtained. It is important to re-emphasize that optimization methods play a crucial role in fitting regression models in both R and Python, as MLE analytical solutions are typically not available. For example, when we use a fitting function like glm() in R or various fitting functions from the statsmodels (Seabold and Perktold 2010) library in Python, we are actually invoking an optimizer. This optimizer employs a specific method to search for the set of parameter values that best explain the observed data according to the chosen log-likelihood function.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span>"
    ]
  },
  {
    "objectID": "book/02-stats-review.html#sec-basics-inf",
    "href": "book/02-stats-review.html#sec-basics-inf",
    "title": "2  Basic Cuisine: A Review on Probability and Frequentist Statistical Inference",
    "section": "\n2.3 Basics of Frequentist Statistical Inference",
    "text": "2.3 Basics of Frequentist Statistical Inference\n\n\n\n\n\nFigure 2.7: Results stage from the data science workflow in Figure 1.1. This stage is directly followed by storytelling and preceded by goodness of fit.\n\n\n\n\n\n\n\nFigure 2.8: A classical-based hypothesis testing workflow structured in four substages: general settings, hypotheses definitions, test flavour and components, and inferential conclusions.\n\n\n\n2.3.1 General Settings\n\n\n\n\n\nFigure 2.9: General settings substage from the classical-based hypothesis testing workflow in Figure 2.8. This substage is directly followed by the hypotheses definitions.\n\n\nBased on the work by Soch et al. (2024), let us check some key definitions.\n\n\nDefinition of hypothesis\n\n\nSuppose you observe some data \\(y\\) from some population(s) or system(s) of interest governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nMoreover, let us assume that random variable \\(Y\\) follows certain probability distribution \\(\\mathcal{D}(\\cdot)\\) in a generative model \\(m\\) as in\n\\[\n\\text{$m$: } Y \\sim \\mathcal{D}(\\boldsymbol{\\theta}).\n\\]\nBeginning from the fact that \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\) where \\(\\boldsymbol{\\Theta} \\in \\mathbb{R}^k\\), a statistical hypothesis is a general statement about some parameter vector \\(\\boldsymbol{\\theta}\\) in regards to specific values contained in vector \\(\\boldsymbol{\\Theta}^*\\) such that\n\\[\n\\text{$H$: } \\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}^* \\quad \\text{where} \\quad \\boldsymbol{\\Theta}^* \\subset \\boldsymbol{\\Theta}.\n\\]\n\n\n\n\nDefinition of null hypothesis\n\n\nIn a hypothesis(s) testing, a null hypothesis is denoted by \\(H_0\\). The whole inferential process is designed to assess the strength of the evidence in favour or against this null hypothesis. In plain words, \\(H_0\\) is an inferential statement associated to the status quo in some population(s) or system(s) of interest, which might refer to no signal for the researcher in question.\nAgain, suppose random variable \\(Y\\) from some population(s) or system(s) of interest is governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nMoreover, we assume that random variable \\(Y\\) follows certain probability distribution \\(\\mathcal{D}(\\cdot)\\) in a generative model \\(m\\) as in\n\\[\n\\text{$m$: } Y \\sim \\mathcal{D}(\\boldsymbol{\\theta}).\n\\]\nLet \\(\\boldsymbol{\\Theta}_0 \\subset \\boldsymbol{\\theta}\\) denote the status quo for the parameter(s) to be tested. Then, the null hypothesis is mathematically defined as\n\\[\n\\text{$H_0$: } \\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}_0 \\quad \\text{where} \\quad \\boldsymbol{\\Theta}_0 \\subset \\boldsymbol{\\theta}.\n\\tag{2.76}\\]\n\n\n\n\nDefinition of alternative hypothesis\n\n\nIn a hypothesis testing, an alternative hypothesis is denoted by \\(H_1\\). This hypothesis corresponds to the complement (i.e., the opposite) of the null hypothesis \\(H_0\\). Since the whole inferential process is designed to assess the strength of the evidence in favour or against of \\(H_0\\), any inferential conclusion against \\(H_0\\) can be worded as “rejecting \\(H_0\\) in favour of \\(H_1\\).” In plain words, \\(H_1\\) is an inferential statement associated to a non-status quo in some population(s) or system(s) of interest, which might refer to actual signal for the researcher in question.\nLet us assume random variable \\(Y\\) from some population(s) or system(s) of interest is governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nMoreover, suppose random variable \\(Y\\) follows certain probability distribution \\(\\mathcal{D}(\\cdot)\\) in a generative model \\(m\\) as in\n\\[\n\\text{$m$: } Y \\sim \\mathcal{D}(\\boldsymbol{\\theta}).\n\\]\nLet \\(\\boldsymbol{\\Theta}_0^c \\subset \\boldsymbol{\\theta}\\) denote the non-status quo for the parameter(s) to be tested. Then, the alternative hypothesis is mathematically defined as\n\\[\n\\text{$H_1$: } \\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}_0^c \\quad \\text{where} \\quad \\boldsymbol{\\Theta}_0^c \\subset \\boldsymbol{\\theta}.\n\\tag{2.77}\\]\n\n\n\n\nDefinition of hypothesis testing\n\n\nA hypothesis testing is the decision rule we have to apply between the null and alternative hypotheses, via our sample data, to fail to reject or reject the null hypothesis.\n\n\n\n\nDefinition of type I error (false positive)\n\n\nType I error is defined as incorrectly rejecting the null hypothesis \\(H_0\\) in favour of the alternative hypothesis \\(H_1\\) when, in fact, \\(H_0\\) is true. Analogously, this type of error is also called false positive .\n\n\n\n\nDefinition of type II error (false negative)\n\n\nType II error is defined as incorrectly failing to reject the null hypothesis \\(H_0\\) in favour of the alternative hypothesis \\(H_1\\) when, in fact, \\(H_0\\) is false. Analogously, this type of error is also called false negative . Table 2.5 summarizes the types of inferential conclusions in function on whether \\(H_0\\) is true or not.\n\n\nTable 2.5: Types of inferential conclusions in a frequentist hypothesis testing.\n\n\n\n\n\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\nReject \\(H_0\\)\nType I error (False positive)\nCorrect\n\n\nFail to reject \\(H_0\\)\nCorrect\nType II error (False negative)\n\n\n\n\n\n\n\n\n\n\nDefinition of significance level\n\n\nThe significance level \\(\\alpha\\) is defined as the conditional probability of rejecting the null hypothesis \\(H_0\\) given that \\(H_0\\) is true. This can be mathematically represented as\n\\[\nP \\left( \\text{Reject $H_0$} | \\text{$H_0$ is true} \\right) = \\alpha.\n\\]\nIn plain words, \\(\\alpha \\in [0, 1]\\) allows us to probabilistically control for type I error since we are dealing with random variables in our inferential process. The significance level can be thought as one of the main hypothesis testing and power analysis settings.\n\n\n\n\nDefinition of power\n\n\nThe statistical power of a test \\(1 -\\beta\\) is the complement of the conditional probability \\(\\beta\\) of failing to reject the null hypothesis \\(H_0\\) given that \\(H_0\\) is false, which is mathematically represented as\n\\[\nP \\left( \\text{Failing to reject $H_0$} | \\text{$H_0$ is false} \\right) = \\beta;\n\\]\nyielding\n\\[\n\\text{Power} = 1 - \\beta.\n\\]\nIn plain words, \\(1 - \\beta \\in [0, 1]\\) is the probabilistic ability of our hypothesis testing to detect any signal in our inferential process, if there is any. The larger the power in our power analysis, the less prone we are to commit a type II error.\n\n\n\n\nDefinition of power analysis\n\n\nPower analysis is a set of statistical tools used to compute the minimum required sample size \\(n\\) for any given inferential study. These tools require the significance level, power, and effect size (i.e., the magnitude of the signal) the researcher aims to detect via their inferential study. This analysis seeks to determine whether observed results are likely due to chance or represent a true and meaningful effect.\n\n\nImage by Manfred Steger via Pixabay.\n\n\n\nThe larger the significance level in our power analysis and hypothesis testing, the less prone we are to commit a type I error.\n\n2.3.2 Hypotheses Definitions\n\n\n\n\n\nFigure 2.10: Hypotheses definitions substage from the classical-based hypothesis testing workflow in Figure 2.8. This substage is directly preceded by general settings and followed by test flavour and components.\n\n\n\n2.3.3 Test Flavour and Components\n\n\n\n\n\nFigure 2.11: Test flavour and components substage from the classical-based hypothesis testing workflow in Figure 2.8. This substage is directly preceded by hypotheses definitions and followed by inferential conclusions.\n\n\n\n\nDefinition of observed effect\n\n\nAn observed effect is the difference between the estimate provided the observed random sample (of size \\(n\\), as in \\(y_1, \\dots, y_n\\)) to the hypothesized value(s) of the population parameter(s) depicted in the statistical hypotheses.\n\n\n\n\nDefinition of standard error\n\n\nThe standard error allows us to quantify the extent to which an estimate coming from an observed random sample (of size \\(n\\), as in \\(y_1, \\dots, y_n\\)) may deviate from the expected value under the assumption that the null hypothesis is true.\nIt plays a critical role in determining whether an observed effect is likely attributable to random variation or represents a statistically significant finding. In the absence of the standard error, it would not be possible to rigorously assess the reliability or precision of an estimate.\n\n\nImage by Manfred Steger via Pixabay.\n\n\n\n\n\nDefinition of test statistic\n\n\nThe test statistic is a function of the random sample of size \\(n\\), i.e., it is in the function of the random variables \\(Y_1, \\dots, Y_n\\). Therefore, the test statistic will also be a random variable, whose observed value will describe how closely the probability distribution from which the random sample comes from matches the probability distribution of the null hypothesis \\(H_0\\).\nMore specifically, once we have obtained the observed effect and standard error from our observed random sample, we can compute the corresponding observed test statistic. This test statistic computation will be placed on the corresponding \\(x\\)-axis of the probability distribution of \\(H_0\\) so we can reject or fail to reject it accordingly.\n\n\nImage by Manfred Steger via Pixabay.\n\n\n\n\n2.3.4 Inferential Conclusions\n\n\n\n\n\nFigure 2.12: Inferential conclusions substage from the classical-based hypothesis testing workflow in Figure 2.8. This substage is directly preceded by rest flavour and components and followed by the corresponding delivery significance conclusion within the results stage of the data science workflow as shown in Figure 2.7.\n\n\n\n\nDefinition of critical value\n\n\nThe critical value of a hypothesis testing defines the region for which we might reject \\(H_0\\) in favour of \\(H_1\\). This critical value is in the function of the significance level \\(\\alpha\\) and test flavour. It is located on the corresponding \\(x\\)-axis of the probability distribution of \\(H_0\\). Hence, this value acts as a threshold to decide either of the following:\n\nIf the observed test statistic exceeds a given critical value, then we have enough statistical evidence to reject \\(H_0\\) in favour of \\(H_1\\).\nIf the observed test statistic does not exceed a given critical value, then we have enough statistical evidence to fail to reject \\(H_0\\).\n\n\n\n\n\nDefinition of \\(p\\)-value\n\n\nA \\(p\\)-value refers to the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic coming from our observed random sample of size \\(n\\). This \\(p\\)-value is obtained via the probability distribution of \\(H_0\\) and the observed test statistic.\nAlternatively to a critical value, we can reject or fail to reject the null hypothesis \\(H_0\\) using this \\(p\\)-value as follows:\n\nIf the \\(p\\)-value associated to the observed test statistic exceeds a given significance level \\(\\alpha\\), then we have enough statistical evidence to reject \\(H_0\\) in favour of \\(H_1\\).\nIf the \\(p\\)-value associated to the observed test statistic does not exceed a given significance level \\(\\alpha\\), then we have enough statistical evidence to fail to reject \\(H_0\\).\n\n\n\n\n\nDefinition of confidence interval\n\n\nA confidence interval provides an estimated range of values within which the true population parameter is likely to fall, based on the sample data. It reflects the degree of uncertainty associated with the obtained estimate. For instance, a 95% confidence interval means that if the study were repeated many times using different random samples from the same population or system of interest, approximately 95% of the resulting intervals would contain the true parameter.\n\n\nImage by Manfred Steger via Pixabay.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span>"
    ]
  },
  {
    "objectID": "book/02-stats-review.html#sec-chapter-2-summary",
    "href": "book/02-stats-review.html#sec-chapter-2-summary",
    "title": "2  Basic Cuisine: A Review on Probability and Frequentist Statistical Inference",
    "section": "\n2.4 Chapter Summary",
    "text": "2.4 Chapter Summary\n\n\n\n\nBellhouse, D. R. 2004. “The Reverend Thomas Bayes, FRS: A Biography to Celebrate the Tercentenary of His Birth.” Statistical Science 19 (1): 3–43. https://doi.org/10.1214/088342304000000189.\n\n\nBrent, Richard P. 1973. “Chapter 4: An Algorithm with Guaranteed Convergence for Finding a Zero of a Function.” In Algorithms for Minimization Without Derivatives. Englewood Cliffs, NJ: Prentice-Hall.\n\n\nCasella, G., and R. Berger. 2024. Statistical Inference. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://www.taylorfrancis.com/books/mono/10.1201/9781003456285/statistical-inference-roger-berger-george-casella.\n\n\nCauchy, Augustin-Louis. 1847. “Méthode Générale Pour La Résolution Des Systèmes d’équations Simultanées.” Comptes Rendus Hebdomadaires Des Séances de l’Académie Des Sciences 25: 536. https://gallica.bnf.fr/ark:/12148/bpt6k32298/f548.item.\n\n\nFletcher, Roger. 1987. Practical Methods of Optimization. 2nd ed. New York: John Wiley & Sons.\n\n\nJohnson, A. A., M. Q. Ott, and M. Dogucu. 2022. Bayes Rules!: An Introduction to Applied Bayesian Modeling. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://www.bayesrulesbook.com/.\n\n\nLeemis, Larry. n.d. “Univariate Distribution Relationship Chart.” https://www.math.wm.edu/~leemis/chart/UDR/UDR.html.\n\n\nLiu, Dong C., and Jorge Nocedal. 1989. “On the Limited Memory BFGS Method for Large Scale Optimization.” Mathematical Programming 45: 503–28. https://doi.org/10.1007/BF01589116.\n\n\nNewton, Isaac, and John Colson. 1736. Methodus Fluxionum Et Serierum Infinitarum. London: Printed by Henry Woodfall;; sold by John Nourse. https://www.loc.gov/item/42048007/.\n\n\nO’Donnell, T. 1936. History of Life Insurance in Its Formative Years. Compiled from Approved Sources by T. O’Donnell. Chicago.\n\n\nRaphson, Joseph. 1697. Analysis Æquationum Universalis. 2nd ed. London: Thomas Bradyll. https://doi.org/10.3931/e-rara-13516.\n\n\nSeabold, Skipper, and Josef Perktold. 2010. “Statsmodels: Econometric and Statistical Modeling with Python.” In 9th Python in Science Conference.\n\n\nSoch, Joram, The Book of Statistical Proofs, Maja, Pietro Monticone, Thomas J. Faulkenberry, Alex Kipnis, Kenneth Petrykowski, et al. 2024. “StatProofBook/StatProofBook.github.io: StatProofBook 2023.” Zenodo. https://doi.org/10.5281/zenodo.10495684.\n\n\nVirtanen, Pauli, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, et al. 2020. “SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python.” Nature Methods 17: 261–72. https://doi.org/10.1038/s41592-019-0686-2.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Basic Cuisine: A Review on Probability and Frequentist Statistical Inference</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html",
    "href": "book/03-ols.html",
    "title": "3  Ordinary Least-squares Regression",
    "section": "",
    "text": "3.1 Introduction\nWhen looking at data, we often want to know how different factors affect each other. For instance, if you have data on student finances, you might ask:\nOnce you have this financial data, the next step is to analyze it to find answers. One straightforward method for doing this is through regression analysis, and the simplest form is called Ordinary Least Squares (OLS).",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#introduction",
    "href": "book/03-ols.html#introduction",
    "title": "3  Ordinary Least-squares Regression",
    "section": "",
    "text": "How does having a job affect a student’s leftover money at the end of the month?\nWhat impact does receiving a monthly allowance have on their net savings?",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#what-is-ordinary-least-squares-ols",
    "href": "book/03-ols.html#what-is-ordinary-least-squares-ols",
    "title": "3  Ordinary Least-squares Regression",
    "section": "\n3.2 What is Ordinary Least Squares (OLS)?",
    "text": "3.2 What is Ordinary Least Squares (OLS)?\nOrdinary Least Squares (OLS) is a fundamental method in regression analysis for estimating the relationship between a dependent variable and one or more independent variables. In simple terms, OLS is like drawing the best straight line through a scatterplot of data points. Imagine you plotted students’ net savings on a graph, and each point represents a student’s financial outcome. OLS finds the line that best follows the trend of these points by minimizing the overall distance (error) between what the line predicts and what the actual data shows.\nOLS is widely used because it is:\n\n\nSimple: Easy to understand and compute.\n\nClear: Provides straightforward numbers (coefficients) that tell you how much each factor influences the outcome.\n\nVersatile: Applicable in many fields, from economics to social sciences, to help make informed decisions.\n\nIn this chapter, we will break down how OLS works in plain language, explore its underlying assumptions, and discuss its practical applications and limitations. This will give you a solid foundation in regression analysis, paving the way for more advanced techniques later on.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#the-best-line",
    "href": "book/03-ols.html#the-best-line",
    "title": "3  Ordinary Least-squares Regression",
    "section": "\n3.3 The “Best Line”",
    "text": "3.3 The “Best Line”\nWhen using Ordinary Least Squares (OLS) to fit a regression line, our goal is to find the line that best represents the relationship between our dependent variable \\(Y\\) and independent variable \\(X\\). But what does “best” mean?\nImagine you have a scatter plot of data points. Now, consider drawing two different lines through this plot. Each one of these lines represent a set of predictions. They also represent a way to represent the relationship between the dependent variable \\(Y\\) and independent variable \\(X\\)\n\n\nLine A (Blue): A line that follows the general trend of the data very well.\n\nLine B (Red): A line that doesn’t capture the trend as accurately.\n\n\n\nR Code\nPython Code\n\n\n\n# Sample data\nset.seed(42)\nX &lt;- c(1000, 1200, 1500, 1800, 2000)\nY &lt;- c(200, 230, 250, 290, 310)\n\n# Create a data frame\ndf &lt;- data.frame(Size = X, Price = Y)\n\n# Fit the correct OLS model\ncorrect_model &lt;- lm(Price ~ Size, data = df)\n\n# Create predictions for the two lines\ndf$Predicted_Correct &lt;- predict(correct_model, newdata = df)\ndf$Predicted_Wrong &lt;- 110 + 0.08 * df$Size  # Adjusted manually\n\n# Reshape data for ggplot (to add legend)\ndf_long &lt;- data.frame(\n  Size = rep(df$Size, 2),\n  Price = c(df$Predicted_Correct, df$Predicted_Wrong),\n  Line = rep(c(\"Line A (Best Fit)\", \"Line B (Worse Fit)\"), each = nrow(df))\n)\n\n# Store the plot with a legend\nlibrary(ggplot2)\nplot &lt;- ggplot() +\n  geom_point(data = df, aes(x = Size, y = Price), size = 3, color = \"black\") +\n  geom_line(data = df_long, aes(x = Size, y = Price, color = Line), linewidth = 1.2) +\n  scale_color_manual(values = c(\"Line A (Best Fit)\" = \"blue\", \"Line B (Worse Fit)\" = \"red\")) +\n  labs(title = \"Comparing Regression Line Fits\",\n       x = \"House Size (sq ft)\",\n       y = \"House Price (in $1000s)\",\n       color = \"Regression Line\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"),\n        legend.position = \"bottom\")\n\nplot\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\n\n# Sample data\nnp.random.seed(42)\nX = np.array([1000, 1200, 1500, 1800, 2000])\nY = np.array([200, 230, 250, 290, 310])\ndf = pd.DataFrame({'Size': X, 'Price': Y})\n\n# Fit the correct OLS model\nX_sm = sm.add_constant(df['Size'])\nmodel = sm.OLS(df['Price'], X_sm).fit()\ndf['Predicted_Correct'] = model.predict(X_sm)\n\n# Manually add the incorrect line\ndf['Predicted_Wrong'] = 110 + 0.08 * df['Size']\n\n# Reshape for plotting\ndf_long = pd.concat([\n    df[['Size', 'Predicted_Correct']].rename(columns={'Predicted_Correct': 'Price'}).assign(Line='Line A (Best Fit)'),\n    df[['Size', 'Predicted_Wrong']].rename(columns={'Predicted_Wrong': 'Price'}).assign(Line='Line B (Worse Fit)')\n])\n\n# Plot\nfig, ax = plt.subplots(figsize=(6, 4))\nax.scatter(df['Size'], df['Price'], color='black', label='Actual Data')\nfor label, group in df_long.groupby('Line'):\n    ax.plot(group['Size'], group['Price'], label=label)\nax.set_title(\"Comparing Regression Line Fits\", fontsize=14, fontweight='bold')\nax.set_xlabel(\"House Size (sq ft)\")\nax.set_ylabel(\"House Price (in $1000s)\")\nax.legend(title=\"Regression Line\", loc='lower right')\nplt.grid(True)\nplt.show()\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.1 Understanding Residuals\nFor each data point, the residual is the vertical distance between the actual \\(Y\\) value and the predicted \\(Y\\) value (denoted \\(\\hat{Y}\\)) on the line. In simple terms, it tells us how far off our prediction is for each point given the same \\(X\\) value. If a line fits well, these residuals will be small, meaning our predictions of the \\(Y\\) variable are close to the actual value.\nOLS quantifies how well a line fits the data by calculating the Sum of Squared Errors (SSE). The SSE is obtained by:\n\nComputing the residual for each data point.\nSquaring each residual (this ensures that errors do not cancel each other out).\nSumming all these squared values.\n\n\\[\nSSE=\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2\n\\]\nA lower SSE indicates a line that is closer to the actual data points. OLS chooses the best line by finding the one with the smallest SSE.\n\n3.3.2 Quantifying the Fit with SSE\nWe can compare the two lines by computing their SSE. The code below calculates and prints the SSE for each line:\n\n\nR Code\nPython Code\n\n\n\n# Calculate the Sum of Squared Errors for the correct model (Blue)\nsse_correct &lt;- sum((df$Price - df$Predicted_Correct)^2)\n\n# Calculate the Sum of Squared Errors for the manually adjusted model (Red)\nsse_wrong &lt;- sum((df$Price - df$Predicted_Wrong)^2)\n\n# Print the SSEs for each line\ncat(\"SSE for Best-Fit Line (Blue line):\", sse_correct, \"\\n\")\ncat(\"SSE for Worse-Fit Line (Red line):\", sse_wrong, \"\\n\")\n\n\n# Calculate the Sum of Squared Errors for the correct model (Blue)\nsse_correct = np.sum((df['Price'] - df['Predicted_Correct']) ** 2)\n\n# Calculate the Sum of Squared Errors for the manually adjusted model (Red)\nsse_wrong = np.sum((df['Price'] - df['Predicted_Wrong']) ** 2)\n\n# Print the SSEs for each line\nprint(f\"SSE for Best-Fit Line (Blue line): {sse_correct}\")\nprint(f\"SSE for Worse-Fit Line (Red line): {sse_wrong}\")\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\nSSE for Best-Fit Line (Blue line): 83.23529 \n\n\nSSE for Worse-Fit Line (Red line): 3972 \n\n\n\n\n\n\nSSE for Best-Fit Line (Blue line): 83.23529411764711\n\n\nSSE for Worse-Fit Line (Red line): 3972.0\n\n\n\n\n\nWhen you run this code, you’ll observe that the blue line (Line A) has a much lower SSE compared to the red line (Line B). This tells us that the blue line is a better fit for the data because its predictions are, on average, closer to the actual values.\nIn summary, OLS selects the “best line” by minimizing the sum of squared errors, ensuring that the total error between predicted and actual values is as small as possible.\n\n3.3.3 Why Squared Errors?\nWhen measuring how far off our predictions are, errors can be positive (if our prediction is too low) or negative (if it’s too high). If we simply added these errors together, they could cancel each other out, hiding the true size of the mistakes. By squaring each error, we convert all numbers to positive values so that every mistake counts.\nIn addition, squaring makes big errors count a lot more than small ones. This means that a large mistake will have a much bigger impact on the overall error, encouraging the model to reduce those large errors and improve its overall accuracy.\n\n3.3.4 The Mathematical Formulation of the OLS Model\nNow that we understand how OLS finds the best-fitting line by minimizing the differences between the actual and predicted values, let’s look at the math behind it.\nIn a simple linear regression with one predictor, we express the relationship between the outcome \\(Y\\) and the predictor \\(X\\) using the following equation. Note that OLS fits a straight line to the data, which is why the equation takes the familiar form of a straight line:\n\\[\nY=\\beta_0+\\beta_1X+\\epsilon\n\\]\nHere’s what each part of the equation means:\n\n\n\\(Y\\) is the dependent variable or the outcome we want to predict.\n\n\\(X\\) is the independent variable or the predictor that we believe influences \\(Y\\).\n\n\\(\\beta_0\\) is the intercept. It represents the predicted value of \\(Y\\) when \\(X=0\\).\n\n\\(\\beta_1\\) is the slope. It tells us how much \\(Y\\) is expected to change for each one-unit increase in \\(X\\).\n\n\\(\\epsilon\\) is the error term. It captures the random variation in \\(Y\\) that cannot be explained by \\(X\\).\n\nThis equation provides a clear mathematical framework for understanding how changes in \\(X\\) are expected to affect \\(Y\\), while also accounting for random variation. In the upcoming section, we will explore our toy dataset to showcase this equation and OLS in action.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#case-study-understanding-financial-behaviors",
    "href": "book/03-ols.html#case-study-understanding-financial-behaviors",
    "title": "3  Ordinary Least-squares Regression",
    "section": "\n3.4 Case Study: Understanding Financial Behaviors",
    "text": "3.4 Case Study: Understanding Financial Behaviors\nTo demonstrate Ordinary Least Squares (OLS) in action, we will walk through a case study using a toy dataset. This case study will help us understand the financial behaviors of students and identify the factors that influence their Net_Money, the amount of money left over at the end of each month. We will approach this case study using the data science workflow described in a previous chapter, ensuring a structured approach to problem-solving and model building.\n\n3.4.1 The Dataset\nOur dataset captures various aspects of students’ financial lives. Each row represents a student, and the columns describe different characteristics. Below is a breakdown of the variables:\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\nHas_Job\nWhether the student has a job (0 = No, 1 = Yes).\n\n\nYear_of_Study\nThe student’s current year of study (e.g., 1st year, 2nd year, etc.).\n\n\nFinancially_Dependent\nWhether the student is financially dependent on someone else (0 = No, 1 = Yes).\n\n\nMonthly_Allowance\nThe amount of financial support the student receives each month.\n\n\nCooks_at_Home\nWhether the student prepares their own meals (0 = No, 1 = Yes).\n\n\nLiving_Situation\nThe student’s living arrangement (e.g., living with family, in a shared apartment, etc.).\n\n\nHousing_Type\nThe type of housing the student lives in (e.g., rented, owned, dormitory).\n\n\nGoes_Out_Spends_Money\nHow frequently the student goes out and spends money (1 = rarely, 5 = very often).\n\n\nDrinks_Alcohol\nWhether the student drinks alcohol (0 = No, 1 = Yes).\n\n\nNet_Money\nThe amount of money the student has left at the end of the month after income and expenses.\n\n\nMonthly_Earnings\nThe student’s earnings from any part-time jobs or other income sources.\n\n\n\nHere’s a sample of the dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHas_Job\nYear_of_Study\nFinancially_Dependent\nMonthly_Allowance\nCooks_at_Home\nLiving_Situation\nHousing_Type\nGoes_Out_Spends_Money\nDrinks_Alcohol\nNet_Money\nMonthly_Earnings\n\n\n\n0\n1\n0\n658.99\n0\n3\n1\n6\n0\n529.34\n0.00\n\n\n1\n3\n0\n592.55\n0\n3\n2\n3\n1\n992.72\n941.92\n\n\n1\n4\n1\n602.54\n0\n2\n2\n2\n1\n557.30\n876.57\n\n\n\n\nThis dataset provides a structured way to analyze the financial habits of students and determine which factors contribute most to their financial stability.\n\n3.4.2 The Problem We’re Trying to Solve\nOur goal in this case study is to understand which factors impact a student’s net money. Specifically, we aim to identify which characteristics, such as having a job, monthly earnings, or financial support, explain why some students have more money left over at the end of the month than others.\nThe key question we want to answer is:\n\nWhich factors have the biggest influence on a student’s net money?\n\nBy applying OLS to this dataset, we can:\n\nMeasure how much each factor contributes to variations in net money. For example, we can determine the increase in net money associated with a one-unit increase in monthly earnings.\nIdentify whether each factor has a positive or negative effect on net money.\nUnderstand the unique contribution of each variable while accounting for the influence of others. This helps us isolate the effect of, say, having a job from that of receiving financial support.\nPredict a student’s net money based on their characteristics. These insights could help institutions design targeted financial literacy programs or interventions to improve financial stability.\nEvaluate the overall performance of our model using statistical measures such as R-squared and p-values. This not only confirms the significance of our findings but also guides improvements in future analyses.\n\nIn summary, using OLS in this case study allows us to break down complex financial behaviors into understandable components. This powerful tool provides clear, actionable insights into which factors are most important, paving the way for more informed decisions and targeted interventions.\n\n3.4.3 Study Design\nNow that we’ve introduced our case study and dataset, it’s time to follow the data science workflow step by step. The first step is to define the main statistical inquiries we want to address. As mentioned earlier, our key question is:\n\nWhich factors have the biggest influence on a student’s net money?\n\nTo answer this question, we will adopt an inferential analysis approach rather than a predictive analysis approach. Let’s quickly review the difference between these two methods:\nInferential vs. Predictive Analysis\n\n\nInferential Analysis explores and quantifies the relationships between explanatory variables (e.g., student characteristics) and the response variable (Net_Money). For example, we might ask: Does having a part-time job significantly affect a student’s net money, and by how much? The goal here is to understand these effects and assess their statistical significance.\n\nPredictive Analysis focuses on accurately forecasting the response variable using new data. In this case, the question could be: Can we predict a student’s net money based on factors like monthly earnings, living situation, and spending habits? The emphasis is on building a model that produces reliable predictions, even if it doesn’t fully explain the underlying relationships.\n\n3.4.4 Applying Study Design to Our Case Study\nFor our case study, we are interested in understanding how factors such as Has_Job, Monthly_Earnings, and Spending_Habits affect a student’s Net_Money. This leads us to adopt an inferential approach. We aim to answer questions like:\n\nDoes having a part-time job lead to significantly higher net money?\nHow much do a student’s monthly earnings influence their financial situation?\nDo spending habits, like going out frequently, decrease a student’s net money?\n\nUsing OLS, we will estimate the impact of each factor and determine whether these effects are statistically significant. This inferential analysis will help us understand which variables have the greatest influence on students’ financial outcomes.\nIf our goal were instead to predict a student’s future Net Money based on their characteristics, we would adopt a predictive approach. Although our focus here is on inference, it’s important to recognize that OLS is versatile and can be applied in both contexts.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#data-collection-and-wrangling",
    "href": "book/03-ols.html#data-collection-and-wrangling",
    "title": "3  Ordinary Least-squares Regression",
    "section": "\n3.5 Data Collection and Wrangling",
    "text": "3.5 Data Collection and Wrangling\nWith the statistical questions clearly defined, the next step is to ensure that the data is appropriately prepared for analysis. Although we already have the dataset, it is valuable to consider how this data could have been collected to better understand its context and potential limitations.\n\n3.5.1 Data Collection\nFor a study like ours, data on students’ financial behaviors could have been collected through various methods:\n\n\nSurveys: Students might have been asked about their employment status, earnings, and spending habits through structured questionnaires. While surveys can capture self-reported financial behaviors, they may suffer from recall bias or social desirability bias.\n\nAdministrative Data: Universities or employers may maintain records on student income and employment, providing a more objective source of financial information. However, access to such data may be limited due to privacy regulations.\n\nFinancial Tracking Apps: Digital financial management tools can offer detailed, real-time data on student income and spending patterns. While these apps provide high granularity, they may introduce selection bias, as only students who use such apps would be represented in the dataset.\n\nRegardless of the data collection method, each approach presents challenges, such as missing data, reporting errors, or sample biases. Addressing these issues is a critical aspect of data wrangling.\n\n3.5.2 Data Wrangling\nNow that our dataset is ready, the next step is to clean and organize it so that it’s in the best possible shape for analysis using OLS. Data wrangling involves several steps that ensure our data is accurate, consistent, and ready for modelling. Here are some key tasks:\nHandling Missing Data\nThe first task is to ensure data integrity by checking for missing values. Missing data can occur for various reasons, such as unrecorded responses or errors in data entry. When we find missing values—for example, if some students don’t have recorded earnings or net money—we must decide how to handle these gaps. Common strategies include:\n\n\nRemoving incomplete records: If the amount of missing data is minimal or missingness is random.\n\nImputing missing values: Using logical estimates or averages if missingness follows a systematic pattern.\n\nIn our toy dataset, there are no missing values, as confirmed by:\n\n\nR Code\nPython Code\n\n\n\n\ncolSums(is.na(data))\n\n              Has_Job         Year_of_Study Financially_Dependent \n                    0                     0                     0 \n    Monthly_Allowance         Cooks_at_Home      Living_Situation \n                    0                     0                     0 \n         Housing_Type Goes_Out_Spends_Money        Drinks_Alcohol \n                    0                     0                     0 \n            Net_Money      Monthly_Earnings \n                    0                     0 \n\n\n\n\n\n# Count missing values in each column\ndata.isna().sum()\n\nHas_Job                  0\nYear_of_Study            0\nFinancially_Dependent    0\nMonthly_Allowance        0\nCooks_at_Home            0\nLiving_Situation         0\nHousing_Type             0\nGoes_Out_Spends_Money    0\nDrinks_Alcohol           0\nNet_Money                0\nMonthly_Earnings         0\ndtype: int64\n\n\n\n\n\nEncoding Categorical Variables\nFor regression analysis, we need to convert categorical variables into numerical representations. In R, binary variables like Has_Job and Drinks_Alcohol should be transformed into factors so that the model correctly interprets them as categorical data rather than continuous numbers. For example:\n\n\nR Code\nPython Code\n\n\n\n\n# Convert binary categorical variables to factors\ndata &lt;- data |&gt;\n  mutate(Has_Job = as.factor(Has_Job),\n         Drinks_Alcohol = as.factor(Drinks_Alcohol),\n         Financially_Dependent = as.factor(Financially_Dependent),\n         Cooks_at_Home = as.factor(Cooks_at_Home))\n\n\n\n\n# Convert binary columns to categorical dtype\ncols_to_convert = [\"Has_Job\", \"Drinks_Alcohol\", \"Financially_Dependent\", \"Cooks_at_Home\"]\ndata[cols_to_convert] = data[cols_to_convert].astype(\"category\")\n\n\n\n\nDetecting and Handling Outliers\nOutliers in continuous variables like Monthly_Earnings and Net_Money can distort the regression analysis by skewing results. We use the Interquartile Range (IQR) method to identify these extreme values. Specifically, any observation falling below 1.5 times the IQR below the first quartile (Q1) or above 1.5 times the IQR above the third quartile (Q3) is flagged as an outlier. These outliers are then treated as missing values and removed:\n\n\nR Code\nPython Code\n\n\n\n\n# Using IQR method to filter out extreme values in continuous variables\nremove_outliers &lt;- function(x) {\n  Q1 &lt;- quantile(x, 0.25, na.rm = TRUE)\n  Q3 &lt;- quantile(x, 0.75, na.rm = TRUE)\n  IQR &lt;- Q3 - Q1\n  x[x &lt; (Q1 - 1.5 * IQR) | x &gt; (Q3 + 1.5 * IQR)] &lt;- NA\n  return(x)\n}\n\ndata &lt;- data |&gt;\n  mutate(across(c(Monthly_Earnings, Net_Money), remove_outliers))\n\n# Remove rows with newly introduced NAs due to outlier handling\ndata &lt;- na.omit(data)\n\n\n\n\n# Define the IQR outlier-removal function\ndef remove_outliers(series):\n    Q1 = series.quantile(0.25)\n    Q3 = series.quantile(0.75)\n    IQR = Q3 - Q1\n    return series.where((series &gt;= Q1 - 1.5 * IQR) & (series &lt;= Q3 + 1.5 * IQR))\n\n# Apply to specific continuous columns\ndata[\"Monthly_Earnings\"] = remove_outliers(data[\"Monthly_Earnings\"])\ndata[\"Net_Money\"] = remove_outliers(data[\"Net_Money\"])\n\n# Drop rows with any newly introduced NAs (from outliers)\ndata = data.dropna()\n\n\n\n\nSplitting the Data for Model Training\nTo ensure that our OLS model generalizes well to unseen data, we split the dataset into training and testing subsets. The training set is used to estimate the model parameters, and the testing set is used to evaluate the model’s performance. This split is typically done in an 80/20 ratio, as shown below:\n\n\nR Code\nPython Code\n\n\n\n\n# Splitting the dataset by row order: first 80% for training, last 20% for testing\nn &lt;- nrow(data)\nsplit_index &lt;- floor(0.8 * n)\ntrain_data &lt;- data[1:split_index, ]\ntest_data &lt;- data[(split_index + 1):n, ]\n\n\n\n\n# Splitting the dataset by row order: first 80% for training, last 20% for testing\nn = len(data)\nsplit_index = int(0.8 * n)\ntrain_data = data.iloc[:split_index]\ntest_data = data.iloc[split_index:]\n\n\n\n\nAlthough random sampling is generally preferred, since it helps ensure the training and testing sets are representative of the overall dataset, we deliberately split the data by row index here to produce consistent results across R and Python. This allows for reproducible comparisons between implementations in both languages.\nBy following these steps, checking for missing values, encoding categorical variables, handling outliers, and splitting the data, we ensure that our dataset is clean, well-organized, and ready for regression analysis using OLS.\nIt is important to note, however, that these are just a few of the many techniques available during the data wrangling stage. Depending on the dataset and the specific goals of your analysis, you might also consider additional strategies such as feature scaling, normalization, advanced feature engineering, handling duplicate records, or addressing imbalanced data. Each of these techniques comes with its own set of solutions, and the optimal approach will depend on the unique challenges and objectives of your case.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#exploratory-data-analysis-eda",
    "href": "book/03-ols.html#exploratory-data-analysis-eda",
    "title": "3  Ordinary Least-squares Regression",
    "section": "\n3.6 Exploratory Data Analysis (EDA)",
    "text": "3.6 Exploratory Data Analysis (EDA)\nBefore diving into data modelling, it is crucial to develop a deep understanding of the relationships between variables in the dataset. This stage, known as Exploratory Data Analysis (EDA), helps us visualize and summarize the data, uncover patterns, detect anomalies, and test key assumptions that will inform our modelling decisions.\n\n3.6.1 Classifying Variables\nThe first step in EDA is to classify variables according to their types. This classification guides the selection of appropriate visualization techniques and modelling strategies. In our toy dataset, we categorize variables as follows:\n\n\nNet_Money serves as the response variable, representing a continuous outcome constrained by realistic income and expenses.\n\nThe regressors include a mix of binary, categorical, ordinal, and continuous variables.\n\nBinary variables, such as Has_Job and Drinks_Alcohol, take on only two values and need to be encoded for modelling.\nCategorical variables, like Living_Situation and Housing_Type, represent qualitative distinctions between different student groups.\nSome predictors, like Year_of_Study and Goes_Out_Spends_Money, follow an ordinal structure, meaning they have a meaningful ranking but no consistent numerical spacing.\nFinally, Monthly_Allowance and Monthly_Earnings are continuous variables, requiring attention to their distributions and potential outliers.\n\nBy classifying variables correctly at the outset, we ensure that they are analyzed and interpreted appropriately throughout the modelling process.\n\n3.6.2 Visualizing Variable Distributions\nOnce variables are classified, the next step is to explore their distributions. Understanding how variables are distributed is crucial for identifying potential issues such as skewness, outliers, or missing values. We employ different visualizations depending on the variable type:\nContinuous Variables\nWe begin by examining continuous variables, which are best visualized using histograms and boxplots.\nHistograms\nHistograms display the frequency distribution of a continuous variable. They allow us to assess the overall shape, central tendency, and spread of the data. For example, the histogram of Net_Money helps us determine if the variable follows a roughly normal distribution or if it is skewed. A normal distribution often appears bell-shaped, while skewness can indicate that the data might benefit from transformations (like logarithmic transformations) to meet the assumptions of regression analysis. In our case, the histogram below shows that Net_Money appears roughly normal.\n\n\nR Code\nPython Code\n\n\n\n\n# Histogram of Net_Money\nhist(train_data$Net_Money, \n     main = \"Distribution of Net Money\", \n     xlab = \"Net Money\", \n     col = \"blue\", \n     border = \"white\")\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Histogram of Net_Money\nplt.hist(train_data[\"Net_Money\"].dropna(), bins=8, color=\"blue\", edgecolor=\"white\")\nplt.title(\"Distribution of Net Money\")\nplt.xlabel(\"Net Money\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBoxplots\nBoxplots provide a concise summary of a variable’s distribution by displaying its quartiles and highlighting potential outliers. Outliers are typically defined as data points that fall below \\(Q1 - 1.5 * IQR\\) or above \\(Q3 + 1.5 * IQR\\). The boxplot below visualizesNet_Money and helps us quickly assess if there are any extreme values that might skew the analysis. In this case, the boxplot suggests that there are no significant outliers according to the IQR method (the method commonly used by ggplot to identify outliers).\n\n\nR Code\nPython Code\n\n\n\n\n# Boxplot of Net Money\nboxplot(train_data$Net_Money, \n        main = \"Boxplot of Net Money\", \n        ylab = \"Net Money\", \n        col = \"lightblue\")\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(4, 6))\nax.boxplot(train_data[\"Net_Money\"].dropna(), patch_artist=True,\n           boxprops=dict(facecolor=\"lightblue\"));\nax.set_title(\"Boxplot of Net Money\")\nax.set_ylabel(\"Net Money\")\nax.set_xticks([1])\nax.set_xticklabels([\"\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBy visualizing the distribution of Net_Money with these plots, we gain valuable insights into its behavior. This understanding not only informs whether transformations are needed but also prepares us for deeper analysis as we move forward with regression modelling.\nCategorical and Ordinal Variables\nCategorical variables require a different approach from continuous ones because they represent distinct groups rather than numerical values. For these variables, bar charts are very effective. They display the frequency of each category, helping us understand the distribution of qualitative attributes.\nFor example, consider the variable Living_Situation. The bar chart below shows how many students fall into each category. From the chart, we can see that category 1 is more heavily represented, while categories 2 and 3 have roughly similar counts. This insight can be critical—if a category is underrepresented, you might need to consider grouping it with similar categories or applying techniques such as one-hot encoding to ensure that each category contributes appropriately to the model.\n\n\nR Code\nPython Code\n\n\n\n\n# Bar plot of Living Situation\nbarplot(table(train_data$Living_Situation), \n        main = \"Living Situation Distribution\", \n        xlab = \"Living Situation\", \n        ylab = \"Frequency\", \n        col = \"purple\")\n\n\n\n\n\n\n\n\n\n\n# Count occurrences of each category\nliving_counts = train_data[\"Living_Situation\"].value_counts()\n\n# Bar plot of Living Situation\nplt.bar(living_counts.index, living_counts.values, color=\"purple\")\nplt.title(\"Living Situation Distribution\")\nplt.xlabel(\"Living Situation\")\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation=45)  # Optional: rotate labels if they overlap\n\n(array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. ]), [Text(0.0, 0, '0.0'), Text(0.5, 0, '0.5'), Text(1.0, 0, '1.0'), Text(1.5, 0, '1.5'), Text(2.0, 0, '2.0'), Text(2.5, 0, '2.5'), Text(3.0, 0, '3.0'), Text(3.5, 0, '3.5'), Text(4.0, 0, '4.0')])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFor ordinal variables (which have a natural order but not a fixed numerical interval), you might still use bar charts to show the ranking or frequency of each level. Additionally, understanding these distributions can help you decide whether to treat them as categorical variables or convert them into numeric scores for analysis.\nExploring Relationships Between Variables\nBeyond examining individual variables, it is crucial to explore how they interact with one another, especially the predictors and the response variable. Understanding these relationships helps identify which predictors might be influential in the model and whether any issues, like multicollinearity, could affect regression estimates.\nCorrelation Matrices\nFor continuous variables, correlation matrices provide a numerical summary of how strongly pairs of variables are related. High correlations between predictors might signal multicollinearity, which can distort model estimates. For demonstration, consider the correlation matrix computed for Net_Money, Monthly_Allowance, and Monthly_Earnings:\n\n\nR Code\nPython Code\n\n\n\n\n# Correlation matrix\ncor_matrix &lt;- cor(train_data[, c(\"Net_Money\", \"Monthly_Allowance\", \"Monthly_Earnings\")], use = \"complete.obs\")\nprint(cor_matrix)\n\n                  Net_Money Monthly_Allowance Monthly_Earnings\nNet_Money         1.0000000         0.2975895        0.7525736\nMonthly_Allowance 0.2975895         1.0000000       -0.0319396\nMonthly_Earnings  0.7525736        -0.0319396        1.0000000\n\n\n\n\n\n# Select relevant columns and drop rows with missing values\ncor_matrix = train_data[[\"Net_Money\", \"Monthly_Allowance\", \"Monthly_Earnings\"]].corr(method=\"pearson\")\nprint(cor_matrix)\n\n                   Net_Money  Monthly_Allowance  Monthly_Earnings\nNet_Money           1.000000            0.29759          0.752574\nMonthly_Allowance   0.297590            1.00000         -0.031940\nMonthly_Earnings    0.752574           -0.03194          1.000000\n\n\n\n\n\nIn the output, we observe a strong positive correlation (corr = 0.757) between Monthly_Earnings and Net_Money. This result is intuitive. Higher earnings typically lead to more money left at the end of the month, resulting in a higher Net_Money.\nScatterplots\nScatter plots visually depict the relationship between two continuous variables. For example, plotting Monthly_Allowance against Net_Money helps us assess whether students with higher allowances tend to have higher or lower net savings. In the scatter plot below, a slightly positive trend is visible. However, the points are quite scattered, indicating that while there may be a relationship, it is not overwhelmingly strong. Such visual insights might prompt further investigation, perhaps considering polynomial transformations or interaction terms if nonlinearity is suspected.\n\n\nR Code\nPython Code\n\n\n\n\n# Scatter plot of Monthly Allowance vs. Net Money\nplot(train_data$Monthly_Allowance, train_data$Net_Money, \n     main = \"Net Money vs. Monthly Allowance\", \n     xlab = \"Monthly Allowance\", \n     ylab = \"Net Money\", \n     col = \"blue\", \n     pch = 19)\nabline(lm(Net_Money ~ Monthly_Allowance, data = train_data), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Scatter plot with regression line\nplt.figure(figsize=(6, 4))\nsns.regplot(\n    data=train_data,\n    x=\"Monthly_Allowance\",\n    y=\"Net_Money\",\n    scatter_kws={\"color\": \"blue\", \"s\": 40},  # s = point size\n    line_kws={\"color\": \"red\", \"linewidth\": 2}\n)\nplt.title(\"Net Money vs. Monthly Allowance\")\nplt.xlabel(\"Monthly Allowance\")\nplt.ylabel(\"Net Money\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBoxplots for Categorical Variables\nFor categorical predictors, boxplots are an excellent tool to compare the distribution of the response variable across different groups. For instance, examining how Net_Money varies by Living_Situation can reveal whether students in different living arrangements experience different financial outcomes. In the boxplot below, the distributions of Net_Money across categories of Living_Situation appear quite similar. This similarity may suggest that Living_Situation has little impact on Net_Money in our dataset.\n\n\nR Code\nPython Code\n\n\n\n\n# Boxplot of Net Money by Living Situation\nboxplot(Net_Money ~ Living_Situation, \n        data = train_data, \n        main = \"Net Money by Living Situation\", \n        xlab = \"Living Situation\", \n        ylab = \"Net Money\", \n        col = \"lightgreen\")\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Boxplot of Net Money by Living Situation\nplt.figure(figsize=(6, 4))\nsns.boxplot(\n    data=train_data,\n    x=\"Living_Situation\",\n    y=\"Net_Money\",\n    color=\"lightgreen\"\n)\nplt.title(\"Net Money by Living Situation\")\nplt.xlabel(\"Living Situation\")\nplt.ylabel(\"Net Money\")\nplt.xticks(rotation=45)  # Optional: rotate if labels overlap\n\n([0, 1, 2], [Text(0, 0, '1.0'), Text(1, 0, '2.0'), Text(2, 0, '3.0')])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSummary Statistics\nIn addition to visual exploration, descriptive statistics provide a numerical summary of the dataset that is especially useful for beginners. Summary statistics give you a snapshot of the central tendency and spread of your data, helping you quickly grasp its overall characteristics.\nFor instance, if you notice that the mean of Monthly_Earning is significantly higher than its median, it might suggest that a few high values (or outliers) are skewing the data.\n\n\nR Code\nPython Code\n\n\n\n\n# Summary statistics for numerical variables\nsummary(train_data[, c(\"Net_Money\", \"Monthly_Allowance\", \"Monthly_Earnings\")])\n\n   Net_Money        Monthly_Allowance Monthly_Earnings\n Min.   :-1719.38   Min.   :  51.33   Min.   :   0.0  \n 1st Qu.: -387.69   1st Qu.: 399.83   1st Qu.:   0.0  \n Median :   66.15   Median : 494.10   Median : 348.0  \n Mean   :  117.76   Mean   : 497.44   Mean   : 500.5  \n 3rd Qu.:  589.36   3rd Qu.: 592.96   3rd Qu.: 998.4  \n Max.   : 1932.42   Max.   :1088.94   Max.   :1839.9  \n\n\n\n\n\n# Summary statistics for numerical variables\ntrain_data[[\"Net_Money\", \"Monthly_Allowance\", \"Monthly_Earnings\"]].describe()\n\n\n\n\n\n\nNet_Money\nMonthly_Allowance\nMonthly_Earnings\n\n\n\ncount\n797.000000\n797.000000\n797.000000\n\n\nmean\n117.758595\n497.442424\n500.451432\n\n\nstd\n675.877166\n147.209448\n536.948133\n\n\nmin\n-1719.378062\n51.329604\n0.000000\n\n\n25%\n-387.687620\n399.833067\n0.000000\n\n\n50%\n66.145622\n494.103907\n347.994676\n\n\n75%\n589.360845\n592.956717\n998.419667\n\n\nmax\n1932.416524\n1088.935656\n1839.947244\n\n\n\n\n\n\n\n\n\n\n3.6.3 Key Takeaways from EDA\nConducting Exploratory Data Analysis (EDA) allows us to gain an initial understanding of the data and its underlying patterns before moving on to model building. Through EDA, we identify the types of variables present, examine their distributions, and uncover potential issues such as skewness, outliers, or multicollinearity. This process helps to highlight which variables might be strong predictors and which may require additional transformation or treatment. For instance, a strong correlation between two variables, like Monthly_Earnings and Net_Money, signals that earnings are likely a key driver of net savings. At the same time, observing differences in distributions or spotting similar patterns across groups in boxplots can inform us about the impact of categorical factors like Living_Situation.\nIt is important to remember that the insights gained from EDA are preliminary and primarily serve to inform further analysis. When we explore relationships between only two variables, we might overlook the influence of other factors, which could lead to misleading conclusions if taken in isolation. EDA is a crucial step for forming initial hypotheses and guiding decisions regarding data transformations, feature engineering, and the overall modelling strategy. With this foundation, we are better prepared to build a robust Ordinary Least Squares (OLS) regression model on data that has been carefully examined and understood.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#data-modelling",
    "href": "book/03-ols.html#data-modelling",
    "title": "3  Ordinary Least-squares Regression",
    "section": "\n3.7 Data Modelling",
    "text": "3.7 Data Modelling\nAfter conducting Exploratory Data Analysis (EDA), we transition to the modelling stage, where we apply a structured approach to uncover relationships between variables and predict outcomes. In this section, we focus on Ordinary Least Squares (OLS) regression, a widely used statistical technique for modelling linear relationships.\nOLS aims to estimate the effect of multiple predictors on an outcome variable by minimizing the sum of squared differences between observed and predicted values. This approach helps quantify financial behaviors, allowing us to interpret the impact of various factors on students’ net financial balance.\n\n3.7.1 Choosing a Suitable Regression Model\nThe choice of regression model depends on the patterns identified in EDA and the objectives of our analysis. Regression techniques vary in complexity, with some handling simple linear relationships and others accounting for more nuanced effects. Below are common approaches:\n\n\nSimple Linear Regression models the relationship between a single predictor and the response variable. This approach is suitable when we suspect a dominant factor driving financial balance.\n\nMultiple Linear Regression extends simple regression by incorporating multiple predictors, allowing us to account for various financial influences simultaneously.\n\nPolynomial Regression captures non-linear relationships by introducing polynomial terms of predictors, useful when relationships observed in scatter plots are curved rather than strictly linear.\n\nLog-Linear Models transform skewed distributions to improve interpretability and meet regression assumptions.\n\nRegularized Regression (Ridge and Lasso) applies penalties to regression coefficients to handle multicollinearity and enhance model generalization by reducing overfitting.\n\nGiven that our goal is to examine how multiple factors-such as income, expenses, and living arrangements—affect students’ financial balance, we select Multiple Linear Regression via OLS. This method allows us to quantify the influence of each predictor while controlling for confounding effects.\n\n3.7.2 Defining modelling Parameters\nOnce we select OLS regression, we define the key modelling components: the response variable (dependent variable) and the predictor variables (independent variables).\nResponse Variable (Y):\nThe response variable, also known as the dependent variable, represents the financial outcome we aim to explain:\n\n\nNet_Money: The dependent variable representing financial balance.\nPredictor Variables (X):\nEach predictor variable is chosen based on its theoretical and statistical relevance in explaining financial behavior:\n\n\nHas_Job (Binary) – Indicates whether the student has a job (1 = Yes, 0 = No).\n\nFinancially_Dependent (Binary) – Identifies students who rely on external financial support.\n\nYear_of_Study (Ordinal) – Represents academic seniority (higher values indicate later years).\n\nGoes_Out_Spends_Money (Ordinal) – Measures spending behavior on a scale from 1 to 6.\n\nDrinks_Alcohol (Binary) – Identifies whether a student consumes alcohol, which may impact discretionary spending.\n\nMonthly_Allowance (Continuous) – Represents financial support received from family or scholarships.\n\nMonthly_Earnings (Continuous) – Reflects the student’s personal income from work.\n\nLiving_Situation (Categorical) – Encodes different living arrangements (e.g., dormitory, shared apartment, living with family).\n\nHousing_Type (Categorical) – Further distinguishes between different types of housing situations.\n\nCooks_at_Home (Binary) – Indicates whether the student regularly prepares meals at home.\n\nThese predictors capture a mix of economic, behavioral, and lifestyle factors, providing a comprehensive view of the drivers of student financial balance.\n\n3.7.3 Setting Up the modelling Equation\nWith all predictors defined, the OLS regression equation models the relationship between Net_Money and the predictor variables:\n\n\\[\n\\begin{align*}\n\\text{Net\\_Money} &= \\beta_0 \\\\\n&\\quad + \\beta_1 \\times \\text{Has\\_Job} \\\\\n&\\quad + \\beta_2 \\times \\text{Financially\\_Dependent} \\\\\n&\\quad + \\beta_3 \\times \\text{Year\\_of\\_Study} \\\\\n&\\quad + \\beta_4 \\times \\text{Goes\\_Out\\_Spends\\_Money} \\\\\n&\\quad + \\beta_5 \\times \\text{Drinks\\_Alcohol} \\\\\n&\\quad + \\beta_6 \\times \\text{Monthly\\_Allowance} \\\\\n&\\quad + \\beta_7 \\times \\text{Monthly\\_Earnings} \\\\\n&\\quad + \\beta_8 \\times \\text{Living\\_Situation} \\\\\n&\\quad + \\beta_9 \\times \\text{Housing\\_Type} \\\\\n&\\quad + \\beta_{10} \\times \\text{Cooks\\_at\\_Home} \\\\\n&\\quad + \\epsilon\n\\end{align*}\n\\]\n\nwhere:\n\n\n\\(\\beta_0\\) represents the intercept, or the baseline Net_Money when all predictors are set to zero.\n\n\\(\\beta_1, \\beta_2, ..., \\beta_{10}\\) are the regression coefficients, quantifying the impact of each predictor on financial balance.\n\n\\(\\epsilon\\) is the error term, accounting for unexplained variability and random noise.\n\nEach coefficient provides insight into how Net_Money changes when a specific predictor increases by one unit, holding all other factors constant. For example:\n\n\n\\(\\beta_5\\) (Drinks_Alcohol) measures the financial impact of alcohol consumption, which may reflect higher discretionary spending.\n\n\\(\\beta_6\\) (Monthly_Allowance) quantifies the increase in Net_Money per additional dollar of allowance.\n\n\\(\\beta_10\\) (Cooks_at_Home) indicates how much more (or less) financially stable students are when they cook at home instead of eating out.\n\nIf significant interaction effects exist—such as students who live independently having a different financial impact from increased earnings compared to those living with family—we can extend the model by adding interaction terms.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#estimation",
    "href": "book/03-ols.html#estimation",
    "title": "3  Ordinary Least-squares Regression",
    "section": "\n3.8 Estimation",
    "text": "3.8 Estimation\nWith the data modelling stage completed, we now move to estimation, where we fit the Ordinary Least Squares (OLS) regression model to the data and obtain numerical estimates for the regression coefficients. These estimates quantify how much each predictor contributes to the response variable, allowing us to measure their individual effects on Net Money.\nThe goal of estimation is to determine the best-fitting regression line by minimizing the sum of squared residuals—the differences between the observed and predicted values. This step provides a mathematical basis for analyzing financial behaviors in students.\n\n3.8.1 Fitting the Model\nTo estimate the regression coefficients, we fit the OLS model to the training data using Python (statsmodels) or R (lm). The model is trained using least squares estimation, which finds the coefficients that minimize the total squared error between observed values and predictions.\nIn R, we can fit the regression model using the lm() function:\n\n\nR Code\nPython Code\n\n\n\n\n# Load necessary library\nlibrary(stats)\n\n# Fit the OLS model\nols_model &lt;- lm(Net_Money ~ Has_Job + Financially_Dependent + Year_of_Study + Goes_Out_Spends_Money + Drinks_Alcohol + Monthly_Allowance + Monthly_Earnings + Living_Situation + Housing_Type + Cooks_at_Home, data = train_data)\n\n# Display summary of model results\nsummary(ols_model)\n\n\nCall:\nlm(formula = Net_Money ~ Has_Job + Financially_Dependent + Year_of_Study + \n    Goes_Out_Spends_Money + Drinks_Alcohol + Monthly_Allowance + \n    Monthly_Earnings + Living_Situation + Housing_Type + Cooks_at_Home, \n    data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-752.76 -142.00    9.64  154.20  679.52 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            -647.24459   48.05092 -13.470  &lt; 2e-16 ***\nHas_Job1                 91.24439   39.55815   2.307   0.0213 *  \nFinancially_Dependent1 -488.10094   15.87713 -30.742  &lt; 2e-16 ***\nYear_of_Study          -100.14255    6.94097 -14.428  &lt; 2e-16 ***\nGoes_Out_Spends_Money   -47.85370    4.10987 -11.644  &lt; 2e-16 ***\nDrinks_Alcohol1        -139.96472   16.10396  -8.691  &lt; 2e-16 ***\nMonthly_Allowance         1.51624    0.05361  28.283  &lt; 2e-16 ***\nMonthly_Earnings          0.92083    0.03686  24.981  &lt; 2e-16 ***\nLiving_Situation        107.91555    9.57487  11.271  &lt; 2e-16 ***\nHousing_Type             57.77879    9.89696   5.838 7.72e-09 ***\nCooks_at_Home1          -86.39027   16.22054  -5.326 1.31e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 221.7 on 786 degrees of freedom\nMultiple R-squared:  0.8937,    Adjusted R-squared:  0.8924 \nF-statistic:   661 on 10 and 786 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nimport statsmodels.formula.api as smf\n\n# Fit the OLS model using a formula\nols_model = smf.ols(\n    formula=\"Net_Money ~ Has_Job + Financially_Dependent + Year_of_Study + Goes_Out_Spends_Money + Drinks_Alcohol + Monthly_Allowance + Monthly_Earnings + Living_Situation + Housing_Type + Cooks_at_Home\",\n    data=train_data\n).fit()\n\n# Display summary of model results\nprint(ols_model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              Net_Money   R-squared:                       0.894\nModel:                            OLS   Adj. R-squared:                  0.892\nMethod:                 Least Squares   F-statistic:                     661.0\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):               0.00\nTime:                        20:13:03   Log-Likelihood:                -5430.3\nNo. Observations:                 797   AIC:                         1.088e+04\nDf Residuals:                     786   BIC:                         1.093e+04\nDf Model:                          10                                         \nCovariance Type:            nonrobust                                         \n================================================================================================\n                                   coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------------\nIntercept                     -647.2446     48.051    -13.470      0.000    -741.568    -552.921\nHas_Job[T.1.0]                  91.2444     39.558      2.307      0.021      13.592     168.897\nFinancially_Dependent[T.1.0]  -488.1009     15.877    -30.742      0.000    -519.268    -456.934\nDrinks_Alcohol[T.1.0]         -139.9647     16.104     -8.691      0.000    -171.577    -108.353\nCooks_at_Home[T.1.0]           -86.3903     16.221     -5.326      0.000    -118.231     -54.550\nYear_of_Study                 -100.1425      6.941    -14.428      0.000    -113.768     -86.518\nGoes_Out_Spends_Money          -47.8537      4.110    -11.644      0.000     -55.921     -39.786\nMonthly_Allowance                1.5162      0.054     28.283      0.000       1.411       1.621\nMonthly_Earnings                 0.9208      0.037     24.981      0.000       0.848       0.993\nLiving_Situation               107.9155      9.575     11.271      0.000      89.120     126.711\nHousing_Type                    57.7788      9.897      5.838      0.000      38.351      77.206\n==============================================================================\nOmnibus:                        4.197   Durbin-Watson:                   1.944\nProb(Omnibus):                  0.123   Jarque-Bera (JB):                4.115\nSkew:                          -0.138   Prob(JB):                        0.128\nKurtosis:                       3.218   Cond. No.                     5.14e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.14e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\n\n\n3.8.2 Interpreting the Coefficients\nAfter fitting the model, we examine the estimated coefficients to understand their impact. Each coefficient obtained from the OLS regression represents the expected change in Net_Money for a one-unit increase in the corresponding predictor, holding all other variables constant. The estimated regression equation can be expressed as:\n\n\\[\n\\begin{align*}\n\\text{Net\\_Money} &= -647.24 \\\\\n&\\quad + 91.24 \\times \\text{Has\\_Job} \\\\\n&\\quad - 488.10 \\times \\text{Financially\\_Dependent} \\\\\n&\\quad - 100.14 \\times \\text{Year\\_of\\_Study} \\\\\n&\\quad - 47.85 \\times \\text{Goes\\_Out\\_Spends\\_Money} \\\\\n&\\quad - 139.96 \\times \\text{Drinks\\_Alcohol} \\\\\n&\\quad + 1.52 \\times \\text{Monthly\\_Allowance} \\\\\n&\\quad + 0.92 \\times \\text{Monthly\\_Earnings} \\\\\n&\\quad + 107.92 \\times \\text{Living\\_Situation} \\\\\n&\\quad + 57.78 \\times \\text{Housing\\_Type} \\\\\n&\\quad - 86.39 \\times \\text{Cooks\\_at\\_Home} \\\\\n&\\quad + \\epsilon\n\\end{align*}\n\\]\n\nFor example:\n\nThe intercept (\\(\\beta_0 = -647.24\\)) represents the expected financial balance for a student in the reference group—i.e., a student who has no job, is not financially dependent, does not go out to spend money or drink alcohol, receives no allowance or earnings, and is in the baseline category for all categorical variables.\nA 1 dollar increase in Monthly_Allowance (\\(\\beta\\) = 1.52$) is associated with a $1.52 increase in Net_Money, suggesting that students with larger allowances tend to have a higher financial balance, all else being equal.\n\nThese estimates provide an initial understanding of the direction and magnitude of relationships between predictors and financial balance. However, before drawing conclusions, we need to validate model assumptions and evaluate the statistical significance of each coefficient.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#goodness-of-fit",
    "href": "book/03-ols.html#goodness-of-fit",
    "title": "3  Ordinary Least-squares Regression",
    "section": "\n3.9 Goodness of Fit",
    "text": "3.9 Goodness of Fit\nAfter estimating the regression coefficients, the next step is to assess how well the model fits the data and whether it satisfies the assumptions of Ordinary Least Squares (OLS) regression. This evaluation ensures that the model is not only statistically valid but also generalizes well to unseen data. A well-fitting model should explain a substantial proportion of variation in the response variable while adhering to key statistical assumptions. If these assumptions are violated, model estimates may be biased, leading to misleading conclusions.\n\n3.9.1 Checking Model Assumptions\nOLS regression is built on several fundamental assumptions:\n\nlinearity\nindependence of errors\nhomoscedasticity\nnormality of residuals\n\nIf these assumptions hold, OLS provides unbiased, efficient, and consistent estimates. We assess each assumption through diagnostic plots and statistical tests.\nLinearity\nA core assumption of OLS is that the relationship between each predictor and the response variable is linear. If this assumption is violated, the model may systematically under- or overestimate Net_Money, leading to biased predictions. The Residuals vs. Fitted values plot is a common diagnostic tool for checking linearity. In a well-specified linear model, residuals should be randomly scattered around zero, without any discernible patterns. If the residuals exhibit a U-shaped or curved pattern, this suggests a non-linear relationship, indicating that transformations such as logarithmic, square root, or polynomial terms may be necessary.\nTo visualize linearity, we plot the residuals against the fitted values:\n\n\nR Code\nPython Code\n\n\n\n\n# Residuals vs Fitted plot (R)\nplot(ols_model$fitted.values, residuals(ols_model), \n     main = \"Residuals vs Fitted\", xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Extract fitted values and residuals\nfitted_vals = ols_model.fittedvalues\nresiduals = ols_model.resid\n\n# Plot Residuals vs Fitted\nplt.figure(figsize=(6, 4))\nplt.scatter(fitted_vals, residuals, alpha=0.7)\nplt.axhline(y=0, color='red', linestyle='--')\nplt.title(\"Residuals vs Fitted\")\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIf the residual plot displays a clear trend, polynomial regression or feature engineering may be required to better capture the underlying data structure. However, in this case, the residuals appear to be randomly scattered around the horizontal axis, with no obvious patterns such as curves or systematic structure. This suggests that the linearity assumption of the OLS model holds reasonably well in this case. Therefore, no immediate transformation or addition of nonlinear terms is necessary to capture the relationship between predictors and the response variable.\nIndependence of Errors\nThe residuals, or errors, in an OLS model should be independent of one another. This assumption is particularly relevant in time-series or sequential data, where errors from one observation might influence subsequent observations, leading to autocorrelation. If the errors are correlated, the estimated standard errors will be biased, making hypothesis testing unreliable.\nThe Durbin-Watson test is commonly used to detect autocorrelation. This test produces a statistic that ranges between 0 and 4, where values close to 2 indicate no significant autocorrelation, while values near 0 or 4 suggest positive or negative correlation in the residuals.\n\n\nR Code\nPython Code\n\n\n\n\ndwtest(ols_model)\n\n\n    Durbin-Watson test\n\ndata:  ols_model\nDW = 1.9437, p-value = 0.2133\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n\n\nfrom statsmodels.stats.stattools import durbin_watson\n\n# Durbin-Watson test for autocorrelation of residuals\ndw_stat = durbin_watson(ols_model.resid)\n\nprint(f\"Durbin-Watson statistic: {dw_stat:.3f}\")\n\nDurbin-Watson statistic: 1.944\n\n\n\n\n\nBased on the Durbin-Watson test, the test statistic is 1.9437 with a p-value of 0.2133. Since the statistic is close to 2 and the p-value is not statistically significant, we do not find evidence of autocorrelation in the residuals. This suggests that the assumption of independence of errors holds for this model.\nIf the test suggests autocorrelation, a possible solution is to use time-series regression models such as Autoregressive Integrated Moving Average (ARIMA) or introduce lagged predictors to account for dependencies in the data.\nHomoscedasticity (Constant Variance of Errors)\nOLS regression assumes that the variance of residuals remains constant across all fitted values. If this assumption is violated, the model exhibits heteroscedasticity, where the spread of residuals increases or decreases systematically. This can result in inefficient coefficient estimates, making some predictors appear statistically significant when they are not.\nTo check for heteroscedasticity, we plot residuals against the fitted values and conduct a Breusch-Pagan test, which formally tests whether residual variance is constant.\n\n\nR Code\nPython Code\n\n\n\n\nbptest(ols_model)  # Uses all regressors, like Python\n\n\n    studentized Breusch-Pagan test\n\ndata:  ols_model\nBP = 14.381, df = 10, p-value = 0.1563\n\n\n\n\n\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n\n# Extract residuals and design matrix from the fitted model\nresiduals = ols_model.resid\nexog = ols_model.model.exog  # independent variables (with intercept)\n\n# Perform Breusch-Pagan test\nbp_test = het_breuschpagan(residuals, exog)\n\n# Unpack results\nbp_stat, bp_pvalue, empty, empty = bp_test\n\nprint(f\"Breusch-Pagan test statistic: {bp_stat:.3f}\")\n\nBreusch-Pagan test statistic: 14.381\n\nprint(f\"P-value: {bp_pvalue:.4f}\")\n\nP-value: 0.1563\n\n\n\n\n\nSince the p-value (0.1563) is greater than the conventional threshold of 0.05, we fail to reject the null hypothesis of constant variance. This indicates no significant evidence of heteroscedasticity, and thus the assumption of homoscedasticity appears to hold for this model.\nIf heteroscedasticity is detected, solutions include applying weighted least squares (WLS) regression, transforming the dependent variable (e.g., using a log transformation), or computing robust standard errors to correct for variance instability.\nNormality of Residuals\nFor valid hypothesis testing and confidence interval estimation, OLS assumes that residuals follow a normal distribution. If residuals deviate significantly from normality, statistical inference may be unreliable, particularly for small sample sizes.\nA Q-Q plot (Quantile-Quantile plot) is used to assess normality. If residuals are normally distributed, the points should lie along the reference line.\n\n\nR Code\nPython Code\n\n\n\n\nqqnorm(residuals(ols_model))\nqqline(residuals(ols_model), col = \"red\")\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\n# Extract residuals from the model\nresiduals = ols_model.resid\n\n# Q-Q plot of residuals\nplt.figure(figsize=(6, 4))\nempty = stats.probplot(residuals, dist=\"norm\", plot=plt)  # Suppress output by assigning to _\nplt.title(\"Q-Q Plot of Residuals\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe Q-Q plot shows that the residuals closely follow the 45-degree reference line, indicating that the residuals are approximately normally distributed. While there are some mild deviations at the tails, these are minimal and do not suggest serious violations of the normality assumption.\nIf the plot reveals heavy tails or skewness, potential solutions include applying log or Box-Cox transformations to normalize the distribution. In cases where normality is severely violated, using a non-parametric model or bootstrapping confidence intervals may be appropriate.\n\n3.9.2 Evaluating Model Fit\nA good model should explain a large proportion of variance in the response variable.\nR-Squared\nBeyond checking assumptions, it is essential to assess how well the model explains variability in the response variable. One of the most commonly used metrics is R-Squared (\\(R^2\\)), which measures the proportion of variance in Net_Money that is explained by the predictors. An \\(R^2\\) value close to 1 indicates a strong model fit, whereas a low value suggests that important predictors may be missing or that the model is poorly specified.\nWe can retrieve the R-squared and Adjusted R-squared values from the model summary:\n\n\nR Code\nPython Code\n\n\n\n\nsummary(ols_model)$r.squared  # R-squared value\n\n[1] 0.8937252\n\nsummary(ols_model)$adj.r.squared  # Adjusted R-squared\n\n[1] 0.8923731\n\n\n\n\n\n# R-squared\nr_squared = ols_model.rsquared\n\n# Adjusted R-squared\nadj_r_squared = ols_model.rsquared_adj\n\n# Print values\nprint(f\"R-squared: {r_squared:.4f}\")\n\nR-squared: 0.8937\n\nprint(f\"Adjusted R-squared: {adj_r_squared:.4f}\")\n\nAdjusted R-squared: 0.8924\n\n\n\n\n\nIn this model, the R-squared is 0.894 and the adjusted R-squared is 0.892, indicating that around 89% of the variance in Net_Money is explained by the model.\nHowever, while \\(R^2\\) provides insight into model fit, it has limitations. Adding more predictors will always increase \\(R^2\\), even if those predictors have little explanatory power. That’s why Adjusted R-squared is often preferred, as it adjusts for the number of predictors and only increases when a new variable meaningfully improves the model.\nFinally, a high \\(R^2\\) should not be interpreted as evidence of causation, nor does it guarantee the model is free from issues like omitted variable bias or multicollinearity. Always complement goodness-of-fit metrics with residual diagnostics and statistical inference to ensure model reliability.\nIdentifying Outliers and Influential Points\nOutliers and influential observations can distort regression estimates, making it crucial to detect and address them appropriately. One way to identify extreme residuals is through residual plots, where large deviations from zero may indicate problematic data points.\n\n\nR Code\nPython Code\n\n\n\n\nplot(residuals(ols_model), main = \"Residual Plot\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Extract residuals\nresiduals = ols_model.resid\n\n# Residual plot (residuals vs observation index)\nplt.figure(figsize=(6, 4))\nplt.plot(residuals, marker='o', linestyle='none', alpha=0.7)\nplt.axhline(y=0, color='red', linestyle='--')\nplt.title(\"Residual Plot\")\nplt.xlabel(\"Observation Index\")\nplt.ylabel(\"Residuals\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIn the residual plot above, the residuals appear to be evenly and randomly scattered around zero, with no clear pattern or extreme values. This suggests that there are no obvious outliers or highly influential observations in the dataset. The model residuals behave as expected, reinforcing the assumption that the data points do not exert undue influence on the regression fit.\nAnother important diagnostic tool is Cook’s Distance, which measures the influence of each observation on the regression results. Data points with Cook’s Distance values greater than 0.5 may significantly impact model estimates.\n\n\nR Code\nPython Code\n\n\n\n\ncook_values &lt;- cooks.distance(ols_model)\nplot(cook_values, type = \"h\", main = \"Cook's Distance\")\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Get Cook's distance values\ninfluence = ols_model.get_influence()\ncooks_d = influence.cooks_distance[0]  # Values\n\n# Plot Cook's distance\nfig, ax = plt.subplots(figsize=(6, 4))\nmarkerline, stemlines, baseline = ax.stem(cooks_d, markerfmt=\",\")\nax.set_title(\"Cook's Distance\")\nax.set_xlabel(\"Observation Index\")\nax.set_ylabel(\"Cook's Distance\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIn the plot above, all observations have Cook’s Distance values well below the 0.5 threshold. This indicates that no individual data point has an undue influence on the model’s estimates.\nIf influential points are identified, the next steps involve investigating data quality, testing robust regression techniques, or applying Winsorization, which involves replacing extreme values with more moderate ones to reduce their impact.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#results",
    "href": "book/03-ols.html#results",
    "title": "3  Ordinary Least-squares Regression",
    "section": "\n3.10 Results",
    "text": "3.10 Results\nAfter validating the goodness of fit, we now assess how well the model performs in both predictive analysis and inferential analysis. This step involves using the trained model to generate predictions on unseen data and evaluating how well it generalizes beyond the training set. Additionally, we analyze the estimated regression coefficients to draw meaningful conclusions about student financial behaviors.\n\n3.10.1 Predictive Analysis\nA key objective of regression modelling is to generate reliable predictions. To assess how well our model generalizes, we apply it to the test dataset—a portion of the original data that was not used for model training. If the model’s predictions align closely with actual outcomes, we can conclude that it has strong predictive power.\nIn R, we use the predict() function to apply the trained OLS model to the test set. In Python, we use the predict() method from the statsmodels library after ensuring the test data is properly formatted.\n\n\nR Code\nPython Code\n\n\n\n\n# Generate predictions on the test set\ny_pred &lt;- predict(ols_model, newdata=test_data)\n\n\n\n\nimport statsmodels.api as sm\n\n# Prepare the test data: add constant term to match training\nX_test = test_data[[\n    \"Has_Job\", \"Financially_Dependent\", \"Year_of_Study\",\n    \"Goes_Out_Spends_Money\", \"Drinks_Alcohol\", \"Monthly_Allowance\",\n    \"Monthly_Earnings\", \"Living_Situation\", \"Housing_Type\", \"Cooks_at_Home\"\n]]\nX_test = sm.add_constant(X_test)  # Add intercept term\n\n# Generate predictions\ny_pred = ols_model.predict(X_test)\n\n\n\n\nOnce predictions are generated, we evaluate their accuracy using common regression error metrics.\nPerformance Metrics\nModel accuracy is assessed using four standard error metrics:\n\n\nMean Absolute Error (MAE) measures the average absolute differences between predicted and actual values. A lower MAE indicates better model accuracy.\n\nMean Squared Error (MSE) calculates the average squared differences between predicted and actual values, penalizing larger errors more heavily.\n\nRoot Mean Squared Error (RMSE) is the square root of MSE, making it easier to interpret since it retains the same units as the dependent variable (Net_Money).\n\nR-squared (\\(R^2\\)) quantifies the proportion of variance in Net_Money explained by the model. A higher \\(R^2\\) value indicates better model performance.\n\nThese metrics can be computed as follows:\n\n\nR Code\nPython Code\n\n\n\n\n# Extract response variable from test data\ny_test &lt;- test_data$Net_Money\n\n# Calculate metrics in R\nmae &lt;- mean(abs(y_test - y_pred))\nmse &lt;- mean((y_test - y_pred)^2)\nrmse &lt;- sqrt(mse)\nr2 &lt;- summary(ols_model)$r.squared\n\ncat(sprintf(\"MAE: %.2f, MSE: %.2f, RMSE: %.2f, R-squared: %.2f\", mae, mse, rmse, r2))\n\nMAE: 175.48, MSE: 45299.10, RMSE: 212.84, R-squared: 0.89\n\n\n\n\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport numpy as np\n\n# Extract the true values\ny_test = test_data[\"Net_Money\"]\n\n# Calculate metrics\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\n# Print results\nprint(f\"MAE: {mae:.2f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}, R-squared: {r2:.2f}\")\n\nMAE: 175.48, MSE: 45299.10, RMSE: 212.84, R-squared: 0.91\n\n\n\n\n\nIf the RMSE is significantly larger than the MAE, it suggests that the model is highly sensitive to large prediction errors, meaning that certain extreme values are having a disproportionate impact on the model’s performance. Similarly, a low R-squared value may indicate that important predictors are missing from the model or that the relationship is more complex than a simple linear pattern can capture.\nIn this case, the model’s performance metrics are encouraging. With a MAE of 175.48, the model’s predictions deviate from actual values by about $175 on average. The RMSE of 212.84, which is not drastically higher than the MAE, suggests that large errors are present but not excessively dominant. An R-squared value of 0.89 confirms that the model explains a substantial portion of the variance in Net_Money, indicating strong predictive performance and a solid overall model fit.\n\n3.10.2 Inferential Analysis\nBeyond prediction, OLS regression allows us to interpret the estimated coefficients to uncover patterns in students’ financial behaviors. Each coefficient represents the expected change in Net_Money for a one-unit increase in the corresponding predictor, assuming all other variables remain constant.\nInsights from Regression Coefficients\nHere are a few insights that we can extract from the regression model result:\n\nFinancial dependency is one of the strongest negative predictors of financial balance. Students who rely on others—such as parents or guardians—for financial support tend to have $488.10 less in Net_Money. This may be due to having limited personal income, reduced autonomy in managing expenses, or tighter financial constraints imposed by dependency on external sources.\nSpending habits, particularly related to social activities, also play a crucial role. Students who report going out and spending money more frequently experience a $47.85 decrease in Net_Money for each unit increase in this behavior. This result aligns with expectations: frequent socializing often involves discretionary expenses such as dining out or entertainment, which can erode savings or disposable income over time.\nAnother lifestyle factor, alcohol consumption, is associated with a $139.96 drop in Net_Money. This relatively large effect suggests that students who drink alcohol regularly may also engage in broader spending behaviors linked to nightlife or entertainment, significantly impacting their overall financial standing. It also highlights alcohol as a strong proxy for costly social habits.\nFinally, the student’s living situation is associated with financial advantage. Certain types of living arrangements correspond to an increase of $107.92 in Net_Money. This may reflect the financial benefits of shared housing, university-subsidized residences, or lower-cost arrangements. Choosing a cost-effective living setup appears to have a notable influence on students’ financial outcomes.\n\nThese findings suggest that students can improve their financial position through a combination of employment, controlled social spending, and strategic housing choices. Conversely, financial dependency and certain lifestyle habits like frequent alcohol use and going out may contribute to lower net balances. These insights can inform both personal budgeting decisions and institutional support strategies for student financial wellness.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#storytelling",
    "href": "book/03-ols.html#storytelling",
    "title": "3  Ordinary Least-squares Regression",
    "section": "\n3.11 Storytelling",
    "text": "3.11 Storytelling\nThe final step in our data science workflow is storytelling, where we translate our analytical findings into actionable insights. This stage ensures that our results are clearly understood by both technical and non-technical audiences. Effective storytelling involves summarizing insights, using visuals for clarity, and making data-driven recommendations.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/03-ols.html#practice-problems",
    "href": "book/03-ols.html#practice-problems",
    "title": "3  Ordinary Least-squares Regression",
    "section": "\n3.12 Practice Problems",
    "text": "3.12 Practice Problems\nIn this final section, you can test your understanding with the following conceptual interactive exercises, and then proceed to work through an inferential analysis of a simple dataset.\n\n\n3.12.1 Conceptual Questions\n\nWhy does OLS minimize the sum of squared residuals instead of just the sum of residuals? What problem would arise if we used absolute errors?\n\nClick to reveal answer\n\nSquaring residuals ensures errors don’t cancel out, and penalizes large errors more. Absolute errors lack differentiability for optimization.\n\nIn the house price example (Figure 3.1), if Line A has an SSE of 83 and Line B has an SSE of 3972, which line would OLS choose? Why?\n\nClick to reveal answer\n\nLine A, as lower SSE implies a better fit. OLS minimizes SSE.\n\nCould a regression line ever have an SSE of zero? What would that imply about the data?\n\nClick to reveal answer\n\nYes, if all points lie exactly on the line. This would indicate a perfect fit with no noise, which is highly unlikely with real data.\n\nIn the student finance dataset, EDA reveals a strong correlation between Monthly_Earnings and Net_Money. Can we conclude that large monthly earnings lead to the students have greater net money?\n\nClick to reveal answer\n\nNo, since Correlation \\(\\neq\\) causation. Omitted variables may link earnings to savings.\n\nIf a histogram of Net_Money showed extreme right skewness, how could this violate OLS assumptions? What transformation could be done to address this?\n\nClick to reveal answer\n\nRight skew violates normality, a log transformation could help resolve this.\n\nWhy did we convert binary variables (such as Has_Job) to factors in R? What could happen if they were left as numeric 1s and 0s?\n\nClick to reveal answer\n\nFactors ensure correct interpretation as categories (not continuous variables). Numeric 0/1 could work but increases risks of misspecification.\n\nThe regression equation includes Living_Situation as a categorical predictor. How would the coefficient for Living_Situation=2 be interpreted compared to the baseline category?\n\nClick to reveal answer\n\nCompared to baseline, Living_Situation=2 predicts $107.92 higher Net_Money, holding other variables constant.\n\nThe coefficient for Drinks_Alcohol is −$139.96. How would you explain this in simple terms to a university administrator advocating for alcohol-free campuses?\n\nClick to reveal answer\n\nWe could say something along the lines of: “Students who drink alcohol average $140 fewer savings. Policies reducing drinking may improve students’ financial well-being.”\n\nIf Monthly_Allowance had been measured in hundreds of dollars (6.58 instead of 658.99), how would the coefficient of Monthly_Allowance change?\n\nClick to reveal answer\n\nThe coefficient simply scales from 1.52 to 152.\n\nThe Breusch-Pagan test for heteroscedasticity returns a p-value of 0.16. What does this imply about the residuals?\n\nClick to reveal answer\n\np &gt; 0.05 implies no heteroscedasticity.\n\nIn the Q-Q p plot shown in 3.9.1, if the residuals deviated sharply from the red line at the tails, what would this suggest? How might you address it?\n\nClick to reveal answer\n\nThis would suggest non-normally distributed tails. You could consider transformations (such as logging) or robust regression techniques.\n\nThe model’s \\(R^2\\) is 0.89. What might a high \\(R^2\\) indicate? Is a higher \\(R^2\\) always better?\n\nClick to reveal answer\n\nA high \\(R^2\\) may hide overfitting or omitted variables. It is important to also check residuals for the full picture.\n\nThe coefficient for Financially_Dependent is −$488.10. Propose a few real-world mechanisms that could explain this relationship.\n\nClick to reveal answer\n\nDependent students earn less; (2) Dependents may have stricter budgets\n\nIf you suspected that the effect of Monthly_Earnings on Net_Money differs for students who Cook_at_Home, how would you modify the regression equation?\n\nClick to reveal answer\n\nAdd a term: Monthly_Earnings * Cooks_at_Home. This would allow slope differences.\n\nThe dataset lacks variables like “student debt” or “scholarships.” How might this lead to omitted variable bias?\n\nClick to reveal answer\n\nMissing debt/scholarships could bias the coefficients (for instance overstating job impact).\n\n3.12.2 Coding Question\nProblem 1\nIn this problem, you will investigate a dataset of surveyed students taking a programming course with a final exam at the end. Each student in this dataset was asked for their current grade out of 100, how many previous programming courses they took, how many hours they spent studying for the final, and how much they scored on the final exam out of 30, all stored as variables current_grade, prior_courses, hours_studied and exam_score respectively. Below is a full breakdown of the variables:\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\ncurrent_grade\nThe current average of the student in the course. Float from 0 to 100 (e.g. 87.9, 50.18, etc.).\n\n\nprior_courses\nThe number of prior courses the student took. Integer (e.g. 0, 4, etc.).\n\n\nhours_studied\nThe number of hours that the student spent studying for the final exam. Float (e.g. 10.7, 3.4, etc.).\n\n\nexam_score\nThe score that the student received on their final exam. Float from 0 to 30 (e.g. 27.5, 29.1, etc.).\n\n\n\nFor future iterations of the course, you are curious to know if it is possible to understand how well a student will do on the final given these three factors.\nTo answer this, the questions below will guide you through an OLS analysis on this data. Specifically, by the end of these questions, you will be able to answer the following concrete questions: 1. How does a 5% increase in a particular variable (one of current_grade, prior_courses, hours_studied) affect the exam_score? 2. Inference: How does hours_studied infulence exam_score, holding other variables constant? 3. Prediction: Given a particular set of predictor values, what is the expected exam_score? Dive in to see how you can answer these questions using OLS regression!\nA. Data Wrangling and Exploratory Data Analysis\n\nRead in the data from problem-1-data.csv, checking for any missing entries.\nPlot histograms of each variable. Are any distributions skewed or bimodal? Do you notice anything peculiar? Are there any extreme outliers that may affect the OLS?\nPlot relationships between each predictor and the outcome exam_score. Do any appear non-linear?\n\nClick to reveal answer\n\n\nR Code\nPython Code\n\n\n\n\n# Load required libraries\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n# Read data\ndf &lt;- read_csv(\"./03-ols_files/simulations/problem-1-data.csv\")\n\nRows: 200 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): current_grade, prior_courses, hours_studied, exam_score\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# A.1 Check for missing entries\ncat(\"Missing values:\\n\")\n\nMissing values:\n\nprint(colSums(is.na(df)))\n\ncurrent_grade prior_courses hours_studied    exam_score \n            5             0             5             0 \n\n# A.2 Plot histograms (using ggplot2)\n# Pivot the data longer so we can facet\ndf_long &lt;- df %&gt;% pivot_longer(everything(), names_to = \"variable\", values_to = \"value\")\n\nggplot(df_long, aes(x = value)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", alpha = 0.5) +\n  facet_wrap(~ variable, scales = \"free\", nrow = 1) +\n  theme_minimal() +\n  ggtitle(\"Distribution of each variable\")\n\nWarning: Removed 10 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n# A.3 Plot relationships with outcome\npredictors &lt;- c(\"current_grade\", \"prior_courses\", \"hours_studied\")\n\nfor (pred in predictors) {\n  p &lt;- ggplot(df, aes(x = .data[[pred]], y = exam_score)) +\n    geom_point(alpha = 0.6) +\n    ggtitle(paste(\"exam_score vs\", pred)) +\n    theme_minimal()\n  print(p)\n}\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom scipy import stats\n\n# Read data\n# point this to the data location\ndf = pd.read_csv(\"./03-ols_files/simulations/problem-1-data.csv\") \n\n# A.1 Check for missing entries\nprint(\"Missing values:\\n\", df.isnull().sum())\n\nMissing values:\n current_grade    5\nprior_courses    0\nhours_studied    5\nexam_score       0\ndtype: int64\n\n# A.2 Plot histograms\nfig, axes = plt.subplots(1, 4, figsize=(16, 4), dpi=300)\nfor i, col in enumerate(df.columns):\n    axes[i].hist(df[col], bins=30, alpha=0.5)\n    axes[i].set_title(f'Distribution of {col}')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n# A.3 Plot relationships with outcome\nfig, axes = plt.subplots(1, 3, figsize=(15, 5), dpi=300)\npredictors = ['current_grade', 'prior_courses', 'hours_studied']\nfor i, pred in enumerate(predictors):\n    sns.scatterplot(data=df, x=pred, y='exam_score', ax=axes[i])\n    axes[i].set_title(f'exam_score vs {pred}')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\ncurrent_grade appears roughly uniform over its range, with no strong skew or extreme outliers. prior_courses is discrete and right-skewed, with most students having taken 0–2 prior courses and very few above 4. hours_studied is relatively uniform. exam_score is unimodal and left-skewed, with most scores concentrated between roughly 60 and 90. Now for the relationships with exam_score: exam_score vs current_grade shows a fairly weak positive trend, with a lot of noise. exam_score vs prior_courses shows strong variation at low integers, and no strong monotonic trend. This suggests a weak or nonlinear effect, and motivates categorical treatment. exam_score vs hours_studied is strongly increasing but clearly nonlinear, with diminishing returns at high study times. This is potentially consistent with a log-linear effect rather than a linear one.\nB. Data Modelling\n\nFit a multiple linear regression model predicting exam_score using the other variables. Display a statistics summary table.\nReport the \\(R^2\\) value and coefficients. Do all of the coefficients match intuition? Can you find a plausible explantion for why they would not? We will revisit this.\nPlot residuals against the fitted values. Is heteroscedasticity present?\nCreate a Q-Q plot of the residuals. Are they normally distributed?\n\nClick to reveal answer\n\n\nR Code\nPython Code\n\n\n\n\n# B.1 Fit initial model\nmodel1 &lt;- lm(exam_score ~ current_grade + prior_courses + hours_studied, data = df)\nsummary(model1)\n\n\nCall:\nlm(formula = exam_score ~ current_grade + prior_courses + hours_studied, \n    data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.6836  -3.6079   0.7019   4.3444  13.4680 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   19.02669    1.40902   13.50  &lt; 2e-16 ***\ncurrent_grade  0.22100    0.01649   13.40  &lt; 2e-16 ***\nprior_courses -2.29225    0.39655   -5.78 3.09e-08 ***\nhours_studied  5.82093    0.14959   38.91  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.337 on 186 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.9004,    Adjusted R-squared:  0.8988 \nF-statistic: 560.3 on 3 and 186 DF,  p-value: &lt; 2.2e-16\n\n# B.2 Report R² and coefficients\ncat(sprintf(\"\\nR-squared: %.3f\\n\", summary(model1)$r.squared))\n\n\nR-squared: 0.900\n\ncat(\"Coefficients:\\n\")\n\nCoefficients:\n\nprint(coef(model1))\n\n  (Intercept) current_grade prior_courses hours_studied \n   19.0266898     0.2209959    -2.2922451     5.8209292 \n\n# B.3 Residuals vs Fitted\nggplot(data = NULL, aes(x = fitted(model1), y = resid(model1))) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(x = \"Fitted values\", y = \"Residuals\", title = \"Residuals vs Fitted\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n# B.4 Q-Q plot\nggplot(data = NULL, aes(sample = resid(model1))) +\n  stat_qq(alpha = 0.6) +\n  stat_qq_line(color = \"red\") +\n  labs(title = \"Q-Q Plot of Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n# B.1 Fit initial model\nmodel1 = ols('exam_score ~ current_grade + prior_courses + hours_studied', data=df.dropna()).fit()\nprint(model1.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             exam_score   R-squared:                       0.900\nModel:                            OLS   Adj. R-squared:                  0.899\nMethod:                 Least Squares   F-statistic:                     560.3\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           7.35e-93\nTime:                        20:13:06   Log-Likelihood:                -618.40\nNo. Observations:                 190   AIC:                             1245.\nDf Residuals:                     186   BIC:                             1258.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept        19.0267      1.409     13.503      0.000      16.247      21.806\ncurrent_grade     0.2210      0.016     13.403      0.000       0.188       0.254\nprior_courses    -2.2922      0.397     -5.780      0.000      -3.075      -1.510\nhours_studied     5.8209      0.150     38.913      0.000       5.526       6.116\n==============================================================================\nOmnibus:                       23.170   Durbin-Watson:                   1.787\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               30.095\nSkew:                          -0.775   Prob(JB):                     2.92e-07\nKurtosis:                       4.182   Cond. No.                         171.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n# B.2 Report R² and coefficients\nprint(f\"\\nR-squared: {model1.rsquared:.3f}\")\n\n\nR-squared: 0.900\n\nprint(\"Coefficients:\\n\", model1.params)\n\nCoefficients:\n Intercept        19.026690\ncurrent_grade     0.220996\nprior_courses    -2.292245\nhours_studied     5.820929\ndtype: float64\n\n# B.3 Residuals vs fitted\nsns.residplot(x=model1.fittedvalues, y=model1.resid, lowess=True)\nplt.xlabel('Fitted values')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Fitted')\nplt.show()\n\n\n\n\n\n\n# B.4 Q-Q plot\nsm.qqplot(model1.resid, line='45')\nplt.title('Q-Q Plot of Residuals')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe coefficients do not entirely match intuition—prior_courses comes out negative in the linear model. A plausible explanation could be a selection bias: students who took more prior courses may be doing so because they struggled previously, leading to lower exam scores. Or, pior_courses is a poor linear proxy.\nHeteroscedasticity appears present in the residuals vs fitted plot and the Breusch-Pagan test, which shown non-constant variance. The Q-Q plot shows residuals which deviate from a normal distribution.\nC. Goodness of Fit\n\nPerform the Durbin-Watson test to check for autocorrelation in the residuals. What does the test statistic tell you about the independence of errors assumption?\nTest for Homoscedasticity: You already plotted residuals against fitted values. Perform the Breusch-Pagan test to formally assess homoscedasticity. What does the p-value of the test indicate about the model?\nIdentify Outliers: Create a plot of the residuals against the observation index. Does this plot reveal any obvious outliers or problematic patterns?\nCheck for Influential Points: Calculate and plot the Cook’s Distance for each observation. Are there any data points that have a disproportionately large influence on the model’s coefficients?\n\nClick to reveal answer\n\n\nR Code\nPython Code\n\n\n\n\n# C.1 Durbin-Watson Test (Independence)\n\ndw &lt;- dwtest(model1)\nprint(dw$statistic)\n\n      DW \n1.787067 \n\n# C.2 Breusch-Pagan Test (Homoscedasticity)\n\nbp &lt;- bptest(model1)\nprint(bp$statistic)\n\n      BP \n26.76084 \n\nprint(bp$p.value)\n\n          BP \n6.607713e-06 \n\n# C.3 Residuals vs Observation Index\n\n# Create data frame for index plotting\n\ndf_diag &lt;- data.frame(\nresid = residuals(model1),\nindex = 1:length(residuals(model1)),\ncooks = cooks.distance(model1)\n)\n\nggplot(df_diag, aes(x = index, y = resid)) +\ngeom_point(alpha = 0.7) +\ngeom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\nlabs(title = \"Residual Plot vs. Observation Index\",\nx = \"Observation Index\", y = \"Residuals\") +\ntheme_minimal()\n\n\n\n\n\n\n# C.4 Cook's Distance\n\nggplot(df_diag, aes(x = index, y = cooks)) +\ngeom_segment(aes(xend = index, yend = 0)) + \ngeom_point() +\ngeom_hline(yintercept = 0.5, color = \"red\", linetype = \"dashed\") +\nlabs(title = \"Cook's Distance\", x = \"Observation Index\", y = \"Cook's Distance\") +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\n\nimport statsmodels.stats.api as sms\n# C.1 Perform the Durbin-Watson test...\ndw_stat = durbin_watson(model1.resid)\nprint(f\"Durbin-Watson statistic: {dw_stat:.4f}\")\n\nDurbin-Watson statistic: 1.7871\n\n# C.2 Perform the Breusch-Pagan test...\n\nfrom statsmodels.stats.diagnostic import het_breuschpagan\n\n# Perform Breusch-Pagan test\nbp_test = het_breuschpagan(model1.resid, model1.model.exog)\n\n# Unpack results\nbp_stat, bp_pvalue, empty, empty = bp_test\n\nprint(f\"Breusch-Pagan test statistic: {bp_stat:.3f}\")\n\nBreusch-Pagan test statistic: 26.761\n\nprint(f\"P-value: {bp_pvalue:.4f}\")\n\nP-value: 0.0000\n\n# bp_test = sms.het_breuschpagan(residuals, model.model.exog)\n# labels = ['LM Statistic', 'LM-Test p-value', 'F-Statistic', 'F-Test p-value']\n# bp_results = dict(zip(labels, bp_test))\n# print(f\"Breusch-Pagan test p-value: {bp_results['LM-Test p-value']:.4f}\")\n# \n# C.3 Plot the residuals against the observation index...\nplt.figure(figsize=(6, 4))\nplt.plot(residuals, marker='o', linestyle='none', alpha=0.7)\nplt.axhline(y=0, color='red', linestyle='--')\nplt.title(\"Residual Plot vs. Observation Index\")\nplt.xlabel(\"Observation Index\")\nplt.ylabel(\"Residuals\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n# C.4 Calculate and plot Cook's Distance...\ninfluence = model.get_influence()\ncooks_d = influence.cooks_distance[0]\n\nplt.figure(figsize=(6, 4))\n# use_line_collection=True is for compatibility\nplt.stem(np.arange(len(cooks_d)), cooks_d, markerfmt=\",\")\nplt.axhline(y=0.5, color='red', linestyle='--', label='Threshold (0.5)')\nplt.title(\"Cook's Distance\")\nplt.xlabel(\"Observation Index\")\nplt.ylabel(\"Cook's Distance\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe Durbin-Watson test produces a value of 1.79, close to 2, meaning there is no strong evidence of autocorrelation (errors are approximately independent). The p-value of the Breusch-Pagan test is essentially 0, and thus we reject homoscedasticity: residual variance changes with fitted values. Looking at the residuals vs observation index, there is no obvious time/order pattern implied here. Using Cook’s Distance plot, there are no point that appear to have disproportionately large influence on the fitted coefficients.\nD. Data Modelling - Improvements\n\nShould prior_courses be converted to another variable type? If so, why and which one? If you answered yes, try converting it and re-fit. Do you notice any improvements?\nCreate log_hours_studied, the log of hours_studied. Re-fit the OLS using this predictor instead. Does \\(R^2\\) improve? &gt; Hint: the true data generation process used log(hours_studied)!\nAdd an interaction term, current_grade * prior_courses and re-fit. Is this interaction significant?\n\nClick to reveal answer\n\n\nR Code\nPython Code\n\n\n\n\n# D.1 prior_courses is an integer. We can try convert prior_courses to categorical and fit the model again:\ndf &lt;- df %&gt;% mutate(prior_courses_category = factor(prior_courses))\n\nmodel2 &lt;- lm(exam_score ~ current_grade + prior_courses_category + hours_studied, data = df)\ncat(\"Model 2:\\n\")\n\nModel 2:\n\nsummary(model2)\n\n\nCall:\nlm(formula = exam_score ~ current_grade + prior_courses_category + \n    hours_studied, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.9321  -3.1306   0.2158   4.3482  12.7484 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              17.81383    1.53192  11.628  &lt; 2e-16 ***\ncurrent_grade             0.21710    0.01669  13.007  &lt; 2e-16 ***\nprior_courses_category1  -0.34711    1.25488  -0.277  0.78240    \nprior_courses_category2  -2.41685    1.35086  -1.789  0.07527 .  \nprior_courses_category3  -4.80176    1.74599  -2.750  0.00656 ** \nprior_courses_category4  -9.94565    2.44134  -4.074 6.91e-05 ***\nprior_courses_category5 -13.86771    4.62599  -2.998  0.00310 ** \nprior_courses_category6 -19.77704    6.44826  -3.067  0.00249 ** \nhours_studied             5.82145    0.15179  38.351  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.312 on 181 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.9038,    Adjusted R-squared:  0.8996 \nF-statistic: 212.6 on 8 and 181 DF,  p-value: &lt; 2.2e-16\n\ncat(\"##########################################################\\n\")\n\n##########################################################\n\n# D.2 Log transform hours_studied and fit model\ndf &lt;- df %&gt;% mutate(log_hours = log(hours_studied))\n\nmodel3 &lt;- lm(exam_score ~ current_grade + prior_courses + log_hours, data = df)\ncat(\"Model 3:\\n\")\n\nModel 3:\n\nsummary(model3)\n\n\nCall:\nlm(formula = exam_score ~ current_grade + prior_courses + log_hours, \n    data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.5498 -2.3069  0.1171  1.8023  9.1291 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    2.064443   0.874596    2.36   0.0193 *  \ncurrent_grade  0.222380   0.008533   26.06   &lt;2e-16 ***\nprior_courses -2.487138   0.205384  -12.11   &lt;2e-16 ***\nlog_hours     31.958236   0.407101   78.50   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.28 on 186 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.9733,    Adjusted R-squared:  0.9729 \nF-statistic:  2262 on 3 and 186 DF,  p-value: &lt; 2.2e-16\n\ncat(\"##########################################################\\n\")\n\n##########################################################\n\n# D.3 Add interaction term\nmodel4 &lt;- lm(exam_score ~ current_grade * prior_courses + hours_studied, data = df)\ncat(\"Model 4:\\n\")\n\nModel 4:\n\nsummary(model4)\n\n\nCall:\nlm(formula = exam_score ~ current_grade * prior_courses + hours_studied, \n    data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-21.5523  -3.5381   0.6858   4.3840  13.4922 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 18.80963    1.73495  10.842  &lt; 2e-16 ***\ncurrent_grade                0.22527    0.02583   8.722 1.56e-15 ***\nprior_courses               -2.14965    0.77195  -2.785  0.00592 ** \nhours_studied                5.82504    0.15118  38.531  &lt; 2e-16 ***\ncurrent_grade:prior_courses -0.00304    0.01411  -0.216  0.82961    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.354 on 185 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.9004,    Adjusted R-squared:  0.8982 \nF-statistic: 418.1 on 4 and 185 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n# D.1 prior_courses is an integer. We can try convert prior_courses to categorical and fit the model again:\ndf[\"prior_courses_category\"] = df['prior_courses'].astype('category')\nmodel2 = ols('exam_score ~ current_grade + prior_courses_category + hours_studied', \n             data=df.dropna()).fit()\nprint(model2.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             exam_score   R-squared:                       0.904\nModel:                            OLS   Adj. R-squared:                  0.900\nMethod:                 Least Squares   F-statistic:                     212.6\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           9.11e-88\nTime:                        20:13:07   Log-Likelihood:                -615.06\nNo. Observations:                 190   AIC:                             1248.\nDf Residuals:                     181   BIC:                             1277.\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n=================================================================================================\n                                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------------------------\nIntercept                        17.8138      1.532     11.628      0.000      14.791      20.837\nprior_courses_category[T.1.0]    -0.3471      1.255     -0.277      0.782      -2.823       2.129\nprior_courses_category[T.2.0]    -2.4168      1.351     -1.789      0.075      -5.082       0.249\nprior_courses_category[T.3.0]    -4.8018      1.746     -2.750      0.007      -8.247      -1.357\nprior_courses_category[T.4.0]    -9.9456      2.441     -4.074      0.000     -14.763      -5.129\nprior_courses_category[T.5.0]   -13.8677      4.626     -2.998      0.003     -22.996      -4.740\nprior_courses_category[T.6.0]   -19.7770      6.448     -3.067      0.002     -32.500      -7.054\ncurrent_grade                     0.2171      0.017     13.007      0.000       0.184       0.250\nhours_studied                     5.8215      0.152     38.351      0.000       5.522       6.121\n==============================================================================\nOmnibus:                       24.086   Durbin-Watson:                   1.757\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               31.292\nSkew:                          -0.803   Prob(JB):                     1.60e-07\nKurtosis:                       4.171   Cond. No.                         783.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nprint(\"##########################################################\")\n\n##########################################################\n\n# D.2 Log transform hours_studied\ndf['log_hours'] = np.log(df['hours_studied'])\nmodel3 = ols('exam_score ~ current_grade + prior_courses + log_hours', \n             data=df.dropna()).fit()\nprint(model3.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             exam_score   R-squared:                       0.973\nModel:                            OLS   Adj. R-squared:                  0.973\nMethod:                 Least Squares   F-statistic:                     2262.\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):          4.70e-146\nTime:                        20:13:07   Log-Likelihood:                -493.24\nNo. Observations:                 190   AIC:                             994.5\nDf Residuals:                     186   BIC:                             1007.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         2.0644      0.875      2.360      0.019       0.339       3.790\ncurrent_grade     0.2224      0.009     26.060      0.000       0.206       0.239\nprior_courses    -2.4871      0.205    -12.110      0.000      -2.892      -2.082\nlog_hours        31.9582      0.407     78.502      0.000      31.155      32.761\n==============================================================================\nOmnibus:                        0.248   Durbin-Watson:                   2.061\nProb(Omnibus):                  0.883   Jarque-Bera (JB):                0.358\nSkew:                           0.077   Prob(JB):                        0.836\nKurtosis:                       2.853   Cond. No.                         217.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nprint(\"##########################################################\")\n\n##########################################################\n\n# D.3 Add interaction term\nmodel4 = ols('exam_score ~ current_grade * prior_courses + hours_studied', \n             data=df.dropna()).fit()\nprint(model4.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             exam_score   R-squared:                       0.900\nModel:                            OLS   Adj. R-squared:                  0.898\nMethod:                 Least Squares   F-statistic:                     418.1\nDate:                Wed, 21 Jan 2026   Prob (F-statistic):           1.85e-91\nTime:                        20:13:07   Log-Likelihood:                -618.38\nNo. Observations:                 190   AIC:                             1247.\nDf Residuals:                     185   BIC:                             1263.\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n===============================================================================================\n                                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------\nIntercept                      18.8096      1.735     10.842      0.000      15.387      22.232\ncurrent_grade                   0.2253      0.026      8.722      0.000       0.174       0.276\nprior_courses                  -2.1496      0.772     -2.785      0.006      -3.673      -0.627\ncurrent_grade:prior_courses    -0.0030      0.014     -0.216      0.830      -0.031       0.025\nhours_studied                   5.8250      0.151     38.531      0.000       5.527       6.123\n==============================================================================\nOmnibus:                       23.090   Durbin-Watson:                   1.783\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               29.962\nSkew:                          -0.774   Prob(JB):                     3.12e-07\nKurtosis:                       4.179   Cond. No.                         457.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\nprior_courses should be converted to categorical, because it is discrete and its relationship with exam_score isn’t clearly linear. We see that this gives a small improvement in the fit (\\(R^2\\) increases). Following the log-transform of hours_studied, \\(R^2\\) improves and reaches 0.973. This is consistent with diminishing returns in studying. The current_grade * prior_courses interaction is not very significant, having a p-value of 0.830. There is therefore no evidence of a dependence of current_grade on prior_courses here.\nE. Final Evaluation\n\nSelect the final best performing model from the combinations of improvements above, by comparing \\(R^2\\) and RMSE.\nInterpret the final model: How does a 5% increase in current_grade affect exam_score?\nWhy might having taken more prior_courses show diminishing returns in exam_score?\nInference: Based on your final model, describe the relationship between hours_studied and exam_score. Namely, if a student increases their study time by 10%, how does their expected exam score change?\nPrediction: Suppose a student has a current grade of 82, has taken 2 prior programming courses, and studied for 12.5 hours. Based on your final model, what is the predicted exam score of this student?\n\nClick to reveal answer\n\n\nR Code\nPython Code\n\n\n\n\n# E.1 Compare models\n# Create a comparison table of R² and RMSE\nmodels &lt;- list(\n  Initial = model1,\n  Categorical = model2,\n  Log_Transform = model3,\n  Interaction = model4\n)\n\nresults &lt;- tibble(\n  Model = names(models),\n  R_squared = sapply(models, function(m) summary(m)$r.squared),\n  RMSE = sapply(models, function(m) sqrt(mean(resid(m)^2)))\n)\n\n# Show models sorted by R-squared\nresults %&gt;% arrange(desc(R_squared)) %&gt;% print()\n\n# A tibble: 4 × 3\n  Model         R_squared  RMSE\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1 Log_Transform     0.973  3.24\n2 Categorical       0.904  6.16\n3 Interaction       0.900  6.27\n4 Initial           0.900  6.27\n\n# E.2 Interpretation of final model (model3)\n# Interpretation: effect of a 5% increase in current_grade\ncoef &lt;- coef(model3)[\"current_grade\"]\neffect &lt;- 0.05 * coef\n\ncat(sprintf(\"\\nA 5%% increase in current_grade increases exam_score by %.5f points\\n\", effect))\n\n\nA 5% increase in current_grade increases exam_score by 0.01112 points\n\n# E.3 Diminishing returns explanation\n# see below!\n\n# E.4 Inference: Effect of hours_studied\n\n# Since Model 3 uses log(hours_studied), this is a Level-Log interpretation.\n\n# A 10% increase in X is associated with a (beta / 100) change in Y.\n# Delta Y = beta * ln(1 + 0.10)\n\ncoef_log_hours &lt;- coef(model3)[\"log_hours\"]\neffect_hours &lt;- coef_log_hours * log(1.10)\n\ncat(\"\\nA 10% increase in study time is associated with an exam_score increase of:\\n\")\n\n\nA 10% increase in study time is associated with an exam_score increase of:\n\nprint(effect_hours)\n\nlog_hours \n 3.045945 \n\n# E.5 Prediction\n\n# Define the new student profile\n\nnew_student &lt;- data.frame(\ncurrent_grade = 82,\nprior_courses = 2,\nhours_studied = 12.5\n)\n\n# Note: model3 was trained on a dataframe with a 'log_hours' column.\n# We must create that column in the new data to predict successfully.\nnew_student &lt;- new_student %&gt;%\nmutate(log_hours = log(hours_studied))\n\npredicted_score &lt;- predict(model3, newdata = new_student)\n\ncat(sprintf(\"\\nPredicted exam score for the student: %.2f\\n\", predicted_score))\n\n\nPredicted exam score for the student: 96.04\n\n\n\nComparing \\(R^2\\) and RMSE, one should choose the Log Transform model, which has the highest \\(R^2=0.9733\\) and lowest RMSE\\(\\approx3.28\\).\n\nThere are a number of plausible explanations for why taking more prior courses may show diminishing returns:\n\nStudents who have taken many prior courses may be repeating courses due to past struggles, indicating a negative selection bias.\nThe knowledge gained from additional courses may overlap, leading to less incremental benefit for each additional course taken.\nStudents with many prior courses may have less time or motivation (or more overconfidence!) to study effectively for the current course.\n\n\n\n\n\n\n# E.1 Compare models\nmodels = {\n    'Initial': model1,\n    'Categorical': model2,\n    'Log Transform': model3,\n    'Interaction': model4\n}\n\nresults = pd.DataFrame({\n    'Model': models.keys(),\n    'R-squared': [m.rsquared for m in models.values()],\n    'RMSE': [np.sqrt(m.mse_resid) for m in models.values()]\n})\nprint(results.sort_values('R-squared', ascending=False))\n\n           Model  R-squared      RMSE\n2  Log Transform   0.973318  3.279605\n1    Categorical   0.903819  6.312044\n3    Interaction   0.900395  6.353581\n0        Initial   0.900370  6.337274\n\n# E.2 Interpretation of final model (model3)\nfinal_model = model3\ncoef = final_model.params['current_grade']\nprint(f\"\\nA 5% increase in current_grade increases exam_score by {0.05*coef:.5f} points\")\n\n\nA 5% increase in current_grade increases exam_score by 0.01112 points\n\n# E.3 Diminishing returns explanation\n# see below!\n\n\n# E.4 Inference: Effect of hours_studied (10% increase)\n# Interpretation of Level-Log model: Delta Y = beta * ln(1.10)\ncoef_log_hours = final_model.params['log_hours']\neffect_hours = coef_log_hours * np.log(1.10)\n\nprint(\"\\nA 10% increase in study time is associated with an exam_score increase of:\")\n\n\nA 10% increase in study time is associated with an exam_score increase of:\n\nprint(effect_hours)\n\n3.0459452256912294\n\n# E.5 Prediction\n# Create a DataFrame for the new student\nnew_student = pd.DataFrame({\n    'current_grade': [82],\n    'prior_courses': [2],\n    'hours_studied': [12.5]\n})\n\n# We must add the log_hours feature as used in model3 training\nnew_student['log_hours'] = np.log(new_student['hours_studied'])\n\n# Predict\nprediction = final_model.predict(new_student)\nprint(\"\\nPredicted exam score for the student:\")\n\n\nPredicted exam score for the student:\n\nprint(prediction[0])\n\n96.04319041340568\n\n\n\nComparing \\(R^2\\) and RMSE, one should choose the Log Transform model, which has the highest \\(R^2=0.9733\\) and lowest RMSE\\(\\approx3.28\\).\n\nThere are a number of plausible explanations for why taking more prior courses may show diminishing returns:\n\nStudents who have taken many prior courses may be repeating courses due to past struggles, indicating a negative selection bias.\nThe knowledge gained from additional courses may overlap, leading to less incremental benefit for each additional course taken.\nStudents with many prior courses may have less time or motivation (or more overconfidence!) to study effectively for the current course.\n\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 3.2\n\n\n\n\n\n\n\n\n\n\n\nClark, Michael. 2022. “Generalized Additive Models.” https://m-clark.github.io/generalized-additive-models/.\n\n\nDodge, Yadolah. 2008. “Weighted Least-Squares Method.” In The Concise Encyclopedia of Statistics, 566–69. New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-32833-1_422.\n\n\nHastie, Trevor, and Robert Tibshirani. 1986. “Generalized Additive Models.” Statistical Science 1 (3): 297–310. http://www.jstor.org/stable/2245459.\n\n\nRousseeuw, Peter J., and Annick M. Leroy. 1987. Robust Regression and Outlier Detection. Wiley Series in Probability and Mathematical Statistics. John Wiley & Sons. https://doi.org/10.1002/0471725382.\n\n\nWu, Lang. 2009. Mixed Effects Models for Complex Data. 1st ed. Chapman; Hall/CRC. https://doi.org/10.1201/9781420074086.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ordinary Least-squares Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html",
    "href": "book/04-gamma.html",
    "title": "4  Gamma Regression",
    "section": "",
    "text": "4.1 Introduction\nSome types of data are always positive and tend to have a few really large values. For example, imagine we are looking at how much people spend at a grocery store. Most people might spend a small to medium amount, but a few people might spend a lot more. This kind of data is not evenly spread out, and it does not look like a nice bell curve. Instead, it is skewed to the right, with a long tail of higher numbers.\nIn these situations, the usual type of regression (called ordinary least squares, or OLS) does not work very well. OLS assumes that the data is evenly spread out around the average and that big values and small values are equally likely. But when we have only positive numbers and some very big ones, we need a different approach.\nThis is where Gamma regression can help.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html#what-is-gamma-regression",
    "href": "book/04-gamma.html#what-is-gamma-regression",
    "title": "4  Gamma Regression",
    "section": "\n4.2 What is Gamma Regression",
    "text": "4.2 What is Gamma Regression\nGamma regression is a tool we use when the outcome we are trying to predict is always positive and can vary a lot. For example, we might want to predict:\n\nhow long it takes for someone to complete a task\nhow much money someone spends at a store\nthe cost of a medical procedure\n\nThese types of outcomes are positive, continuous, and often right-skewed: most values are small or moderate, but a few can be extremely large. This uneven spread makes the data a poor fit for regular linear regression, which assumes that the outcomes are roughly symmetric and that the variability stays constant across all levels of the predictors.\nGamma regression is designed for situations like this. Instead of trying to fit a straight line with constant variance, it uses a model that:\n\nrespects the fact that the outcome must be positive, and\nallows the variance to grow with the mean, which matches how many real-world cost or duration variables behave.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html#understanding-the-gamma-distribution",
    "href": "book/04-gamma.html#understanding-the-gamma-distribution",
    "title": "4  Gamma Regression",
    "section": "\n4.3 Understanding the Gamma Distribution",
    "text": "4.3 Understanding the Gamma Distribution\nGamma regression is built on something called the Gamma distribution. This distribution is used to describe values that are:\n\nalways above zero\nand can have a long tail of larger values\n\nWhat makes the Gamma distribution flexible is a number called the shape parameter. This controls how the data is spread out.\n\nWhen the shape is small (like 1), the distribution is very skewed. Most values are small, but a few are much larger.\nWhen the shape is larger (like 5 or 9), the curve becomes more balanced, though it still never includes negative numbers.\n\nLet’s look at what this means visually:\n\n\n\n\n\n\n\n\nThe plot above shows how the shape parameter changes the look of the Gamma distribution. It helps explain why Gamma regression is such a good match for skewed data. In the frequentist paradigm, we aim to find the Gamma distribution with the shape parameter that best fits the observed data.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html#how-gamma-regression-works",
    "href": "book/04-gamma.html#how-gamma-regression-works",
    "title": "4  Gamma Regression",
    "section": "\n4.4 How Gamma Regression Works",
    "text": "4.4 How Gamma Regression Works\nNow that we know what Gamma regression is and the kinds of problems it solves, let’s look at how it actually works.\n\nThe key idea is that Gamma regression doesn’t model the outcome values directly. Instead, it focuses on the average outcome and relates it to predictors in a way that respects the positive, skewed nature of the data.\n\nTo understand this, we need to take a step back and talk about a broader family of models called Generalized Linear Models, or GLMs. These are a flexible class of models that let us deal with many types of data, not just bell-shaped curves. GLMs allow us to model outcomes that are counts, proportions, or, as in our case, continuous and strictly positive. In this next section, we’ll break down what this means.\n\n4.4.1 Generalized Linear Models\n\n\nDefinition of Generalized Linear Models\n\n\nGeneralized Linear Models (GLMs) are a powerful extension of linear regression. They allow us to model a wide variety of response variables beyond just those that follow a normal distribution. This makes GLMs especially useful in real-world situations where data often breaks the assumptions of ordinary least squares (OLS) regression.\n\n\nBefore we introduce the full GLM framework, let’s start with a simple example that shows why a standard linear model isn’t always appropriate. This will help motivate why we need a more general approach in the first place.\nProblem with Linear Regression for Non-Normal Data\nSuppose we want to model the number of daily website visits for a company, based on the company’s advertising budget. This outcome is:\n\nalways positive\nright-skewed\nand often grows rapidly with budget\n\nLet’s simulate this data:\n\n\n\n\n\n\n\n\nLinear regression tries to fit a straight line through this data, but the result is clearly problematic.\n\n\n\n\n\n\n\n\nWe run into several issues here:\n\nPredictions can be negative, which makes no sense for visit counts\nPredictions do not capture the multiplicative or curved nature of the relationship\nVariability grows with the mean, violating the constant variance assumption\n\nThese problems stem from assumptions built into OLS:\n\nThe outcome is normally distributed\nThe outcome has constant variance\nThe relationship between predictors and outcome is linear\n\nWhen those assumptions are broken, OLS can give misleading or invalid results.\nWhat GLMs Do Differently\nDespite these problems, linear regression has a big strength: it’s interpretable. The formula tells us exactly how the outcome changes with the predictor. But wouldn’t it be great if we could retain this interpretability while also accounting for the data’s quirks? That’s what Generalized Linear Models do.\nThe original linear regression formula looks like this:\n\\[\n\\mu = \\beta_0 + \\beta_1 x\n\\]\nAs demonstrated above, this assumes \\(\\mu\\) (the expected outcome) can take any real value, including negatives, and grows in a straight-line fashion, which just doesn’t make much sense given the data.\nNow, imagine we make one small change, apply a log transformation to the mean:\n\\[\n\\log(\\mu) = \\beta_0 + \\beta_1 x\n\\]\nThis small adjustment makes a huge difference. Here’s how the fitted curve now looks with this small change:\n\n\n\n\n\n\n\n\nThis new setup has several advantages:\n\nAll predicted values of \\(\\mu\\) are strictly positive\nThe relationship between predictors and response is multiplicative, not additive\nThe variance increases with the mean, which reflects how real-world data often behaves\nWe still retain a straight-line interpretation, but now it’s on the log scale\n\nWhat you have just witnessed is an example of Generalized Linear Model. It is a flexible extension of linear regression that allows you to model outcomes in a way that reflects how they are actually distributed, while also preserving the ability to interpret coefficients like in linear regression\nThe Three Components of GLM\nNow that we have seen an example of a GLM, let’s properly introduce the components that make up a GLM. Every Generalized Linear Model (GLM) is built on the same three key components. This structure gives GLMs the flexibility to model many types of data while still being easy to interpret.\n1. Random Component (Distribution of the Response)\nThis specifies how the response variable is distributed. In linear regression, the response is normally distributed. But GLMs allow for many other distributions, depending on what kind of data you are modeling.\n\n\nType of Data\nExample Distribution\n\n\n\nContinuous, symmetric\nNormal\n\n\nCount\nPoisson\n\n\nProportion or binary\nBinomial\n\n\nPositive, skewed\nGamma\n\n\n\nThis is where the flexibility of GLMs comes from. You choose a distribution that matches your outcome.\n2. Systematic Component (Linear Predictor)\nThis is the familiar part from linear regression. It is a linear combination of the predictors:\n\\[\n\\begin{align*}\n\\eta &= \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_p x_{p}\n\\end{align*}\n\\]\nThis linear predictor, often denoted by \\(\\eta\\), is the foundation of the model. The link function (coming next) will connect this to the mean of the response variable.\n3. Link Function (Connecting Predictors to the Mean)\nThe link function transforms the expected value of the response variable so that it can be modeled as a linear combination of predictors.\n\\[\n\\begin{align*}\ng(\\mu) = \\eta &= \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\cdots + \\beta_p x_{p}\n\\end{align*}\n\\]\nDifferent distributions tend to pair with different link functions:\n\n\n\n\n\n\n\nDistribution\nTypical Link Function\nEquation Example\n\n\n\nNormal\nIdentity\n\\(\\mu = \\eta\\)\n\n\nBinomial\nLogit\n\\(\\log\\left( \\frac{\\mu}{1 - \\mu} \\right) = \\eta\\)\n\n\nPoisson\nLog\n\\(\\log(\\mu) = \\eta\\)\n\n\nGamma\nLog\n\\(\\log(\\mu) = \\eta\\)\n\n\n\nThe link function helps ensure that predictions stay within a valid range (e.g., probabilities stay between 0 and 1, counts stay positive, etc.).\nGamma Regression as a GLM\nNow that we’ve introduced the components of a GLM, we can see that Gamma regression is not a completely new type of model. Instead, it is one specific case within the broader GLM framework. What makes Gamma regression special is that it is designed for situations where the outcome:\n\nis strictly positive\ntends to be right-skewed\nand often shows larger variance when the mean is higher\n\nTo make this concrete, let’s specify the three GLM components as they apply to Gamma regression:\n1. Random Component (Gamma)\nThe response variable \\(Y\\) is assumed to follow a Gamma distribution, which is continuous, strictly positive, and right-skewed. The variance of \\(Y\\) increases with its mean, often expressed as: ​ \\[\n\\operatorname{Var}(Y_i) = \\phi \\mu_i^2\n\\]\nThis makes Gamma regression a good match for data where larger expected values come with more uncertainty.\n2. Systematic Component (Gamma)\nJust like in linear regression, the predictors are combined linearly:\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}\n\\]\nThis linear predictor \\(\\eta_i\\) serves as the backbone of the model.\n3. Link Function (Gamma)\nTo connect the predictors to the expected outcome \\(\\mu_i = \\mathbb{E}(Y_i)\\), Gamma regression uses the log link:\n\\[\n\\log(\\mu_i) = \\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}\n\\]\nThis ensures that:\n\nAll predicted values of \\(\\mu_i\\) are strictly positive\nThe relationship between predictors and the mean is multiplicative, not additive\nWe can interpret coefficients on the log scale, preserving interpretability\n\nIn other words, Gamma regression is just a GLM with a Gamma distribution and a log link. It does not model the outcome values directly. Instead, it focuses on the average outcome and relates it to predictors in a way that respects the positive, skewed nature of the data.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html#when-to-use-and-not-use-gamma-regression",
    "href": "book/04-gamma.html#when-to-use-and-not-use-gamma-regression",
    "title": "4  Gamma Regression",
    "section": "\n4.5 When to Use and Not Use Gamma Regression",
    "text": "4.5 When to Use and Not Use Gamma Regression\nWith the core ideas of Gamma regression in place, we can now summarize the practical guidelines: the situations where Gamma regression is most effective, and the situations where it is not recommended.\n\n\nWhen to Use and Not Use Gamma Regression\n\n\nGamma regression is powerful, but it isn’t a one-size-fits-all tool. It shines in some situations and breaks down in others. The checklist below can help you decide.\nUse Gamma regression when:\n\nThe outcome is continuous and strictly positive (e.g., insurance claim amounts, pollutant concentrations, operational spending).\nObservations are statistically independent.\nThe distribution is right-skewed, where variance grows faster than the mean.\nYou want to model the logarithm of the mean as a link function, which guarantees positive predictions.\n\nAvoid Gamma regression when:\n\nThe outcome includes zeros or negative values.\nThe outcome is a count variable. In that case, consider alternatives like Classical Poisson, Negative Binomial, Zero-Inflated Poisson or Generalized Poisson.\nThe variance increases at a constant rate with the mean, rather than at a faster rate (a sign another GLM family may be more appropriate).",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html#case-study-modeling-web-server-performance",
    "href": "book/04-gamma.html#case-study-modeling-web-server-performance",
    "title": "4  Gamma Regression",
    "section": "\n4.6 Case Study: Modeling Web Server Performance",
    "text": "4.6 Case Study: Modeling Web Server Performance\nTo demonstrate Gamma regression in action, we will walk through a case study using a toy dataset. This case study will help us understand what affects the response time of a web server. Specifically, how long it takes (in milliseconds) for the server to respond to a user’s request.\nWe will approach this case study using the data science workflow introduced in the first chapter. This ensures a structured approach to framing the problem, selecting a model, and building an interpretation based on the model’s results.\n\n4.6.1 The Dataset\nThe toy dataset used to demonstrate captures various features of web requests across different servers and regions. Each row represents a single request and contains information about the server load, the number of users at the time, how complex the request was, and other related factors. Below is a summary of the variables:\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\nresponse_time_ms\nThe time it takes for the server to respond to a request (in milliseconds).\n\n\nconcurrent_users\nNumber of users on the server at the time of the request.\n\n\ndatabase_queries\nNumber of database queries triggered by the request.\n\n\ncache_hit_rate\nPercentage of data requests served from the cache (0 to 100).\n\n\nserver_load\nServer CPU load at the time of the request (0 to 100).\n\n\ntime_of_day\nHour of the day when the request was made (0 = midnight, 23 = 11 PM).\n\n\nday_of_week\nDay of the week (e.g., Monday, Tuesday).\n\n\ngeographic_region\nGeographic region where the request originated.\n\n\nrequest_complexity\nCategorical value describing request complexity: “Simple”, “Moderate”, or “Complex”.\n\n\ncdn_usage\nWhether a content delivery network was used (“Yes” or “No”).\n\n\nmemory_usage\nMemory used on the server at the time of request (in percent).\n\n\n\nHere’s a snapshot of the dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresponse_time_ms\nconcurrent_users\ndatabase_queries\ncache_hit_rate\nserver_load\ntime_of_day\nday_of_week\ngeographic_region\nrequest_complexity\ncdn_usage\nmemory_usage\n\n\n\n54.2\n40\n10\n94.8\n61.4\n8\nSaturday\nNorth_America\nModerate\nYes\n63.6\n\n\n28.6\n148\n1\n56.1\n43.2\n18\nThursday\nNorth_America\nComplex\nYes\n25.5\n\n\n50.2\n195\n14\n94.6\n67.0\n1\nThursday\nNorth_America\nComplex\nNo\n26.6\n\n\n24.8\n82\n10\n77.9\n83.9\n14\nMonday\nAfrica\nSimple\nYes\n67.5\n\n\n127.1\n85\n10\n58.7\n47.1\n7\nThursday\nSouth_America\nModerate\nYes\n54.4\n\n\n\n\nThis dataset allows us to analyze performance differences in web servers under different conditions and determine what contributes to longer or shorter response times.\n\n4.6.2 The Problem We’re Trying to Solve\nIn this case study, our goal is to analyze what drives differences in web server response times. Because response time is continuous, strictly positive, and often right-skewed, Gamma regression is a natural choice for modeling it.\nThe key question we aim to answer is:\n\nWhich server or request characteristics have the biggest influence on response time?\n\nAnswering this question requires more than just prediction. It requires understanding the relationships between server conditions, request features, and performance. In the next section, we’ll clarify our study design and show how Gamma regression supports this goal.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html#study-design",
    "href": "book/04-gamma.html#study-design",
    "title": "4  Gamma Regression",
    "section": "\n4.7 Study Design",
    "text": "4.7 Study Design\nIn this case study, our goal is to understand what drives differences in web server response times. Because response time is continuous, strictly positive, and often right-skewed, Gamma regression is a natural modeling choice.\nThe broader objective is to uncover which server or request characteristics meaningfully influence response time. This requires more than simply forecasting future values. It requires understanding how different factors relate to average performance.\nTo make that purpose precise, we turn to the study design.\n\n4.7.1 Inferential vs. Predictive Analysis\nFollowing the data science workflow, our first step is to define the purpose of the analysis. As mentioned, our guiding question is:\n\nWhich server or request characteristics have the biggest influence on response time?\n\nThis question can be approached in two different ways, inferential or predictive, depending on what we want the analysis to achieve.\n\nInferential analysis helps us understand and quantify how different variables affect the average outcome. For example: Does using a content delivery network (CDN) significantly reduce response time? How much slower are complex requests compared to simple ones?\nPredictive analysis, by contrast, is about making accurate forecasts. For instance: Can we predict the response time of a future request based on its characteristics?\n\nGamma regression can be used for either purpose, but our focus in this chapter is inferential, since our goal is to explain and quantify how different factors influence response time.\nTo that end, we will use Gamma regression to estimate how each feature affects the average response time while properly accounting for skewness in the outcome. This allows us to answer questions such as:\n\nHow do concurrent users affect response time?\nDoes request complexity lead to slower responses?\nWhat is the typical effect of CDN usage?\nHow are server load and memory usage related to delays?\n\nOur goal is not to build a black-box predictor, but to uncover which factors matter most and how they shape server performance, insights that can guide system tuning, policy decisions, and resource planning.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html#data-collection-and-wrangling",
    "href": "book/04-gamma.html#data-collection-and-wrangling",
    "title": "4  Gamma Regression",
    "section": "\n4.8 Data Collection and Wrangling",
    "text": "4.8 Data Collection and Wrangling\nWith our statistical question clearly defined, the next step is to make sure the data is suitable for analysis. Although we already have the dataset, it is useful to reflect on how this type of server performance data might have been collected. Understanding the data collection process helps us recognize potential limitations and prepare the data appropriately.\n\n4.8.1 Data Collection\nFor a study like this, where we analyze server response times across different conditions, data could have been collected through several common methods:\n\nServer Logs: Most web servers automatically record detailed logs of every request. These logs typically include information like timestamps, request types, response time, server load, and geographic origin. This is a likely source for our dataset and provides reliable, machine-recorded data.\nMonitoring Tools: Infrastructure monitoring platforms such as New Relic, Datadog, or AWS CloudWatch can capture performance metrics in real time. These tools often collect a richer set of metrics, including CPU usage, memory usage, and cache hit rates.\nSimulated Load Testing: In controlled environments, engineers may run scripted performance tests to evaluate how different types of requests behave under varying traffic loads. While simulated data offers consistency, it may not reflect real-world user behavior.\n\nEach data collection method has its own strengths and trade-offs:\n\nServer logs are objective but may lack high-level context (such as request complexity or user intent).\nMonitoring tools are detailed but might involve sampling or missing values due to downtime.\nSimulated tests offer control but may lack realism.\n\nRegardless of how the data was collected, it’s important to check for common issues such as:\n\nMissing values in fields like server load or memory usage\n\nIncorrect or inconsistent formatting (e.g., “Yes” vs “yes” vs “TRUE”)\n\nOutliers in response time that may skew the results\n\nCategorical variables that need to be converted into a numerical format for modeling\n\nAll of these issues will be addressed in the next stage, data wrangling.\n\n4.8.2 Data Wrangling\nOnce the data has been collected, the next step is to prepare it for modeling. Even reliable sources like server logs or monitoring tools produce raw data that needs cleaning before it can be used effectively.\nFor Gamma regression, data wrangling typically involves:\n\nValidating the response variable to ensure all values are strictly positive and meaningful.\nTransforming categorical variables (e.g., request complexity, geographic region) into numerical formats that the model can handle.\nChecking for missing values or extreme outliers that could bias results.\nScaling or normalizing predictors if large differences in measurement units make interpretation difficult.\n\nLet’s walk through how to apply these steps to our dataset.\n1. Validating the Response Variable\nGamma regression assumes the outcome is strictly positive and continuous, making it a good fit for response_time_ms, which records how long the server takes to respond to a request.\nBefore fitting the model, we need to confirm that all values in response_time_ms are greater than zero:\n\n\nR Code\nPython Code\n\n\n\n\nsummary(gamma_server$response_time_ms)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.50   19.18   49.05   93.32  111.45 1561.00 \n\n\n\n\n\n# Summary statistics for the response variable\nprint(data[\"response_time_ms\"].describe())\n\ncount    1000.000000\nmean       93.320300\nstd       141.940277\nmin         0.500000\n25%        19.175000\n50%        49.050000\n75%       111.450000\nmax      1561.000000\nName: response_time_ms, dtype: float64\n\n# Check for any non-positive values\nprint((data[\"response_time_ms\"] &lt;= 0).sum())\n\n0\n\n\n\n\n\nThe summary confirms that all values are strictly positive, so response_time_ms is suitable for Gamma regression. If zeros or negative values had been present, we may need to remove them, add a small offset, or just opt for another regression technique.\n2. Encoding Categorical Variables\nLike most regression and machine learning models, Gamma regression requires all inputs to be numeric. This means categorical variables must be converted into numerical form. In our dataset, this includes:\n\nday_of_week\ngeographic_region\nrequest_complexity\ncdn_usage\n\nThe standard approach is one-hot encoding, where each category is represented by a new binary column (0 or 1). For example, the variable request_complexity with three categories (Simple, Moderate, Complex) would become:\n\nrequest_complexity_Simple\nrequest_complexity_Moderate\nrequest_complexity_Complex\n\nTo avoid multicollinearity (perfect correlation among dummies), we drop one category from each group. The dropped category becomes the reference level that all other categories are compared against.\n\n\nR Code\nPython Code\n\n\n\n\ngamma_server_encoded &lt;- fastDummies::dummy_cols(\n  gamma_server,\n  select_columns = c(\"day_of_week\", \"geographic_region\", \"request_complexity\", \"cdn_usage\"),\n  remove_first_dummy = TRUE,\n  remove_selected_columns = TRUE\n)\n\n# Print resulting column names  \ncolnames(gamma_server_encoded)  \n\n [1] \"response_time_ms\"                \"concurrent_users\"               \n [3] \"database_queries\"                \"cache_hit_rate\"                 \n [5] \"server_load\"                     \"time_of_day\"                    \n [7] \"memory_usage\"                    \"day_of_week_Monday\"             \n [9] \"day_of_week_Saturday\"            \"day_of_week_Sunday\"             \n[11] \"day_of_week_Thursday\"            \"day_of_week_Tuesday\"            \n[13] \"day_of_week_Wednesday\"           \"geographic_region_Asia_Pacific\" \n[15] \"geographic_region_Europe\"        \"geographic_region_North_America\"\n[17] \"geographic_region_South_America\" \"request_complexity_Moderate\"    \n[19] \"request_complexity_Simple\"       \"cdn_usage_Yes\"                  \n\n\n\n\n\nimport pandas as pd\n\n# Perform one-hot encoding, drop first level from each category\ncategorical_vars = [\"day_of_week\", \"geographic_region\", \"request_complexity\", \"cdn_usage\"]\n\n# Create dummy variables, drop first category from each\ndata_encoded = pd.get_dummies(data, columns=categorical_vars, drop_first=True)\n\n# Print resulting column names  \nprint(data_encoded.columns.tolist())  \n\n['response_time_ms', 'concurrent_users', 'database_queries', 'cache_hit_rate', 'server_load', 'time_of_day', 'memory_usage', 'day_of_week_Monday', 'day_of_week_Saturday', 'day_of_week_Sunday', 'day_of_week_Thursday', 'day_of_week_Tuesday', 'day_of_week_Wednesday', 'geographic_region_Asia_Pacific', 'geographic_region_Europe', 'geographic_region_North_America', 'geographic_region_South_America', 'request_complexity_Moderate', 'request_complexity_Simple', 'cdn_usage_Yes']\n\n\n\n\n\nThis step ensures that all features going into the Gamma regression model are numeric and suitable for estimation. It also allows us to interpret the effect of each category relative to the baseline, which is the dropped category from each group.\nFor example, the variable request_complexity originally had three categories: Simple, Moderate, and Complex. After encoding, the dataset contains two dummy variables:\n\nrequest_complexity_Simple\nrequest_complexity_Moderate\n\nHere, Complex has been dropped and serves as the reference category. This means:\n\nThe coefficient for request_complexity_Simple tells us how much faster or slower requests marked as Simple are compared to Complex.\nThe coefficient for request_complexity_Moderate tells us the difference between Moderate and Complex.\n\nThis same logic applies to all other categorical variables: one level is chosen as the baseline, and all coefficients are interpreted relative to it.\n3. Checking for Missing Values\nBefore fitting a regression model, it’s important to confirm that the dataset has no gaps. Missing values can cause estimation errors or lead to dropped observations if not handled properly.\n\n\nR Code\nPython Code\n\n\n\n\n# Count missing values in each column\ncolSums(is.na(gamma_server_encoded))\n\n               response_time_ms                concurrent_users \n                              0                               0 \n               database_queries                  cache_hit_rate \n                              0                               0 \n                    server_load                     time_of_day \n                              0                               0 \n                   memory_usage              day_of_week_Monday \n                              0                               0 \n           day_of_week_Saturday              day_of_week_Sunday \n                              0                               0 \n           day_of_week_Thursday             day_of_week_Tuesday \n                              0                               0 \n          day_of_week_Wednesday  geographic_region_Asia_Pacific \n                              0                               0 \n       geographic_region_Europe geographic_region_North_America \n                              0                               0 \ngeographic_region_South_America     request_complexity_Moderate \n                              0                               0 \n      request_complexity_Simple                   cdn_usage_Yes \n                              0                               0 \n\n\n\n\n\n# Count missing values in each column\ndata_encoded.isna().sum()\n\nresponse_time_ms                   0\nconcurrent_users                   0\ndatabase_queries                   0\ncache_hit_rate                     0\nserver_load                        0\ntime_of_day                        0\nmemory_usage                       0\nday_of_week_Monday                 0\nday_of_week_Saturday               0\nday_of_week_Sunday                 0\nday_of_week_Thursday               0\nday_of_week_Tuesday                0\nday_of_week_Wednesday              0\ngeographic_region_Asia_Pacific     0\ngeographic_region_Europe           0\ngeographic_region_North_America    0\ngeographic_region_South_America    0\nrequest_complexity_Moderate        0\nrequest_complexity_Simple          0\ncdn_usage_Yes                      0\ndtype: int64\n\n\n\n\n\nIf missing values were present, we would need to either:\n\nRemove rows with missing values (simple but may reduce sample size), or\nImpute values using the mean, median, or another suitable method.\n\nIn our toy dataset, all variables are complete, so no further action is needed here.\n4. Examining Potential Outliers\nOutliers can distort regression results, so it’s always good practice to check the distribution of the response variable. In our case, we visualize response_time_ms:\n\n\nR Code\nPython Code\n\n\n\n\nlibrary(ggplot2)\n\nggplot(gamma_server_encoded, aes(x = response_time_ms)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Distribution of Response Time\", x = \"Response Time (ms)\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Histogram of response time\nplt.figure(figsize=(8, 5))\nsns.histplot(data_encoded[\"response_time_ms\"], bins=30, color=\"steelblue\", edgecolor=\"white\")\nplt.title(\"Distribution of Response Time\")\nplt.xlabel(\"Response Time (ms)\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe histogram confirms that response_time_ms is right-skewed, which aligns with our expectations and supports the use of Gamma regression.\n\n\nSkewed Data\n\n\nIt’s important to note that with Gamma-distributed data, large values in the upper tail are often part of the natural variation, not necessarily outliers to be removed. Instead of trimming them away, we rely on a model like Gamma regression that is designed to accommodate this skewness.\n\n\n5. Scaling Continuous Variables (Optional)\nGamma regression does not require predictors to be on the same scale. However, scaling can sometimes help with model convergence and make coefficients easier to interpret, especially when predictors have very different ranges (for example, concurrent_users in the hundreds vs. cache_hit_rate as a percentage).\nFor this dataset, it may be helpful to standardize the following continuous variables:\n\nconcurrent_users\ndatabase_queries\ncache_hit_rate\nserver_load\nmemory_usage\ntime_of_day\n\n\n\nR Code\nPython Code\n\n\n\n\n# gamma_server_encoded &lt;- gamma_server_encoded %&gt;%\n#   mutate(across(c(concurrent_users, database_queries, cache_hit_rate, server_load, memory_usage, time_of_day), scale))\n\n\n\n\n# from sklearn.preprocessing import StandardScaler\n\n# # List of columns to scale\n# scale_cols = [\"concurrent_users\", \"database_queries\", \"cache_hit_rate\", \"server_load\", \"memory_usage\", \"time_of_day\"]\n\n# # Initialize scaler and apply to selected columns\n# scaler = StandardScaler()\n# data_encoded[scale_cols] = scaler.fit_transform(data_encoded[scale_cols])\n\n\n\n\nStandardizing doesn’t change the underlying fit of the Gamma regression, but it can make estimation numerically more stable and the coefficients easier to compare side by side.\nAfter scaling, coefficients are interpreted differently: they now represent the effect of a one-standard-deviation increase in the predictor, rather than a one-unit increase.\n\n\nExample of Interpretation After Scaling\n\n\nWithout scaling, suppose the coefficient for cache_hit_rate (measured as a percentage from 0–100) is -0.01. This means that for every 1 percentage point increase in cache hit rate, the expected response time decreases slightly.\nWith scaling, suppose the coefficient for cache_hit_rate becomes -0.25. This means that a one-standard-deviation increase in cache hit rate (say, about 15 percentage points in this dataset) is associated with a 0.25 decrease (on the log scale) in expected response time.\nThis rescaling doesn’t change the underlying relationship. It just shifts the unit of interpretation from “1 raw unit” to “1 standard deviation,” which often makes cross-variable comparisons more meaningful.\n\n\nFor simplicity, we will not scale the variables in this case study, but it’s a useful option to keep in mind.\nA Note on Data Splitting\nIn many machine learning tasks, it is common to split the data into training and test sets. However, in this case study, we are not aiming to build a predictive model. Our goal is to understand how different server and request characteristics influence the average response time, which is an inferential task.\nBecause we are focused on interpreting the model rather than predicting new outcomes, we will use the entire dataset for model fitting. This allows us to make the best use of our data when estimating the effects of each variable.\nIf our goal were to build a model that predicts server response time for future requests, we would consider splitting the dataset and evaluating prediction accuracy on a holdout set.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html#exploratory-data-analysis-eda",
    "href": "book/04-gamma.html#exploratory-data-analysis-eda",
    "title": "4  Gamma Regression",
    "section": "\n4.9 Exploratory Data Analysis (EDA)",
    "text": "4.9 Exploratory Data Analysis (EDA)\nThe data is now ready and cleaned, but before we begin modeling, it is essential to explore the dataset to understand the structure of the variables and how they relate to one another. This step, known as Exploratory Data Analysis (EDA), helps uncover patterns, identify potential issues such as skewness or outliers, and inform the modeling decisions that follow.\n\n4.9.1 Classifying Variables\nA good starting point in EDA is to classify the variables in our dataset, as this guides the choice of plots and informs how we will encode each variable when building the model.\nOur response variable is:\n\n\nresponse_time_ms: a continuous, strictly positive variable measuring how long a server takes to respond to a request. This is well-suited for Gamma regression.\n\nOur regressors fall into two groups:\n\n\nContinuous variables:\n\n\nconcurrent_users, database_queries, cache_hit_rate, server_load, memory_usage, time_of_day\n\n\n\n\nCategorical variables:\n\n\ncdn_usage, request_complexity, day_of_week, geographic_region\n\n\n\n\n4.9.2 Visualizing Variable Distributions\nWith the variables classified, we can now choose appropriate plots to explore their distributions and relationships.\nResponse Variable: response_time_ms\n\nWe begin by examining the distribution of the response variable.\n\n\nR Code\nPython Code\n\n\n\n\nlibrary(ggplot2)\n\nggplot(gamma_server_encoded, aes(x = response_time_ms)) +\n  geom_histogram(bins = 10, fill = \"skyblue\", color = \"white\") +\n  labs(title = \"Histogram of Response Time (ms)\",\n       x = \"Response Time (ms)\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.histplot(data_encoded[\"response_time_ms\"], bins=10, kde=True, color=\"skyblue\")\nplt.title(\"Histogram of Response Time (ms)\")\nplt.xlabel(\"Response Time (ms)\")\nplt.ylabel(\"Frequency\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe histogram confirms that response_time_ms is strictly positive and right-skewed—just as we saw during data wrangling when we checked for negative values and visualized potential outliers.\nCategorical Variables\nWe next examine how the categorical predictors are distributed. This visualization is based on the original dataset, before encoding. Visualizing these variables helps us check if any levels are underrepresented or imbalanced, which can affect model stability and interpretation.\n\n\nR Code\nPython Code\n\n\n\n\nlibrary(ggplot2)\n\ncategorical_vars &lt;- c(\"cdn_usage\", \"request_complexity\", \"day_of_week\", \"geographic_region\")\n\nfor (var in categorical_vars) {\n  p &lt;- ggplot(gamma_server, aes(x = .data[[var]])) +\n    geom_bar(fill = \"steelblue\") +\n    labs(\n      title = paste(\"Distribution of\", var),\n      x = var,\n      y = \"Count\"\n    ) +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  \n  print(p)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncategorical_vars = [\"cdn_usage\", \"request_complexity\", \"day_of_week\", \"geographic_region\"]\n\nfor var in categorical_vars:\n    sns.countplot(\n        x=var,\n        data=data,\n        hue=var,\n        palette=\"pastel\",\n        legend=False\n    )\n    plt.title(f\"{var.replace('_', ' ').title()} Distribution\")\n    plt.xlabel(var.replace(\"_\", \" \").title())\n    plt.ylabel(\"Count\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n&lt;Axes: xlabel='cdn_usage', ylabel='count'&gt;\nText(0.5, 1.0, 'Cdn Usage Distribution')\nText(0.5, 0, 'Cdn Usage')\nText(0, 0.5, 'Count')\n([0, 1], [Text(0, 0, 'Yes'), Text(1, 0, 'No')])\n&lt;Axes: xlabel='request_complexity', ylabel='count'&gt;\nText(0.5, 1.0, 'Request Complexity Distribution')\nText(0.5, 0, 'Request Complexity')\nText(0, 0.5, 'Count')\n([0, 1, 2], [Text(0, 0, 'Moderate'), Text(1, 0, 'Complex'), Text(2, 0, 'Simple')])\n&lt;Axes: xlabel='day_of_week', ylabel='count'&gt;\nText(0.5, 1.0, 'Day Of Week Distribution')\nText(0.5, 0, 'Day Of Week')\nText(0, 0.5, 'Count')\n([0, 1, 2, 3, 4, 5, 6], [Text(0, 0, 'Saturday'), Text(1, 0, 'Thursday'), Text(2, 0, 'Monday'), Text(3, 0, 'Friday'), Text(4, 0, 'Tuesday'), Text(5, 0, 'Wednesday'), Text(6, 0, 'Sunday')])\n&lt;Axes: xlabel='geographic_region', ylabel='count'&gt;\nText(0.5, 1.0, 'Geographic Region Distribution')\nText(0.5, 0, 'Geographic Region')\nText(0, 0.5, 'Count')\n([0, 1, 2, 3, 4], [Text(0, 0, 'North_America'), Text(1, 0, 'Africa'), Text(2, 0, 'South_America'), Text(3, 0, 'Europe'), Text(4, 0, 'Asia_Pacific')])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBy examining these distributions, we can ensure that each category has enough observations to be meaningfully included in the model. If a category is rarely observed, it may need to be combined with others or omitted.\nNumerical Variables\nNext, we examine the distributions of the continuous predictors to understand their range, shape, and potential skewness.\n\n\nR Code\nPython Code\n\n\n\n\nlibrary(ggplot2)\n\ncontinuous_vars &lt;- c(\n  \"concurrent_users\", \"database_queries\", \"cache_hit_rate\",\n  \"server_load\", \"memory_usage\", \"time_of_day\"\n)\n\nfor (var in continuous_vars) {\n  p &lt;- ggplot(gamma_server, aes(x = .data[[var]])) +\n    geom_histogram(bins = 10, fill = \"cornflowerblue\", color = \"white\") +\n    labs(\n      title = paste(\"Distribution of\", var),\n      x = var,\n      y = \"Frequency\"\n    ) +\n    theme_minimal()\n  \n  print(p)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontinuous_vars = [\"concurrent_users\", \"database_queries\", \"cache_hit_rate\", \"server_load\", \"memory_usage\", \"time_of_day\"]\n\nfor var in continuous_vars:\n    sns.histplot(data[var], bins=10, kde=True, color=\"cornflowerblue\")\n    plt.title(f\"Distribution of {var}\")\n    plt.xlabel(var)\n    plt.ylabel(\"Frequency\")\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs we can see from the histograms, nothing appears too concerning. Both concurrent_users and database_queries show clear right-skewness, while the remaining continuous variables have more uniform or evenly spread distributions. memory_usage shows a lower count in its final (largest) bin, but this pattern is not unusual and does not raise any major concerns for our analysis.\n\n4.9.3 Visualizing Predictor Relationships to Response\nHaving explored the univariate distributions, we can now turn to bivariate visualizations to examine how each predictor relates to the response.\nCategorical Predictors vs. Response\nFor categorical predictors, box plots provide a clear and effective way to visualize how response times vary across different groups.\n\n\nR Code\nPython Code\n\n\n\n\ncategorical_vars &lt;- c(\"cdn_usage\", \"request_complexity\", \"day_of_week\", \"geographic_region\")\n\nfor (var in categorical_vars) {\n  p &lt;- ggplot(gamma_server, aes(x = .data[[var]], y = response_time_ms)) +\n    geom_boxplot(fill = \"lightblue\") +\n    labs(\n      title = paste(\"Response Time by\", var),\n      x = var,\n      y = \"Response Time (ms)\"\n    ) +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  \n  print(p)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor var in [\"cdn_usage\", \"request_complexity\", \"day_of_week\", \"geographic_region\"]:\n    sns.boxplot(\n        x=var,\n        y=\"response_time_ms\",\n        data=data,\n        hue=var,         # required if using palette\n        palette=\"Set3\",\n        dodge=False      # keep single box per category\n    )\n    plt.legend([], [], frameon=False)  # hide duplicated legend\n    plt.title(f\"Response Time by {var.replace('_', ' ').title()}\")\n    plt.xlabel(var.replace('_', ' ').title())\n    plt.ylabel(\"Response Time (ms)\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()\n\n&lt;Axes: xlabel='cdn_usage', ylabel='response_time_ms'&gt;\n&lt;matplotlib.legend.Legend object at 0x14d292990&gt;\nText(0.5, 1.0, 'Response Time by Cdn Usage')\nText(0.5, 0, 'Cdn Usage')\nText(0, 0.5, 'Response Time (ms)')\n([0, 1], [Text(0, 0, 'Yes'), Text(1, 0, 'No')])\n&lt;Axes: xlabel='request_complexity', ylabel='response_time_ms'&gt;\n&lt;matplotlib.legend.Legend object at 0x14d6c2850&gt;\nText(0.5, 1.0, 'Response Time by Request Complexity')\nText(0.5, 0, 'Request Complexity')\nText(0, 0.5, 'Response Time (ms)')\n([0, 1, 2], [Text(0, 0, 'Moderate'), Text(1, 0, 'Complex'), Text(2, 0, 'Simple')])\n&lt;Axes: xlabel='day_of_week', ylabel='response_time_ms'&gt;\n&lt;matplotlib.legend.Legend object at 0x14d7a7090&gt;\nText(0.5, 1.0, 'Response Time by Day Of Week')\nText(0.5, 0, 'Day Of Week')\nText(0, 0.5, 'Response Time (ms)')\n([0, 1, 2, 3, 4, 5, 6], [Text(0, 0, 'Saturday'), Text(1, 0, 'Thursday'), Text(2, 0, 'Monday'), Text(3, 0, 'Friday'), Text(4, 0, 'Tuesday'), Text(5, 0, 'Wednesday'), Text(6, 0, 'Sunday')])\n&lt;Axes: xlabel='geographic_region', ylabel='response_time_ms'&gt;\n&lt;matplotlib.legend.Legend object at 0x14d772dd0&gt;\nText(0.5, 1.0, 'Response Time by Geographic Region')\nText(0.5, 0, 'Geographic Region')\nText(0, 0.5, 'Response Time (ms)')\n([0, 1, 2, 3, 4], [Text(0, 0, 'North_America'), Text(1, 0, 'Africa'), Text(2, 0, 'South_America'), Text(3, 0, 'Europe'), Text(4, 0, 'Asia_Pacific')])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese box plots help us assess whether server response times differ meaningfully across categories. From these visualizations, request_complexity appears to have a noticeable impact on response_time_ms. At this stage, this is an educated guess based on patterns in the plots, but we’ll need statistical modeling to confirm whether the differences are truly meaningful. Apparent differences in the box plots could simply be due to random variation, limited sample size in certain groups, or interactions with other predictors.\nContinuous Predictors vs. Response\nFor continuous predictors, scatterplots provide a clear view of how each variable is associated with the response.\n\n\nR Code\nPython Code\n\n\n\n\ncontinuous_vars &lt;- c(\n  \"concurrent_users\", \"database_queries\", \"cache_hit_rate\",\n  \"server_load\", \"memory_usage\", \"time_of_day\"\n)\n\nfor (var in continuous_vars) {\n  p &lt;- ggplot(gamma_server, aes(x = .data[[var]], y = response_time_ms)) +\n    geom_point(color = \"#008080\") +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    labs(\n      title = paste(var, \"vs. Response Time\"),\n      x = var,\n      y = \"Response Time (ms)\"\n    ) +\n    theme_minimal()\n  \n  print(p)\n}\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nfor var in [\"concurrent_users\", \"database_queries\", \"cache_hit_rate\", \"server_load\", \"memory_usage\", \"time_of_day\"]:\n\n    sns.scatterplot(x=var, y=\"response_time_ms\", data=data, color=\"teal\")\n    sns.regplot(x=var, y=\"response_time_ms\", data=data, scatter=False, color=\"red\")\n    plt.title(f\"{var.replace('_', ' ').title()} vs. Response Time\")\n    plt.xlabel(var.replace('_', ' ').title())\n    plt.ylabel(\"Response Time (ms)\")\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese plots help reveal trends or patterns that might inform the model structure, such as the need for transformations or interaction terms. In our case, however, none of the scatterplots show a particularly strong or obvious pattern that would suggest immediate adjustments. This is not unusual when working with skewed outcomes like response times: relationships may become clearer only after applying the link function (e.g., the log link in Gamma regression) or after controlling for multiple predictors simultaneously.\n\n4.9.4 Key Takeaways from EDA\nFrom our exploratory analysis, we can make a few early observations:\n\nThe response variable (response_time_ms) is strictly positive and right-skewed, making Gamma regression an appropriate choice.\nMost requests come from North America, while Africa and South America have relatively fewer entries.\nThe concurrent_users variable is also right-skewed, indicating that most requests happen under low to moderate user load, but some occur under heavy load.\nComplex requests tend to have higher response times on average, as seen in the boxplots.\n\nThese patterns help us understand the structure of the data and form preliminary hypotheses about which factors may influence response time. Keep these observations in mind. We’ll soon see whether the Gamma regression model reinforces or challenges them.\n\n\nTip\n\n\nEDA is meant to explore patterns, not confirm conclusions. While these insights help guide model building, we should wait until after model fitting and diagnostics before making final interpretations.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html#data-modeling",
    "href": "book/04-gamma.html#data-modeling",
    "title": "4  Gamma Regression",
    "section": "\n4.10 Data Modeling",
    "text": "4.10 Data Modeling\nAfter conducting Exploratory Data Analysis (EDA), we now move into the modeling stage, where we apply a statistical model to better understand how different factors influence server response times.\n\n4.10.1 Choosing a Suitable Regression Model\nThe choice of regression model depends on both the nature of the response variable and the goal of the analysis. In our case:\n\nThe response variable, response_time_ms, is continuous, strictly positive, and right-skewed.\nThe goal is to explain which factors influence the average response time, not to predict future outcomes.\n\nGiven these characteristics, a Gamma regression model with a log link is appropriate. While we had a sense of this at the beginning, the decision is best made after examining the data and understanding its distribution and structure. Once the exploratory analysis confirms positivity and skewness, Gamma regression becomes a natural choice.\nGamma regression allows us to:\n\nmodel the response in its original (positive-only) scale,\nhandle right-skewed data,\nand interpret results in terms of multiplicative changes in the mean response.\n\nIn contrast, Ordinary Least Squares (OLS) assumes constant variance and symmetry around the mean, assumptions that clearly do not hold in this dataset. This makes Gamma regression a more suitable and robust option for our analysis.\n\n4.10.2 Defining Modeling Parameters\nWith the model selected, our next step is to specify the variables that will go into the equation. In this analysis, we use all available predictors. Together, these variables capture a mix of technical metrics (e.g., server load, memory usage), behavioral context (e.g., request complexity, time of day), and infrastructure details (e.g., CDN usage, geographic region). Using the full set of variables provides a well-rounded view of the factors that may influence response time. Therefore, we will include all predictors in the model.\n\n4.10.3 Setting Up the Modeling Equation\nBy including all predictors in the Gamma regression, we arrive at the following model equation. This equation represents the log of the average response time as a linear function of the predictors.\n\n\\[\n\\begin{align*}\n\\log(\\mu_i) &= \\beta_0 \\\\\n&\\quad + \\beta_1 \\times \\text{concurrent\\_users}_i \\\\\n&\\quad + \\beta_2 \\times \\text{database\\_queries}_i \\\\\n&\\quad + \\beta_3 \\times \\text{cache\\_hit\\_rate}_i \\\\\n&\\quad + \\beta_4 \\times \\text{server\\_load}_i \\\\\n&\\quad + \\beta_5 \\times \\text{memory\\_usage}_i \\\\\n&\\quad + \\beta_6 \\times \\text{time\\_of\\_day}_i \\\\\n&\\quad + \\beta_7 \\times \\text{cdn\\_usage}_i \\\\\n&\\quad + \\beta_8 \\times \\text{request\\_complexity}_i \\\\\n&\\quad + \\beta_9 \\times \\text{day\\_of\\_week}_i \\\\\n&\\quad + \\beta_{10} \\times \\text{geographic\\_region}_i\n\\end{align*}\n\\]\n\nThe beauty of mathematics is that we can then back-transform the predicted value from the log scale using the exponential function:\n\\[\n\\mu_i = \\exp(\\text{linear combination of predictors})\n\\]\nwhere:\n\n\n\\(\\mu_i\\) is the expected average response time for the \\(i\\)-th request\n\n\n\\(\\beta_0\\) is the intercept\n\n\n\\(\\beta_1, \\beta_2, \\ldots \\beta_{10}\\) are the coefficients for each predictor\n\nThis means that each coefficient (\\(\\beta_n\\)) describes the multiplicative effect on the mean response time:\n\n\n\\(\\beta_1\\): How response time changes as the number of concurrent users increases\n\n\n\\(\\beta_7\\): The impact of CDN usage versus not using a CDN\n\n\n\\(\\beta_8\\): How request complexity levels (Moderate or Simple) compare to the base level (Complex)\n\n\n\\(\\beta_{10}\\): Regional differences in server response behavior\n\nThis model structure allows us to capture both technical and contextual effects on server performance using a regression framework suited for positive and skewed outcomes.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html#estimation",
    "href": "book/04-gamma.html#estimation",
    "title": "4  Gamma Regression",
    "section": "\n4.11 Estimation",
    "text": "4.11 Estimation\nWith the data modeling stage completed, we now move on to estimation, where we actually fit the Gamma regression model to the data and obtain numerical estimates for each coefficient. These estimates allow us to quantify how each predictor influences the average server response time under the specified model.\n\n\nHow Gamma Regression Is Fit\n\n\nUnlike OLS, which minimizes squared errors, Gamma regression relies on maximum likelihood estimation (MLE). MLE is more appropriate when the outcome is strictly positive and right-skewed, as it takes into account the distributional assumptions of the Gamma model.\n\n\n\n4.11.1 Fitting the Model\nIn R, you can use the glm() function with family = Gamma(link = \"log\"). In Python, we use statsmodels with the Gamma family and log link:\n\n\nR Code\nPython Code\n\n\n\n\n# Load required library\nlibrary(stats)\n\n# Fit Gamma regression model\ngamma_model &lt;- glm(\n  response_time_ms ~ concurrent_users + database_queries + cache_hit_rate +\n    server_load + time_of_day + memory_usage +\n    day_of_week_Monday + day_of_week_Saturday + day_of_week_Sunday +\n    day_of_week_Thursday + day_of_week_Tuesday + day_of_week_Wednesday +\n    geographic_region_Asia_Pacific + geographic_region_Europe +\n    geographic_region_North_America + geographic_region_South_America +\n    request_complexity_Moderate + request_complexity_Simple +\n    cdn_usage_Yes,\n  family = Gamma(link = \"log\"),\n  data = gamma_server_encoded\n)\n\n# Display model summary\nsummary(gamma_model)\n\n\nCall:\nglm(formula = response_time_ms ~ concurrent_users + database_queries + \n    cache_hit_rate + server_load + time_of_day + memory_usage + \n    day_of_week_Monday + day_of_week_Saturday + day_of_week_Sunday + \n    day_of_week_Thursday + day_of_week_Tuesday + day_of_week_Wednesday + \n    geographic_region_Asia_Pacific + geographic_region_Europe + \n    geographic_region_North_America + geographic_region_South_America + \n    request_complexity_Moderate + request_complexity_Simple + \n    cdn_usage_Yes, family = Gamma(link = \"log\"), data = gamma_server_encoded)\n\nCoefficients:\n                                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      4.7603924  0.1971945  24.141  &lt; 2e-16 ***\nconcurrent_users                 0.0029522  0.0002651  11.134  &lt; 2e-16 ***\ndatabase_queries                 0.0737204  0.0045095  16.348  &lt; 2e-16 ***\ncache_hit_rate                  -0.0192125  0.0012367 -15.536  &lt; 2e-16 ***\nserver_load                      0.0171491  0.0010331  16.599  &lt; 2e-16 ***\ntime_of_day                      0.0020302  0.0035487   0.572  0.56740    \nmemory_usage                     0.0096178  0.0012127   7.931 5.91e-15 ***\nday_of_week_Monday              -0.0353228  0.0908594  -0.389  0.69754    \nday_of_week_Saturday            -0.1781832  0.0968001  -1.841  0.06596 .  \nday_of_week_Sunday              -0.2334680  0.0954806  -2.445  0.01465 *  \nday_of_week_Thursday            -0.1025488  0.0899490  -1.140  0.25453    \nday_of_week_Tuesday              0.0298651  0.0932648   0.320  0.74887    \nday_of_week_Wednesday           -0.0845206  0.0909808  -0.929  0.35312    \ngeographic_region_Asia_Pacific  -0.3520001  0.1261621  -2.790  0.00537 ** \ngeographic_region_Europe        -0.5514433  0.1226661  -4.495 7.77e-06 ***\ngeographic_region_North_America -0.7498536  0.1193010  -6.285 4.91e-10 ***\ngeographic_region_South_America -0.1160509  0.1385021  -0.838  0.40229    \nrequest_complexity_Moderate     -0.5889707  0.0797133  -7.389 3.17e-13 ***\nrequest_complexity_Simple       -1.3210568  0.0768739 -17.185  &lt; 2e-16 ***\ncdn_usage_Yes                   -0.3824307  0.0542098  -7.055 3.27e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.6007392)\n\n    Null deviance: 1447.44  on 999  degrees of freedom\nResidual deviance:  565.82  on 980  degrees of freedom\nAIC: 10028\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Fit Gamma regression model using log link\ngamma_model = smf.glm(\n    formula=\"\"\"\n        response_time_ms ~ concurrent_users + database_queries + cache_hit_rate +\n        server_load + time_of_day + memory_usage +\n        day_of_week_Monday + day_of_week_Saturday + day_of_week_Sunday +\n        day_of_week_Thursday + day_of_week_Tuesday + day_of_week_Wednesday +\n        geographic_region_Asia_Pacific + geographic_region_Europe +\n        geographic_region_North_America + geographic_region_South_America +\n        request_complexity_Moderate + request_complexity_Simple +\n        cdn_usage_Yes\n    \"\"\",\n    data=data_encoded,\n    family=sm.families.Gamma(link=sm.families.links.log())\n).fit()\n\n/Users/alexi/Library/Caches/org.R-project.R/R/reticulate/uv/cache/archive-v0/9Inoul8SNCRkiWJj10t6A/lib/python3.11/site-packages/statsmodels/genmod/families/links.py:13: FutureWarning: The log link alias is deprecated. Use Log instead. The log link alias will be removed after the 0.15.0 release.\n  warnings.warn(\n\n# Display model summary\nprint(gamma_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:       response_time_ms   No. Observations:                 1000\nModel:                            GLM   Df Residuals:                      980\nModel Family:                   Gamma   Df Model:                           19\nLink Function:                    log   Scale:                         0.60074\nMethod:                          IRLS   Log-Likelihood:                -4996.9\nDate:                Wed, 21 Jan 2026   Deviance:                       565.82\nTime:                        20:13:25   Pearson chi2:                     589.\nNo. Iterations:                    17   Pseudo R-squ. (CS):             0.7695\nCovariance Type:            nonrobust                                         \n===========================================================================================================\n                                              coef    std err          z      P&gt;|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------------------------------\nIntercept                                   4.7604      0.197     24.140      0.000       4.374       5.147\nday_of_week_Monday[T.True]                 -0.0353      0.091     -0.389      0.697      -0.213       0.143\nday_of_week_Saturday[T.True]               -0.1782      0.097     -1.841      0.066      -0.368       0.012\nday_of_week_Sunday[T.True]                 -0.2335      0.095     -2.445      0.014      -0.421      -0.046\nday_of_week_Thursday[T.True]               -0.1026      0.090     -1.140      0.254      -0.279       0.074\nday_of_week_Tuesday[T.True]                 0.0299      0.093      0.320      0.749      -0.153       0.213\nday_of_week_Wednesday[T.True]              -0.0845      0.091     -0.929      0.353      -0.263       0.094\ngeographic_region_Asia_Pacific[T.True]     -0.3520      0.126     -2.790      0.005      -0.599      -0.105\ngeographic_region_Europe[T.True]           -0.5514      0.123     -4.495      0.000      -0.792      -0.311\ngeographic_region_North_America[T.True]    -0.7499      0.119     -6.285      0.000      -0.984      -0.516\ngeographic_region_South_America[T.True]    -0.1161      0.139     -0.838      0.402      -0.388       0.155\nrequest_complexity_Moderate[T.True]        -0.5890      0.080     -7.389      0.000      -0.745      -0.433\nrequest_complexity_Simple[T.True]          -1.3211      0.077    -17.185      0.000      -1.472      -1.170\ncdn_usage_Yes[T.True]                      -0.3824      0.054     -7.055      0.000      -0.489      -0.276\nconcurrent_users                            0.0030      0.000     11.134      0.000       0.002       0.003\ndatabase_queries                            0.0737      0.005     16.348      0.000       0.065       0.083\ncache_hit_rate                             -0.0192      0.001    -15.536      0.000      -0.022      -0.017\nserver_load                                 0.0171      0.001     16.599      0.000       0.015       0.019\ntime_of_day                                 0.0020      0.004      0.572      0.567      -0.005       0.009\nmemory_usage                                0.0096      0.001      7.931      0.000       0.007       0.012\n===========================================================================================================\n\n\n\n\n\n\n4.11.2 Interpreting the Coefficients\nThe modelling was a success! Based on our estimates, the fitted equation is as follow:\n\n\\[\n\\begin{align*}\n\\log(\\mu) &= 5.85 \\\\\n&\\quad + 0.276 \\times \\text{concurrent\\_users} \\\\\n&\\quad + 0.407 \\times \\text{database\\_queries} \\\\\n&\\quad - 0.384 \\times \\text{cache\\_hit\\_rate} \\\\\n&\\quad + 0.411 \\times \\text{server\\_load} \\\\\n&\\quad + 0.197 \\times \\text{memory\\_usage} \\\\\n&\\quad + \\cdots \\\\\n&\\quad - 0.116 \\times \\text{geographic\\_region\\_South\\_America}\n\\end{align*}\n\\]",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html#goodness-of-fit",
    "href": "book/04-gamma.html#goodness-of-fit",
    "title": "4  Gamma Regression",
    "section": "\n4.12 Goodness of Fit",
    "text": "4.12 Goodness of Fit\nAll of the numbers above look encouraging, but it’s important to make sure they are reliable. Before drawing conclusions, we need to check whether the model has successfully captured the main patterns in the data, or whether it’s missing something important. This brings us to the next stage: goodness of fit, where we evaluate how well the Gamma regression model represents the underlying data.\n\n4.12.1 Deviance Residuals vs. Fitted Values\nIn generalized linear models like Gamma regression, we use something called deviance residuals to examine goodness of fit. These are special residuals that account for the type of model we’re using. They’re designed to help us detect if the model is systematically making mistakes.\n\n\nDefinition of Deviance Residual\n\n\nDeviance Residuals measure how far each observation is from what the model expects, using the correct distribution (in this case, the Gamma distribution). You can think of it as:\n\nA standardized measure of “how surprising” each data point is under the fitted model.\n\nSome key points:\n\nDeviance residuals are always centered around zero.\nLarge positive or negative values indicate observations the model struggles to fit.\nPatterns in a residual–fitted plot can reveal model misspecification (e.g., wrong link function, missing predictors, nonlinear relationships).\n\n\n\nBelow is a plot of deviance residuals against the model’s fitted values (the predicted average response time for each observation):\n\n\nR Code\nPython Code\n\n\n\n\n# Plot deviance residuals vs fitted values\nplot(gamma_model$fitted.values, residuals(gamma_model, type = \"deviance\"),\n     main = \"Deviance Residuals vs Fitted Values\",\n     xlab = \"Fitted Values\",\n     ylab = \"Deviance Residuals\",\n     pch = 19, col = \"blue\")\nabline(h = 0, lty = 2, col = \"red\")\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n# Fitted values and deviance residuals\nfitted_values = gamma_model.fittedvalues\ndeviance_residuals = gamma_model.resid_deviance\n\n# Plot\nplt.scatter(fitted_values, deviance_residuals, color=\"blue\", s=30)\nplt.axhline(0, linestyle=\"--\", color=\"red\")\nplt.title(\"Deviance Residuals vs Fitted Values\")\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Deviance Residuals\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIn a well-fitting model, we expect the deviance residuals to be centered around zero, without any noticeable patterns or trends, and to be spread out fairly evenly across the range of fitted values. When we look at this plot, it meets these expectations quite well.\n\n\nWhy the Spread Looks Flat\n\n\nYou might expect residuals to spread out more as the predicted value increases, especially in a Gamma model, where variance increases with the mean. But remember: deviance residuals have already been scaled to account for that.\nThat’s why the spread of points looks more even instead of fanning outward. This is actually a good sign. It tells us the model is appropriately handling the increasing variability.\n\n\n\n4.12.2 Deviance and AIC\nThe deviance residual plot helps us see whether individual observations behave as the model expects. However, residuals only tell part of the story. They show where the model fits well or poorly, but not how well the model fits overall.\nTo assess overall model quality, we turn to model-level deviance, which summarizes the total remaining error after fitting the Gamma regression.\nResidual Deviance\nIn linear regression, we often use the sum of squared errors to summarize model fit. In Gamma regression, an analogous measure is deviance, which quantifies the discrepancy between the model and the observed data in a way appropriate for the Gamma distribution.\nThere are two key deviance values:\n\n\nNull deviance is the error from a model that uses only the overall mean response time.\n\nResidual deviance is the error remaining after fitting the Gamma regression with all predictors included.\n\nWe also compare this to the null deviance, which is the error you’d get if the model only used the overall average and ignored all predictors.\nHere are the values from our analysis:\n\n\nNull deviance: 1447.44\n\n\nResidual deviance: 565.82\n\nThe large drop from the null deviance to the residual deviance indicates that our predictors substantially improve the model’s ability to explain variation in response time.\nAIC (Akaike Information Criterion)\nAnother useful summary statistic is the Akaike Information Criterion (AIC). You can think of AIC as a score that balances two goals:\n\nHow well the model fits the data\n\nHow simple the model is (fewer predictors = simpler)\n\nA lower AIC indicates a better overall balance between fit and complexity. Our model’s AIC is:\n\n\nAIC: 10028\n\nThis number isn’t very informative on its own. It becomes meaningful when we compare it to the AIC of alternative models (for example, a model with fewer predictors or a different link function). In such comparisons, the model with the lower AIC is preferred, because it captures the data more efficiently without unnecessary complexity.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html#results",
    "href": "book/04-gamma.html#results",
    "title": "4  Gamma Regression",
    "section": "\n4.13 Results",
    "text": "4.13 Results\nWith the goodness-of-fit checks completed, we can be reasonably confident that our model provides a reliable representation of the data. Now we can return to our original question:\n\nWhat factors influence the average response time of a server?\n\nBefore interpreting the coefficients, remember that we used a Gamma regression with a log link. This means that, unlike ordinary least squares, coefficients operate multiplicatively on the original scale: a one-unit change in a predictor corresponds to a percentage change in the expected response time, holding all other variables constant.\n\n4.13.1 Statistically Significant Predictors\nSeveral predictors in our model have p-values less than 0.05, which means there is strong statistical evidence that they influence average server response time:\n\nConcurrent users: The coefficient is 0.276. This means for each additional concurrent user, the expected response time increases by a factor of \\(e^{0.276} \\approx 1.32\\), or 32%. This is a substantial increase.\nDatabase queries: With a coefficient of 0.407, each additional database query increases the response time by a factor of \\(e^{0.407} \\approx 1.50\\), or 50%. More queries mean heavier processing load.\nCache hit rate: The coefficient is \\(-0.384\\). This means that a one-unit increase in cache hit rate (e.g., from 60% to 61%) is associated with a decrease in response time by \\(e^{-0.384} \\approx 0.68\\), or 32%.\nServer load: The coefficient is 0.411. This corresponds to a 50.9% increase in response time per unit increase in server load (\\(e^{0.411} \\approx 1.51\\)), which aligns with intuition.\nMemory usage: This has a positive coefficient of 0.197. Each unit increase in memory usage increases expected response time by \\(e^{0.197} \\approx 1.22\\), or 22%.\nCDN usage: The coefficient is \\(-0.382\\). Using a content delivery network is associated with a 31.8% decrease in expected response time (\\(e^{-0.382} \\approx 0.68\\)).\nRequest complexity (Moderate and Simple): Compared to complex requests, moderate requests reduce response time by about 44.7% (\\(e^{-0.589} \\approx 0.55\\)), and simple requests reduce it by about 73.4% (\\(e^{-1.321} \\approx 0.27\\)).\nGeographic region: Requests from Asia Pacific, Europe, and North America all show significantly shorter response times compared to the reference region. For example, the coefficient for North America is \\(-0.750\\), meaning a 52.8% shorter response time on average (\\(e^{-0.750} \\approx 0.47\\)).\n\n4.13.2 Non-Significant Predictors\nThe following predictors have p-values greater than 0.05 and are not statistically significant in this model:\n\nTime of day: This does not appear to have a measurable effect on response time.\nDay of week: Most weekday indicators are not significant, suggesting that response time is consistent across the week.\nSouth America: The coefficient is small and the p-value is high, indicating no strong evidence of different response times compared to the baseline region.\n\nThese variables might still play a role in different datasets or under different modeling assumptions, but here they do not provide strong enough evidence to conclude an effect.\n\n\nInterpretation Reminder\n\n\nThese effects represent associations, not causal guarantees. They reflect average differences after adjusting for other predictors in the model. System-level performance is complex, and real-world behavior may involve interactions or nonlinearities not captured here.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html#storytelling",
    "href": "book/04-gamma.html#storytelling",
    "title": "4  Gamma Regression",
    "section": "\n4.14 Storytelling",
    "text": "4.14 Storytelling\nFinally, to wrap up the workflow, the final step is to consider how the results can actually be used. This part of the workflow is often referred to as storytelling, where we connect the analysis back to practical decision-making.\nThe Gamma regression model highlights several factors that consistently influence server response times. Packaging these findings into insights helps different stakeholders like engineers, product teams, and managers make informed decisions. Below, we translate the model’s results into practical implications.\n\n4.14.1 Key Insights From the Model\nSystem load emerges as one of the strongest drivers of slower performance. Variables such as concurrent users, database queries, and server load all increase response time substantially, especially under peak conditions. These results suggest that heavy user traffic and complex queries consistently create processing bottlenecks. To mitigate these issues, teams should invest in more robust load balancing, optimize query performance, and refine autoscaling strategies to better accommodate traffic spikes.\nCaching proves to be one of the most powerful levers for improving response times. Both cache hit rate and CDN usage lead to substantial reductions in latency, indicating that even small improvements in caching efficiency can have meaningful impact. Because CDNs also provide significant performance gains by serving content closer to users, teams should prioritize strengthening their caching strategy, expanding CDN coverage, and regularly monitoring cache effectiveness as a key performance metric.\nRequest complexity also plays a major role in determining speed, with simple and moderately complex requests completing far faster than complex ones. This implies that certain high-load endpoints may be doing unnecessary or overly expensive work. Refactoring these endpoints, breaking large operations into smaller components, or offloading intensive tasks to asynchronous, batched, or precomputed workflows can significantly improve overall responsiveness.\nFinally, regional differences highlight potential network-level optimization opportunities. Regions such as North America, Europe, and Asia Pacific experience much faster response times than the baseline region, likely due to better routing or closer proximity to infrastructure. To improve performance for underserved regions, teams should consider deploying additional edge servers, optimizing routing paths, or enhancing regional infrastructure to reduce cross-region latency.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/04-gamma.html#conclusion",
    "href": "book/04-gamma.html#conclusion",
    "title": "4  Gamma Regression",
    "section": "\n4.15 Conclusion",
    "text": "4.15 Conclusion\nAnd that wraps up the entire data science workflow. By walking through a complete example from start to finish, we have seen how Gamma regression fits naturally into each stage of the process, from study design and exploratory analysis to modeling, diagnostics, and storytelling. This chapter showed how Gamma regression provides a principled way to model continuous, strictly positive, and right-skewed outcomes, and how it can transform raw data into meaningful insights.\nGamma regression is a powerful tool, but the workflow that surrounds it is even more important. Asking the right question, understanding the data, choosing an appropriate model, checking that the model fits well, and interpreting the results in context are the steps that make an analysis reliable and useful. With these ideas in place, you can confidently apply Gamma regression to real problems and guide decisions with evidence.\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 4.1: Regression analysis mind map depicting all modelling techniques explored so far in this book. Depending on the type of outcome \\(Y\\), these techniques are split into two large zones: discrete and continuous.",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Gamma Regression</span>"
    ]
  },
  {
    "objectID": "book/05-beta.html",
    "href": "book/05-beta.html",
    "title": "5  Soup-erb Beta Regression",
    "section": "",
    "text": "Fun fact!\n\n\nSoup-erb! Soup that’s so heartwarming it feels like a cozy hug.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 5.1",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Soup-erb Beta Regression</span>"
    ]
  },
  {
    "objectID": "book/06-parametric-survival.html",
    "href": "book/06-parametric-survival.html",
    "title": "6  Crunchified Parametric Survival Regression",
    "section": "",
    "text": "Fun fact!\n\n\nCrunchified! Extra crunchy, borderline noisy; could probably shatter glass.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 6.1\n\n\n\n\nDefinition of cumulative distribution function\n\n\nLet \\(Y\\) be a random variable either discrete or continuous. Its cumulative distribution function (CDF) \\(F_Y(y)  : \\mathbb{R} \\rightarrow [0, 1]\\) refers to the probability that \\(Y\\) is less or equal than an observed value \\(y\\):\n\\[\nF_Y(y) = P(Y \\leq y).\n\\tag{6.1}\\]\nThen, we have the following by type of random variable:\n\nWhen \\(Y\\) is discrete, whose support is \\(\\mathcal{Y}\\), suppose it has a PMF \\(P_Y(Y = y)\\). Then, the CDF is mathematically represented as:\n\n\\[\nF_Y(y) = \\sum_{\\substack{t \\in \\mathcal{Y} \\\\ t \\leq y}} P_Y(Y = t).\n\\tag{6.2}\\]\n\nWhen \\(Y\\) is continuous, whose support is \\(\\mathcal{Y}\\), suppose it has a PDF \\(f_Y(y)\\). Then, the CDF is mathematically represented as:\n\n\\[\nF_Y(y) = \\int_{-\\infty}^y f_Y(t) \\mathrm{d}t.\n\\tag{6.3}\\]\nNote that in Equation 6.2 and Equation 6.3, we use the auxiliary variable \\(t\\) since we do not compute the summation or integral over the observed \\(y\\) given its role on either the PMF or PDF. Therefore, we use this auxiliary variable \\(t\\).\n\n\n\n\nHeads-up on the properties of the cumulative distribution function!\n\n\nIt is important to clarify that a valid CDF \\(F_Y(y)\\) fulfils the following properties:\n\n\n\\(F_Y(y)\\) must never be a decreasing function.\nGiven that \\(F_Y(y)  : \\mathbb{R} \\rightarrow [0, 1]\\), it must never evaluate to be \\(&lt; 0\\) or \\(&gt; 1\\). The output of a CDF is a cumulative probability, hence the previous bounds.\nWhen \\(y \\rightarrow -\\infty\\), if follows that \\(F_Y(y) \\rightarrow 0\\).\nWhen \\(y \\rightarrow \\infty\\), if follows that \\(F_Y(y) \\rightarrow 1\\).\n\nNow, in the case of a CDF corresponding to a continuous random variable \\(Y\\), there is an additional handy property that relates the CDF \\(F_Y(y)\\) to the PDF \\(f_Y(y)\\):\n\\[\nf_Y(y) = \\frac{\\mathrm{d}}{\\mathrm{d}y} F_Y(y).\n\\tag{6.4}\\]\nEquation 6.4 indicates that the PDF of \\(Y\\) can be obtained by taking the first derivative of the CDF with respect to \\(y\\).",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Crunchified Parametric Survival Regression</span>"
    ]
  },
  {
    "objectID": "book/07-semiparametric-survival.html",
    "href": "book/07-semiparametric-survival.html",
    "title": "7  Butteryfied Semiparametric Survival Regression",
    "section": "",
    "text": "Fun fact!\n\n\nButteryfied! So rich and buttery it practically slides off the plate.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 7.1",
    "crumbs": [
      "Continuous Cuisine",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Butteryfied Semiparametric Survival Regression</span>"
    ]
  },
  {
    "objectID": "book/08-binary-logistic.html",
    "href": "book/08-binary-logistic.html",
    "title": "8  Sauce-sational Binary Logistic Regression",
    "section": "",
    "text": "Fun fact!\n\n\nSauce-sational! When the sauce is so good, it’s basically soup.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 8.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Sauce-sational Binary Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/09-binomial-logistic.html",
    "href": "book/09-binomial-logistic.html",
    "title": "9  Cheesified Binomial Logistic Regression",
    "section": "",
    "text": "Fun fact!\n\n\nCheesified! Oozing with cheese in every crevice; a cheese lover’s paradise.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma &lt;br/&gt;Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 9.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Cheesified Binomial Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/10-classical-poisson.html",
    "href": "book/10-classical-poisson.html",
    "title": "10  Classical Poisson Regression",
    "section": "",
    "text": "10.1 Introduction\nThis chapter introduces a generalized linear regression model that can be applied on Poisson-distributed count data. Compared to ordinary regression, which assume normality and constant variance—homoskedasticity, Poisson regression models count data that is skewed and heteroskedastic.\nSome research questions you might explore using Poisson regression:",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Classical Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/10-classical-poisson.html#introduction",
    "href": "book/10-classical-poisson.html#introduction",
    "title": "10  Classical Poisson Regression",
    "section": "",
    "text": "How is the number of insurance claims filed by policyholders in a year associated with ages, vehicle types, and regions?\nHow can the number of complaints received by a telecom company from customers be explained by service types and contract lengths?\nHow does the distribution of counts of satellite male horseshoe crabs residing around a female horseshoe crab nest vary by the phenotypic characteristics of the female horseshoe crabs?\n\n\n10.1.1 Poisson Regression Assumptions\n\nIndependence: Poisson regression assumes the responses to be counts–(nonnegative integers: 0, 1, 2,…). Each response is mutually independent to each other with mean parameter \\(\\lambda_i,i=1,\\dots,n\\).\nLog-linearity: Poisson regression models the log-link function of the response as a linear combination of the explanatory variables, i.e., \\(\\log(\\lambda_i) = \\beta_0 + \\beta_1 X_{i1}+\\dots+\\beta_p X_{ip}\\) with \\(\\lambda_i&gt; 0\\) for all \\(i=1,\\dots,n\\).\nHeteroskedasticity: Poisson regression assumes heteoroskedastic response, i.e., the variance of the response increases along with the mean increasing; in contrast to the ordinary regression with Gaussian noise, whose the variance is constant and independent to the mean. Poisson regression assumes equidispersion, i.e., the variance is the same as the expectation of the response; compared to overdispersion in negative binomial distribution where the variance is greater than the mean.\n\n10.1.2 A Graphical Look\nBelow is a graphical demonstration of the comparison between the ordinary linear regression with Gaussian distributed response and Poisson regression with Poisson distributed response. The left panel illustrates the price of smartphones (in 100 USD) increase with the number of days listed on an online marketplace. The right panel illustrates the number of bacteria colonies on a petri dish versus the hours of incubation.\nThe ordinary linear regression has a linear fitted blue line (in the left panel), while the Poisson regression, due to the use of log-link function, has a fitted blue curve (in the right panel). Segment the explanatory variable (in the x axis) in each scenario into five sections using the gray dashed vertical lines. The red horizontal segments represent the frequencies of binned response in each section, which represents the rotated histogram of the response. In ordinary linear regression, the response in each section is symmetrically distributed with similar variation across five sections (i.e., homoskedasticity); while in Poisson regression, the response is skewed with heteroskedasticity (specifically, increasing variances as the responses increase) across sections.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Classical Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/10-classical-poisson.html#case-study-horseshoe-crab-satellites",
    "href": "book/10-classical-poisson.html#case-study-horseshoe-crab-satellites",
    "title": "10  Classical Poisson Regression",
    "section": "\n10.2 Case Study: Horseshoe Crab Satellites",
    "text": "10.2 Case Study: Horseshoe Crab Satellites\nLet’s take a closer look at the example in the third question mentioned in the Introduction—exploring the differential distribution of the number of satellite male horseshoe crabs residing around a female horseshoe crab nest across various phenotypic characteristics. Using the dataset crabs provided by Brockmann, 1996, let’s load the dataset and do some data wrangling.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Classical Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/10-classical-poisson.html#data-collection-and-wrangling",
    "href": "book/10-classical-poisson.html#data-collection-and-wrangling",
    "title": "10  Classical Poisson Regression",
    "section": "\n10.3 Data Collection and Wrangling",
    "text": "10.3 Data Collection and Wrangling\nThere are 173 records of the female horseshoe crab nests and their male satellites with 5 features: satell: satellite size of each nest (i.e., the number of male horseshoe crabs around each female horseshoe crab nest), width: shell width in cm, color: 1 = medium light, 2 = medium, 3 = medium dark, 4 = dark, spine: spine condition: 1 = both good, 2 = one broken, 3 = both broken, and weight: weight in kg (referring to Section 3.3.3 in Agresti, 2018).\n\n\nR Code\nPython Code\n\n\n\nlibrary(asbio)\ndata(crabs)\ncrabs &lt;- as_tibble(crabs)\n# reorder the columns\ncrabs &lt;- crabs %&gt;%\n  select(satell, width, everything())\n# display the dataset with dimension and column types\ncrabs\n\n\nimport pandas as pd\nprint(crabs.dtypes)\ncrabs\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n# A tibble: 173 × 5\n   satell width color spine weight\n    &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n 1      8  28.3 2     3       3.05\n 2      0  22.5 3     3       1.55\n 3      9  26   1     1       2.3 \n 4      0  24.8 3     3       2.1 \n 5      4  26   3     3       2.6 \n 6      0  23.8 2     3       2.1 \n 7      0  26.5 1     1       2.35\n 8      0  24.7 3     2       1.9 \n 9      0  23.7 2     1       1.95\n10      0  25.6 3     3       2.15\n# ℹ 163 more rows\n\n\n\n\n\n\nsatell       int64\nwidth      float64\ncolor     category\nspine     category\nweight     float64\ndtype: object\n\n\n\n\n\n\n\nsatell\nwidth\ncolor\nspine\nweight\n\n\n\n0\n8\n28.3\n2\n3\n3.050\n\n\n1\n0\n22.5\n3\n3\n1.550\n\n\n2\n9\n26.0\n1\n1\n2.300\n\n\n3\n0\n24.8\n3\n3\n2.100\n\n\n4\n4\n26.0\n3\n3\n2.600\n\n\n...\n...\n...\n...\n...\n...\n\n\n168\n3\n26.1\n3\n3\n2.750\n\n\n169\n4\n29.0\n3\n3\n3.275\n\n\n170\n0\n28.0\n1\n1\n2.625\n\n\n171\n0\n27.0\n4\n3\n2.625\n\n\n172\n0\n24.5\n2\n2\n2.000\n\n\n\n173 rows × 5 columns\n\n\n\n\n\n\nLet’s now split the data into training and test sets.\n\n\nR Code\nPython Code\n\n\n\n# Use 70% data for training and 30% for testing\npar &lt;- 0.7\n\n# Set seed for reproducible splitting\nset.seed(1046)\nn &lt;- nrow(crabs)\ntrain_indices &lt;- sample(seq_len(n), size = floor(par * n))\n\n# Create training and test sets\ntrain_set &lt;- crabs[train_indices, ]\ntest_set &lt;- crabs[-train_indices, ]\n\n# Print the number of training and test samples\ncat(\"Number of training samples:\", nrow(train_set), \"\\n\")\ncat(\"Number of test samples:\", nrow(test_set), \"\\n\")\n\n\n# Use 70% data for training and 30% for testing\npar = 0.7\n\n# Set seed for reproducible splitting\nnp.random.seed(1046)\nn = len(crabs)\ntrain_indices = np.random.choice(n, size=int(par * n), replace=False)\n\n# Create training and test sets\ntrain_set = crabs.iloc[train_indices]\ntest_set = crabs.drop(index=train_indices)\n\n# Print the number of training and test samples\nprint(f\"Number of training samples: {len(train_set)}\")\nprint(f\"Number of test samples: {len(test_set)}\")\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\nNumber of training samples: 122 \n\n\nNumber of test samples: 51 \n\n\n\n\n\n\nNumber of training samples: 122\n\n\nNumber of test samples: 51",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Classical Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/10-classical-poisson.html#exploratory-data-analysis",
    "href": "book/10-classical-poisson.html#exploratory-data-analysis",
    "title": "10  Classical Poisson Regression",
    "section": "\n10.4 Exploratory Data Analysis",
    "text": "10.4 Exploratory Data Analysis\nSuppose that the relationship between the satellite size per nest (satell) and width of the female horseshoe crab (width in cm) is our main research interest. Let’s explore their relationship in the following scatterplot with the distribution of satell on the margin.\n\n\nR Code\nPython Code\n\n\n\n# draw the scatterplot of satellite size versus width \np &lt;- crabs %&gt;%\n  ggplot(aes(y = satell, x = width)) + \n  geom_point(alpha = 0.8) + \n  labs(x = \"Width (cm)\", y = \"Satellite Size\") +\n  scale_x_continuous(breaks = seq(floor(min(crabs$width)), ceiling(max(crabs$width)), by = 2)) + \n  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +\n  theme_bw()\n# add a marginal histogram with binwidth of 1, i.e., bins for integers; equivalent to the barplot\nggMarginal(p, type = \"histogram\", margins = \"y\", binwidth = 1, boundary = -0.5) # start with `satell=-0.5` to avoid the first bins (0s and 1s) being combined in one bin\n\n\nimport seaborn as sns\nsns.set(style = \"whitegrid\")\n# create bins of width 1 for the marginal histogram\nbin_edges = np.arange(crabs['satell'].min(), crabs['satell'].max() + 2, 1)  # bins of width 1\n# draw the scatterplot of satellite size versus width with marginal distributions\ng = sns.jointplot(\n    data = crabs, \n    x = \"width\", \n    y = \"satell\", \n    color = \"black\",\n    kind = \"scatter\",\n    marginal_ticks = False,\n    marginal_kws = dict(bins = bin_edges, fill = True, color=\"black\", alpha=0.6)\n)\ng.ax_marg_x.remove() # remove the marginal histogram on x axis\n# add labels\ng.ax_joint.set_xlabel(\"Width (cm)\") \ng.ax_joint.set_ylabel(\"Satellite Size\")\n# add axes' limits\nymin, ymax = crabs['satell'].min(), crabs['satell'].max()\nxmin, xmax = crabs['width'].min(), crabs['width'].max()\ng.ax_joint.set_xlim(xmin - 1, xmax + 1);\ng.ax_joint.set_ylim(ymin - 0.5, ymax + 0.5);\ng.ax_marg_y.set_ylim(ymin - 0.5, ymax + 0.5);\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe distribution of satellite sizes is highly right skewed, which violates the normality assumption of the ordinary linear regression.\nIn the scatter plot above, we can tell that the satellite size gets more spread out as the width increases. The averaged satellite sizes turn to increase along the width increasing. Let’s now split the width into a few intervals and compute the representative point for each interval to have a clear look at the trend.\n\n\nR Code\nPython Code\n\n\n\n# set up the number of intervals\nn_intervals &lt;- 10\n\n# compute the average points for each interval of `width`\ncrabs_binned &lt;- crabs %&gt;%\n  mutate(width_inl = cut(width, breaks = n_intervals)) %&gt;%\n  group_by(width_inl) %&gt;%\n  summarize(\n    mean_width = mean(width),\n    mean_satell = mean(satell),\n    .groups = \"drop\"\n  )\ncrabs_binned\n\n\n# set up the number of intervals\nn_intervals = 10\n\n# compute the average points for each interval of `width`\ncrabs['width_inl'] = pd.cut(crabs['width'], bins=n_intervals)\ncrabs_binned = (\n    crabs\n    .groupby('width_inl', observed=True)\n    .agg(mean_width=('width', 'mean'), mean_satell=('satell', 'mean'))\n    .reset_index()\n    .round(2)\n)\ncrabs_binned\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n# A tibble: 10 × 3\n   width_inl   mean_width mean_satell\n   &lt;fct&gt;            &lt;dbl&gt;       &lt;dbl&gt;\n 1 (21,22.2]         21.5        0   \n 2 (22.2,23.5]       23.0        1   \n 3 (23.5,24.8]       24.2        1.77\n 4 (24.8,26]         25.5        2.98\n 5 (26,27.2]         26.6        2.53\n 6 (27.2,28.5]       27.9        4.15\n 7 (28.5,29.8]       29.1        4   \n 8 (29.8,31]         30.1        4.86\n 9 (31,32.2]         31.8        3   \n10 (32.2,33.5]       33.5        7   \n\n\n\n\n\n\n\n\n\n\n\nwidth_inl\nmean_width\nmean_satell\n\n\n\n0\n(20.988, 22.25]\n21.50\n0.00\n\n\n1\n(22.25, 23.5]\n22.97\n1.00\n\n\n2\n(23.5, 24.75]\n24.25\n1.77\n\n\n3\n(24.75, 26.0]\n25.49\n2.98\n\n\n4\n(26.0, 27.25]\n26.60\n2.53\n\n\n5\n(27.25, 28.5]\n27.92\n4.15\n\n\n6\n(28.5, 29.75]\n29.08\n4.00\n\n\n7\n(29.75, 31.0]\n30.11\n4.86\n\n\n8\n(31.0, 32.25]\n31.80\n3.00\n\n\n9\n(32.25, 33.5]\n33.50\n7.00\n\n\n\n\n\n\n\n\n\nWe’ve prepared the summarized dataset for the average points—each entry includes an interval of width, an averaged width, and an averaged satellite size per interval. Now, let’s visualize the representative points on the scatter plot to take a closer look at the trend.\n\n\nR Code\nPython Code\n\n\n\n# add the average points onto the scatterplot, connect them, and mark intervals\nbreaks &lt;- seq(min(crabs$width), max(crabs$width), length.out = n_intervals + 1)\np + \n  geom_vline(xintercept = breaks, linetype = \"dashed\", color = \"gray50\") +\n  geom_point(data = crabs_binned, aes(x = mean_width, y = mean_satell), color = \"red\", size = 2) +\n  geom_line(data = crabs_binned, aes(x = mean_width, y = mean_satell), color = \"red\", linewidth = 1)\n\n\n# draw the scatterplot of satellite size versus width\np = (\n    ggplot(crabs, aes(x='width', y='satell')) +\n    geom_point(alpha=0.7, color='black') +  # points in gray\n    labs(x=\"Width (cm)\", y=\"Satellite Size\") +\n    scale_x_continuous(breaks=range(int(np.floor(crabs['width'].min())), int(np.ceil(crabs['width'].max())) + 1, 2)) +\n    scale_y_continuous(breaks=range(int(np.floor(crabs['satell'].min())), int(np.ceil(crabs['satell'].max())) + 1, 2)) +\n    theme_bw()\n)\n# add the average points onto the scatterplot, connect them, and mark intervals\nbreaks = np.linspace(crabs['width'].min(), crabs['width'].max(), n_intervals + 1)\np_final = (\n    p +\n    geom_vline(xintercept=breaks, linetype='dashed', color='gray') +\n    geom_point(data=crabs_binned, mapping=aes(x='mean_width', y='mean_satell'), color='red', size=2) +\n    geom_line(data=crabs_binned, mapping=aes(x='mean_width', y='mean_satell'), color='red', size=1)\n)\np_final.show()\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see a general increasing trend of the satellite size as the nest width grows.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Classical Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/10-classical-poisson.html#data-modelling",
    "href": "book/10-classical-poisson.html#data-modelling",
    "title": "10  Classical Poisson Regression",
    "section": "\n10.5 Data Modelling",
    "text": "10.5 Data Modelling\nThe Poisson regression model assumes a random sample of \\(n\\) count observations \\(Y_i\\)s, hence independent (but not identically distributed!), which have the following distribution: \\[Y_i \\sim \\mathrm{Poisson}(\\lambda_i).\\] Each \\(i\\)th observation has its own \\(\\mathbb{E}(Y_i)=\\lambda_i&gt;0\\), which also implicates \\(Var(Y_i)=\\lambda_i&gt;0\\).\nParameter \\(\\lambda_i\\) is the risk of an event occurrence, coming from the definition of the Poisson random variable, in a given timeframe or even a space. It is a continuous distributional parameter! For the crabs dataset, the events are the number of satellite male horseshoe crabs around a space: the female breeding nest.\nSince the Poisson Regression model is also a GLM, we need to set up a link function for the mean. Let \\(X_{i,\\text{width}}\\) be the \\(i\\)th value for the regressor width in our dataset. An easy modelling solution would be an identity link function as in \\[\nh(\\lambda_i)=\\lambda_i=\\beta_0+\\beta_1 X_{i,\\text{width}}. \\label{eq:pois-uni-iden}\n\\]\nHowever, we have a response range issue!\nThe model (eq:pois-uni-iden?) for has a significant drawback: the right-hand side is allowed to take on even negative values, which does not align with the nature of the parameter \\(\\lambda_i\\) (that always has to be non-negative).\nRecall the essential characteristic of a GLM that should come into play for a link function. In this class of models, the direct relationship between the original response and the regressors may be non-linear as in \\(h(\\lambda_i)\\). Hence, instead of using the identity link function, we will use the natural logarithm of the mean: \\(\\log(\\lambda_i)\\).\nBefore continuing with the crabs dataset, let us generalize the Poisson regression model with \\(k\\) regressors as: \\[\n  h(\\lambda_i) = \\log(\\lambda_i)=\\beta_0+\\beta_1 X_{i,1}+\\dots+\\beta_k X_{i,k}. \\label{eq:pois-k}\n\\] We could make more sense in the interpretation by exponentiating (eq:pois-k?): \\[\n  \\lambda_i = \\exp(\\beta_0+\\beta_1 X_{i,1}+\\dots+\\beta_k X_{i,k}), \\label{eq:pois-k-exp}\n\\] where an increase in one unit in any of the \\(k\\) regressors (while keeping the rest of them constant) multiplies the mean \\(\\lambda_i\\) by a factor \\(\\exp(\\beta_j)\\), for all \\(j=i,\\dots,k\\).\nIn the crabs dataset with width as a regressor, the Poisson regression model is depicted as: \\[\n  \\log(\\lambda_i)=\\lambda_i=\\beta_0+\\beta_1 X_{i,\\text{width}}. \\label{eq:pois-uni}\n\\]",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Classical Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/10-classical-poisson.html#estimation",
    "href": "book/10-classical-poisson.html#estimation",
    "title": "10  Classical Poisson Regression",
    "section": "\n10.6 Estimation",
    "text": "10.6 Estimation\nIn generalized linear models (GLMs), parameter estimation is typically performed using maximum likelihood estimation (MLE). A widely used algorithm to compute the MLE in GLMs is the iteratively reweighted least squares (IRLS) method, which learns the regression coefficients by solving the weighted least squares problems through iteration. Below we learn the essential elements of the IRLS method and the MLE that it solves. We use Poisson regression as an example, but the procedure applies to other distributions straightforwardly.\nThe general idea to solve the generalized linear regression is to estimate the parameters by maximizing the likelihood. For Poisson regression in (eq:pois-k?) specifically, it is written as: \\[\n  \\prod_{i=1}^n p_{Y_i}(y_i) = \\prod_{i=1}^n \\frac{e^{-\\lambda_i}{\\lambda_i}^{y_i}}{y_i!},\n\\] where \\(p_{Y_i}(y_i)\\) is the probability mass function of \\(Y_i\\), for \\(i=1,\\dots,n\\).\nLet \\(\\boldsymbol{\\beta}:=(\\beta_0,\\dots,\\beta_k)^{\\top}\\) be the coefficient vector, and then (eq:pois-k-exp?) can be rewritten as \\(\\lambda_i=\\exp(X_{i}^{\\top}\\boldsymbol{\\beta})\\), where \\(X_{i}^{\\top}\\) is the row \\(i\\) in the design matrix \\(X\\). Maximizing the likelihood is equivalent to minimizing the log-likelihood in terms of solving for the parameters \\(\\beta_j,j=1,\\dots,k\\): \\[\n  {\\arg\\min}_{\\boldsymbol{\\lambda}} - \\prod_{i=1}^n\\frac{e^{-\\lambda_i}{\\lambda_i}^{y_i}}{y_i!} = {\\arg\\min}_{\\boldsymbol{\\lambda}} \\sum_{i=1}^n \\lambda_i - y_i \\log(\\lambda_i).\n\\] Plugging in \\(\\lambda_i=\\exp(X_{i}^{\\top}\\boldsymbol{\\beta})\\) gives the minimization problem with respect to the Poisson regression coefficients: \\[\n  {\\arg\\min}_{\\boldsymbol{\\beta}} \\sum_{i=1}^n \\exp(X_{i}^{\\top}\\boldsymbol{\\beta}) - y_i X_{i}^{\\top}\\boldsymbol{\\beta}. \\label{eq:pois-log}\n\\]\nSince the objective in (eq:pois-log?) is not quadratic and there is no closed form solution, we could not solve the equation exactly. IRLS solves an approximated solution with a specified accuracy or tolerance. The basic idea is that IRLS uses Fisher scoring (or Newton-Raphson) to solve an approximated problem of the original objective (eq:pois-log?) and solve it iteratively with weights updated through iteration until the specified accuracy is achieved, i.e., the objective is converged.\nThe MLE \\(\\hat{\\boldsymbol{\\beta}}\\) satisfies the score function \\(U(\\boldsymbol{\\beta}) = \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) = 0\\). Applying the Fisher scoring update gives: \\[\n  \\boldsymbol{\\beta}^{t+1} \\leftarrow \\boldsymbol{\\beta}^{t} + {\\mathcal{I}(\\boldsymbol{\\beta}^t)}^{-1} U(\\boldsymbol{\\beta}^{t}),\n\\] where \\(\\mathcal{I}(\\boldsymbol{\\beta}^t) := \\mathbb{E}\\big[- \\nabla^2_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}^t)\\big]\\) is the Fisher information, which is the expectation of the negative second-order derivative of the log-likelihood with respect to the parameters.\nThe Fisher scoring update corresponds to solve the following approximated problem of the original objective at iterate \\(t+1\\), which is a weighted least squares problem written as: \\[\n  \\begin{align}\n    \\boldsymbol{\\beta}^{t+1} &= {\\arg\\min}_{\\boldsymbol{\\beta}} \\sum_{i=1}^n w_i^t (y_i^t - X_i^{\\top}\\boldsymbol{\\beta})^2 \\\\\n    &= {\\arg\\min}_{\\boldsymbol{\\beta}} \\sum_{i=1}^n w_i^t (y_i^t - \\beta_0 - \\beta_1 x_{i,\\text{width}})^2 \\label{eq:irls-approx}\n  \\end{align}\n\\] given the estimates at time \\(t\\) for the univariate case using crabs dataset. The approximated problem has a closed-form solution and is much easier to solve than the original objective. The observations \\(y_i\\) and weights \\(w_i\\) should be updated per iterate. The arbitrary observations \\(y_i^{t+1}\\) is a function of the original observations \\(y_i^0\\) and the coefficient estimates. The weights is updated by \\[w_i^{t+1} = \\Big(\\frac{1}{Var(Y_i)}\\Big)\\Big(\\frac{1}{g'(\\lambda_i)}\\Big)^2,\\] where \\(\\lambda_i\\) is the mean of the response \\(y_i\\), \\(\\eta_i=g(\\lambda_i)\\) is the link function of the mean. In our case, the weights at iterate \\(t+1\\) can be updated given the coefficient estimates at \\(t\\) using the following formula: \\[\n  w_i^{t+1} = \\exp(X_i^{\\top} \\boldsymbol{\\beta}^t), \\forall i=1,\\dots,n.\n\\]\nThe IRLS iteration procedure is as follows.\n\nChoose a set of initial coefficients \\(\\boldsymbol{\\lambda}^0 = (\\lambda_0^0,\\dots,\\lambda_k^0)\\), which can be a vector of zeros.\nCompute the weights based on the estimates from the previous iterate, or the inital coefficients at the first iterate.\nSolve the approximated problem (irls-approx?).\nCheck te convergence condition.\nReturn estimates if the objective is converged; go back to step 2 if not.\n\nLet’s now fit the model on the training data.\n\n\nR Code\nPython Code\n\n\n\n# Fit Poisson regression\npoisson_model &lt;- glm(satell ~ width, family = poisson(link = \"log\"), data = train_data)\n\n# View summary\nsummary(poisson_model)\n\n\n# Fit Poisson regression\npoisson_model = glm(formula='satell ~ width', data=train_data, family=sm.families.Poisson(link=sm.families.links.log())).fit()\n\n# View summary\nprint(poisson_model.summary())\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\nCall:\nglm(formula = satell ~ width, family = poisson(link = \"log\"), \n    data = train_set)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -3.43023    0.64716  -5.300 1.16e-07 ***\nwidth        0.16973    0.02386   7.112 1.14e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 476.83  on 121  degrees of freedom\nResidual deviance: 428.20  on 120  degrees of freedom\nAIC: 679.01\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                 satell   No. Observations:                  122\nModel:                            GLM   Df Residuals:                      120\nModel Family:                 Poisson   Df Model:                            1\nLink Function:                    log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -337.50\nDate:                Wed, 21 Jan 2026   Deviance:                       428.20\nTime:                        20:14:03   Pearson chi2:                     418.\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.3287\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -3.4302      0.647     -5.300      0.000      -4.699      -2.162\nwidth          0.1697      0.024      7.112      0.000       0.123       0.216\n==============================================================================",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Classical Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/10-classical-poisson.html#goodness",
    "href": "book/10-classical-poisson.html#goodness",
    "title": "10  Classical Poisson Regression",
    "section": "\n10.7 Goodness of Fit",
    "text": "10.7 Goodness of Fit\nIn GLMs, we can use residual deviance and Chi-squared test to check the goodness of fit of the model. The residual deviance measures how well the fitted model explains the observed outcomes, compared to a perfect model (the saturated model) that explains the data exactly. A smaller residual deviance means a better fit of the data. It is computed as \\[\\text{Residual Deviance } = 2 (\\ell_{\\mathrm{saturated}} - \\ell_{\\mathrm{fitted}}),\\] where \\(\\ell_A\\) is the log-likelihood of model \\(A\\). For Poisson regression specifically, the residual deviance is \\(2\\sum_{i=1}^n \\Big[y_i\\log\\big(\\frac{y_i}{\\hat{\\lambda}_i}\\big) - (y_i-\\hat{\\lambda}_i)\\Big]\\).\nWhen the model is a good fit, the residual deviance is expected to follow a chi-squared distribution with degrees of freedom \\(df = n - p\\), where \\(n\\) is the number of observations and \\(p\\) is the number of parameters, asymptotically for a large enough sample size. We then can compute the \\(p\\)-value using the chi-squared distribution: \\[\n  p\\text{-value} = 1-P(\\chi^2_{\\mathrm{df}} \\leq \\text{residual deviance}).\n\\]\n\\(p\\)-value is the probability of observing a residual deviance as large as (or larger than) what we got, if the model is truly correct. A large p-value (e.g., &gt; 0.05) means that the observed deviance is not surprisingly large. Out model is a good fit and could plausibly have generated the data. A small p-value (e.g., &lt; 0.05) means that the deviance is too large to be due to chance. The model is a poor fit and likely missing something important.\n\n\nR Code\nPython Code\n\n\n\n# Residual deviance and degrees of freedom\nres_dev &lt;- deviance(poisson_model)\ndf_res &lt;- df.residual(poisson_model)\n\n# Chi-squared goodness-of-fit test\np_value &lt;- 1 - pchisq(res_dev, df_res)\n\ncat(\"Residual Deviance:\", round(res_dev, 4), \"\\n\")\ncat(\"Degrees of Freedom:\", df_res, \"\\n\")\ncat(\"Goodness-of-fit p-value:\", round(p_value, 4), \"\\n\")\n\n\nres_dev = poisson_model.deviance\ndf_res = poisson_model.df_resid\n\np_value = 1 - stats.chi2.cdf(res_dev, df_res)\n\nprint(f\"Residual Deviance: {res_dev:.4f}\")\nprint(f\"Degrees of Freedom: {df_res}\")\nprint(f\"Goodness-of-fit p-value: {p_value:.4f}\")\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\nResidual Deviance: 428.2018 \n\n\nDegrees of Freedom: 120 \n\n\nGoodness-of-fit p-value: 0 \n\n\n\n\n\n\nResidual Deviance: 428.2018\n\n\nDegrees of Freedom: 120\n\n\nGoodness-of-fit p-value: 0.0000\n\n\n\n\n\nThe probability of observing a deviance as large as this if the model is truly correct (i.e., the goodness-of-fit \\(p\\)-value) is esentially 0, saying that there is significant evidence of lack-of-fit. There can be several reasons for lack-of-fit. The model is misspecified, e.g., it is missing important covariates or nonlinear effects. The link function might be incorrect, so that the model is systematically overestimate or underestimate the data. There can be outliers or influential points that inflate the deviance. The data might be overdispersed, so that the residual deviance is inflated by the large variance.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Classical Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/10-classical-poisson.html#inference",
    "href": "book/10-classical-poisson.html#inference",
    "title": "10  Classical Poisson Regression",
    "section": "\n10.8 Inference",
    "text": "10.8 Inference\nThe estimated model can be used for two purposes: inference and prediction. In terms of inference, we use the fitted model to identify the relationship between the response and regressors. Wald’s test is a general method for hypothesis testing in maximum likelihood estimation (MLE), including generalized linear models (GLMs) like Poisson regression. It tests the hypotheses of the form: \\[\n  \\begin{align}\n    H_0 &: \\beta_j = 0, \\\\\n    H_a &: \\beta_j \\neq 0.\n  \\end{align}\n\\] using the fact that \\(\\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)} \\sim \\mathcal{N}(0,1)\\) asymptotically under \\(H_0\\), where \\(\\beta_j\\) is the \\(j\\)th estimated regression coefficient and \\(SE(\\hat{\\beta}_j)\\) is its corresponding variability which is reflected in the standard error of the estimate. This ratio is the Wald’s test statistic \\[z_j = \\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)}\\] that is used to determine the statistical significance of \\(\\hat{\\beta}_j\\) using the fact that the squared statistic asymptotically follows a one-degree-of-freedom chi-squared test, i.e., \\[W_j = \\Big(\\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)}\\Big)^2\\sim \\chi_1^2.\\]\nRemark 1: A statistic like \\(t_j=\\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)}\\) in t test is referred to as a \\(t\\)-value. It assumes finite-sample normality—a student’s \\(t\\)-distribution under the null hypothesis with \\(H_0\\) with \\(n-k-1\\) degrees of freedom. It can only be used for linear models with normal errors. While the Wald’s test applies to any MLE (e.g., Poisson or logistic) to be used in any GLMs, and assumes asymptotic normality—standard normal or \\(\\chi^2\\) distribution under \\(H_0\\).\nRemark 2: Wald’s test is validate under several conditions: 1-large sample size, so that the test statistic is asymptotically normal, 2-regularity conditions for MLE, including differentiable likelihood, positive definite Fisher information, correctly specified model and parameter space, 3-well-estimated parameters, i.e., parameters are not near the bounaries, 4-stable and finite standard errors, and 5-no outliers or high leverage points.\nWe can obtain the corresponding \\(p\\)-values for each \\(\\beta_j\\) associated to the Wald’s test statistic under the null hypothesis \\(H_0\\). The smaller the \\(p\\)-value, the stronger the evidence against the null hypothesis \\(H_0\\) in our sample. Hence, small \\(p\\)-values (less than the significance level \\(\\alpha\\)) indicate that the data provides evidence in favour of association (or causation if that is the case) between the response variable and the \\(j\\)th regressor.\nSimilarly, given a specified \\((1-\\alpha)\\times 100%\\) level of confidence, we can construct confidence intervals for the corresponding true value of \\(\\beta_j\\): \\[\\hat{\\beta}_j \\pm t_{\\alpha/2,n-k-1} SE(\\hat{\\beta}_j),\\] where \\(t_{\\alpha/2,n-k-1}\\) is the upper \\(\\alpha/2\\) quantile of the \\(t\\)-distribution with \\(n-k-1\\) degrees of freedom.\nLet’s now compute the 95% confidence interval.\n\n\nR Code\nPython Code\n\n\n\ntidy(poisson_model, conf.int = TRUE) %&gt;% mutate_if(is.numeric, round, 3)\n\n\n# Get coefficient table\nsummary_df = poisson_model.summary2().tables[1]\n\n# Compute confidence intervals\nconf_int = poisson_model.conf_int()\nconf_int.columns = ['conf_low', 'conf_high']\n\n# Combine with coefficient table\nsummary_df = summary_df.join(conf_int)\n\n# Round all numeric columns to 3 decimals\nsummary_df = summary_df.round(3)\n\nprint(summary_df)\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    -3.43     0.647     -5.3        0   -4.70     -2.16 \n2 width           0.17     0.024      7.11       0    0.123     0.216\n\n\n\n\n\n\n           Coef.  Std.Err.      z  P&gt;|z|  [0.025  0.975]  conf_low  conf_high\nIntercept  -3.43     0.647 -5.300    0.0  -4.699  -2.162    -4.699     -2.162\nwidth       0.17     0.024  7.112    0.0   0.123   0.216     0.123      0.216\n\n\n\n\n\nOur sample gives us evidence to reject \\(H_0\\) (with a nearly-zero \\(p\\) value), which suggests that carapace width is statistically associated to the logarithm of the satellite size. Now, it is time to plot the fitted values coming from poisson_model to check whether it provides a positive relationship between width and the original scale of the response satell.\n\n\nR Code\nPython Code\n\n\n\np +\n  geom_smooth(\n    data = train_set, aes(width, satell),\n    method = \"glm\", formula = y ~ x,\n    method.args = list(family = poisson), se = FALSE\n  ) +\n  labs(title=\"Poisson Regression Fitted Curve\")\n\n\n# Compute the fitted values\nwidth_range = np.linspace(train_set['width'].min(), train_set['width'].max(), 100)\npredict_df = pd.DataFrame({'width': width_range})\npredict_df['predicted'] = poisson_model.predict(predict_df)\n\n# Draw the scatterplot\nsns.scatterplot(data=train_set, x='width', y='satell', label='Observed')\n\n# Add the Poisson regression line\nsns.lineplot(data=predict_df, x='width', y='predicted', color='red', label='Poisson fit')\n\n# Add title\nplt.title('Poisson Regression Fitted Curve')\nplt.xlabel('Width')\nplt.ylabel('Satellite size')\nplt.legend()\nplt.show()\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe blue line in the plot above is the fitted Poisson regression of satell versus width. The positive relationship is now clear with this regression line.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Classical Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/10-classical-poisson.html#results",
    "href": "book/10-classical-poisson.html#results",
    "title": "10  Classical Poisson Regression",
    "section": "\n10.9 Results",
    "text": "10.9 Results\nLet us fit a second model with two regressors: width (\\(X_{\\mathrm{width}_i}\\)) and color (\\(X_{\\mathrm{color_2}_i}\\), \\(X_{\\mathrm{color_3}_i}\\), \\(X_{\\mathrm{color_4}_i}\\)) for the \\(i\\)th observation: \\[\n  h(\\lambda_i) = \\log(\\lambda_i) = \\beta_0 + \\beta_1 X_{\\mathrm{width}_i} + \\beta_2 X_{\\mathrm{color_2}_i} + \\beta_3 X_{\\mathrm{color_3}_i} + \\beta_4 X_{\\mathrm{color_4}_i}.\n\\] The explanatory variable color is of factor-type (discrete) and nominal (its levels do not follow any specific order). There are four levels: 1 = medium light, 2 = medium, 3 = medium dark, 4 = dark.\n\n\nR Code\nPython Code\n\n\n\n# Factorize the variable\ntrain_set$color &lt;- as.factor(train_set$color)\n# Print color categories\nlevels(train_set$color)\n\n\n# Factorize the variable\ntrain_set['color'] = train_set['color'].astype('category')\n# Print color categories\ntrain_set['color'].cat.categories\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n[1] \"1\" \"2\" \"3\" \"4\"\n\n\n\n\n\n\nIndex([1, 2, 3, 4], dtype='int64')\n\n\n\n\n\nUsing categorical variables such as color involves using dummy variables as in Binary Logistic regression. Taking the baseline 1 = medium light by default, this Poisson regression model will incorporate three dummy variables: \\(X_{\\mathrm{color_2}_i}\\), \\(X_{\\mathrm{color_3}_i}\\), and \\(X_{\\mathrm{color_4}_i}\\). For different levels of color, these dummy variables take on the following values:\n\nWhen color is 1 = medium light, then all three dummy variables \\(X_{\\mathrm{color_2}_i} = X_{\\mathrm{color_3}_i} = X_{\\mathrm{color_4}_i}=0\\).\nWhen color is 2 = medium, then \\(X_{\\mathrm{color_2}_i}=1\\) while the other two dummy variables \\(X_{\\mathrm{color_3}_i} = X_{\\mathrm{color_4}_i}=0\\).\nWhen color is 3 = medium dark, then \\(X_{\\mathrm{color_3}_i}=1\\) while the other two dummy variables \\(X_{\\mathrm{color_2}_i} = X_{\\mathrm{color_4}_i}=0\\).\nWhen color is 4 = dark, then \\(X_{\\mathrm{color_4}_i}=1\\) while the other two dummy variables \\(X_{\\mathrm{color_2}_i} = X_{\\mathrm{color_3}_i}=0\\).\n\nNote that the level 1 = medium light is depicted as the baseline here. Hence, the interpretation of the coefficients in the model for each dummy variable will be compared to this baseline.\nNow, let us fit this second Poisson regression model and print the model summary including coefficient estimates, standard errors, Wald’s test statistic (\\(z\\)-test), the corresponding \\(p\\)-value, and 95% confidence intervals.\n\n\nR Code\nPython Code\n\n\n\n# Fit the Poisson model\npoisson_model2 &lt;- glm(satell ~ width + color, family = poisson, data = train_set)\n# Summarise the model output\nsummary_df &lt;- tidy(poisson_model2, conf.int = TRUE) %&gt;% mutate_if(is.numeric, round, 3)\n# Display the output in table\nkable(summary_df, \"pipe\")\n\n\n# Fit the Poisson regression model\npoisson_model2 = glm(formula='satell ~ width + color', data=train_set, family=sm.families.Poisson()).fit()\n# Extract the summary table\nsummary_df = poisson_model2.summary2().tables[1]\n# Display the table\nprint(summary_df.round(3))\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n-2.915\n0.687\n-4.244\n0.000\n-4.257\n-1.564\n\n\nwidth\n0.161\n0.025\n6.495\n0.000\n0.112\n0.209\n\n\ncolor2\n-0.224\n0.180\n-1.244\n0.213\n-0.562\n0.145\n\n\ncolor3\n-0.458\n0.212\n-2.160\n0.031\n-0.867\n-0.034\n\n\ncolor4\n-0.410\n0.231\n-1.773\n0.076\n-0.862\n0.047\n\n\n\n\n\n\n\n\n\n            Coef.  Std.Err.      z  P&gt;|z|  [0.025  0.975]\nIntercept  -2.915     0.687 -4.244  0.000  -4.261  -1.568\ncolor[T.2] -0.224     0.180 -1.244  0.213  -0.576   0.129\ncolor[T.3] -0.458     0.212 -2.160  0.031  -0.873  -0.042\ncolor[T.4] -0.410     0.231 -1.773  0.076  -0.862   0.043\nwidth       0.161     0.025  6.495  0.000   0.112   0.209\n\n\n\n\n\nWe can see that width and color3 are significant according to the \\(p\\)-value column under the significance level \\(\\alpha=0.05\\).\n\n10.9.1 Interpreting the results of a continuous variable\nFirst, let us focus on the coefficient corresponding to width, while keeping color constant. Consider an observation with a given shell width \\(X_{\\mathrm{width}}=\\texttt{w}\\) cm, and another observation with a given \\(X_{\\mathrm{width+1}}=\\texttt{w}+1\\) cm (i.e., an increase of \\(1\\) cm). Denote the corresponding expected satell sizes as \\(\\lambda_{\\mathrm{width}}\\) and \\(\\lambda_{\\mathrm{width+1}}\\) respectively. Then we have their corresponding regression equations:\n\\[\n\\begin{align}\n  \\log \\left( \\lambda_{\\mathrm{width}} \\right) &= \\beta_0 + \\beta_1 \\overbrace{\\texttt{w}}^{X_{\\mathrm{width}}} + \\overbrace{\\beta_2 X_{\\mathrm{color_2}} + \\beta_3 X_{\\mathrm{color_3}} + \\beta_4 X_{\\mathrm{color_4}}}^{\\mathrm{Constant}} \\\\\n  \\log \\left( \\lambda_{\\mathrm{width + 1}} \\right) &= \\beta_0 + \\beta_1 \\underbrace{(\\texttt{w} + 1)}_{X_{\\mathrm{width + 1}}} + \\underbrace{\\beta_2 X_{\\mathrm{color_2}} + \\beta_3 X_{\\mathrm{color_3}} + \\beta_4 X_{\\mathrm{color_4}}}_{\\mathrm{Constant}}.\n\\end{align}\n\\]\nWe take the difference between both equations as: \\[\n\\begin{align}\n\\log \\left( \\lambda_{\\mathrm{width + 1}} \\right) - \\log \\left( \\lambda_{\\mathrm{width}} \\right) &= \\beta_1 (\\texttt{w} + 1) - \\beta_1 \\texttt{w} \\\\\n&= \\beta_1.\n\\end{align}\n\\]\nWe apply the logarithm property for a ratio: \\[\n\\begin{align*}\n\\log \\left( \\frac{\\lambda_{\\mathrm{width + 1}} }{\\lambda_{\\mathrm{width}}} \\right) &= \\log \\left( \\lambda_{\\mathrm{width + 1}} \\right) - \\log \\left( \\lambda_{\\mathrm{width}} \\right) \\\\\n&= \\beta_1.\n\\end{align*}\n\\]\nFinally, we have to exponentiate the previous equation: \\[\\frac{\\lambda_{\\mathrm{width+1}}}{\\lambda_{\\mathrm{width}}} = e^{\\beta_1},\\] which indicates that the satellite size varies in a multiplicative way when 1 cm is increased in satellite width.\nTherefore, by using the estimate \\(\\hat{\\beta_1}\\) (note the hat notation) coming from the model poisson_model2, we calculate this multiplicative effect as follows (via exponentiate = TRUE in tidy()):\n\n\nR Code\nPython Code\n\n\n\nsummary_exp_df &lt;- tidy(poisson_model2, exponentiate = TRUE, conf.int = TRUE) \n  %&gt;% mutate_if(is.numeric, round, 3)\nknitr::kable(summary_exp_df, \"pipe\")\n\n\nsummary_exp_df = summary_df.copy()\nsummary_exp_df['Coef.'] = np.exp(summary_df['Coef.'])\nsummary_exp_df['[0.025'] = np.exp(summary_df['[0.025'])\nsummary_exp_df['0.975]'] = np.exp(summary_df['0.975]'])\nsummary_exp_df = summary_exp_df.round(3)\nprint(summary_exp_df)\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n0.054\n0.687\n-4.244\n0.000\n0.014\n0.209\n\n\nwidth\n1.174\n0.025\n6.495\n0.000\n1.118\n1.232\n\n\ncolor2\n0.800\n0.180\n-1.244\n0.213\n0.570\n1.156\n\n\ncolor3\n0.633\n0.212\n-2.160\n0.031\n0.420\n0.967\n\n\ncolor4\n0.664\n0.231\n-1.773\n0.076\n0.422\n1.048\n\n\n\n\n\n\n\n\n\n            Coef.  Std.Err.      z  P&gt;|z|  [0.025  0.975]\nIntercept   0.054     0.687 -4.244  0.000   0.014   0.208\ncolor[T.2]  0.800     0.180 -1.244  0.213   0.562   1.137\ncolor[T.3]  0.633     0.212 -2.160  0.031   0.418   0.958\ncolor[T.4]  0.664     0.231 -1.773  0.076   0.422   1.044\nwidth       1.174     0.025  6.495  0.000   1.119   1.232\n\n\n\n\n\n\\(\\frac{\\hat{\\lambda}_{\\mathrm{width + 1}} }{\\hat{\\lambda}_{\\mathrm{width}}} = e^{\\hat{\\beta}_1} = 1.174\\) indicates that the averaged satellite size (satell) of male horseshoe crabs around a female breeding nest increases by \\(17.4\\%\\) when increasing the crab width by \\(1\\) cm, while keeping color constant. The interpretation of the significant coefficients corresponding to difference between the color groups and the baseline level.\n\n10.9.2 Interpreting the results of a categorical variable\nConsider two observations, one with color 1 = medium light of the prosoma (the baseline) and another with 4 = dark. Their corresponding responses are denoted as \\(\\lambda_{\\mathrm{D}}\\) (for dark) and \\(\\lambda_{\\mathrm{L}}\\) (for medium light). While holding \\(X_{\\mathrm{width}}\\) constant, their regression equations are: \\[\n\\begin{gather*}\n\\log \\left( \\lambda_{\\mathrm{D}} \\right) = \\beta_0 + \\overbrace{\\beta_1 X_{\\mathrm{width}}}^{\\text{Constant}} + \\beta_2 X_{\\mathrm{color_2}_{\\mathrm{D}}} + \\beta_3 X_{\\mathrm{color_3}_{\\mathrm{D}}} + \\beta_4 X_{\\mathrm{color_4}_{\\mathrm{D}}} \\\\\n\\log \\left( \\lambda_{\\mathrm{L}} \\right) = \\beta_0 + \\underbrace{\\beta_1 X_{\\mathrm{width}}}_{\\text{Constant}} + \\beta_2 X_{\\mathrm{color_2}_{\\mathrm{L}}} + \\beta_3 X_{\\mathrm{color_3}_{\\mathrm{L}}} + \\beta_4 X_{\\mathrm{color_4}_{\\mathrm{L}}}\n\\end{gather*}\n\\]\nThe corresponding color indicator variables for both \\(\\lambda_{\\mathrm{D}}\\) and \\(\\lambda_{\\mathrm{L}}\\) take on these values: \\[\n\\begin{align*}\n\\log \\left( \\lambda_{\\mathrm{D}} \\right) &= \\beta_0 + \\overbrace{\\beta_1 X_{\\mathrm{width}}}^{\\text{Constant}} + \\beta_2 X_{\\mathrm{color_2}_{\\mathrm{D}}} + \\beta_3 X_{\\mathrm{color_3}_{\\mathrm{D}}} + \\beta_4 X_{\\mathrm{color_4}_{\\mathrm{D}}} \\\\\n&= \\beta_0 + \\beta_1 X_{\\mathrm{width}}+ \\beta_2 \\times 0 + \\beta_3 \\times 0 + \\beta_4 \\times 1 \\\\\n&= \\beta_0 + \\beta_1 X_{\\mathrm{width}} + \\beta_4, \\\\\n\\log \\left( \\lambda_{\\mathrm{L}} \\right) &= \\beta_0 + \\beta_1 X_{\\mathrm{width}} + \\beta_2 X_{\\mathrm{color_2}_{\\mathrm{L}}} + \\beta_3 X_{\\mathrm{color_3}_{\\mathrm{L}}} + \\beta_4 X_{\\mathrm{color_4}_{\\mathrm{L}}} \\\\\n&= \\beta_0 + \\beta_1 X_{\\mathrm{width}}+ \\beta_2 \\times 0 + \\beta_3 \\times 0 + \\beta_4 \\times 0 \\\\\n&= \\beta_0 + \\underbrace{\\beta_1 X_{\\mathrm{width}}}_{\\text{Constant}}.\n\\end{align*}\n\\]\nTherefore, what is the association of the level medium light with respect to dark? Let us take the differences again: \\[\n\\begin{align*}\n\\log \\left( \\frac{\\lambda_{\\mathrm{D}} }{\\lambda_{\\mathrm{L}}} \\right) &= \\log \\left( \\lambda_{\\mathrm{D}} \\right) - \\log \\left( \\lambda_{\\mathrm{L}} \\right) \\\\\n&= \\beta_4.\n\\end{align*}\n\\]\nThen, we exponentiate the previous equation:\n\\[\n\\frac{\\lambda_{\\mathrm{D}} }{\\lambda_{\\mathrm{L}}} = e^{\\beta_4}.\n\\]\nThe expression \\(\\frac{\\lambda_{\\mathrm{D}} }{\\lambda_{\\mathrm{L}}} = e^{\\beta_4}\\) indicates that the response varies in a multiplicative way when the color of the prosoma changes from medium light to dark.\n\\(\\frac{\\hat{\\lambda}_{\\mathrm{D}} }{\\hat{\\lambda}_{\\mathrm{L}}} = e^{\\hat{\\beta}_4} = 0.66\\) indicates that the mean satellite size (satell) of male horseshoe crabs around a female breeding nest decreases by \\(34\\%\\) when the color of the prosoma changes from medium light to dark, while keeping the carapace width constant.\n\n10.9.3 Prediction using test data\nLet us now predict on the test set. Besides the residual deviance discussed in Goodness of fit, the model performance can also be measured by Kullback-Leibler (KL) divergence, which is defined as \\[\nD_{\\mathrm{KL}}(\\hat{y_i} \\Vert \\hat{\\lambda}_i) = y_i \\log\\left(\\frac{\\hat{y_i}}{\\hat{\\lambda}_i} \\right) - y_i + \\hat{\\lambda}_i.\n\\] Kullback–Leibler (KL) divergence is a measure of how one probability distribution diverges from a reference distribution. In the context of Poisson regression, KL divergence quantifies how much the predicted Poisson distribution diverges from the true observed data.\nAveraging \\(\\overline{D}_{\\mathrm{KL}}(\\hat{y}\\Vert \\hat{\\lambda}) = \\frac{1}{n} \\sum_{i=1}^n D_{\\mathrm{KL}}(\\hat{y_i} \\Vert \\hat{\\lambda}_i)\\) across all observations gives a measure of how well the model captures the true data distribution. Smaller averaged KL values indicate better fit.\n\n\nR Code\nPython Code\n\n\n\n# Factorize `color` in test set\ntest_set$color &lt;- as.factor(test_set$color)\n# Predict\npredictions &lt;- predict(poisson_model2, newdata = test_set, type = \"response\")\nactuals &lt;- test_set$satell\n# KL divergence\nkf = 1\n\ncat(\"KL divergence:\", kf, \"\\n\")\n\n\n# Factorize `color` in test set\ntest_set['color'] = test_set['color'].astype('category')\n# Predict\npredictions = poisson_model2.predict(test_set)\nactuals = test_set['satell']\n# KL divergence\nkf = 1\n\nprint(f\"KF divergence: {kf:.4f}\")\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\nKL divergence: 1 \n\n\n\n\n\n\nKL divergence: 1.0000\n\n\n\n\n\nPoisson residual deviance equals 2 times the KL divergence of Poisson distribution summed across all points. It measures the difference in log-likelihood between a saturated model (perfect fit) and the fitted model as defined in Goodness of fit. Lower deviance values indicate the model predictions are closer to the observed data.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Classical Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/10-classical-poisson.html#storytelling",
    "href": "book/10-classical-poisson.html#storytelling",
    "title": "10  Classical Poisson Regression",
    "section": "\n10.10 Storytelling",
    "text": "10.10 Storytelling\nThe data case explores the relationship between satellite size, i.e., the number of male horseshoe crabs around the female horseshoe breeding nests, and several phenotypic characteristis of the female horseshoe crabs. The graphical display suggests that there is a dependence between satellite sizes and female horseshoe crab widths. The Poisson regression modelling suggests that widths is significantely related to satellite size and the dark color is significantly different to the medium light color of the female horseshoe size, given constant confoudning variables at the 5% significance level.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Classical Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/10-classical-poisson.html#practice-problems",
    "href": "book/10-classical-poisson.html#practice-problems",
    "title": "10  Classical Poisson Regression",
    "section": "\n10.11 Practice Problems",
    "text": "10.11 Practice Problems\nIn this final section, you can test your understanding with the following conceptual interactive exercises, and then proceed to work through an inferential analysis of a simple dataset.\n\n\n10.11.1 Conceptual Questions\n\nWhat makes Poisson regression appropriate for modelling the number of satellites per crab?\n\nClick to reveal answer\n\nThe response variable (number of satellites) is a non-negative integer, shows right skewness, and represents event counts for a fixed unit (one crab), all of which make Poisson regression appropriate.\n\nWhat is the key assumption made about the distribution of the response by a classical Poisson regression model?\n\nClick to reveal answer\n\nEquidispersion: The conditional distribution of the response given the covariates follows a Poisson distribution with mean equal to variance.\n\nWhy do we use the log function in Poisson regression?\n\nClick to reveal answer\n\nThe log function (technically the log link) ensures that the predicted mean of the response variable is always positive, which is necessary since counts cannot be negative.\n\nIn the univariate model, what does a positive coefficient on carapace width mean?\n\nClick to reveal answer\n\nLarger female crabs are expected to attract more satellite males, holding all other variables constant.\n\nWhat can the incidence rate ratio (IRR) tell us in the context of carapace width for this dataset?\n\nClick to reveal answer\n\nThe IRR indicates the multiplicative change in the expected number of satellites for each one-unit increase in carapace width, holding other variables constant.\n\nIn this chapter, why did we use binning (with grouped means) before fitting to a Poisson model?\n\nClick to reveal answer\n\nBinning helps to visualize the relationship between carapace width and satellite size, by highlighting trends in the mean response of the data, to assess whether a log-linear trend is plausible.\n\nWhat does residual deviance measure in a Poisson regression model?\n\nClick to reveal answer\n\nHow far the fitted model is from the saturated model (perfect fit) – lower values indicate a better fit.\n\nHow do we assess the goodness-of-fit of a Poisson regression model?\n\nClick to reveal answer\n\nUse the residual deviance, and its associated p-value from a chi-squared test to assess goodness-of-fit: A high p-value indicates a good fit, while a low p-value indicates a lack-of-fit.\n\nWhat does overdispersion mean? Why is it a problem for Poisson regression?\n\nClick to reveal answer\n\nOverdispersion occurs when the variance of the response variable exceeds its mean. This violates the equidispersion assumption, which can lead to underestimated standard errors and overstated significance of predictors.\n\nHow can we diagnoze overdispersion in the horseshoe crab dataset?\n\nClick to reveal answer\n\nWe can compare the residual deviance to the degrees of freedom. If the ratio is significantly greater than 1, this indicates overdispersion.\n\nWhy are categorical predictors, such as color, included as indicator variables in Poisson regression?\n\nClick to reveal answer\n\nThey allow the model to estimate separate effects for each category relative to a baseline category, which allows for the analysis of how different levels of the categorical variable influence the response.\n\nWhy do we interpret on the response scale (exponentiated coefficients) rather than the log scale in Poisson regression?\n\nClick to reveal answer\n\nThis is simply more intuitive, as it tells us the multiplicative effect on the expected count of the response variable for a one-unit change in the predictor.\n\nHow do we interpret the coefficient for a categorical variable in Poisson regression?\n\nClick to reveal answer\n\nThe coefficient (exponentiated!) represents the change in the expected count of the response variable when moving from the baseline category to the category represented by the coefficient (holding other variables constant).\n\n10.11.2 Coding Question\nProblem 1\nIn this problem, you are given a set of 700 individuals with different demographic and socioeconomic characteristics, along with their reported number of annual doctor visits. Your task will be to perform a full poisson regression analysis following a similar structure to this chapter. The variables involved are:\n\n\nVariable Name\nDescription\n\n\n\ndoctor_visits\nThe number of visits in the past year (0-20+)\n\n\nage\nAge in years\n\n\nincome\nAnnual income in USD\n\n\neducation_years\nTotal years of education\n\n\nmarried\nMarried or not (0 or 1)\n\n\nurban\nUrban residence or not (0 or 1)\n\n\ninsurance\nHealth insurance or not (0 or 1)\n\n\n\nBy the end of these questions, you will be able to describe how age, insurance and socioeconomic factors affect the number of doctor visits individuals need. You will determine whether the Poisson model is appropriate, and which predictors matter most in answering this problem.\nA. Data Wrangling and Exploratory Data Analysis\nLet us check if doctor_visists meets the assumptions for a classical Poisson model:\n\nCheck if it is a non-negative integer.\nExamine its distribution, computing the mean and variance.\nIs equidispersion (Var \\(\\approx\\) Mean) plausible here?\n\nClick to reveal answer\n\n\nR Code\nPython Code\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.1     ✔ purrr     1.1.0\n✔ lubridate 1.9.4     ✔ stringr   1.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ broom::bootstrap()   masks asbio::bootstrap()\n✖ gridExtra::combine() masks dplyr::combine()\n✖ magrittr::extract()  masks tidyr::extract()\n✖ dplyr::filter()      masks stats::filter()\n✖ dplyr::lag()         masks stats::lag()\n✖ lubridate::pm()      masks asbio::pm()\n✖ purrr::set_names()   masks magrittr::set_names()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    crabs\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(broom)\nlibrary(dplyr)\nlibrary(zoo)\n\n\nAttaching package: 'zoo'\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(DHARMa)\n\nThis is DHARMa 0.4.7. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\nselect &lt;- dplyr::select\n\n## setwd(\"/home/michael/coding/stats-textbook/regression-cookbook\") \n\nload(\"./data/poisson_regression.rda\")\ndf &lt;- poisson_regression  \n\n# Convert to categorical \ndf &lt;- df %&gt;%\n  mutate(\n    married   = factor(married),\n    urban     = factor(urban),\n    insurance = factor(insurance)\n  )\n\n############################################################\n# A. Poisson Regression Assumptions\n############################################################\n\nsummary(df$doctor_visits)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    3.00    5.00    5.13    7.00   21.00 \n\nmean_val &lt;- mean(df$doctor_visits)\nvar_val  &lt;- var(df$doctor_visits)\nc(mean = mean_val, variance = var_val)\n\n    mean variance \n5.130000 9.624725 \n\ntable(df$doctor_visits)\n\n\n  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  20  21 \n 18  59 108 155 140 151  93  84  62  43  28  23  11  10   1   3   5   3   2   1 \n\ncat(\"Mean =\", mean_val, \"Variance =\", var_val, \"\\n\")\n\nMean = 5.13 Variance = 9.624725 \n\n\n\n\n\n############################################################\n# Imports\n############################################################\n\nimport pyreadr\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy.stats import chi2\n\nsns.set(style=\"whitegrid\")\n\n############################################################\n# Load data (RDA)\n############################################################\n\nresult = pyreadr.read_r(\"./data/poisson_regression.rda\")\ndf = result[\"poisson_regression\"]\n\n# Convert categorical variables\nfor col in [\"married\", \"urban\", \"insurance\"]:\n    df[col] = df[col].astype(\"category\")\n\n############################################################\n# A. Poisson Regression Assumptions\n############################################################\n\nprint(df[\"doctor_visits\"].describe())\n\ncount    1000.000000\nmean        5.130000\nstd         3.102374\nmin         0.000000\n25%         3.000000\n50%         5.000000\n75%         7.000000\nmax        21.000000\nName: doctor_visits, dtype: float64\n\nmean_val = df[\"doctor_visits\"].mean()\nvar_val  = df[\"doctor_visits\"].var()\n\nprint({\"mean\": mean_val, \"variance\": var_val})\n\n{'mean': np.float64(5.13), 'variance': np.float64(9.624724724724727)}\n\nprint(df[\"doctor_visits\"].value_counts().sort_index())\n\ndoctor_visits\n0.0      18\n1.0      59\n2.0     108\n3.0     155\n4.0     140\n5.0     151\n6.0      93\n7.0      84\n8.0      62\n9.0      43\n10.0     28\n11.0     23\n12.0     11\n13.0     10\n14.0      1\n15.0      3\n16.0      5\n17.0      3\n20.0      2\n21.0      1\nName: count, dtype: int64\n\n\n\n\n\nWe see that the distribution is right-skewed, with many small counts and a few larger values; the sample mean and variance are of similar magnitude, suggesting that equidispersion is plausible here.\nB. A Graphical Look\n\nCreate a scatterplot of doctor_visits vs age, adding an offset to the points with jitter for visibility.\nComment on the (1) skewness, (2) heteroskedasticity and (3) whether the variability increases with the mean, relating this to your observation in question A3.\n\nClick to reveal answer\n\n\nR Code\nPython Code\n\n\n\n\n############################################################\n# B. A Graphical Look\n############################################################\n\n# Jitter scatter with marginal histograms\np_scatter &lt;- ggplot(df, aes(x = age, y = doctor_visits)) +\n  geom_jitter(alpha = 0.3, width = 0.2, height = 0.2) +\n  labs(title = \"Doctor Visits vs Age\")\n\np_hist_x &lt;- ggplot(df, aes(x = age)) + geom_histogram(bins = 30)\np_hist_y &lt;- ggplot(df, aes(x = doctor_visits)) + geom_histogram(bins = 30)\n\nprint(p_scatter)\n\n\n\n\n\n\nprint(p_hist_x)\n\n\n\n\n\n\nprint(p_hist_y)\n\n\n\n\n\n\n\n\n\n\n############################################################\n# B. A Graphical Look\n############################################################\n\nplt.figure(figsize=(6,4))\nplt.scatter(df[\"age\"], df[\"doctor_visits\"], alpha=0.3)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Doctor Visits\")\nplt.title(\"Doctor Visits vs Age\")\nplt.show()\n\n\n\n\n\n\nplt.figure(figsize=(6,4))\nplt.hist(df[\"age\"], bins=30)\nplt.title(\"Age Distribution\")\nplt.show()\n\n\n\n\n\n\nplt.figure(figsize=(6,4))\nplt.hist(df[\"doctor_visits\"], bins=30)\nplt.title(\"Doctor Visits Distribution\")\nplt.show()\n\n\n\n\n\n\n\n\nThe distribution of doctor_visits is right-skewed, the spread increases with age, and the variability appears larger at higher mean values. This is consistent with the Poisson mean–variance relationship noted in A3.\n\n\n\nC. Data Wrangling\nNow, prepare the data for modelling:\n\nConvert all appropriate variables to factors. Report the number of levels in each factor to verify this is working.\nDrop any missing observations.\nProduce a summary table of all variables.\n\nClick to reveal answer\n\n\nR Code\nPython Code\n\n\n\n\n############################################################\n# C. Data Wrangling\n############################################################\n\nsummary(df)\n\n      age            income      education_years urban   married insurance\n Min.   : 2.70   Min.   :-7021   Min.   :10.00   0:293   0:498   0:422    \n 1st Qu.:28.18   1st Qu.:40416   1st Qu.:12.00   1:707   1:502   1:578    \n Median :34.60   Median :50727   Median :15.00                            \n Mean   :34.60   Mean   :50126   Mean   :15.07                            \n 3rd Qu.:41.70   3rd Qu.:59808   3rd Qu.:18.00                            \n Max.   :70.70   Max.   :92761   Max.   :20.00                            \n doctor_visits  \n Min.   : 0.00  \n 1st Qu.: 3.00  \n Median : 5.00  \n Mean   : 5.13  \n 3rd Qu.: 7.00  \n Max.   :21.00  \n\nlapply(df[c(\"married\",\"urban\",\"insurance\")], table)\n\n$married\n\n  0   1 \n498 502 \n\n$urban\n\n  0   1 \n293 707 \n\n$insurance\n\n  0   1 \n422 578 \n\n\n\n\n\n############################################################\n# C. Data Wrangling\n############################################################\n\nprint(df.describe(include=\"all\"))\n\n                age        income  ...  insurance  doctor_visits\ncount   1000.000000   1000.000000  ...     1000.0    1000.000000\nunique          NaN           NaN  ...        2.0            NaN\ntop             NaN           NaN  ...        1.0            NaN\nfreq            NaN           NaN  ...      578.0            NaN\nmean      34.604300  50125.837370  ...        NaN       5.130000\nstd       10.013233  14377.333877  ...        NaN       3.102374\nmin        2.700000  -7020.670000  ...        NaN       0.000000\n25%       28.175000  40416.130000  ...        NaN       3.000000\n50%       34.600000  50726.830000  ...        NaN       5.000000\n75%       41.700000  59808.250000  ...        NaN       7.000000\nmax       70.700000  92760.620000  ...        NaN      21.000000\n\n[11 rows x 7 columns]\n\nfor col in [\"married\", \"urban\", \"insurance\"]:\n    print(f\"\\n{col} counts:\")\n    print(df[col].value_counts())\n\n\nmarried counts:\nmarried\n1.0    502\n0.0    498\nName: count, dtype: int64\n\nurban counts:\nurban\n1.0    707\n0.0    293\nName: count, dtype: int64\n\ninsurance counts:\ninsurance\n1.0    578\n0.0    422\nName: count, dtype: int64\n\n\n\n\n\n\nThe variables married, urban, and insurance are categorical with a small number of discrete levels, and can thus be treated as factors.\nD. Exploratory Data Analysis\nFollowing the horsehsoe crab example closely:\n\nBin age into 10 equally-sized groups, and compute the mean age and doctor_visits of each.\nOverlay the bin means on the scatterplot produced in question B1.\nComment on the trend: How does doctor visitation vary with age? Does this trend (if any) appear linear?\n\nClick to reveal answer\n\n\nR Code\nPython Code\n\n\n\n\n############################################################\n# D. Exploratory Data Analysis\n############################################################\n\ndf_binned &lt;- df %&gt;%\n  mutate(age_bin = ntile(age, 10)) %&gt;%\n  group_by(age_bin) %&gt;%\n  summarize(\n    mean_age = mean(age),\n    mean_visits = mean(doctor_visits),\n    .groups = \"drop\"\n  )\n\nggplot(df, aes(x = age, y = doctor_visits)) +\n  geom_jitter(alpha = 0.25) +\n  geom_point(data = df_binned, aes(x = mean_age, y = mean_visits),\n             color = \"red\", size = 3) +\n  geom_line(data = df_binned, aes(x = mean_age, y = mean_visits),\n            color = \"red\") +\n  labs(title = \"Binned Mean Doctor Visits vs Age\")\n\n\n\n\n\n\n\n\n\n\n############################################################\n# D. Exploratory Data Analysis (Binning)\n############################################################\n\ndf[\"age_bin\"] = pd.qcut(df[\"age\"], 10, labels=False)\n\ndf_binned = (\n    df.groupby(\"age_bin\")\n      .agg(mean_age=(\"age\", \"mean\"),\n           mean_visits=(\"doctor_visits\", \"mean\"))\n      .reset_index()\n)\n\nplt.figure(figsize=(6,4))\nplt.scatter(df[\"age\"], df[\"doctor_visits\"], alpha=0.25)\nplt.plot(df_binned[\"mean_age\"], df_binned[\"mean_visits\"],\n         color=\"red\", marker=\"o\")\nplt.title(\"Binned Mean Doctor Visits vs Age\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Doctor Visits\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe number of doctor visits tend to increase with age on average—the binned means show a smooth upward trend that is not strictly linear on the response scale.\nE. Simple Data Modelling & Estimation\nWe will model the expected number of visits of some individual \\(i\\) as \\(\\log(\\lambda_i) = \\beta_0 + \\beta_1  X_{i,\\text{age}}\\).\n\nFit the univariate Poisson regression above.\nPrint a coefficient table.\nCompute and display \\(\\text{IRR}(\\text{exp}(\\beta_1))\\).\nCompute and display the 95% confidence interval for \\(\\beta_1\\).\n\nClick to reveal answer\n\n\nR Code\nPython Code\n\n\n\n\n############################################################\n# E. Simple Data Modelling & Estimation\n############################################################\n\nmod_uni &lt;- glm(doctor_visits ~ age, family = poisson(), data = df)\nsummary(mod_uni)\n\n\nCall:\nglm(formula = doctor_visits ~ age, family = poisson(), data = df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) 0.545217   0.054529   9.999   &lt;2e-16 ***\nage         0.030181   0.001401  21.537   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1822.8  on 999  degrees of freedom\nResidual deviance: 1357.5  on 998  degrees of freedom\nAIC: 4662.7\n\nNumber of Fisher Scoring iterations: 4\n\ntidy(mod_uni, conf.int = TRUE) %&gt;%\n  mutate(IRR = exp(estimate),\n         IRR_low = exp(conf.low),\n         IRR_high = exp(conf.high))\n\n# A tibble: 2 × 10\n  term   estimate std.error statistic   p.value conf.low conf.high   IRR IRR_low\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 (Inte…   0.545    0.0545      10.00 1.54e- 23   0.438     0.652   1.72    1.55\n2 age      0.0302   0.00140     21.5  7.04e-103   0.0274    0.0329  1.03    1.03\n# ℹ 1 more variable: IRR_high &lt;dbl&gt;\n\n\n\n\n\n############################################################\n# E. Simple Poisson Regression\n############################################################\n\nmod_uni = smf.glm(\n    \"doctor_visits ~ age\",\n    data=df,\n    family=sm.families.Poisson()\n).fit()\n\nprint(mod_uni.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:          doctor_visits   No. Observations:                 1000\nModel:                            GLM   Df Residuals:                      998\nModel Family:                 Poisson   Df Model:                            1\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -2329.3\nDate:                Wed, 21 Jan 2026   Deviance:                       1357.5\nTime:                        20:14:06   Pearson chi2:                 1.30e+03\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.3720\nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.5452      0.055      9.999      0.000       0.438       0.652\nage            0.0302      0.001     21.536      0.000       0.027       0.033\n==============================================================================\n\n# IRRs + confidence intervals\nuni_params = mod_uni.params\nuni_ci = mod_uni.conf_int()\n\nuni_results = pd.DataFrame({\n    \"estimate\": uni_params,\n    \"conf_low\": uni_ci[0],\n    \"conf_high\": uni_ci[1],\n    \"IRR\": np.exp(uni_params),\n    \"IRR_low\": np.exp(uni_ci[0]),\n    \"IRR_high\": np.exp(uni_ci[1])\n})\n\nprint(uni_results)\n\n           estimate  conf_low  conf_high       IRR   IRR_low  IRR_high\nIntercept  0.545217  0.438340   0.652093  1.724982  1.550132  1.919554\nage        0.030181  0.027435   0.032928  1.030641  1.027814  1.033476\n\n\n\n\n\n\nThe IRR is the multiplicative change in the expected number of doctor visits associated with a one-year increase in age. The 95% confidence interval for \\(\\beta_1\\) does not include zero, indicating a statistically significant association between age and doctor visits.\nF. Goodness of Fit\nUsing your univariate model from question E:\n\nReport the residual deviance and the degrees of freedom.\nCompute a chi-square goodness-of-fit test.\nCompute the Pearson dispersion statistic.\nIs this an overdispersion or underdispersion, and how do you know?\n\nClick to reveal answer\n\n\nR Code\nPython Code\n\n\n\n\n############################################################\n# F. Goodness of Fit\n############################################################\n\nres_dev  &lt;- deviance(mod_uni)\ndf_res   &lt;- df.residual(mod_uni)\ngof_pval &lt;- 1 - pchisq(res_dev, df_res)\n\npearson_X2 &lt;- sum(residuals(mod_uni, type = \"pearson\")^2)\ndispersion &lt;- pearson_X2 / df_res\n\ncat(\"Residual deviance =\", res_dev, \"df =\", df_res, \"GOF p-value =\", gof_pval, \"\\n\")\n\nResidual deviance = 1357.524 df = 998 GOF p-value = 1.957323e-13 \n\ncat(\"Pearson X2 =\", pearson_X2, \"Dispersion =\", dispersion, \"\\n\")\n\nPearson X2 = 1299.367 Dispersion = 1.301971 \n\n\n\n\n\n############################################################\n# F. Goodness of Fit\n############################################################\n\nres_dev = mod_uni.deviance\ndf_res  = mod_uni.df_resid\ngof_pval = 1 - chi2.cdf(res_dev, df_res)\n\npearson_X2 = np.sum(mod_uni.resid_pearson**2)\ndispersion = pearson_X2 / df_res\n\nprint(f\"Residual deviance = {res_dev:.3f}\")\n\nResidual deviance = 1357.524\n\nprint(f\"df = {df_res}\")\n\ndf = 998\n\nprint(f\"GOF p-value = {gof_pval:.4f}\")\n\nGOF p-value = 0.0000\n\nprint(f\"Pearson X2 = {pearson_X2:.3f}\")\n\nPearson X2 = 1299.367\n\nprint(f\"Dispersion = {dispersion:.3f}\")\n\nDispersion = 1.302\n\n\n\n\n\n\nThe dispersion statistic is greater than 1, indicating mild overdispersion relative to the Poisson assumption.\nG. Inference\n\nPerform the Wald test for \\(\\beta_1\\).\nDetermine the 95% confidence interval for the IRR computed in question E3.\n\nExplain how the estimated coefficient \\(\\hat{\\beta}_1\\) affects the relationship between age and the expected number of doctor visits. In particular:\n\nDescribe how the expected number of doctor visits changes as age increases.\nIs the relationship linear, exponential, or something else? Explain your reasoning.\nDo equal increases in age lead to multiplicative or additive changes in the expected number of visits? Why?\n\n\n\nClick to reveal answer\n\n\nR Code\nPython Code\n\n\n\n\n############################################################\n# G. Inference\n############################################################\n\ncoef_uni &lt;- tidy(mod_uni, conf.int = TRUE)\ncoef_uni\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   0.545    0.0545      10.00 1.54e- 23   0.438     0.652 \n2 age           0.0302   0.00140     21.5  7.04e-103   0.0274    0.0329\n\ncat(\"IRR for age =\", exp(coef(mod_uni)[\"age\"]), \"\\n\")\n\nIRR for age = 1.030641 \n\n\nThe estimated coefficient \\(\\hat{\\beta}_1\\) is included in the model through the log link function, \\[\n\\log(\\lambda_i) = \\beta_0 + \\beta_1 \\,\\text{age}_i.\n\\] This implies that the expected number of doctor visits satisfies \\[\n\\lambda_i = \\exp(\\beta_0)\\exp(\\beta_1 \\,\\text{age}_i).\n\\]\nAs age increases, the expected number of doctor visits increases multiplicatively rather than additively: Each one-year increase in age multiplies the expected number of visits by a constant factor equal to the incidence rate ratio (IRR), i.e. \\(\\exp(\\hat{\\beta}_1)\\).\nTherefore, the relationship between age and expected doctor visits is exponential in the response scale and linear in the log scale. Equal increases in age produce proportionally equal changes in the expected count (a feature of all Poisson regression models).\n\n\n\n############################################################\n# G. Inference\n############################################################\n\nprint(\"IRR for age =\", np.exp(mod_uni.params[\"age\"]))\n\nIRR for age = 1.0306414682533183\n\n\n\nThe estimated coefficient \\(\\hat{\\beta}_1\\) is included in the model through the log link function, \\[\n\\log(\\lambda_i) = \\beta_0 + \\beta_1 \\,\\text{age}_i.\n\\] This implies that the expected number of doctor visits satisfies \\[\n\\lambda_i = \\exp(\\beta_0)\\exp(\\beta_1 \\,\\text{age}_i).\n\\]\nAs age increases, the expected number of doctor visits increases multiplicatively rather than additively: Each one-year increase in age multiplies the expected number of visits by a constant factor equal to the incidence rate ratio (IRR), i.e. \\(\\exp(\\hat{\\beta}_1)\\).\nTherefore, the relationship between age and expected doctor visits is exponential in the response scale and linear in the log scale. Equal increases in age produce proportionally equal changes in the expected count (a feature of all Poisson regression models).\n\n\n\nH. Multivariate Poisson Regression\nNow, extend the model to: \\[\\log(\\lambda_i) = \\beta_0 + \\sum_{j}\\beta_i X_{i,j},\\] where \\(j\\) runs over age, income, education_years, married, urban, and insurance.\n\nFit the multivariable model above.\nCompute the IRRs and 95% confidence intervals.\nInterpret the effect of insurance (insured or not).\nWhich predictors are the most significant? How do you know?\nFit another model with age * insurance, and perform a likelihood-ratio test vs the additive model above.\nBased on the result, does insurance modify the effect of age?\n\nClick to reveal answer\n\n\nR Code\nPython Code\n\n\n\n\n############################################################\n# H. Multivariate Poisson Regression\n############################################################\n\nform_multi &lt;- doctor_visits ~ age + income + education_years +\n  married + urban + insurance\n\nmod_multi &lt;- glm(form_multi, family = poisson(), data = df)\nsummary(mod_multi)\n\n\nCall:\nglm(formula = form_multi, family = poisson(), data = df)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      1.710e-02  1.073e-01   0.159    0.873    \nage              2.913e-02  1.396e-03  20.863  &lt; 2e-16 ***\nincome          -1.165e-05  9.493e-07 -12.276  &lt; 2e-16 ***\neducation_years  5.647e-02  4.571e-03  12.353  &lt; 2e-16 ***\nmarried1         1.229e-01  2.814e-02   4.369 1.25e-05 ***\nurban1           1.742e-01  3.181e-02   5.474 4.40e-08 ***\ninsurance1       1.359e-01  2.872e-02   4.730 2.24e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1822.8  on 999  degrees of freedom\nResidual deviance: 1002.2  on 993  degrees of freedom\nAIC: 4317.3\n\nNumber of Fisher Scoring iterations: 4\n\ntidy(mod_multi, conf.int = TRUE) %&gt;%\n  mutate(IRR = exp(estimate),\n         IRR_low = exp(conf.low),\n         IRR_high = exp(conf.high))\n\n# A tibble: 7 × 10\n  term    estimate std.error statistic  p.value conf.low conf.high   IRR IRR_low\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 (Inter…  1.71e-2   1.07e-1     0.159 8.73e- 1 -1.94e-1   2.27e-1 1.02    0.824\n2 age      2.91e-2   1.40e-3    20.9   1.15e-96  2.64e-2   3.19e-2 1.03    1.03 \n3 income  -1.17e-5   9.49e-7   -12.3   1.22e-34 -1.35e-5  -9.79e-6 1.000   1.000\n4 educat…  5.65e-2   4.57e-3    12.4   4.67e-35  4.75e-2   6.54e-2 1.06    1.05 \n5 marrie…  1.23e-1   2.81e-2     4.37  1.25e- 5  6.78e-2   1.78e-1 1.13    1.07 \n6 urban1   1.74e-1   3.18e-2     5.47  4.40e- 8  1.12e-1   2.37e-1 1.19    1.12 \n7 insura…  1.36e-1   2.87e-2     4.73  2.24e- 6  7.97e-2   1.92e-1 1.15    1.08 \n# ℹ 1 more variable: IRR_high &lt;dbl&gt;\n\n# Interaction: age * insurance\nmod_int &lt;- glm(doctor_visits ~ age * insurance + income + education_years +\n                 married + urban, family = poisson(), data = df)\n\nanova(mod_multi, mod_int, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: doctor_visits ~ age + income + education_years + married + urban + \n    insurance\nModel 2: doctor_visits ~ age * insurance + income + education_years + \n    married + urban\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1       993     1002.2                     \n2       992     1001.8  1  0.43642   0.5089\n\n\n\n\n\n############################################################\n# H. Multivariate Poisson Regression\n############################################################\n\nformula_multi = (\n    \"doctor_visits ~ age + income + education_years \"\n    \"+ married + urban + insurance\"\n)\n\nmod_multi = smf.glm(\n    formula_multi,\n    data=df,\n    family=sm.families.Poisson()\n).fit()\n\nprint(mod_multi.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:          doctor_visits   No. Observations:                 1000\nModel:                            GLM   Df Residuals:                      993\nModel Family:                 Poisson   Df Model:                            6\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -2151.7\nDate:                Wed, 21 Jan 2026   Deviance:                       1002.2\nTime:                        20:14:07   Pearson chi2:                     940.\nNo. Iterations:                     4   Pseudo R-squ. (CS):             0.5598\nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept            0.0171      0.107      0.159      0.873      -0.193       0.227\nmarried[T.1.0]       0.1229      0.028      4.369      0.000       0.068       0.178\nurban[T.1.0]         0.1742      0.032      5.474      0.000       0.112       0.237\ninsurance[T.1.0]     0.1359      0.029      4.730      0.000       0.080       0.192\nage                  0.0291      0.001     20.863      0.000       0.026       0.032\nincome           -1.165e-05   9.49e-07    -12.276      0.000   -1.35e-05   -9.79e-06\neducation_years      0.0565      0.005     12.353      0.000       0.048       0.065\n====================================================================================\n\nmulti_params = mod_multi.params\nmulti_ci = mod_multi.conf_int()\n\nmulti_results = pd.DataFrame({\n    \"estimate\": multi_params,\n    \"conf_low\": multi_ci[0],\n    \"conf_high\": multi_ci[1],\n    \"IRR\": np.exp(multi_params),\n    \"IRR_low\": np.exp(multi_ci[0]),\n    \"IRR_high\": np.exp(multi_ci[1])\n})\n\nprint(multi_results)\n\n                  estimate  conf_low  conf_high       IRR   IRR_low  IRR_high\nIntercept         0.017104 -0.193212   0.227420  1.017251  0.824308  1.255357\nmarried[T.1.0]    0.122949  0.067791   0.178106  1.130826  1.070142  1.194953\nurban[T.1.0]      0.174160  0.111803   0.236516  1.190246  1.118293  1.266828\ninsurance[T.1.0]  0.135868  0.079568   0.192167  1.145530  1.082820  1.211873\nage               0.029131  0.026394   0.031868  1.029559  1.026746  1.032381\nincome           -0.000012 -0.000014  -0.000010  0.999988  0.999986  0.999990\neducation_years   0.056471  0.047511   0.065431  1.058096  1.048658  1.067619\n\n# Interaction model\nmod_int = smf.glm(\n    \"doctor_visits ~ age * insurance + income + education_years + married + urban\",\n    data=df,\n    family=sm.families.Poisson()\n).fit()\n\nlr_stat = 2 * (mod_int.llf - mod_multi.llf)\ndf_diff = mod_int.df_model - mod_multi.df_model\np_lr = 1 - chi2.cdf(lr_stat, df_diff)\n\nprint(\"LR test statistic =\", lr_stat)\n\nLR test statistic = 0.43641994780045934\n\nprint(\"df =\", df_diff)\n\ndf = 1\n\nprint(\"p-value =\", p_lr)\n\np-value = 0.5088554748118505\n\n\n\n\n\n\nHolding all other variables constant, insured individuals are expected to have about 14.6% more doctor visits than uninsured individuals (IRR ≈ 1.15). The 95% confidence interval for the IRR does not include 1, indicating this effect is statistically significant.\nThe most statistically significant predictors are age, education_years, married, urban, and insurance, as their confidence intervals for the IRR do not include 1. Among these, age shows the most precise and consistent effect. education_years also shows a clear positive association with doctor_visits. income has a very small effect size despite being statistically significant.\nI. Prediction & KL Divergence\nUsing the multivariate model from question H:\n\nPredict the expected number of doctor visits for a grid of age values, from 20 to 80.\nPlot predicted visits for insured and uninsured groups, including 95% confidence interval bands.\nCompute the KL divergence. How does this value reflect the model fit?\n\nClick to reveal answer\n\n\nR Code\nPython Code\n\n\n\n\n############################################################\n# I. Prediction & KL Divergence\n############################################################\n\n# Prediction grid for ages\nage_grid &lt;- seq(min(df$age), max(df$age), length.out = 100)\nnewdata &lt;- expand.grid(\n  age = age_grid,\n  income = mean(df$income),\n  education_years = mean(df$education_years),\n  married = levels(df$married)[1],\n  urban = levels(df$urban)[1],\n  insurance = levels(df$insurance)\n)\n\npred_link &lt;- predict(mod_multi, newdata, type = \"link\", se.fit = TRUE)\n\nnewdata$fit &lt;- exp(pred_link$fit)\nnewdata$lower95 &lt;- exp(pred_link$fit - 1.96 * pred_link$se.fit)\nnewdata$upper95 &lt;- exp(pred_link$fit + 1.96 * pred_link$se.fit)\n\nggplot(newdata, aes(x = age, y = fit, color = insurance)) +\n  geom_line(linewidth = 1) +\n  geom_ribbon(aes(ymin = lower95, ymax = upper95, fill = insurance),\n              alpha = 0.15, color = NA) +\n  labs(y = \"Predicted Doctor Visits\", title = \"Predicted Visits by Age and Insurance\")\n\n\n\n\n\n\n# KL Divergence\nlambda_hat &lt;- fitted(mod_multi)\ny &lt;- df$doctor_visits\nKL_i &lt;- ifelse(y == 0,\n               -y + lambda_hat,\n               y * log(y / lambda_hat) - y + lambda_hat)\nKL_avg &lt;- mean(KL_i)\n\ncat(\"Average KL divergence =\", KL_avg, \"\\n\")\n\nAverage KL divergence = 0.5010995 \n\n\n\n\n\n############################################################\n# I. Prediction & KL Divergence\n############################################################\n\nage_grid = np.linspace(df[\"age\"].min(), df[\"age\"].max(), 100)\n\nnewdata = pd.DataFrame({\n    \"age\": np.repeat(age_grid, df[\"insurance\"].cat.categories.size),\n    \"income\": df[\"income\"].mean(),\n    \"education_years\": df[\"education_years\"].mean(),\n    \"married\": df[\"married\"].cat.categories[0],\n    \"urban\": df[\"urban\"].cat.categories[0],\n    \"insurance\": np.tile(df[\"insurance\"].cat.categories, 100)\n})\n\n# Ensure categories match\nfor col in [\"married\", \"urban\", \"insurance\"]:\n    newdata[col] = newdata[col].astype(\"category\")\n    newdata[col] = newdata[col].cat.set_categories(df[col].cat.categories)\n\npred = mod_multi.get_prediction(newdata)\npred_summary = pred.summary_frame()\n\nnewdata[\"fit\"] = np.exp(pred_summary[\"mean\"])\nnewdata[\"lower95\"] = np.exp(pred_summary[\"mean_ci_lower\"])\nnewdata[\"upper95\"] = np.exp(pred_summary[\"mean_ci_upper\"])\n\nplt.figure(figsize=(7,5))\nsns.lineplot(data=newdata, x=\"age\", y=\"fit\", hue=\"insurance\")\nplt.fill_between(\n    newdata[\"age\"],\n    newdata[\"lower95\"],\n    newdata[\"upper95\"],\n    alpha=0.15\n)\nplt.ylabel(\"Predicted Doctor Visits\")\nplt.title(\"Predicted Visits by Age and Insurance\")\nplt.show()\n\n\n\n\n\n\n# KL divergence\nlambda_hat = mod_multi.fittedvalues.values\ny = df[\"doctor_visits\"].values\n\nKL_i = np.where(\n    y == 0,\n    -y + lambda_hat,\n    y * np.log(y / lambda_hat) - y + lambda_hat\n)\n\n&lt;string&gt;:5: RuntimeWarning: divide by zero encountered in log\n&lt;string&gt;:5: RuntimeWarning: invalid value encountered in multiply\n\nKL_avg = KL_i.mean()\nprint(\"Average KL divergence =\", KL_avg)\n\nAverage KL divergence = 0.5010994579709896\n\n\n\n\n\n\nA smaller average KL divergence indicates that the fitted Poisson model produces predicted counts that are close to the observed data, reflecting better model fit.\nJ. Storytelling\nThe goal of this analysis was to understand how individual characteristics are associated with the expected number of doctor visits.\n\nSummarize the main findings of the Poisson regression analysis. Which variables are most strongly associated with doctor visitation, and in what direction?\nHow do age, insurance status, and education jointly shape expected doctor visits? Describe the relationship in substantive terms rather than model coefficients.\nAssess whether the classical Poisson model appears to be an appropriate choice for this dataset. What evidence supports or weakens this conclusion?\nBased on the fitted model, what broad conclusions can be drawn about patterns of healthcare use in this population?\n\nClick to reveal answer\nThe Poisson regression analysis shows that doctor visitation increases systematically with age, education, insurance coverage, marital status, and urban residence. Within these, age and education show strong and precise positive associations, while having insurance coverage is associated with an increase in expected visits (vs being uninsured).\nOlder individuals and those with higher levels of education tend to visit doctors more frequently, and insured individuals have higher expected doctor use across all ages. Insurance primarily shifts the overall level of doctor visits upward rather than changing how visits increase with age.\nThe classical Poisson model appears reasonably appropriate for this dataset. The outcome variable is a non-negative count, the mean–variance relationship is approximately consistent with Poisson assumptions, and goodness-of-fit diagnostics show only mild overdispersion. Alternative models could be considered, but there is no strong evidence that the Poisson model is fundamentally misspecified.\nOverall, the results suggest that healthcare use in this population is strongly patterned by age and socioeconomic factors, with insurance coverage playing an important role in increasing access to healthcare care. The model provides a coherent and interpretable summary of how these factors relate to doctor visitation rates.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Classical Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/11-negative-binomial.html",
    "href": "book/11-negative-binomial.html",
    "title": "11  Umami-zing Negative Binomial Regression",
    "section": "",
    "text": "Fun fact!\n\n\nUmami-zing! Savory to the point where you start craving a second plate… and a third.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 11.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Umami-zing Negative Binomial Regression</span>"
    ]
  },
  {
    "objectID": "book/12-zero-inflated-poisson.html",
    "href": "book/12-zero-inflated-poisson.html",
    "title": "12  Spicetacular Zero-Inflated Poisson Regression",
    "section": "",
    "text": "Fun fact!\n\n\nSpicetacular! The kind of spicy that requires a fire extinguisher at the ready.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n        {{Overdispersed or &lt;br/&gt;Underdispersed &lt;br/&gt;Data}}\n        {{Zero Inflated &lt;br/&gt;Data}}\n          )Chapter 12: &lt;br/&gt;Zero Inflated Poisson &lt;br/&gt;Regression(\n            (Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 12.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Spicetacular Zero-Inflated Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/13-generalized-poisson.html",
    "href": "book/13-generalized-poisson.html",
    "title": "13  Herbalicious Generalized Poisson Regression",
    "section": "",
    "text": "Fun fact!\n\n\nHerbalicious! Loaded with herbs and greens; like chewing through a botanical garden.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: &lt;br/&gt;Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n        {{Zero Inflated &lt;br/&gt;Data}}\n          )Chapter 12: &lt;br/&gt;Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed or &lt;br/&gt;Underdispersed &lt;br/&gt;Data}}\n          )Chapter 13: &lt;br/&gt;Generalized &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Generalized &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n\n\n\n\n\n\n\n\nFigure 13.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Herbalicious Generalized Poisson Regression</span>"
    ]
  },
  {
    "objectID": "book/14-multinomial-logistic.html",
    "href": "book/14-multinomial-logistic.html",
    "title": "14  Multinomial Logistic Regression",
    "section": "",
    "text": "14.1 Introduction\nMultinomial logistic regression is a statistical modeling technique used to analyze relationships between a multi-class categorical response variable and a set of explanatory variables. Unlike binary logistic regression, which models outcomes with two categories, multinomial logistic regression extends the framework to outcomes with more than two unordered categories. It estimates the probability of each possible outcome as a function of the predictors by modeling the log-odds of each category relative to a reference category. This method is widely used in fields such as transportation, marketing, and social sciences where the goal is to understand or predict categorical choices among multiple alternatives.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multinomial Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/14-multinomial-logistic.html#introduction",
    "href": "book/14-multinomial-logistic.html#introduction",
    "title": "14  Multinomial Logistic Regression",
    "section": "",
    "text": "14.1.1 Multinomial Logistic Regression Assumptions\n\nThe response or dependent variable should be measured at the nominal level with more than two values.\nThere should be one or more independent variables that are continuous, ordinal or nominal (including dichotomous variables).\n\nLogit linearity assumption: There needs to be a linear relationship between any continuous independent variables and the logit transformation of the dependent variable.\n\nIndependent observations: The observations should be independent and the dependent variable should have mutually exclusive and exhaustive categories (i.e. no individual belonging to two different categories).\n\nNo Multicollinearity: Multicollinearity occurs when you have two or more independent variables that are highly correlated with each other.\nThere should be no outliers, high leverage values or highly influential points.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multinomial Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/14-multinomial-logistic.html#case-study",
    "href": "book/14-multinomial-logistic.html#case-study",
    "title": "14  Multinomial Logistic Regression",
    "section": "\n14.2 Case Study",
    "text": "14.2 Case Study\nWe will explore a dataset containing information on individuals’ commuting behaviors and demographics, with a focus on their primary mode of transportation. We explore a couple of research problems based on the dataset.\n\nHow is the transportation mode of people (driving own cars or taking public transportation specifically) associated by their commuting distances, given the individuals’ commuting behaviors and demographics including if they have a car, if they commute on weekends, and ages?\nCan we accurately predict the transportation mode of an individual based on their commuting behaviors and demographics including if they have a car, if they commute on weekends, their commute distance, and age?\n\nThe two research problems correspond to inferential and predictive statistical objectives respectively:\n\nTo estimate the association between the explanatory variable—commute distance—and the probability of each category of the response—using each transport mode (car v.s. public transportation specifically), given the confounding effects of age, car availability, and weekend commuter status.\nTo develop a predictive model that classifies individuals into their most likely transport mode using the predictors commute distance, age, car availability, and weekend commuting status.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multinomial Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/14-multinomial-logistic.html#data-collection-and-wrangling",
    "href": "book/14-multinomial-logistic.html#data-collection-and-wrangling",
    "title": "14  Multinomial Logistic Regression",
    "section": "\n14.3 Data Collection and Wrangling",
    "text": "14.3 Data Collection and Wrangling\nThe dataset includes the following five variables:\n\n\ntransport_mode (categorical, 4 levels):\n\nThe primary mode of transport used by the individual. Categories: “Bicycle”, “Car”, “Public Transit”, “Walking”. This variable is used as the response in data modelling.\n\n\n\ncommute_distance (numeric):\n\nThe one-way commuting distance in kilometers.\n\n\n\nage (numeric):\n\nThe age of the individual in years.\n\n\n\nhas_car_available (binary categorical):\n\nIndicates whether the individual has access to a car. Levels: “Yes” or “No”.\n\n\n\nweekend_commuter (binary categorical):\n\nIndicates whether the individual commutes during weekends. Levels: “Yes” or “No”.\n\n\n\n\n\nR Code\nPython Code\n\n\n\ndata(multinomial_transport)\ndata &lt;- multinomial_transport[,-1]\ncat(\"There are\", nrow(data), \"observations and\", ncol(data), \"variables in the dataset.\")\nprint(data)\n\n\nurl = \"https://raw.githubusercontent.com/andytai7/cookbook/refs/heads/main/raw-data/multinomial_transport.csv\"\ndata = pd.read_csv(url)\ndata = data.iloc[:, :-1]\nprint(f\"There are {data.shape[0]} observations and {data.shape[1]} variables in the dataset.\")\nprint(data)\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\nThere are 1000 observations and 5 variables in the dataset.\n\n\n# A tibble: 1,000 × 5\n   transport_mode commute_distance   age has_car_available weekend_commuter\n   &lt;chr&gt;                     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;           \n 1 Public_Transit             22.9    58 No                No              \n 2 Public_Transit             23.5    21 Yes               Yes             \n 3 Bicycle                     7.5    57 Yes               No              \n 4 Public_Transit             20.8    43 No                No              \n 5 Public_Transit             16.2    41 No                No              \n 6 Car                        13.2    19 No                No              \n 7 Car                        18.5    44 Yes               No              \n 8 Public_Transit              3.8    52 No                No              \n 9 Public_Transit             16.6    29 Yes               No              \n10 Public_Transit             17.8    56 Yes               No              \n# ℹ 990 more rows\n\n\n\n\n\n\nThere are 1000 observations and 5 variables in the dataset.\n\n\n     transport_mode  commute_distance  age has_car_available weekend_commuter\n0    Public_Transit              22.9   58                No               No\n1    Public_Transit              23.5   21               Yes              Yes\n2           Bicycle               7.5   57               Yes               No\n3    Public_Transit              20.8   43                No               No\n4    Public_Transit              16.2   41                No               No\n..              ...               ...  ...               ...              ...\n995  Public_Transit               3.9   30                No               No\n996             Car              13.9   22               Yes               No\n997  Public_Transit               1.9   29                No               No\n998             Car              12.6   34               Yes               No\n999         Bicycle               0.7   63                No               No\n\n[1000 rows x 5 columns]\n\n\n\n\n\nFactorize the categorical variables. There are four types of transport mode which are by bicycle, car, public transit, and walking.\n\n\nR Code\nPython Code\n\n\n\n# Factorize the categorical variables and print their levels\ncategorical_cols &lt;- c('transport_mode', 'has_car_available', 'weekend_commuter')\nfor (col in categorical_cols) {\n  data[[col]] &lt;- as.factor(data[[col]])\n  cat(\"Levels of\", col, \":\", levels(data[[col]]), \"\\n\")\n}\n\n\n# Factorize the categorical variables and print their levels\ncategorical_cols = ['transport_mode', 'has_car_available', 'weekend_commuter']\nfor col in categorical_cols:\n  data[col] = data[col].astype('category')\n  print(f\"Levels of {col}: {list(data[col].cat.categories)}\")\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\nLevels of transport_mode : Bicycle Car Public_Transit Walking \nLevels of has_car_available : No Yes \nLevels of weekend_commuter : No Yes \n\n\n\n\n\n\nLevels of transport_mode: ['Bicycle', 'Car', 'Public_Transit', 'Walking']\nLevels of has_car_available: ['No', 'Yes']\nLevels of weekend_commuter: ['No', 'Yes']\n\n\n\n\n\nNow, we split the dataset into training and test sets, use training set to fit the model, and use test set to assess the model performance.\n\n\nR Code\nPython Code\n\n\n\n# Split the data: first 90% for training, last 10% for test\nn &lt;- nrow(data)\nsplit_idx &lt;- floor(0.9 * n)\ntrain_set &lt;- data[1:split_idx, ]\ntest_set &lt;- data[(split_idx + 1):n, ]\n\ncat(\"Training set size:\", nrow(train_set))\ncat(\"Test set size:\", nrow(test_set))\n\n\n# Split the data: first 90% for training, last 10% for test\nsplit_idx = int(len(data) * 0.9)\ntrain_set = data.iloc[:split_idx].reset_index(drop=True)\ntest_set = data.iloc[split_idx:].reset_index(drop=True)\n\nprint(f\"Training set size: {len(train_set)}\")\nprint(f\"Test set size: {len(test_set)}\")\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\nTraining set size: 900\n\n\nTest set size: 100\n\n\n\n\n\n\nTraining set size: 900\n\n\nTest set size: 100",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multinomial Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/14-multinomial-logistic.html#exploratory-data-analysis",
    "href": "book/14-multinomial-logistic.html#exploratory-data-analysis",
    "title": "14  Multinomial Logistic Regression",
    "section": "\n14.4 Exploratory Data Analysis",
    "text": "14.4 Exploratory Data Analysis\nLet’s visualize the response transport_mode and its relationship with the continuous or categorical(binary) explanatory variables (predictors).\n\n\nR Code\nPython Code\n\n\n\n# Custom color palette `cbPalette` \ncbPalette &lt;- c(\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n# Sort `transport_mode` by frequency\ntrain_set$transport_mode &lt;- factor(train_set$transport_mode, levels = names(sort(table(train_set$transport_mode), decreasing = TRUE)))\n\np1 &lt;- train_set %&gt;%\n  ggplot(aes(x = transport_mode, fill = has_car_available)) +\n  geom_bar(position = \"stack\") +  # use \"stack\" for counts, \"fill\" for proportions\n  labs(y = \"Count\", title = \"With Car Availability\") +\n  scale_fill_manual(values=cbPalette) +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_legend(title = \"Car Availability\"))\n\np2 &lt;- train_set %&gt;%\n  ggplot(aes(x = transport_mode, fill = weekend_commuter)) +\n  geom_bar(position = \"stack\") +  # use \"stack\" for counts, \"fill\" for proportions\n  labs(x = \"Transport Mode\", y = \"Count\", title = \"With Weekend Commuter Status\") +\n  scale_fill_manual(values=cbPalette) +\n  theme_bw() +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_legend(title = \"Weekend Commuter\"))\n\ngrid.arrange(p1, p2, ncol = 2, top = grid::textGrob(\"Distribution of Transport Mode\", gp = grid::gpar(fontsize = 16, fontface = \"bold\")))\n\n\n# Custom color palette `cbPalette` \ncbPalette = [\"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\"]\n\n# Sort transport_mode by frequency\norder = train_set['transport_mode'].value_counts().index\ntrain_set['transport_mode'] = pd.Categorical(train_set['transport_mode'], categories=order, ordered=True)\n\n# Set up the plot\nsns.set_theme(style=\"whitegrid\")\nfig = plt.figure()\nfig.set_size_inches(8, 6)\ngs = GridSpec(1, 2, figure=fig)\n_ = fig.suptitle(\"Distribution of Transport Mode\", fontsize=16, fontweight='bold')\n\n# Plot: Stacked bar for Car Availability\ncar_counts = train_set.groupby(['transport_mode', 'has_car_available']).size().unstack(fill_value=0)\ncar_counts = car_counts.loc[order] \n\nax1 = fig.add_subplot(gs[0, 0])\nbottom = None\nfor i, col in enumerate(car_counts.columns):\n    ax1.bar(car_counts.index, car_counts[col], bottom=bottom, label=col, color=cbPalette[i % len(cbPalette)])\n    bottom = car_counts[col] if bottom is None else bottom + car_counts[col]\nax1.set_title(\"With Car Availability\")\nax1.set_xlabel(\"Transport Mode\")\nax1.set_ylabel(\"Count\")\nax1.legend(title=\"Car Availability\", loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)\n\n# Plot: Stacked bar for Weekend Commuter\nweekend_counts = train_set.groupby(['transport_mode', 'weekend_commuter']).size().unstack(fill_value=0)\nweekend_counts = weekend_counts.loc[order]\n\nax2 = fig.add_subplot(gs[0, 1])\nbottom = None\nfor i, col in enumerate(weekend_counts.columns):\n    ax2.bar(weekend_counts.index, weekend_counts[col], bottom=bottom, label=col, color=cbPalette[i % len(cbPalette)])\n    bottom = weekend_counts[col] if bottom is None else bottom + weekend_counts[col]\nax2.set_title(\"With Weekend Commuter Status\")\nax2.set_xlabel(\"Transport Mode\")\nax2.set_ylabel(\"\")\nax2.legend(title=\"Weekend Commuter\", loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)\n\n# Adjust plot layout\nplt.tight_layout(rect=[0, 0.05, 1, 1])\nplt.show()\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe bar plots displays the distribution of transport mode across car availability and weekend commute status.\nCar is the dominant mode of transport, especially among those who have access to a car (blue bar is much taller than orange). Public Transit is used by both those with and without car availability, but is more common among those without a car (orange section is relatively larger). Bicycle and Walking are rare overall but tend to occur more when no car is available.\nMost weekend commuters do not significantly change their transport mode: the proportions are largely dominated by the “No” (non-weekend commuter) group (orange). However, Public Transit has a noticeable number of weekend commuters (blue), indicating some weekend reliance on public transit. Walking and Bicycling show little weekend commuting activity.\nVisualize the relationship between transport_mode and continuous variables commute_distance and age.\n\n\nR Code\nPython Code\n\n\n\n# visualize boxplots and violin plots of commute_distance v.s. transport_mode\np3 &lt;- train_set %&gt;%\n    ggplot(aes(x = transport_mode, y = commute_distance, fill = transport_mode)) +\n    geom_violin(alpha = 0.6, trim = FALSE) +\n    geom_boxplot(alpha = 1, fill = NA) +\n    labs(x = \"Transport Mode\", y = \"Commute Distance (km)\", title =\n        \"Commute Distance by Transport Mode\") +\n    scale_fill_manual(values = cbPalette[3:6]) +\n    theme_bw() +\n    theme(legend.position = \"none\") +\n    guides(fill = guide_legend(title = \"Transport Mode\"))\n\np4 &lt;- train_set %&gt;%\n    ggplot(aes(x = transport_mode, y = age, fill = transport_mode)) +\n    geom_violin(alpha = 0.6, trim = FALSE) +\n    geom_boxplot(alpha = 1, fill = NA) +\n    labs(x = \"Transport Mode\", y = \"Age\", title =\n        \"Age by Transport Mode\") +\n    scale_fill_manual(values = cbPalette[3:6]) +\n    theme_bw() +\n    theme(legend.position = \"none\") +\n    guides(fill = guide_legend(title = \"Transport Mode\"))\n\ngrid.arrange(p3, p4, ncol = 2, top = grid::textGrob(\"Box Plots and Violin Plots\", gp = grid::gpar(fontsize = 16, fontface = \"bold\")))\n\n\nsns.set_theme(style=\"whitegrid\")\n\n# Set up side-by-side subplots with shared layout\nfig = plt.figure(figsize=(8, 7))\ngs = GridSpec(1, 2, figure=fig)\nfig.suptitle(\"Box Plots and Violin Plots\", fontsize=16, fontweight='bold')\n\n# Plot Commute Distance vs Transport Mode\nax1 = fig.add_subplot(gs[0, 0])\nsns.violinplot(\n    data=train_set,\n    x='transport_mode',\n    y='commute_distance',\n    palette=cbPalette[2:6],\n    ax=ax1,\n    alpha=0.6,\n    inner=None,\n    order=order,\n    cut=3\n)\nsns.boxplot(\n    data=train_set,\n    x='transport_mode',\n    y='commute_distance',\n    showcaps=True,\n    boxprops={'facecolor': 'none', 'edgecolor': 'black'},\n    whiskerprops={'color': 'black'},\n    flierprops={'markerfacecolor': 'black', 'markersize': 3},\n    ax=ax1,\n    order=order\n)\nax1.set_title(\"Commute Distance by Transport Mode\")\nax1.set_xlabel(\"Transport Mode\")\nax1.set_ylabel(\"Commute Distance (km)\")\nax1.get_legend()\n\n# Plot Age vs Transport Mode\nax2 = fig.add_subplot(gs[0, 1])\nsns.violinplot(\n    data=train_set,\n    x='transport_mode',\n    y='age',\n    palette=cbPalette[2:6],\n    ax=ax2,\n    alpha=0.6,\n    inner=None,\n    order=order,\n    cut=3\n)\nsns.boxplot(\n    data=train_set,\n    x='transport_mode',\n    y='age',\n    showcaps=True,\n    boxprops={'facecolor': 'none', 'edgecolor': 'black'},\n    whiskerprops={'color': 'black'},\n    flierprops={'markerfacecolor': 'black', 'markersize': 3},\n    ax=ax2,\n    order=order\n)\nax2.set_title(\"Age by Transport Mode\")\nax2.set_xlabel(\"Transport Mode\")\nax2.set_ylabel(\"Age\")\nax2.get_legend()\n\nplt.tight_layout(rect=[0, 0.05, 1, 0.95])\nplt.show()\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBox and violin plots present that how commute distance and age vary across different transport modes.\nPublic transit users tend to have the longest commutes on average, with a wide distribution and a median around 17–18 km. Car users also have moderately long commutes, with a median around 10-11 km and a relatively broad spread. Bicycle and Walking are associated with shorter commute distances (medians &lt; 5 km), and their distributions are tightly concentrated near the lower end. All modes show some long-distance commuters (long tails), especially public transit and car.\nCar and public transit users have similar age distributions, with median ages in the mid-30s to early 40s. Bicycle users skew slightly younger, but with a long tail of older users. Walking has a much wider spread in age, with a high median (around 60) and some very young walkers as well. The variability in age is greatest for walking, suggesting diverse usage across age groups.\nYou can also visualize the relationship across other explanatory variables or predictors in various types of figures for practice.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multinomial Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/14-multinomial-logistic.html#data-modelling",
    "href": "book/14-multinomial-logistic.html#data-modelling",
    "title": "14  Multinomial Logistic Regression",
    "section": "\n14.5 Data Modelling",
    "text": "14.5 Data Modelling\nA Multinomial Logistic Regression model is a suitable approach to our statistical inquiries given that transport_mode is categorical and nominal (our response of interest) subject to the numerical regressors commute_distance and age and binary regressors has_car_available and weekend_commuter. Moreover, its corresponding regression estimates will allow us to measure variable association.\nThis regression approach assumes a Multinomial distribution where \\(p_{i,1},p_{i,2},\\dots,p_{i,m}\\) are the probabilities that will belong to categories \\(1,2,\\dots,m\\) respectively; i.e., \\[P(Y_i=1)=p_{i,1}, P(Y_i=2)=p_{i,2}, \\dots, P(Y_i=m)=p_{i,m}\\] where \\[\n\\sum_{j = 1}^m p_{i,j} = p_{i,1} + p_{i,2} + \\dots + p_{i,m} = 1.\n\\]\nA particular highlight is that the Binomial distriution is a special Multinomial distribution when \\(m=2\\).\nThe Multinomial Logistic regression also models the logarithm of the odds. However, only one logarithm of the odds (or logit) will not be enough anymore. Recall we can capture the odds between two categories with a single logit function. What about adding some other ones?\nHere is what we can do:\n\nPick one of the categories to be the baseline. For example, the category “\\(1\\)”.\nFor each of the other categories, we model the logarithm of the odds to the baseline category.\n\nNow, what is the math for the general case with \\(m\\) response categories and \\(K\\) regressors? For the \\(i\\)th observation, we end up with a system of \\(m - 1\\) linK functions in the Multinomial Logistic regression model as follows:\n\\[\n\\begin{gather*} \\label{eq:multinomial-model}\n\\eta_i^{(2,1)} = \\log\\left[\\frac{P(Y_i = 2\\mid X_{i,1}, \\ldots, X_{i,K})}{P(Y_i = 1 \\mid X_{i,1}, \\ldots, X_{i,K})}\\right] = \\beta_0^{(2,1)} + \\beta_1^{(2,1)} X_{i, 1} + \\beta_2^{(2,1)} X_{i, 2} + \\ldots + \\beta_K^{(2,1)} X_{i, K} \\\\\n\\eta_i^{(3,1)} = \\log\\left[\\frac{P(Y_i = 3\\mid X_{i,1}, \\ldots, X_{i,K})}{P(Y_i = 1 \\mid X_{i,1}, \\ldots, X_{i,K})}\\right] = \\beta_0^{(3,1)} + \\beta_1^{(3,1)} X_{i, 1} + \\beta_2^{(3,1)} X_{i, 2} + \\ldots + \\beta_K^{(3,1)} X_{i, K} \\\\\n\\vdots \\\\\n\\eta_i^{(m,1)} = \\log\\left[\\frac{P(Y_i = m\\mid X_{i,1}, \\ldots, X_{i,K})}{P(Y_i = 1 \\mid X_{i,1}, \\ldots, X_{i,K})}\\right] = \\beta_0^{(m,1)} + \\beta_1^{(m,1)} X_{i, 1} + \\beta_2^{(m,1)} X_{i, 2} + \\ldots + \\beta_K^{(m,1)} X_{i, K}.\n\\end{gather*}\n\\]\nNote that the superscript \\((j, 1)\\) in (eq:multinomial-model?) indicates that the equation is on level \\(j\\) (for \\(j = 2, \\dots, m\\)) with respect to level \\(1\\). Furthermore, the regression coefficients are different for each link function.\nEach of the logit-linear functions in (eq:multinomial-model?) writes the log of odds between each category \\(j=2,\\dots,m\\) and the baseline category \\(j=1\\) as a linear combination of the regressors. To compare between the categories \\(j=2,\\dots,m\\), we can simply deduct one equation be another, e.g., \\[\n\\begin{gather*} \\label{eq:multinomial-deduct}\n\\eta_i^{(2,1)} - \\eta_i^{(3,1)} = \\log\\left[\\frac{P(Y_i = 2\\mid X_{i,1}, \\ldots, X_{i,K})}{P(Y_i = 1 \\mid X_{i,1}, \\ldots, X_{i,K})}\\right] - \\log\\left[\\frac{P(Y_i = 3\\mid X_{i,1}, \\ldots, X_{i,K})}{P(Y_i = 1 \\mid X_{i,1}, \\ldots, X_{i,K})}\\right] \\\\\n= \\log\\left[\\frac{P(Y_i = 3\\mid X_{i,1}, \\ldots, X_{i,K})}{P(Y_i = 2 \\mid X_{i,1}, \\ldots, X_{i,K})}\\right]\n= (\\beta_0^{(2,1)}-\\beta_0^{(3,1)}) + (\\beta_1^{(2,1)}-\\beta_1^{(3,1)}) X_{i, 1} + (\\beta_2^{(2,1)}-\\beta_2^{(3,1)}) X_{i, 2} + \\ldots\n\\end{gather*}\n\\]\nWith some algebraic manipulation, we can show that the probabilities \\(p_{i,1}, p_{i,2}, \\dots, p_{i,m}\\) of \\(Y_i\\) belonging to categories \\(1, 2, \\dots, m\\) are:\n\\[\n\\begin{gather*} \\label{eq:prob-multinomial}\np_{i,1} = P(Y_i = 1 \\mid X_{i,1}, \\ldots, X_{i,K}) = \\frac{1}{1 + \\sum_{j = 2}^m \\exp \\big( \\eta_i^{(j,1)} \\big)} \\\\\np_{i,2} = P(Y_i = 2 \\mid X_{i,1}, \\ldots, X_{i,K}) = \\frac{\\exp \\big( \\eta_i^{(2,1)} \\big)}{1 + \\sum_{j = 2}^m \\exp \\big( \\eta_i^{(j,1)} \\big)} \\\\\n\\vdots \\\\\np_{i,m} = P(Y_i = m \\mid X_{i,1}, \\ldots, X_{i,K}) = \\frac{\\exp \\big( \\eta_i^{(m,1)} \\big)}{1 + \\sum_{j = 2}^m \\exp \\big( \\eta_i^{(j,1)} \\big)}.\n\\end{gather*}\n\\]\nNote: The equations above are Softmax functions. The multinomial logistic regression is theoretically equivalent to a one-layer neural network with Softmax activation function, where we input \\(X\\), output probability \\(p_{i,j}\\), and minimize the cross-entropy loss function to train the neural network to learn parameters \\(\\beta\\).\nIf we sum all \\(m\\) probabilities in (eq:prob-multinomial?), the sum will be equal to \\(1\\) for the \\(i\\)th observation. This is particularly important when we want to use this model for making predictions in classification matters.\nGoting back to our data example, let us set the Multinomial logistic regression model with transport_mode as the response with four classes: Car, Public_Transit, Bicycle, and Walking (denoted as Car, Public, Bike, Walk respectively) subject to the continuous regressors commute_distance and age, denoted as \\(X_{\\texttt{distance}}\\) and \\(X_{\\texttt{age}}\\) respectively, and binary regressors has_car_available and weekend_commuter, with the dummy variables of class Yes denoted as \\(Z_{\\texttt{car}}\\) and \\(Z_{\\texttt{weekend}}\\) respectively. Use Car class as the baseline model.\n\\[\n\\begin{align*}\n\\eta_i^{(\\texttt{Public},\\texttt{Car})} &= \\log\\left[\\frac{P(Y_i = \\texttt{Public} \\mid X_{i, \\texttt{distance}}, X_{i, \\texttt{age}}, Z_{i, \\texttt{car}}, Z_{i,\\texttt{weekend}})}{P(Y_i = \\texttt{Car} \\mid X_{i, \\texttt{distance}}, X_{i, \\texttt{age}}, Z_{i, \\texttt{car}}, Z_{i,\\texttt{weekend}})}\\right] \\\\\n&= \\beta_0^{(\\texttt{Public},\\texttt{Car})} + \\beta_1^{(\\texttt{Public},\\texttt{Car})} X_{i, \\texttt{distance}} + \\beta_2^{(\\texttt{Public},\\texttt{Car})} X_{i, \\texttt{age}} + \\beta_3^{(\\texttt{Public},\\texttt{Car})} Z_{i, \\texttt{car}} + \\beta_4^{(\\texttt{Public},\\texttt{Car})} Z_{i,\\texttt{weekend}}, \\\\\n\\eta_i^{(\\texttt{Bike},\\texttt{Car})} &= \\log\\left[\\frac{P(Y_i = \\texttt{Bike} \\mid X_{i, \\texttt{distance}}, X_{i, \\texttt{age}}, Z_{i, \\texttt{car}}, Z_{i,\\texttt{weekend}})}{P(Y_i = \\texttt{Car} \\mid X_{i, \\texttt{distance}}, X_{i, \\texttt{age}}, Z_{i, \\texttt{car}}, Z_{i,\\texttt{weekend}})}\\right] \\\\\n&= \\beta_0^{(\\texttt{Bike},\\texttt{Car})} + \\beta_1^{(\\texttt{Bike},\\texttt{Car})} X_{i, \\texttt{distance}} + \\beta_2^{(\\texttt{Bike},\\texttt{Car})} X_{i, \\texttt{age}} + \\beta_3^{(\\texttt{Bike},\\texttt{Car})} Z_{i, \\texttt{car}} + \\beta_4^{(\\texttt{Bike},\\texttt{Car})} Z_{i,\\texttt{weekend}}, \\\\\n\\eta_i^{(\\texttt{Walk},\\texttt{Car})} &= \\log\\left[\\frac{P(Y_i = \\texttt{Walk} \\mid X_{i, \\texttt{distance}}, X_{i, \\texttt{age}}, Z_{i, \\texttt{car}}, Z_{i,\\texttt{weekend}})}{P(Y_i = \\texttt{Car} \\mid X_{i, \\texttt{distance}}, X_{i, \\texttt{age}}, Z_{i, \\texttt{car}}, Z_{i,\\texttt{weekend}})}\\right] \\\\\n&= \\beta_0^{(\\texttt{Walk},\\texttt{Car})} + \\beta_1^{(\\texttt{Walk},\\texttt{Car})} X_{i, \\texttt{distance}} + \\beta_2^{(\\texttt{Walk},\\texttt{Car})} X_{i, \\texttt{age}} + \\beta_3^{(\\texttt{Walk},\\texttt{Car})} Z_{i, \\texttt{car}} + \\beta_4^{(\\texttt{Walk},\\texttt{Car})} Z_{i,\\texttt{weekend}} .\n\\end{align*}\n\\]\nIn a Multinomial Logistic regression model, each link function has its own intercept and regression coefficients.\nTaking exponential on both sides of the model equations gives the ratio of probability of each category Public_Transit, Bicycle, and Walking over the probability of the baseline level Car:\n\\[\n\\begin{align*}\n&\\frac{P(Y_i = \\texttt{Public} \\mid X_{i, \\texttt{distance}}, X_{i, \\texttt{age}}, Z_{i, \\texttt{car}}, Z_{i,\\texttt{weekend}})}{P(Y_i = \\texttt{Car} \\mid X_{i, \\texttt{distance}}, X_{i, \\texttt{age}}, Z_{i, \\texttt{car}}, Z_{i,\\texttt{weekend}})} = \\exp\\left[\\eta_i^{(\\texttt{Public},\\texttt{Car})} \\right]\\\\\n&= \\exp\\left[\\beta_0^{(\\texttt{Public},\\texttt{Car})}\\right] \\exp\\left[\\beta_1^{(\\texttt{Public},\\texttt{Car})} X_{i,\\texttt{distance}}\\right] \\exp\\left[\\beta_2^{(\\texttt{Public},\\texttt{Car})} X_{i,\\texttt{age}}\\right] \\exp\\left[ \\beta_3^{(\\texttt{Public},\\texttt{Car})} Z_{i, \\texttt{car}}\\right] \\exp\\left[ \\beta_4^{(\\texttt{Public},\\texttt{Car})} Z_{i,\\texttt{weekend}}\\right]\\\\\n&\\frac{P(Y_i = \\texttt{Bike} \\mid X_{i, \\texttt{distance}}, X_{i, \\texttt{age}}, Z_{i, \\texttt{car}}, Z_{i,\\texttt{weekend}})}{P(Y_i = \\texttt{Car} \\mid X_{i, \\texttt{distance}}, X_{i, \\texttt{age}}, Z_{i, \\texttt{car}}, Z_{i,\\texttt{weekend}})} = \\exp\\left[\\eta_i^{(\\texttt{Bike},\\texttt{Car})} \\right]\\\\\n&= \\exp\\left[\\beta_0^{(\\texttt{Bike},\\texttt{Car})}\\right] \\exp\\left[\\beta_1^{(\\texttt{Bike},\\texttt{Car})} X_{i,\\texttt{distance}}\\right] \\exp\\left[\\beta_2^{(\\texttt{Bike},\\texttt{Car})} X_{i,\\texttt{age}}\\right] \\exp\\left[ \\beta_3^{(\\texttt{Bike},\\texttt{Car})} Z_{i, \\texttt{car}}\\right] \\exp\\left[ \\beta_4^{(\\texttt{Bike},\\texttt{Car})} Z_{i,\\texttt{weekend}}\\right]\\\\\n&\\frac{P(Y_i = \\texttt{Walk} \\mid X_{i, \\texttt{distance}}, X_{i, \\texttt{age}}, Z_{i, \\texttt{car}}, Z_{i,\\texttt{weekend}})}{P(Y_i = \\texttt{Car} \\mid X_{i, \\texttt{distance}}, X_{i, \\texttt{age}}, Z_{i, \\texttt{car}}, Z_{i,\\texttt{weekend}})} = \\exp\\left[\\eta_i^{(\\texttt{Walk},\\texttt{Car})} \\right]\\\\\n&= \\exp\\left[\\beta_0^{(\\texttt{Walk},\\texttt{Car})}\\right] \\exp\\left[\\beta_1^{(\\texttt{Walk},\\texttt{Car})} X_{i,\\texttt{distance}}\\right] \\exp\\left[\\beta_2^{(\\texttt{Walk},\\texttt{Car})} X_{i,\\texttt{age}}\\right] \\exp\\left[ \\beta_3^{(\\texttt{Walk},\\texttt{Car})} Z_{i, \\texttt{car}}\\right] \\exp\\left[ \\beta_4^{(\\texttt{Walk},\\texttt{Car})} Z_{i,\\texttt{weekend}}\\right]\n\\end{align*}\n\\]\nFinally, the probability of \\(Y_i\\) belonging to categories Car, Public_Transit, Bicycle, and Walking are:\n\\[\n\\begin{align*}\nP(Y_i = \\texttt{Car} \\mid X_{i, \\texttt{distance}}, X_{i, \\texttt{age}}, Z_{i, \\texttt{car}}, Z_{i,\\texttt{weekend}}) &= \\frac{1}{1+\\exp\\left[\\eta_i^{(\\texttt{Public},\\texttt{Car})} \\right]+\\exp\\left[\\eta_i^{(\\texttt{Bike},\\texttt{Car})}\\right]+\\exp\\left[\\eta_i^{(\\texttt{Walk},\\texttt{Car})} \\right]} \\\\\nP(Y_i = \\texttt{Public} \\mid X_{i, \\texttt{distance}}, X_{i, \\texttt{age}}, Z_{i, \\texttt{car}}, Z_{i,\\texttt{weekend}}) &= \\frac{\\exp\\left[\\eta_i^{(\\texttt{Public},\\texttt{Car})} \\right]}{1+\\exp\\left[\\eta_i^{(\\texttt{Public},\\texttt{Car})} \\right]+\\exp\\left[\\eta_i^{(\\texttt{Bike},\\texttt{Car})}\\right]+\\exp\\left[\\eta_i^{(\\texttt{Walk},\\texttt{Car})} \\right]} \\\\\nP(Y_i = \\texttt{Bike} \\mid X_{i, \\texttt{distance}}, X_{i, \\texttt{age}}, Z_{i, \\texttt{car}}, Z_{i,\\texttt{weekend}}) &= \\frac{\\exp\\left[\\eta_i^{(\\texttt{Bike},\\texttt{Car})}\\right]}{1+\\exp\\left[\\eta_i^{(\\texttt{Public},\\texttt{Car})} \\right]+\\exp\\left[\\eta_i^{(\\texttt{Bike},\\texttt{Car})}\\right]+\\exp\\left[\\eta_i^{(\\texttt{Walk},\\texttt{Car})} \\right]}\\\\\nP(Y_i = \\texttt{Walk} \\mid X_{i, \\texttt{distance}}, X_{i, \\texttt{age}}, Z_{i, \\texttt{car}}, Z_{i,\\texttt{weekend}}) &= \\frac{\\exp\\left[\\eta_i^{(\\texttt{Walk},\\texttt{Car})} \\right]}{1+\\exp\\left[\\eta_i^{(\\texttt{Public},\\texttt{Car})} \\right]+\\exp\\left[\\eta_i^{(\\texttt{Bike},\\texttt{Car})}\\right]+\\exp\\left[\\eta_i^{(\\texttt{Walk},\\texttt{Car})} \\right]}\n\\end{align*}\n\\]\nNote: Changing the baseline category, e.g., switching from Car to Public won’t change the estimated probability for each category for each individual.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multinomial Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/14-multinomial-logistic.html#estimation",
    "href": "book/14-multinomial-logistic.html#estimation",
    "title": "14  Multinomial Logistic Regression",
    "section": "\n14.6 Estimation",
    "text": "14.6 Estimation\nWe summarise the key estimation steps in math below. We consider one observation at each population \\(i\\). More estimation details and generalization from one to \\(n_i\\) observations for population \\(i\\) can be found in Section 2.2 Parameter Estimation of Czepiel, S. A. (2002)..\nLet’s follow the notations in (eq:multinomial-model?) and (eq:prob-multinomial?). Define \\(y_{i,j} = 1\\) if \\(i\\)th individual is observed as category \\(j\\) and \\(y_{i,j} = 0\\) otherwise. Define the kernel of the log likelihood function for multinomial logistic regression models is \\[\n\\mathcal{L}(\\beta \\mid y) \\simeq \\prod_{i=1}^n \\prod_{j=1}^m p_{i,j}^{y_{i,j}}.\n\\]\nReplacing \\(y_{i,1}\\) by \\(1-\\sum_{j=2}^{m} y_{i,j}\\) and rearranging the function gives \\[\n\\mathcal{L}(\\beta \\mid y) \\simeq \\prod_{i=1}^n \\prod_{j=2}^{m} \\left(\\frac{p_{i,j}}{p_{i,1}} \\right)^{y_{i,j}} p_{i,1}.\n\\] Substituting the probabilities by the function of linear combination of regressors as in (eq:prob-multinomial?) and taking the logarithm of the likelihood function yields the log likelihood function, which is the loss function to minimize: \\[\n\\ell(\\beta) = \\sum_{i=1}^n \\sum_{j=2}^m \\left(y_{i,j}\\sum_{k=0}^K X_{i,k} \\beta_k^{(j,1)}\\right) - \\log\\left(1+\\sum_{j=2}^m e^{\\sum_{k=0}^K X_{i,k}\\beta_k^{(j,1)}}\\right),\n\\] where \\(X_{i,0}=1\\) corresponds to the intercept in the linear model. We solve the minimization problem by the Newton-Raphson method as in Section 10.6 in the Poisson Regression Section.\n\n14.6.1 The Newton–Raphson Method\nTo carry out maximum-likelihood estimation, we iterate Newton–Raphson in a compact matrix form (multinomial iteratively reweighted least squares (IRLS)). Let \\(q=(K+1)(m-1)\\) be the total number of parameters (an intercept and \\(K\\) slopes for each of the \\(m-1\\) non-baseline classes). Then denote\n\n\n\\(\\beta \\in \\mathbb{R}^q\\) (stack of all \\(\\beta_k^{(j,1)}\\)),\n\n\\(S(\\beta) \\in \\mathbb{R}^q\\) (stacked Score function components),\n\n\\(H(\\beta) \\in \\mathbb{R}^{q\\times q}\\) (stacked second derivatives).\n\nFor model (category) \\(j\\) and regressor \\(k\\), the component of the score function (first-order partial detivative) \\(S(\\beta)\\) is \\[\n\\frac{\\partial \\ell(\\beta)}{\\partial \\beta_k^{(j,1)}} = \\sum_i^n y_{i,j}X_{i,k} - p_{i,j}X_{i,k}.\n\\] For model \\(j\\) and regressor \\(k\\) on the row, the component for category \\(j'\\) and regressor \\(k'\\) on the column of the Hessian matrix (second-order partial derivative) \\(H(\\beta)\\) is \\[\n\\frac{\\partial \\ell^2(\\beta)}{\\partial \\beta_k^{(j,1)} \\partial \\beta_{k'}^{(j',1)}} =\n\\begin{cases}\n-\\sum_{i=1^n} X_{i,k}p_{i,j}(1-p_{i,j})X_{i,k'} & \\text{ if } j'=j, \\\\\n\\sum_{i=1}^n X_{i,k}p_{i,j}p_{i,j'}X_{i,k'} & \\text{ if } j' \\neq j,\n\\end{cases}\n\\] where \\(j,j'=1,\\dots,m\\) and \\(k,k'=0,\\dots,K\\).\nThe Newton-Raphson/IRLS iteration procedure is as follows.\n\nChoose baseline class; initialize \\(\\beta^{(0)}=\\mathbf{0}\\) (stacked over all \\((j,k)\\)).\nRepeat until convergence:\n\nCompute the log-odds \\(\\eta_i^{(j,1)}\\), probability \\(p_{i,j}\\) for all individual \\(i\\) and category \\(j\\) across all regressors \\(k=1,\\dots,K\\), and the log-likelihood function \\(\\ell(\\beta^{(t)})\\).\nForm the stacked score \\(S(\\beta^{(t)})\\) and Hessian functions \\(H(\\beta^{(t)})\\) using the expressions above.\nUpdate \\(\\beta^{(t+1)} \\leftarrow \\beta^{(t)}+\\alpha (H(\\beta^{(t)}))^{-1}S(\\beta^{(t)})\\), with step size \\(\\alpha\\in(0,1]\\).\nCheck convergence criteria.\n\n\n\n14.6.2 Fitting the Model\nNow, we fit the multinomial regression model using the nnet package in R or statsmodel package in Python. To handle the imbalanced groups, we resample the training sets to balance out the groups to eliminate the bias in modelling.\n\n\nR Code\nPython Code\n\n\n\n# Set the baseline model\ntrain_set$transport_mode &lt;- relevel(train_set$transport_mode, ref = \"Car\")\n\n# Fit multinomial logistic regression\nmultinom_fit &lt;- multinom(transport_mode ~ age + commute_distance + has_car_available + weekend_commuter, data = train_set)\n\n# Print the fitted model\nprint(summary(multinom_fit))\n\n\n# Factorize the response and set the baseline model\ntrain_set[\"transport_mode\"] = pd.Categorical(\n    train_set[\"transport_mode\"],\n    categories=[\"Car\", \"Bicycle\", \"Public_Transit\", \"Walking\"],\n    ordered=False\n)\n\n# Design matrix and response\ny = train_set[\"transport_mode\"]\nX = train_set[[\"age\", \"commute_distance\", \"has_car_available\", \"weekend_commuter\"]]\n\n# Add intercept\nX = sm.add_constant(X)\n\n# Convert categorical variables to numeric codes for statsmodels MNLogit\nX_numeric = X.copy()\nfor col in ['has_car_available', 'weekend_commuter']:\n    X_numeric[col] = X_numeric[col].cat.codes\n\n# Fit multinomial logistic regression\nmodel = sm.MNLogit(y, X_numeric)\nmultinom_fit = model.fit()\n\n# Print the fitted model\nprint(multinom_fit.summary())\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n# weights:  24 (15 variable)\ninitial  value 1247.664925 \niter  10 value 694.856621\niter  20 value 604.709852\niter  30 value 603.307254\niter  40 value 603.302790\nfinal  value 603.302497 \nconverged\n\n\nCall:\nmultinom(formula = transport_mode ~ age + commute_distance + \n    has_car_available + weekend_commuter, data = train_set)\n\nCoefficients:\n               (Intercept)           age commute_distance has_car_availableYes\nPublic_Transit  -0.9516669 -0.0001486655        0.1152912           -1.5582633\nBicycle         -3.4108159  0.0237368333       -0.1268209            0.4342351\nWalking         -4.7447162  0.0626851593       -0.2019441           -2.3925813\n               weekend_commuterYes\nPublic_Transit           0.1173869\nBicycle                 -0.7710694\nWalking                  1.0924670\n\nStd. Errors:\n               (Intercept)         age commute_distance has_car_availableYes\nPublic_Transit   0.3027178 0.005650876       0.01190376            0.1692069\nBicycle          0.9621575 0.016624316       0.04314417            0.6361547\nWalking          1.7681241 0.035162949       0.10861331            0.9100075\n               weekend_commuterYes\nPublic_Transit           0.2073222\nBicycle                  0.7566138\nWalking                  0.9078276\n\nResidual Deviance: 1206.605 \nAIC: 1236.605 \n\n\n\n\n\n\n\n\nIn multinomial logistic regression, we model the probability of each outcome category given predictors. The key assumption is that we have enough data for each category to estimate the model parameters reliably. If one category is much more frequent than the others (i.e., unbalanced outcomes), the model may:\n\nBias coefficients toward the majority class because the likelihood function is dominated by majority observations.\nUnderestimate uncertainty for rare categories (small sample size → large variance).\nYield poor predictive performance for minority classes (the model tends to predict the majority class).\nIn extreme cases, parameters for rare categories may not converge or may give infinite estimates (complete separation).\n\nThe intuition behind is if 95% are “car”, then the model has little incentive to adjust parameters for “walk” and “bus,” so estimates for those categories can be unstable. There are two common approaches:\n\nDownsampling: Reduce the majority class size so that each class is roughly balanced. This risks throwing away data, but helps balance influence.\nUpsampling: Duplicate (or synthetically generate) minority class cases until classes are balanced. This risks overfitting, but improves minority estimation.\n\nCheck the class counts of the outcome below.\n\n\nR Code\nPython Code\n\n\n\ncat(\"Class counts before balancing:\", \"\\n\")\nprint(table(train_set$transport_mode))\n\n\nprint(\"Class counts before balancing:\", \"\\n\")\nprint(y.value_counts())\n\n\n\n\n\nR Code\nPython Code\n\n\n\n\n\nClass counts before balancing: \n\n\n\n           Car Public_Transit        Bicycle        Walking \n           532            340             22              6 \n\n\n\n\n\n\n\n\nThe fitted model on upsampled training set is below.\n\n\nR Code\nPython Code\n\n\n\n# Upsample the outcome classes\nset.seed(352) # set a seed for sampling\ntrain_set_balanced &lt;- upSample(x = train_set[, c(\"age\",\"commute_distance\")],\n                  y = train_set$transport_mode,\n                  yname = \"transport_mode\")\n# Fit the multinomial logit regression\nmultinom_fit_balanced &lt;- multinom(transport_mode ~ age + commute_distance + has_car_available + weekend_commuter, data = train_set_balanced)\nprint(summary(multinom_fit_balanced))\n\n\n# Upsample the outcome classes\nbalanced_df = pd.DataFrame()\nfor label in train_set[\"transport_mode\"].unique():\n    subset = train_set[train_set[\"transport_mode\"] == label]\n    balanced_subset = resample(\n        subset,\n        replace=True,  # sample with replacement\n        n_samples=train_set[\"transport_mode\"].value_counts().max(),  # match majority class size\n        random_state=352\n    )\n    balanced_df = pd.concat([balanced_df, balanced_subset])\nX_bal = balanced_df[[\"age\", \"commute_distance\", \"has_car_available\", \"weekend_commuter\"]]\ny_bal = balanced_df[\"transport_mode\"]\nX_bal = sm.add_constant(X_bal)\n\n# Convert categorical variables to numeric codes for statsmodels MNLogit\nX_bal_num = X_bal.copy()\nfor col in ['has_car_available', 'weekend_commuter']:\n    X_bal_num[col] = X_bal_num[col].cat.codes\n\n# Fit the multinomial logit regression\nmodel_balanced = sm.MNLogit(y_bal, X_bal_num)\nmultinom_fit_balanced = model_balanced.fit()\nprint(multinom_fit_balanced.summary())\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n# weights:  24 (15 variable)\ninitial  value 2950.034400 \niter  10 value 2208.856234\niter  20 value 2095.494574\nfinal  value 2095.274369 \nconverged\n\n\nCall:\nmultinom(formula = transport_mode ~ age + commute_distance + \n    has_car_available + weekend_commuter, data = train_set_balanced)\n\nCoefficients:\n               (Intercept)          age commute_distance has_car_availableYes\nPublic_Transit  -0.2515393 -0.004325434       0.10636909           -1.4612980\nBicycle         -0.1474720  0.021796850      -0.09752302            0.1527654\nWalking         -0.0945438  0.082697643      -0.31825842           -3.7439929\n               weekend_commuterYes\nPublic_Transit          -0.1198442\nBicycle                 -0.6250485\nWalking                  2.0022249\n\nStd. Errors:\n               (Intercept)         age commute_distance has_car_availableYes\nPublic_Transit   0.2562676 0.004776676       0.01024380            0.1472358\nBicycle          0.2631115 0.004571458       0.01036838            0.1732867\nWalking          0.3017023 0.006462437       0.02121494            0.2163152\n               weekend_commuterYes\nPublic_Transit           0.1761011\nBicycle                  0.2012288\nWalking                  0.1966513\n\nResidual Deviance: 4190.549 \nAIC: 4220.549",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multinomial Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/14-multinomial-logistic.html#goodness-of-fit",
    "href": "book/14-multinomial-logistic.html#goodness-of-fit",
    "title": "14  Multinomial Logistic Regression",
    "section": "\n14.7 Goodness of Fit",
    "text": "14.7 Goodness of Fit\nLet us assess how well the fitted multinomial logistic regression explains and predicts the observed transport modes. We consider the residual deviance (similar as (goodness?) in the Classical Poisson Regression chapter) and the classification performance (confusion matrix, accuracy, per-class precision, recall, F1 score).\nLet’s visualize and summarize goodness of fit for the training set.\n\n\nR code\nPython Code\n\n\n\ny_pred &lt;- predict(multinom_fit, newdata = train_set)\nconf_matrix &lt;- confusionMatrix(factor(y_pred, levels = levels(train_set$transport_mode)),\n                              factor(train_set$transport_mode, levels = levels(train_set$transport_mode)))\n\ncm_table &lt;- as.table(conf_matrix$table)\ncm_df &lt;- as.data.frame(cm_table)\ncolnames(cm_df) &lt;- c(\"Predicted\", \"Actual\", \"Freq\")\n\nggplot(cm_df, aes(x = Actual, y = Predicted, fill = Freq)) +\n    geom_tile(color = \"white\") +\n    scale_fill_gradient(low = \"white\", high = \"#0072B2\") +\n    geom_text(aes(label = Freq), color = \"black\", size = 4) +\n    labs(title = \"Confusion Matrix Heatmap\", x = \"Actual\", y = \"Predicted\") +\n    theme_bw()\n\n\ny_pred = multinom_fit.predict(X_numeric).idxmax(axis=1)\n\n# Compute confusion matrix\ncm = confusion_matrix(y.cat.codes, y_pred)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=y.cat.categories)\ndisp.plot(cmap='Blues')\nplt.title(\"Confusion Matrix Heatmap\")\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\n\nplt.show()\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou’re encouraged to repeat these diagnostics on the balanced (upsampled) model and on the held‑out test set to compare whether balancing improves minority class recall without overly harming overall calibration.\nLet’s compute the metrics.\n\n\nR Code\nPython Code\n\n\n\n# Classification predictions & confusion matrix\ny_pred &lt;- predict(multinom_fit, newdata = train_set)\nconf_matrix &lt;- confusionMatrix(factor(y_pred, levels = levels(train_set$transport_mode)),\n                               factor(train_set$transport_mode, levels = levels(train_set$transport_mode)))\noverall_accuracy &lt;- conf_matrix$overall['Accuracy']\nmacro_recall &lt;- mean(conf_matrix$byClass[,\"Recall\"], na.rm = TRUE)\nmacro_f1 &lt;- mean(conf_matrix$byClass[,\"F1\"], na.rm = TRUE)\ncat(\"Overall Accuracy:\", round(overall_accuracy, 3), \"\\n\")\ncat(\"Macro Recall:\", round(macro_recall, 3), \"\\n\")\ncat(\"Macro F1:\", round(macro_f1, 3), \"\\n\")\n\n\n# Compute classification predictions & confusion matrix\nprobs = multinom_fit.predict(X_numeric)  # DataFrame of class probabilities\ny_pred = probs.idxmax(axis=1)\n\n# Compute the metrics\nacc = accuracy_score(y.cat.codes, y_pred)\nrecall = recall_score(y.cat.codes, y_pred, average='macro')\nf1 = f1_score(y.cat.codes, y_pred, average='macro')\nprint(f\"Overall Accuracy: {acc:.3f}\")\nprint(f\"Macro Recall: {recall:.3f}\")\nprint(f\"Macro F1-score: {f1:.3f}\")\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\nOverall Accuracy: 0.696 \n\n\nMacro Recall: 0.344 \n\n\nMacro F1: 0.682 \n\n\n\n\n\n\n\n\nAbout 69.6% of all individual transportation modes were classified correctly. On average, the model correctly identifies 34.4% of instances for each transport mode. Recall is relatively low, which suggests the model is biased to the majority classes, even though accuracy is decent. Macro F1 score is 0.682, meaning the model has a moderate balance of precision and recall on average per class. Macro F1 is much higher than macro recall — this can happen if precision is high for the majority class, but recall for minority classes is low. In summary, the model performs okay overall but is biased toward the dominant class(es). It often fails to detect less frequent transport modes, which is why macro recall is low. Accuracy alone would overestimate the model’s effectiveness across all classes. You are encouraged to explore the performance of the fitted model on the balanced sets.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multinomial Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/14-multinomial-logistic.html#inference",
    "href": "book/14-multinomial-logistic.html#inference",
    "title": "14  Multinomial Logistic Regression",
    "section": "\n14.8 Inference",
    "text": "14.8 Inference\nAfter we finish estimating the coefficients, we look at how uncertain they are and whether they differ from zero. We get a variance estimate for each coefficient from the observed information matrix (the negative of the Hessian at the solution). Taking square roots of those variances gives standard errors. Similar as in Poisson regression, each coefficient divided by its standard error gives a (asymptotically normal) Wald Z statistic to test if the true effect is zero. Using the same standard errors we build confidence intervals: estimate \\(\\pm\\) (critical value \\(\\times\\) standard error). To make results easier to interpret, we often exponentiate a coefficient (or its confidence limits) to get an odds ratio—how the odds of choosing a category change for a one‑unit increase in the predictor, holding other variables fixed.\nWe can determine whether a regressor is statistically associated with the logarithm of the odds through hypothesis testing for the parameters \\(\\beta^{(u,v)}_j\\) by link function. Define the Wald statistic \\(z^{(u,v)}_j\\) as: \\[\nz_j^{(u, v)} = \\frac{\\hat{\\beta}_j^{(u, v)}}{\\mbox{se}\\left(\\hat{\\beta}_j^{(u, v)}\\right)}\n\\] and we test the hypotheses: \\[\n\\begin{gather*}\nH_0: \\beta_j^{(u, v)} = 0\\\\\nH_a: \\beta_j^{(u, v)} \\neq 0.\n\\end{gather*}\n\\] Provided the sample size \\(n\\) is large enough, \\(z_j\\) has an approximately Standard Normal distribution under \\(H_0\\).\nThe corresponding \\(p\\)-values for each \\(\\beta^{(u,v)}_j\\) can be computed. The smaller the \\(p\\)-value, the stronger the evidence against the null hypothesis \\(H_0\\). As in the previous regression models, we would set a predetermined significance level (usually taken to be 0.05) to infer if the \\(p\\)-value is small enough. If the \\(p\\)-value is smaller than the predetermined level \\(\\alpha\\), then you could claim that there is evidence to reject the null hypothesis. Hence, \\(p\\)-values that are small enough indicate that the data provides evidence in favour of association (or causation in the case of an experimental study!) between the response variable and the \\(j\\)th regressor.\nGiven a specified level of confidence where \\(\\alpha\\) is the significance level, we can construct approximate \\((1-\\alpha)\\times 100\\%\\) confidence intervals for the corresponding true value of \\(\\beta^{(u,v)}_j\\): \\[\n\\begin{equation*}\n\\hat{\\beta}_j^{(u, v)} \\pm z_{\\alpha/2}\\mbox{se} \\left( \\hat{\\beta}_j^{(u, v)} \\right),\n\\end{equation*}\n\\] where \\(z_{\\alpha/2}\\) is the upper \\(\\alpha/2\\) quantile of the Standard Normal distribution.\nWe compute the 95% confidence intervals below and filter the signicant coefficients under the 5% significance level.\n\n\nR Code\nPython Code\n\n\n\nbroom::tidy(multinom_fit, conf.int = TRUE, exponentiate = TRUE) |&gt;\n  dplyr::mutate_if(is.numeric, round, 3) |&gt;\n  dplyr::filter(p.value &lt; 0.05) |&gt;\n  rename(outcome_class = y.level)\n\n\ndata_dict = {\n    \"estimate\": np.exp(multinom_fit.params),      # odds ratios\n    \"pvalue\": multinom_fit.pvalues,\n    \"std_err\": multinom_fit.bse,\n    \"statistic\": multinom_fit.tvalues\n}\n\n# Outcome class mapping\nclass_map = {0: \"Bicycle\", 1: \"Public_Transit\", 2: \"Walking\"}\n\n# Container for processed tidy tables\ntidy_dict = {}\n\nfor name, df in data_dict.items():\n    tidy = (\n        df.stack()               # long form\n        .swaplevel()             # variable first, outcome second\n        .reset_index()           # convert MultiIndex to columns\n        .rename(columns={\"level_0\": \"outcome_class\", \n                         \"level_1\": \"variable\", \n                         0: name})\n    )\n    tidy[\"outcome_class\"] = tidy[\"outcome_class\"].replace(class_map)\n    tidy = tidy.sort_values(by=\"outcome_class\").reset_index(drop=True)\n    \n    tidy_dict[name] = tidy\n\n# CIs\nconf = multinom_fit.conf_int()\nor_conf = np.exp(conf)\nci_lower_stacked = or_conf[\"lower\"].reset_index()\nci_lower_stacked.columns = ['outcome_class', 'term', 'ci_lower']\nci_upper_stacked = or_conf[\"upper\"].reset_index()\nci_upper_stacked.columns = ['outcome_class', 'term', 'ci_upper']\n\n# Merge the result tables\nref_cols = [\"outcome_class\", \"term\"]\nresults = tidy_dict['estimate'] \\\n    .merge(tidy_dict['pvalue'], on=ref_cols) \\\n    .merge(tidy_dict['std_err'], on=ref_cols) \\\n    .merge(tidy_dict['statistic'], on=ref_cols) \\\n    .merge(ci_lower_stacked, on=ref_cols) \\\n    .merge(ci_upper_stacked, on=ref_cols)\n\n# Filter the rows with significant p-values\nresults_sig = results[results[\"pvalue\"] &lt; 0.05].round(3)\nprint(results_sig)\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n# A tibble: 7 × 8\n  outcome_class  term    estimate std.error statistic p.value conf.low conf.high\n  &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 Public_Transit (Inter…    0.386     0.303     -3.14   0.002    0.213     0.699\n2 Public_Transit commut…    1.12      0.012      9.68   0        1.10      1.15 \n3 Public_Transit has_ca…    0.211     0.169     -9.21   0        0.151     0.293\n4 Bicycle        (Inter…    0.033     0.962     -3.54   0        0.005     0.218\n5 Bicycle        commut…    0.881     0.043     -2.94   0.003    0.809     0.959\n6 Walking        (Inter…    0.009     1.77      -2.68   0.007    0         0.278\n7 Walking        has_ca…    0.091     0.91      -2.63   0.009    0.015     0.544\n\n\n\n\n\n\n\n\nWhether people choose driving or taking public transits varys significantly on average. The commute distance and the availability of cars both have significant effects on the choices of transportation mode.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multinomial Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/14-multinomial-logistic.html#results",
    "href": "book/14-multinomial-logistic.html#results",
    "title": "14  Multinomial Logistic Regression",
    "section": "\n14.9 Results",
    "text": "14.9 Results\nThe predicted model can be written as \\[\n\\log \\big[ \\frac{\\Pr(Y_i = k)}{\\Pr(Y_i = \\text{Car})} \\big] = \\hat{\\beta}_{0k} + \\hat{\\beta}_{1k} X_{1i} + \\hat{\\beta}_{2k} X_{2i} + \\hat{\\beta}_{3k} X_{3i} + \\hat{\\beta}_{4k} X_{4i},\n\\] where \\(k\\in\\{\\) Bicycle, Public Transit, Walking \\(\\}\\) and \\(X_1,X_2,X_3,X_4\\) represent age, commute distance, car availability, and weekend commuter indicator respectively.\n\n14.9.1 Prediction\nWe predict the model on test set and visualize the prediction results in the confusion matrix below.\n\n\nR Code\nPython Code\n\n\n\n# Predict on the test set using the fitted model\ny_pred &lt;- predict(multinom_fit, newdata = test_set)\n\n# Compute confusion matrix\nconf_matrix &lt;- confusionMatrix(factor(y_pred, levels = levels(test_set$transport_mode)),\n                              factor(test_set$transport_mode, levels = levels(test_set$transport_mode)))\n\n# Visualize the confusion matrix\ncm_table &lt;- as.table(conf_matrix$table)\ncm_df &lt;- as.data.frame(cm_table)\ncolnames(cm_df) &lt;- c(\"Predicted\", \"Actual\", \"Freq\")\ncm_df %&gt;%\n  ggplot(aes(x = Predicted, y = Actual, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = Freq), color = \"black\", size = 4) +\n  scale_fill_gradient(low = \"#f7fbff\", high = \"#2171b5\") +\n  labs(title = \"Confusion Matrix (Test Set)\", x = \"Predicted\", y = \"Actual\") +\n  theme_bw()\n\n\n# Predict on the test set using the fitted model\ntest_set[\"transport_mode\"] = pd.Categorical(\n  test_set[\"transport_mode\"],  # align with earlier label spelling\n  categories=[\"Car\", \"Bicycle\", \"Public Transit\", \"Walking\"],\n    ordered=False\n)\ny_test = test_set[\"transport_mode\"]\nX_test = test_set[[\"age\", \"commute_distance\", \"has_car_available\", \"weekend_commuter\"]]\nX_test = sm.add_constant(X_test)\nX_test_num = X_test.copy()\nfor col in ['has_car_available', 'weekend_commuter']:\n    X_test_num[col] = X_test_num[col].cat.codes\ny_pred_test = multinom_fit.predict(X_test_num).idxmax(axis=1)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test.cat.codes, y_pred_test)\n\n# Print confusion matrix\nlabels = y_test.cat.categories\ncm = confusion_matrix(y_test.cat.codes, y_pred_test, labels=range(len(labels)))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=y_test.cat.categories)\ndisp.plot(cmap='Blues')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.show()\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe corresponding metrics are computed below.\n\n\nR Code\nPython Code\n\n\n\n# Compute the metrics\noverall_accuracy &lt;- conf_matrix$overall['Accuracy']\nmacro_recall &lt;- mean(conf_matrix$byClass[,\"Recall\"], na.rm = TRUE)\nmacro_f1 &lt;- mean(conf_matrix$byClass[,\"F1\"], na.rm = TRUE)\ncat(\"Overall Accuracy:\", round(overall_accuracy, 3), \"\\n\")\ncat(\"Macro Recall:\", round(macro_recall, 3), \"\\n\")\ncat(\"Macro F1:\", round(macro_f1, 3), \"\\n\")\n\n\n# Compute the metrics\nacc = accuracy_score(y_test.cat.codes, y_pred_test)\nrecall = recall_score(y_test.cat.codes, y_pred_test, average='macro')\nf1 = f1_score(y_test.cat.codes, y_pred_test, average='macro')\nprint(f\"Overall Accuracy: {acc:.3f}\")\nprint(f\"Macro Recall: {recall:.3f}\")\nprint(f\"Macro F1-score: {f1:.3f}\")\n\n\n\n\n\nR Output\nPython Output\n\n\n\n\n\nOverall Accuracy: 0.62 \n\n\nMacro Recall: 0.411 \n\n\nMacro F1: 0.606 \n\n\n\n\n\n\n\n\nOn the test set, the model’s overall accuracy is slightly lower than training. The higher macro recall suggests the model is now better at detecting less frequent transport modes, though overall prediction quality per class (macro F1) is still moderate. This indicates the model generalizes reasonably well, but there is room to improve detection of all classes evenly.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multinomial Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/14-multinomial-logistic.html#storytelling",
    "href": "book/14-multinomial-logistic.html#storytelling",
    "title": "14  Multinomial Logistic Regression",
    "section": "\n14.10 Storytelling",
    "text": "14.10 Storytelling\nCommuting behaviors and demographic factors are both associated with transportation mode choice and useful for predicting the choice between choosing cars or public transportation. In general, prediction is stronger for more frequent modes. This inferential question provides a comprehensive understanding of how individual characteristics shape transportation decisions and the predictive question can guide planning or personalized recommendations. You may choose one perspective of either inferential or predictive depending on your research goal.",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multinomial Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/15-ordinal-logistic.html",
    "href": "book/15-ordinal-logistic.html",
    "title": "15  Tang-tastic Ordinal Logistic Regression",
    "section": "",
    "text": "Fun fact!\n\n\nTang-tastic! So tangy it could wake you up better than coffee.\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n        {{Overdispersed or &lt;br/&gt;Underdispersed &lt;br/&gt;Data}}\n          )Chapter 13: &lt;br/&gt;Generalized &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Generalized &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n        {{Zero Inflated &lt;br/&gt;Data}}\n          )Chapter 12: &lt;br/&gt;Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n      {{Categorical &lt;br/&gt;Outcome Y}}\n        {{Nominal &lt;br/&gt;Outcome Y}}\n          )Chapter 14: &lt;br/&gt;Multinomial &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Multinomial &lt;br/&gt;Outcome Y)\n        {{Ordinal &lt;br/&gt;Outcome Y}}\n          )Chapter 15: &lt;br/&gt;Ordinal &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Logistic &lt;br/&gt;Distributed &lt;br/&gt;Cumulative Outcome &lt;br/&gt;Probability)\n\n\n\n\n\n\n\n\nFigure 15.1",
    "crumbs": [
      "Discrete Cuisine",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Tang-tastic Ordinal Logistic Regression</span>"
    ]
  },
  {
    "objectID": "book/references.html",
    "href": "book/references.html",
    "title": "References",
    "section": "",
    "text": "Allaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie,\nChristophe Dervieux, and Gordon Woodhull. 2025.\n“Quarto.” https://doi.org/10.5281/zenodo.5960048.\n\n\nBellhouse, D. R. 2004. “The Reverend Thomas\nBayes, FRS: A Biography to Celebrate the Tercentenary of His\nBirth.” Statistical Science 19 (1): 3–43. https://doi.org/10.1214/088342304000000189.\n\n\nBrent, Richard P. 1973. “Chapter 4: An Algorithm with Guaranteed\nConvergence for Finding a Zero of a Function.” In Algorithms\nfor Minimization Without Derivatives. Englewood Cliffs, NJ:\nPrentice-Hall.\n\n\nCasella, G., and R. Berger. 2024. Statistical Inference.\nChapman & Hall/CRC Texts in Statistical Science. CRC Press. https://www.taylorfrancis.com/books/mono/10.1201/9781003456285/statistical-inference-roger-berger-george-casella.\n\n\nCauchy, Augustin-Louis. 1847. “Méthode Générale Pour La Résolution\nDes Systèmes d’équations Simultanées.” Comptes Rendus\nHebdomadaires Des Séances de l’Académie Des Sciences 25: 536. https://gallica.bnf.fr/ark:/12148/bpt6k32298/f548.item.\n\n\nClark, Michael. 2022. “Generalized Additive Models.” https://m-clark.github.io/generalized-additive-models/.\n\n\nConsul, P. C., and G. C. Jain. 1973. “A Generalization of the\nPoisson Distribution.” Technometrics 15 (4): 791–99. http://www.jstor.org/stable/1267389.\n\n\nDodge, Yadolah. 2008. “Weighted Least-Squares Method.” In\nThe Concise Encyclopedia of Statistics, 566–69. New York, NY:\nSpringer New York. https://doi.org/10.1007/978-0-387-32833-1_422.\n\n\nEarlom, Richard. 1793. “Brook Taylor - National Portrait\nGallery.” NPG D6930; Brook Taylor - Portrait - National\nPortrait Gallery. National Portrait Gallery. https://www.npg.org.uk/collections/search/portrait/mw40921/Brook-Taylor.\n\n\nFletcher, Roger. 1987. Practical Methods of Optimization. 2nd\ned. New York: John Wiley & Sons.\n\n\nFrick, Hannah, Fanny Chow, Max Kuhn, Michael Mahoney, Julia Silge, and\nHadley Wickham. 2025. Rsample: General Resampling\nInfrastructure. https://doi.org/10.32614/CRAN.package.rsample.\n\n\nGelbart, Michael. 2017. “Data Science Terminology.” UBC\nMDS. Master of Data Science at the University of British Columbia.\nhttps://ubc-mds.github.io/resources_pages/terminology/.\n\n\nGregory, James. 1668. Vera circuli et\nhyperbolae quadratura cui accedit geometria pars vniuersalis inseruiens\nquantitatum curuarum transmutationi & mensurae. Authore Iacobo\nGregorio Abredonensi. Padua, Italy: Patavii: typis heredum\nPauli Frambotti bibliop. https://archive.org/details/ita-bnc-mag-00001357-001/page/n10/mode/2up.\n\n\nHarding, Edward. 1798. Portrait of Colin MacLaurin.\nCourtesy of the Smithsonian Libraries and Archives. https://library.si.edu/image-gallery/72863.\n\n\nHarris, Charles R., K. Jarrod Millman, Stéfan J. van der Walt, Ralf\nGommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020.\n“Array Programming with NumPy.”\nNature 585 (7825): 357–62. https://doi.org/10.1038/s41586-020-2649-2.\n\n\nHastie, Trevor, and Robert Tibshirani. 1986. “Generalized Additive\nModels.” Statistical Science 1 (3): 297–310. http://www.jstor.org/stable/2245459.\n\n\nJohnson, A. A., M. Q. Ott, and M. Dogucu. 2022. Bayes Rules!: An\nIntroduction to Applied Bayesian Modeling. Chapman & Hall/CRC\nTexts in Statistical Science. CRC Press. https://www.bayesrulesbook.com/.\n\n\nLeemis, Larry. n.d. “Univariate\nDistribution Relationship\nChart.” https://www.math.wm.edu/~leemis/chart/UDR/UDR.html.\n\n\nLiu, Dong C., and Jorge Nocedal. 1989. “On the Limited Memory BFGS\nMethod for Large Scale Optimization.” Mathematical\nProgramming 45: 503–28. https://doi.org/10.1007/BF01589116.\n\n\nLohr, S. L. 2021. Sampling: Design and Analysis. Chapman;\nHall/CRC. https://doi.org/https://doi.org/10.1201/9780429298899.\n\n\nMaclaurin, Colin. 1742. A Treatise of Fluxions. Edinburgh,\nScotland: Printed for the Author by T.W.; T. Ruddimans. https://archive.org/details/treatiseonfluxio02macl/page/n5/mode/2up.\n\n\nNelder, J. A., and R. W. M. Wedderburn. 1972. “Generalized Linear\nModels.” Journal of the Royal Statistical Society, Series a\n(General) 135 (3): 370–84.\n\n\nNewton, Isaac, and John Colson. 1736. Methodus Fluxionum Et Serierum\nInfinitarum. London: Printed by Henry Woodfall;; sold by John\nNourse. https://www.loc.gov/item/42048007/.\n\n\nO’Donnell, T. 1936. History of Life Insurance\nin Its Formative Years. Compiled from Approved Sources by T.\nO’Donnell. Chicago.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O.\nGrisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning\nin Python.” Journal of Machine Learning\nResearch 12: 2825–30.\n\n\nR Core Team. 2024. “R: A Language and Environment for Statistical\nComputing.” Vienna, Austria: R Foundation for Statistical\nComputing. https://www.R-project.org/.\n\n\nRaphson, Joseph. 1697. Analysis Æquationum Universalis. 2nd ed.\nLondon: Thomas Bradyll. https://doi.org/10.3931/e-rara-13516.\n\n\nReinhart, Alex. 2015. Statistics Done Wrong: The Woefully Complete\nGuide. 1st ed. San Francisco, CA: No Starch Press. https://www.statisticsdonewrong.com/index.html.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2025. Broom: Convert\nStatistical Objects into Tidy Tibbles. https://broom.tidymodels.org/.\n\n\nRousseeuw, Peter J., and Annick M. Leroy. 1987. Robust Regression\nand Outlier Detection. Wiley Series in Probability and Mathematical\nStatistics. John Wiley & Sons. https://doi.org/10.1002/0471725382.\n\n\nScotland, National Galleries of. n.d. Professor James Gregory, 1638\n- 1675 (1). Mathematician. Professor James Gregory, 1638 - 1675\n(1). Mathematician | National Galleries. https://www.nationalgalleries.org/art-and-artists/31132/professor-james-gregory-1638-1675-mathematician.\n\n\nSeabold, Skipper, and Josef Perktold. 2010. “Statsmodels:\nEconometric and Statistical Modeling with Python.” In 9th\nPython in Science Conference.\n\n\nSoch, Joram, The Book of Statistical Proofs, Maja, Pietro Monticone,\nThomas J. Faulkenberry, Alex Kipnis, Kenneth Petrykowski, et al. 2024.\n“StatProofBook/StatProofBook.github.io:\nStatProofBook 2023.” Zenodo. https://doi.org/10.5281/zenodo.10495684.\n\n\nTaylor, Brook. 1715. Methodus incrementorum\ndirecta & inversa. Auctore Brook Taylor, LL. D. & Regiae\nSocietatis Secretario. London, England: Typis Pearsonianis\nProstant apud Gul. Innys ad Insignia Principis in Coemeterio Paulino\nMDCCXV. https://archive.org/details/bim_eighteenth-century_methodus-incrementorum-d_taylor-brook_1717.\n\n\nThe Pandas Development Team. 2024. “Pandas-Dev/Pandas:\nPandas.” Zenodo. https://doi.org/10.5281/zenodo.3509134.\n\n\nUshey, Kevin, JJ Allaire, and Yuan Tang. 2025. Reticulate: Interface\nto ’Python’. https://doi.org/10.32614/CRAN.package.reticulate.\n\n\nVan Rossum, Guido, and Fred L. Drake. 2009. Python 3 Reference\nManual. Scotts Valley, CA: CreateSpace.\n\n\nVirtanen, Pauli, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler\nReddy, David Cournapeau, Evgeni Burovski, et al. 2020. “SciPy 1.0: Fundamental Algorithms for\nScientific Computing in Python.” Nature Methods\n17: 261–72. https://doi.org/10.1038/s41592-019-0686-2.\n\n\nWeisstein, Eric W. n.d.a. “Gamma Function.” From\nMathWorld–A Wolfram Web Resource. https://mathworld.wolfram.com/GammaFunction.html.\n\n\n———. n.d.b. “Taylor Series.” From MathWorld–A Wolfram\nWeb Resource. https://mathworld.wolfram.com/TaylorSeries.html.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWu, Lang. 2009. Mixed Effects Models for Complex Data. 1st ed.\nChapman; Hall/CRC. https://doi.org/10.1201/9781420074086.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "book/A-dictionary.html",
    "href": "book/A-dictionary.html",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "",
    "text": "A",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#a",
    "href": "book/A-dictionary.html#a",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "",
    "text": "Alternative hypothesis\nIn a hypothesis testing, an alternative hypothesis is denoted by \\(H_1\\). This hypothesis corresponds to the complement (i.e., the opposite) of the null hypothesis \\(H_0\\). Since the whole inferential process is designed to assess the strength of the evidence in favour or against of \\(H_0\\), any inferential conclusion against \\(H_0\\) can be worded as “rejecting \\(H_0\\) in favour of \\(H_1\\).” In plain words, \\(H_1\\) is an inferential statement associated to a non-status quo in some population(s) or system(s) of interest, which might refer to actual signal for the researcher in question.\nLet us assume random variable \\(Y\\) from some population(s) or system(s) of interest is governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nMoreover, suppose random variable \\(Y\\) follows certain probability distribution \\(\\mathcal{D}(\\cdot)\\) in a generative model \\(m\\) as in\n\\[\n\\text{$m$: } y \\sim \\mathcal{D}(\\boldsymbol{\\theta}).\n\\]\nLet \\(\\boldsymbol{\\Theta}_0^c \\subset \\boldsymbol{\\theta}\\) denote the non-status quo for the parameter(s) to be tested. Then, the alternative hypothesis is mathematically defined as\n\\[\n\\text{$H_1$: } \\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}_0^c \\quad \\text{where} \\quad \\boldsymbol{\\Theta}_0^c \\subset \\boldsymbol{\\theta}.\n\\]\nAttribute\n\n\nEquivalent to:\n\n\nCovariate, exogeneous variable, explanatory variable, feature, independent variable, input, predictor or regressor.\n\n\nAverage\nLet \\(Y\\) be a random variable whose support is \\(\\mathcal{Y}\\). In general, the expected value or mean \\(\\mathbb{E}(Y)\\) of this random variable is defined as a weighted average according to its corresponding probability distribution. In other words, this measure of central tendency \\(\\mathbb{E}(Y)\\) aims to find the middle value of this random variable by weighting all its possible values in its support \\(\\mathcal{Y}\\) as dictated by its probability distribution.\nGiven the above definition, when \\(Y\\) is a discrete random variable whose probability mass function (PMF) is \\(P_Y(Y = y)\\), then its weighted average is mathematically defined as\n\\[\n\\mathbb{E}(Y) = \\sum_{y \\in \\mathcal{Y}} y \\cdot P_Y(Y = y).\n\\]\nWhen \\(Y\\) is a continuous random variable whose probability density function (PDF) is \\(f_Y(y)\\), its weighted average is mathematically defined as\n\\[\n\\mathbb{E}(Y) = \\int_{\\mathcal{Y}} y \\cdot f_Y(y) \\mathrm{d}y.\n\\]\n\n\nEquivalent to:\n\n\nExpected value or mean.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#b",
    "href": "book/A-dictionary.html#b",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "B",
    "text": "B\nBayesian statistics\nThis statistical school of thinking also relies on the frequency of events to estimate specific parameters of interest in a population or system. Nevertheless, unlike frequentist statistics, Bayesian statisticians use prior knowledge on the population parameters to update their estimations on them along with the current evidence they can gather. This evidence is in the form of the repetition of \\(n\\) experiments involving a random phenomenon. All these ingredients allow Bayesian statisticians to make inference by conducting appropriate hypothesis testings, which are designed differently from their mainstream frequentist counterpart.\nUnder the umbrella of this approach, we assume that our governing parameters are random; i.e., they have their own sample space and probabilities associated to their corresponding outcomes. The statistical process of inference is heavily backed up by probability theory mostly in the form of the Bayes’ rule (named after Reverend Thomas Bayes, an English statistician from the 18th century). This rule uses our current evidence along with our prior beliefs to deliver a posterior distribution of our random parameter(s) of interest.\nBayes’ rule\nSuppose you have two events of interest, \\(A\\) and \\(B\\), in a random phenomenon of a population or system of interest. From Equation A.4, we can state the following expression for the conditional probability of \\(A\\) given \\(B\\):\n\\[\nP(A | B) = \\frac{P(A \\cap B)}{P(B)} \\quad \\text{if $P(B) &gt; 0$.}\n\\tag{A.1}\\]\nNote the conditional probability of \\(B\\) given \\(A\\) can be stated as:\n\\[\n\\begin{align*}\nP(B | A) &= \\frac{P(B \\cap A)}{P(A)} \\quad \\text{if $P(A) &gt; 0$} \\\\\n&= \\frac{P(A \\cap B)}{P(A)} \\quad \\text{since $P(B \\cap A) = P(A \\cap B)$.}\n\\end{align*}\n\\tag{A.2}\\]\nThen, we can manipulate Equation A.2 as follows:\n\\[\nP(A \\cap B) = P(B | A) \\times P(A).\n\\]\nThe above result can be plugged into Equation A.1:\n\\[\n\\begin{align*}\nP(A | B) &= \\frac{P(A \\cap B)}{P(B)} \\\\\n&= \\frac{P(B | A) \\times P(A)}{P(B)}.\n\\end{align*}\n\\tag{A.3}\\]\nEquation A.3 is called the Bayes’ rule. We are basically flipping around conditional probabilities.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#c",
    "href": "book/A-dictionary.html#c",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "C",
    "text": "C\nCritical value\nThe critical value of a hypothesis testing defines the region for which we might reject \\(H_0\\) in favour of \\(H_1\\). This critical value is in the function of the significance level \\(\\alpha\\) and test flavour. It is located on the corresponding \\(x\\)-axis of the probability distribution of \\(H_0\\). Hence, this value acts as a threshold to decide either of the following:\n\nIf the observed test statistic exceeds a given critical value, then we have enough statistical evidence to reject \\(H_0\\) in favour of \\(H_1\\).\nIf the observed test statistic does not exceed a given critical value, then we have enough statistical evidence to fail to reject \\(H_0\\).\nConditional probability\nSuppose you have two events of interest, \\(A\\) and \\(B\\), in a random phenomenon, in a population or system of interest. These two events belong to the sample space \\(S\\). Moreover, assume that the probability of event \\(B\\) is such that\n\\[\nP(B) &gt; 0,\n\\]\nwhich is considered the conditioning event.\nHence, the conditional probability event \\(A\\) given event \\(B\\) is defined as\n\\[\nP(A | B) = \\frac{P(A \\cap B)}{P(B)},\n\\tag{A.4}\\]\nwhere \\(P(A \\cap B)\\) is read as the probability of the intersection of events \\(A\\) and \\(B\\).\nConfidence interval\nA confidence interval provides an estimated range of values within which the true population parameter is likely to fall, based on the sample data. It reflects the degree of uncertainty associated with the obtained estimate.\nFor instance, a 95% confidence interval means that if the study were repeated many times using different random samples from the same population or system of interest, approximately 95% of the resulting intervals would contain the true parameter.\nContinuous random variable\nLet \\(Y\\) be a random variable whose support is \\(\\mathcal{Y}\\). If this support \\(\\mathcal{Y}\\) corresponds to an uncountably infinite set of possible values, then \\(Y\\) is considered a continuous random variable.\nNote a continuous random variable could be\n\n\ncompletely unbounded (i.e., its set of possible values goes from \\(-\\infty\\) to \\(\\infty\\) as in \\(-\\infty &lt; y &lt; \\infty\\)),\n\npositively unbounded (i.e., its set of possible values goes from \\(0\\) to \\(\\infty\\) as in \\(0 \\leq y &lt; \\infty\\)),\n\nnegatively unbounded (i.e., its set of possible values goes from \\(-\\infty\\) to \\(0\\) as in \\(-\\infty &lt; y \\leq 0\\)), or\n\nbounded between two values \\(a\\) and \\(b\\) (i.e., its set of possible values goes from \\(a\\) to \\(b\\) as in \\(a \\leq y \\leq b\\)).\nCovariate\n\n\nEquivalent to:\n\n\nAttribute, exogeneous variable, explanatory variable, feature, independent variable, input, predictor or regressor.\n\n\nCumulative distribution function\nLet \\(Y\\) be a random variable either discrete or continuous. Its cumulative distribution function (CDF) \\(F_Y(y)  : \\mathbb{R} \\rightarrow [0, 1]\\) refers to the probability that \\(Y\\) is less or equal than an observed value \\(y\\):\n\\[\nF_Y(y) = P(Y \\leq y).\n\\]\nThen, we have the following by type of random variable:\n\nWhen \\(Y\\) is discrete, whose support is \\(\\mathcal{Y}\\), suppose it has a probability mass function (PMF) \\(P_Y(Y = y)\\). Then, the CDF is mathematically represented as:\n\n\\[\nF_Y(y) = \\sum_{\\substack{t \\in \\mathcal{Y} \\\\ t \\leq y}} P_Y(Y = t).\n\\tag{A.5}\\]\n\nWhen \\(Y\\) is continuous, whose support is \\(\\mathcal{Y}\\), suppose it has a probability density function (PDF) \\(f_Y(y)\\). Then, the CDF is mathematically represented as:\n\n\\[\nF_Y(y) = \\int_{-\\infty}^y f_Y(t) \\mathrm{d}t.\n\\tag{A.6}\\]\nNote that in Equation A.5 and Equation A.6, we use the auxiliary variable \\(t\\) since we do not compute the summation or integral over the observed \\(y\\) given its role on either the PMF or PDF. Therefore, we use this auxiliary variable \\(t\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#d",
    "href": "book/A-dictionary.html#d",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "D",
    "text": "D\nDependent variable\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\nEquivalent to:\n\n\nEndogeneous variable, response variable, outcome, output or target.\n\n\nDiscrete random variable\nLet \\(Y\\) be a random variable whose support is \\(\\mathcal{Y}\\). If this support \\(\\mathcal{Y}\\) corresponds to a finite set or a countably infinite set of possible values, then \\(Y\\) is considered a discrete random variable.\nFor instance, we can encounter discrete random variables which could be classified as\n\n\nbinary (i.e., a finite set of two possible values),\n\ncategorical (either nominal or ordinal, which have a finite set of three or more possible values), or\n\ncounts (which might have a finite set or a countably infinite set of possible values as integers).\nDispersion",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#e",
    "href": "book/A-dictionary.html#e",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "E",
    "text": "E\nEndogeneous variable\n\n\nEquivalent to:\n\n\nDependent variable, outcome, output, response variable or target.\n\n\nEquidispersion\nEstimate\nSuppose we have an observed random sample of size \\(n\\) with values \\(y_1, \\dots , y_n\\). Then, we apply a given estimator mathematical rule to these \\(n\\) observed values. Hence, this numerical computation is called an estimate of our population parameter of interest.\nEstimator\nAn estimator is a mathematical rule involving the random variables \\(Y_1, \\dots, Y_n\\) from our random sample of size \\(n\\). As its name says, this rule allows us to estimate our population parameter of interest.\nExpected value\nLet \\(Y\\) be a random variable whose support is \\(\\mathcal{Y}\\). In general, the expected value or mean \\(\\mathbb{E}(Y)\\) of this random variable is defined as a weighted average according to its corresponding probability distribution. In other words, this measure of central tendency \\(\\mathbb{E}(Y)\\) aims to find the middle value of this random variable by weighting all its possible values in its support \\(\\mathcal{Y}\\) as dictated by its probability distribution.\nGiven the above definition, when \\(Y\\) is a discrete random variable whose probability mass function (PMF) is \\(P_Y(Y = y)\\), then its expected value is mathematically defined as\n\\[\n\\mathbb{E}(Y) = \\sum_{y \\in \\mathcal{Y}} y \\cdot P_Y(Y = y).\n\\tag{A.7}\\]\nWhen \\(Y\\) is a continuous random variable whose probability density function (PDF) is \\(f_Y(y)\\), its expected value is mathematically defined as\n\\[\n\\mathbb{E}(Y) = \\int_{\\mathcal{Y}} y \\cdot f_Y(y) \\mathrm{d}y.\n\\tag{A.8}\\]\n\n\nEquivalent to:\n\n\nAverage or mean.\n\n\nExogeneous variable\n\n\nEquivalent to:\n\n\nAttribute, covariate, explanatory variable, feature, independent variable, input, predictor or regressor.\n\n\nExplanatory variable\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, feature, independent variable, input, predictor or regressor.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#f",
    "href": "book/A-dictionary.html#f",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "F",
    "text": "F\nFalse negative\nA false negative is defined as incorrectly failing to reject the null hypothesis \\(H_0\\) in favour of the alternative hypothesis \\(H_1\\) when, in fact, \\(H_0\\) is false.\n\n\nEquivalent to:\n\n\nType II error.\n\n\nFalse positive\nA false positive is defined as incorrectly rejecting the null hypothesis \\(H_0\\) in favour of the alternative hypothesis \\(H_1\\) when, in fact, \\(H_0\\) is true. Table A.1 summarizes the types of inferential conclusions in function on whether \\(H_0\\) is true or not.\n\n\nTable A.1: Types of inferential conclusions in a frequentist hypothesis testing.\n\n\n\n\n\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\nReject \\(H_0\\)\nType I error (False positive)\nCorrect\n\n\nFail to reject \\(H_0\\)\nCorrect\nType II error (False negative)\n\n\n\n\n\n\n\n\nEquivalent to:\n\n\nType I error.\n\n\nFeature\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, explanatory variable, independent variable, input, predictor or regressor.\n\n\nFrequentist statistics\nThis statistical school of thinking heavily relies on the frequency of events to estimate specific parameters of interest in a population or system. This frequency of events is reflected in the repetition of \\(n\\) experiments involving a random phenomenon within this population or system.\nUnder the umbrella of this approach, we assume that our governing parameters are fixed. Note that, within the philosophy of this school of thinking, we can only make precise and accurate predictions as long as we repeat our \\(n\\) experiments as many times as possible, i.e.,\n\\[\nn \\rightarrow \\infty.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#g",
    "href": "book/A-dictionary.html#g",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "G",
    "text": "G\nGeneralized linear models\nAn umbrella of regression approaches that model the conditional expected value of the response variable \\(Y\\) based on a set of observed regressors \\(x\\). Unlike a traditional model such as the continuous Ordinary Least-squares (OLS) that relies solely on a Normal distribution to make inference, GLMs extend this distributional assumption, allowing for a variety of probability distributions for the response variable. Note that this umbrella encompasses approaches that accommodate continuous or discrete responses \\(Y\\). According to Casella and Berger (2024), a typical GLM consists of three key components:\n\n\nRandom Component: The response variables in a training dataset of size \\(n\\) (i.e., the random variables \\(Y_1, Y_2, \\ldots, Y_n\\)) are statistically independent but not identically distributed. Still, they do belong to the same family of probability distributions (e.g., Gamma, Beta, Poisson, Bernoulli, etc.).\n\nSystematic Component: For the \\(i\\)th observation, this component depicts how the \\(k\\) regressors \\(x_{i, j}\\) (for \\(j = 1, 2, \\ldots, k\\)) come into the GLM as a linear combination involving \\(k + 1\\) regression parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_k\\). This relationship is expressed as\n\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_{i, 1} + \\beta_2 x_{i, 2} + \\ldots + \\beta_k x_{i, k}.\n\\]\n\n\nLink Function: This component connects (or “links”) the systematic component \\(\\eta_i\\) with the mean of the random variable \\(Y_i\\), denoted as \\(\\mu_i\\). The link function is mathematically represented as\n\n\\[\ng(\\mu_i) = \\eta_i.\n\\]\nNelder and Wedderburn (1972) introduced this umbrella term called GLM in the statistical literature and identified a set of distinct statistical models that shared the above three components.\nGenerative model\nSuppose you observe some data \\(y\\) from a population or system of interest. Moreover, let us assume this population or system is governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nIf we state that the random variable \\(Y\\) follows certain probability distribution \\(\\mathcal{D}(\\cdot)\\), then we will have a generative model \\(m\\) such that\n\\[\n\\text{$m$: } Y \\sim \\mathcal{D}(\\boldsymbol{\\theta}).\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#h",
    "href": "book/A-dictionary.html#h",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "H",
    "text": "H\nHypothesis\nSuppose you observe some data \\(y\\) from some population(s) or system(s) of interest governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nMoreover, let us assume that random variable \\(Y\\) follows certain probability distribution \\(\\mathcal{D}(\\cdot)\\) in a generative model \\(m\\) as in\n\\[\n\\text{$m$: } Y \\sim \\mathcal{D}(\\boldsymbol{\\theta}).\n\\]\nBeginning from the fact that \\(\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}\\) where \\(\\boldsymbol{\\Theta} \\in \\mathbb{R}^k\\), a statistical hypothesis is a general statement about some parameter vector \\(\\boldsymbol{\\theta}\\) in regards to specific values contained in vector \\(\\boldsymbol{\\Theta}^*\\) such that\n\\[\n\\text{$H$: } \\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}^* \\quad \\text{where} \\quad \\boldsymbol{\\Theta}^* \\subset \\boldsymbol{\\Theta}.\n\\]\nHypothesis testing\nA hypothesis testing is the decision rule we have to apply between the null and alternative hypotheses, via our sample data, to fail to reject or reject the null hypothesis.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#i",
    "href": "book/A-dictionary.html#i",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "I",
    "text": "I\nIndependence\nSuppose you have two events of interest, \\(A\\) and \\(B\\), in a random phenomenon of a population or system of interest. These two events are statistically independent if event \\(B\\) does not affect event \\(A\\) and vice versa. Therefore, the probability of their corresponding intersection is given by:\n\\[\nP(A \\cap B) = P(A) \\times P(B).\n\\]\nLet us expand the above definition to a random variable framework:\n\nSuppose you have a set of \\(n\\) discrete random variables \\(Y_1, \\dots, Y_n\\) whose supports are \\(\\mathcal{Y_1}, \\dots, \\mathcal{Y_n}\\) with probability mass functions (PMFs) \\(P_{Y_1}(Y_1 = y_1), \\dots, P_{Y_n}(Y_n = y_n)\\) respectively. That said, the joint PMF of these \\(n\\) random variables is the multiplication of their corresponding standalone PMFs:\n\n\\[\n\\begin{align*}\nP_{Y_1, \\dots, Y_n}(Y_1 = y_1, \\dots, Y_n = y_n) &= \\prod_{i = 1}^n P_{Y_i}(Y_i = y_i) \\\\\n& \\qquad \\text{for all} \\\\\n& \\qquad \\quad y_i \\in \\mathcal{Y}_i, i = 1, \\dots, n.\n\\end{align*}\n\\tag{A.9}\\]\n\nSuppose you have a set of \\(n\\) continuous random variables \\(Y_1, \\dots, Y_n\\) whose supports are \\(\\mathcal{Y_1}, \\dots, \\mathcal{Y_n}\\) with probability density functions (PDFs) \\(f_{Y_1}(y_1), \\dots, f_{Y_n}(y_n)\\) respectively. That said, the joint PDF of these \\(n\\) random variables is the multiplication of their corresponding standalone PDFs:\n\n\\[\n\\begin{align*}\nf_{Y_1, \\dots, Y_n}(y_1, \\dots, y_n) &= \\prod_{i = 1}^n f_{Y_i}(y_i) \\\\\n& \\qquad \\text{for all} \\\\\n& \\qquad \\quad y_i \\in \\mathcal{Y}_i, i = 1, \\dots, n.\n\\end{align*}\n\\tag{A.10}\\]\nIndependent variable\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, explanatory variable, feature, input, predictor or regressor.\n\n\nInput\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, explanatory variable, feature, independent variable, predictor or regressor.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#l",
    "href": "book/A-dictionary.html#l",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "L",
    "text": "L\nLikelihood function\nSuppose you observe some data \\(y\\) from a population or system of interest which is governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nThe corresponding random variable \\(Y\\) has a given probability function \\(P_Y(Y = y | \\boldsymbol{\\theta})\\), which can be either a probability mass function (PMF) in the discrete case or a probability density function (PDF) in the continuous case. Then, the likelihood function for the parameter vector \\(\\boldsymbol{\\theta}\\) given the observed data \\(y\\) is mathematically equivalent to the aforementioned probability function such that:\n\\[\nL(\\boldsymbol{\\theta} | y) = P_Y(Y = y | \\boldsymbol{\\theta}).\n\\tag{A.11}\\]\nIt is important to note that the above likelihood is in function of the parameter vector \\(\\boldsymbol{\\theta}\\) and conditioned on the observed data \\(y\\). Additionally, in many continuous cases, the likelihood function may exceed \\(1\\) given the definition of bounds we have already established for a PDF (see Equation A.14).\nLog-likelihood function\nSuppose you observe some data \\(y\\) from a population or system of interest which is governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nThe corresponding random variable \\(Y\\) has a given probability function \\(P_Y(Y = y | \\boldsymbol{\\theta})\\), which can be either a probability mass function (PMF) in the discrete case or a probability density function (PDF) in the continuous case. Moreover, the likelihood function, as described in Equation A.11, is defined as follows:\n\\[\nL(\\boldsymbol{\\theta} | y) = P_Y(Y = y | \\boldsymbol{\\theta}).\n\\]\nThen, the log-likelihood function is merely the logarithm of the above function:\n\\[\n\\log L(\\boldsymbol{\\theta} | y) = \\log \\left[ P_Y(Y = y | \\boldsymbol{\\theta}) \\right].\n\\tag{A.12}\\]\nUsing a log-likelihood function, which is a monotonic transformation of the likelihood function, offers the following practical advantages:\n\nThe logarithmic properties convert products into sums. This is particularly useful in likelihood functions for random samples that involve multiplying probability functions (and its corresponding factors).\nWhen estimating parameters, calculating the derivative of a sum is easier than that of a product.\nIn many cases, the likelihood functions for observed random samples can yield very small values, which may lead to computational instability. By working on a logarithmic scale, these computations become more stable. This stability is crucial for numerical optimization methods applied to a given log-likelihood function, in cases where a closed-form solution for an estimate is not mathematically feasible.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#m",
    "href": "book/A-dictionary.html#m",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "M",
    "text": "M\nMaximum likelihood estimation (MLE)\nSuppose you observe some data \\(y\\) from a population or system of interest which is governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nThe corresponding random variable \\(Y\\) has a given probability function \\(P_Y(Y = y | \\boldsymbol{\\theta})\\), which can be either a probability mass function (PMF) in the discrete case or a probability density function (PDF) in the continuous case. Furthermore, the log-likelihood function is defined as in Equation A.12:\n\\[\n\\log L(\\boldsymbol{\\theta} | y) = \\log \\left[ P_Y(Y = y | \\boldsymbol{\\theta}) \\right].\n\\]\nMaximum likelihood estimation (MLE) aims to find the estimate of \\(\\boldsymbol{\\theta}\\) that maximizes the above log-likelihood function as in:\n\\[\n\\hat{\\boldsymbol{\\theta}}_{\\text{MLE}} = \\underset{\\boldsymbol{\\theta}}{\\operatorname{arg max}} \\log L(\\boldsymbol{\\theta} | y).\n\\]\nIn supervised learning, MLE is analogous to minimizing any given loss function during model training.\nMean\nLet \\(Y\\) be a random variable whose support is \\(\\mathcal{Y}\\). In general, the expected value or mean \\(\\mathbb{E}(Y)\\) of this random variable is defined as a weighted average according to its corresponding probability distribution. In other words, this measure of central tendency \\(\\mathbb{E}(Y)\\) aims to find the middle value of this random variable by weighting all its possible values in its support \\(\\mathcal{Y}\\) as dictated by its probability distribution.\nGiven the above definition, when \\(Y\\) is a discrete random variable whose probability mass function (PMF) is \\(P_Y(Y = y)\\), then its mean is mathematically defined as\n\\[\n\\mathbb{E}(Y) = \\sum_{y \\in \\mathcal{Y}} y \\cdot P_Y(Y = y).\n\\]\nWhen \\(Y\\) is a continuous random variable whose probability density function (PDF) is \\(f_Y(y)\\), its mean is mathematically defined as\n\\[\n\\mathbb{E}(Y) = \\int_{\\mathcal{Y}} y \\cdot f_Y(y) \\mathrm{d}y.\n\\]\n\n\nEquivalent to:\n\n\nAverage or expected value.\n\n\nMeasure of central tendency\nProbabilistically, a measure of central tendency is defined as a metric that identifies a central or typical value of a given probability distribution. In other words, a measure of central tendency refers to a central or typical value that a given random variable might take when we observe various realizations of this variable over a long period.\nMeasure of uncertainty\nProbabilistically, a measure of uncertainty refers to the spread of a given random variable when we observe its different realizations in the long term. Note a larger spread indicates more variability in these realizations. On the other hand, a smaller spread denotes less variability in these realizations.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#n",
    "href": "book/A-dictionary.html#n",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "N",
    "text": "N\nNull hypothesis\nIn a hypothesis(s) testing, a null hypothesis is denoted by \\(H_0\\). The whole inferential process is designed to assess the strength of the evidence in favour or against this null hypothesis. In plain words, \\(H_0\\) is an inferential statement associated to the status quo in some population(s) or system(s) of interest, which might refer to no signal for the researcher in question.\nAgain, suppose random variable \\(Y\\) from some population(s) or system(s) of interest is governed by \\(k\\) parameters contained in the following vector:\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T.\n\\]\nMoreover, we assume that random variable \\(Y\\) follows certain probability distribution \\(\\mathcal{D}(\\cdot)\\) in a generative model \\(m\\) as in\n\\[\n\\text{$m$: } Y \\sim \\mathcal{D}(\\boldsymbol{\\theta}).\n\\]\nLet \\(\\boldsymbol{\\Theta}_0 \\subset \\boldsymbol{\\theta}\\) denote the status quo for the parameter(s) to be tested. Then, the null hypothesis is mathematically defined as\n\\[\n\\text{$H_0$: } \\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}_0 \\quad \\text{where} \\quad \\boldsymbol{\\Theta}_0 \\subset \\boldsymbol{\\theta}.\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#o",
    "href": "book/A-dictionary.html#o",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "O",
    "text": "O\nObserved effect\nAn observed effect is the difference between the estimate provided the observed random sample (of size \\(n\\), as in \\(y_1, \\dots, y_n\\)) to the hypothesized value(s) of the population parameter(s) depicted in the statistical hypotheses.\nOutcome\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\nEquivalent to:\n\n\nDependent variable, endogeneous variable, response variable, output or target.\n\n\nOutput\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\nEquivalent to:\n\n\nDependent variable, endogeneous variable, response variable, outcome or target.\n\n\nOverdispersion",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#p",
    "href": "book/A-dictionary.html#p",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "P",
    "text": "P\nParameter\nIt is a characteristic (numerical or even non-numerical, such as a distinctive category) that summarizes the state of our population or system of interest.\nNote the standard mathematical notation for population parameters are Greek letters (for more insights, you can check Appendix B). Moreover, in practice, these population parameter(s) of interest will be unknown to the data scientist or researcher. Instead, they would use formal statistical inference to estimate them.\nParametric model\nA parametric model is a type of model that assumes a specific functional relationship between the response variable of interest, \\(Y\\), which is considered a random variable, and one or more observed explanatory variables, \\(x\\). This relationship is characterized by a finite set of parameters and can often be expressed as a linear combination of the observed \\(x\\) variables, which favours interpretability.\nMoreover, since \\(Y\\) is a random variable, there is room to make further assumptions on it in the form of a probability distribution, independence or even homoscedasticity (the condition where all responses in the population have the same variance). It is essential to test these assumptions after fitting this type of models, as any deviations may result in misleading or biased estimates, predictions, and inferential conclusions.\nPoint estimate\nLet \\(\\theta\\) denote a population parameter of interest. Suppose you have observed a random sample of size \\(n\\), represented as the vector:\n\\[\n\\boldsymbol{y} = (y_1, y_2, \\ldots, y_n)^T.\n\\]\nThe point estimate \\(\\hat{\\theta}\\) serves as a possible value for \\(\\theta\\) and is expressed as a function of the observed random sample contained in \\(\\boldsymbol{y}\\):\n\\[\n\\hat{\\theta} = h(\\boldsymbol{y}).\n\\]\nPopulation\nIt is a whole collection of individuals or items that share distinctive attributes. As data scientists or researchers, we are interested in studying these attributes, which we assume are governed by parameters. In practice, we must be as specific as possible when defining our given population such that we would frame our entire data modelling process since its very early stages.\nNote that the term population could be exchanged for the term system, given that certain contexts do not specifically refer to individuals or items. Instead, these contexts could refer to processes whose attributes are also governed by parameters.\nPower\nThe statistical power of a test \\(1 -\\beta\\) is the complement of the conditional probability \\(\\beta\\) of failing to reject the null hypothesis \\(H_0\\) given that \\(H_0\\) is false, which is mathematically represented as\n\\[\nP \\left( \\text{Failing to reject $H_0$} | \\text{$H_0$ is false} \\right) = \\beta;\n\\]\nyielding\n\\[\n\\text{Power} = 1 - \\beta.\n\\]\nIn plain words, \\(1 - \\beta \\in [0, 1]\\) is the probabilistic ability of our hypothesis testing to detect any signal in our inferential process, if there is any. The larger the power in our power analysis, the less prone we are to commit a type II error.\n\n\nEquivalent to:\n\n\nTrue positive rate.\n\n\nPower analysis\nThe power analysis is a set of statistical tools used to compute the minimum required sample size \\(n\\) for any given inferential study. These tools require the significance level, power, and effect size (i.e., the magnitude of the signal) the researcher aims to detect via their inferential study. This analysis seeks to determine whether observed results are likely due to chance or represent a true and meaningful effect.\nPredictor\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, explanatory variable, feature, independent variable, input or regressor.\n\n\nProbability\nLet \\(A\\) be an event of interest in a random phenomenon, in a population or system of interest, whose all possible outcomes belong to a given sample space \\(S\\). Generally, the probability for this event \\(A\\) happening can be mathematically depicted as \\(P(A)\\). Moreover, suppose we observe the random phenomenon \\(n\\) times such as we were running some class of experiment, then \\(P(A)\\) is defined as the following ratio:\n\\[\nP(A) = \\frac{\\text{Number of times event $A$ is observed}}{n},\n\\tag{A.13}\\]\nas the \\(n\\) times we observe the random phenomenon goes to infinity.\nEquation A.13 will always put \\(P(A)\\) in the following numerical range:\n\\[\n0 \\leq P(A) \\leq 1.\n\\]\nProbability distribution\nWhen we set a random variable \\(Y\\), we also set a new set of \\(v\\) possible outcomes \\(\\mathcal{Y} = \\{ y_1, \\dots, y_v\\}\\) coming from the sample space \\(S\\). This new set of possible outcomes \\(\\mathcal{Y}\\) corresponds to the support of the random variable \\(Y\\) (i.e., all the possible values that could be taken on once we execute a given random experiment involving \\(Y\\)).\nThat said, let us suppose we have a sample space of \\(u\\) elements defined as\n\\[\nS = \\{ s_1, \\dots, s_u \\},\n\\]\nwhere each one of these elements has a probability assigned via a function \\(P_S(\\cdot)\\) such that\n\\[\nP(S) = \\sum_{i = 1}^u P_S(s_i) = 1.\n\\]\nwhich has to satisfy Equation A.17.\nThen, the probability distribution of \\(Y\\), i.e., \\(P_Y(\\cdot)\\) assigns a probability to each observed value \\(Y = y_j\\) (with \\(j = 1, \\dots, v\\)) if and only if the outcome of the random experiment belongs to the sample space, i.e., \\(s_i \\in S\\) (for \\(i = 1, \\dots, u\\)) such that \\(Y(s_i) = y_j\\):\n\\[\nP_Y(Y = y_j) = P \\left( \\left\\{ s_i \\in S : Y(s_i) = y_j \\right\\} \\right).\n\\]\nProbability density function (PDF)\nLet \\(Y\\) be a continuous random variable whose support is \\(\\mathcal{Y}\\). Furthermore, consider a function \\(f_Y(y)\\) such that\n\\[\nf_Y(y) : \\mathbb{R} \\rightarrow \\mathbb{R}\n\\]\nwith\n\\[\nf_Y(y) \\geq 0.\n\\tag{A.14}\\]\nThen, \\(f_Y(y)\\) is considered a probability density function (PDF) if the probability of \\(Y\\) taking on a value within the range represented by the subset \\(A \\subset \\mathcal{Y}\\) is equal to\n\\[\nP_Y(Y \\in A) = \\int_A f_Y(y) \\mathrm{d}y\n\\]\nwith\n\\[\n\\int_{\\mathcal{Y}} f_Y(y) \\mathrm{d}y = 1.\n\\]\nProbability mass function (PMF)\nLet \\(Y\\) be a discrete random variable whose support is \\(\\mathcal{Y}\\). Moreover, suppose that \\(Y\\) has a probability distribution such that\n\\[\nP_Y(Y = y) : \\mathbb{R} \\rightarrow [0, 1]\n\\]\nwhere, for all \\(y \\notin \\mathcal{Y}\\), we have\n\\[\nP_Y(Y = y) = 0\n\\]\nand\n\\[\n\\sum_{y \\in \\mathcal{Y}} P_Y(Y = y) = 1.\n\\]\nThen, \\(P_Y(Y = y)\\) is considered a probability mass function (PMF).\n\\(p\\)-value\nA \\(p\\)-value refers to the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic coming from our observed random sample of size \\(n\\). This \\(p\\)-value is obtained via the probability distribution of \\(H_0\\) and the observed test statistic.\nAlternatively to a critical value, we can reject or fail to reject the null hypothesis \\(H_0\\) using this \\(p\\)-value as follows:\n\nIf the \\(p\\)-value associated to the observed test statistic exceeds a given significance level \\(\\alpha\\), then we have enough statistical evidence to reject \\(H_0\\) in favour of \\(H_1\\).\nIf the \\(p\\)-value associated to the observed test statistic does not exceed a given significance level \\(\\alpha\\), then we have enough statistical evidence to fail to reject \\(H_0\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#r",
    "href": "book/A-dictionary.html#r",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "R",
    "text": "R\nRandom sample\nA random sample is a collection of random variables \\(Y_1, \\dots, Y_n\\) of size \\(n\\) coming from a given population or system of interest. Note that the most elementary definition of a random sample assumes that these \\(n\\) random variables are mutually independent and identically distributed (which is abbreviated as iid).\nThe fact that these \\(n\\) random variables are identically distributed indicates that they have the same mathematical form for their corresponding probability mass functions (PMFs) or probability density function (PDFs), depending on whether they are discrete or continuous respectively. Hence, under a generative modelling approach in a population or system of interest governed by \\(k\\) parameters contained in the vector\n\\[\n\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\cdots, \\theta_k)^T,\n\\]\nwe can apply the iid property in an elementary random sample to obtain the following joint probability distributions:\n\nIn the case of \\(n\\) iid discrete random variables \\(Y_1, \\dots, Y_n\\) whose common standalone PMF is \\(P_Y(Y = y | \\boldsymbol{\\theta})\\) with support \\(\\mathcal{Y}\\), the joint PMF is mathematically expressed as\n\n\\[\n\\begin{align*}\nP_{Y_1, \\dots, Y_n}(Y_1 = y_1, \\dots, Y_n = y_n | \\boldsymbol{\\theta}) &= \\prod_{i = 1}^n P_Y(Y = y_i | \\boldsymbol{\\theta}) \\\\\n& \\qquad \\text{for all} \\\\\n& \\quad \\quad y_i \\in \\mathcal{Y}, i = 1, \\dots, n.\n\\end{align*}\n\\tag{A.15}\\]\n\nIn the case of \\(n\\) iid continuous random variables \\(Y_1, \\dots, Y_n\\) whose common standalone PDF is \\(f_Y(y | \\boldsymbol{\\theta})\\) with support \\(\\mathcal{Y}\\), the joint PDF is mathematically expressed as\n\n\\[\n\\begin{align*}\nf_{Y_1, \\dots, Y_n}(y_1, \\dots, y_n | \\boldsymbol{\\theta}) &= \\prod_{i = 1}^n f_Y(y_i | \\boldsymbol{\\theta}) \\\\\n& \\qquad \\text{for all} \\\\\n& \\quad \\quad y_i \\in \\mathcal{Y}, i = 1, \\dots, n.\n\\end{align*}\n\\tag{A.16}\\]\nUnlike Equation A.9 and Equation A.10, note that Equation A.15 and Equation A.16 do not indicate a subscript for \\(Y\\) in the corresponding probability distributions since we have identically distributed random variables. Furthermore, the joint distributions are conditioned on the population parameter vector \\(\\boldsymbol{\\theta}\\) which reflects our generative modelling approach.\n\n\nSomewhat equivalent to:\n\n\nTraining dataset.\n\n\nRandom variable\nA random variable is a function where the input values correspond to real numbers assigned to events belonging to the sample space \\(S\\), and whose outcome is one of these real numbers after executing a given random experiment. For instance, a random variable (and its support, i.e., real numbers) is depicted with an uppercase such that\n\\[Y \\in \\mathbb{R}.\\]\nRegression analysis\nRegressor\n\n\nEquivalent to:\n\n\nAttribute, covariate, exogeneous variable, explanatory variable, feature, independent variable, input or predictor.\n\n\nResponse variable\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\nEquivalent to:\n\n\nDependent variable, endogeneous variable, outcome, output or target.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#s",
    "href": "book/A-dictionary.html#s",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "S",
    "text": "S\nSample space\nLet \\(A\\) be an event of interest in a random phenomenon in a population or system of interest. The sample space \\(S\\) of event \\(A\\) denotes the set of all the possible random outcomes we might encounter every time we randomly observe \\(A\\) such as we were running some class of experiment.\nNote each of these outcomes has a determined probability associated with them. If we add up all these probabilities, the probability of the sample \\(S\\) will be one, i.e.,\n\\[\nP(S) = 1.\n\\tag{A.17}\\]\nSemiparametric model\nA semiparametric model is a statistical model that incorporates both parametric and nonparametric parts. In the context of linear regression, these parts can be described as follows:\n\nThe parametric part includes the systematic component where \\(k\\) observed regressors \\(x\\) are modelled along with \\(k + 1\\) regression parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_k\\) in a linear combination.\nThe nonparametric part does not impose specific assumptions on one or more modelling components, allowing the observed training dataset to estimate these elements without requiring any probability distributions.\nSignificance level\nThe significance level \\(\\alpha\\) is defined as the conditional probability of rejecting the null hypothesis \\(H_0\\) given that \\(H_0\\) is true. This can be mathematically represented as\n\\[\nP \\left( \\text{Reject $H_0$} | \\text{$H_0$ is true} \\right) = \\alpha.\n\\]\nIn plain words, \\(\\alpha \\in [0, 1]\\) allows us to probabilistically control for type I error since we are dealing with random variables in our inferential process. The significance level can be thought as one of the main hypothesis testing and power analysis settings.\nStandard error\nThe standard error allows us to quantify the extent to which an estimate coming from an observed random sample (of size \\(n\\), as in \\(y_1, \\dots, y_n\\)) may deviate from the expected value under the assumption that the null hypothesis is true.\nIt plays a critical role in determining whether an observed effect is likely attributable to random variation or represents a statistically significant finding. In the absence of the standard error, it would not be possible to rigorously assess the reliability or precision of an estimate.\nSupervised learning\nSurvival analysis",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#t",
    "href": "book/A-dictionary.html#t",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "T",
    "text": "T\nTarget\nIn supervised learning, it is the main variable of interest we are trying to learn or predict, or equivalently, the variable we are trying explain in a statistical inference framework.\n\n\nEquivalent to:\n\n\nDependent variable, endogeneous variable, response variable, outcome or output.\n\n\nTest statistic\nThe test statistic is a function of the random sample of size \\(n\\), i.e., it is in the function of the random variables \\(Y_1, \\dots, Y_n\\). Therefore, the test statistic will also be a random variable, whose observed value will describe how closely the probability distribution from which the random sample comes from matches the probability distribution of the null hypothesis \\(H_0\\).\nMore specifically, once we have obtained the observed effect and standard error from our observed random sample, we can compute the corresponding observed test statistic. This test statistic computation will be placed on the corresponding \\(x\\)-axis of the probability distribution of \\(H_0\\) so we can reject or fail to reject it accordingly.\nTraining dataset\n\n\nSomewhat equivalent to:\n\n\nRandom sample.\n\n\nTrue positive rate\nThe statistical true positive rate of a test \\(1 -\\beta\\) is the complement of the conditional probability \\(\\beta\\) of failing to reject the null hypothesis \\(H_0\\) given that \\(H_0\\) is false, which is mathematically represented as\n\\[\nP \\left( \\text{Failing to reject $H_0$} | \\text{$H_0$ is false} \\right) = \\beta;\n\\]\nyielding\n\\[\n\\text{Power} = 1 - \\beta.\n\\]\nIn plain words, \\(1 - \\beta \\in [0, 1]\\) is the probabilistic ability of our hypothesis testing to detect any signal in our inferential process, if there is any. The larger the true positive rate in our power analysis, the less prone we are to commit a type II error.\n\n\nEquivalent to:\n\n\nPower.\n\n\nType I error\nType I error is defined as incorrectly rejecting the null hypothesis \\(H_0\\) in favour of the alternative hypothesis \\(H_1\\) when, in fact, \\(H_0\\) is true.\n\n\nEquivalent to:\n\n\nFalse positive.\n\n\nType II error\nType II error is defined as incorrectly failing to reject the null hypothesis \\(H_0\\) in favour of the alternative hypothesis \\(H_1\\) when, in fact, \\(H_0\\) is false. Table A.2 summarizes the types of inferential conclusions in function on whether \\(H_0\\) is true or not.\n\n\nTable A.2: Types of inferential conclusions in a frequentist hypothesis testing.\n\n\n\n\n\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\nReject \\(H_0\\)\nType I error (False positive)\nCorrect\n\n\nFail to reject \\(H_0\\)\nCorrect\nType II error (False negative)\n\n\n\n\n\n\n\n\nEquivalent to:\n\n\nFalse negative.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#u",
    "href": "book/A-dictionary.html#u",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "U",
    "text": "U\nUnderdispersion",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/A-dictionary.html#v",
    "href": "book/A-dictionary.html#v",
    "title": "Appendix A — ML-Stats Dictionary",
    "section": "V",
    "text": "V\nVariance\nLet \\(Y\\) be a discrete or continuous random variable whose support is \\(\\mathcal{Y}\\) with a mean represented by \\(\\mathbb{E}(Y)\\). Then, the variance of \\(Y\\) is the mean of the squared deviation from the corresponding mean as follows:\n\\[\n\\text{Var}(Y) = \\mathbb{E}\\left\\{[ Y - \\mathbb{E}(Y)]^2 \\right\\}. \\\\\n\\]\nNote the expression above is equivalent to:\n\\[\n\\text{Var}(Y) = \\mathbb{E}(Y^2) - \\left[ \\mathbb{E}(Y) \\right]^2.\n\\] Finally, to put the spread measurement on the same units of random variable \\(Y\\), the standard devation of \\(Y\\) is merely the square root of \\(\\text{Var}(Y)\\):\n\\[\n\\text{sd}(Y) = \\sqrt{\\text{Var}(Y)}.\n\\]\n\n\n\n\nCasella, G., and R. Berger. 2024. Statistical Inference. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://www.taylorfrancis.com/books/mono/10.1201/9781003456285/statistical-inference-roger-berger-george-casella.\n\n\nNelder, J. A., and R. W. M. Wedderburn. 1972. “Generalized Linear Models.” Journal of the Royal Statistical Society, Series a (General) 135 (3): 370–84.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>ML-Stats Dictionary</span>"
    ]
  },
  {
    "objectID": "book/B-greek-alphabet.html",
    "href": "book/B-greek-alphabet.html",
    "title": "Appendix B — Greek Alphabet",
    "section": "",
    "text": "Statistical notation can be pretty particular and different from usual mathematical notation. One of these particularities is the constant use of Greek letters to denote unknown population parameters in modelling setup, estimation, and statistical inference. In that spirit, throughout this book, we use diverse Greek letters to denote our regression parameters across each of the outlined models in every chapter.\n\n\n\nImage by meineresterampe via Pixabay.\n\n\nDuring early learning stages of regression modelling, we may feel overwhelmed by these new letters, which could be unfamiliar. Therefore, whenever confusion arises in any of the main chapters in this book regarding the names of these letters, we recommend checking out the Greek alphabet from Table B.1. Note that frequentist statistical inference mostly uses lowercase letters. With practice over time, you would likely end up memorizing most of this alphabet.\n\n\n\nTable B.1: Greek alphabet composed of 24 letters, from left to right you can find the name of letter along with its corresponding uppercase and lowercase forms.\n\n\n\n\n\nName\nUppercase\nLowercase\n\n\n\n\nAlpha\n\\(\\text{A}\\)\n\\(\\alpha\\)\n\n\nBeta\n\\(\\text{B}\\)\n\\(\\beta\\)\n\n\nGamma\n\\(\\Gamma\\)\n\\(\\gamma\\)\n\n\nDelta\n\\(\\Delta\\)\n\\(\\delta\\)\n\n\nEpsilon\n\\(\\text{E}\\)\n\\(\\epsilon\\)\n\n\nZeta\n\\(\\text{Z}\\)\n\\(\\zeta\\)\n\n\nEta\n\\(\\text{H}\\)\n\\(\\eta\\)\n\n\nTheta\n\\(\\Theta\\)\n\\(\\theta\\)\n\n\nIota\n\\(\\text{I}\\)\n\\(\\iota\\)\n\n\nKappa\n\\(\\text{K}\\)\n\\(\\kappa\\)\n\n\nLambda\n\\(\\Lambda\\)\n\\(\\lambda\\)\n\n\nMu\n\\(\\text{M}\\)\n\\(\\mu\\)\n\n\nNu\n\\(\\text{N}\\)\n\\(\\nu\\)\n\n\nXi\n\\(\\Xi\\)\n\\(\\xi\\)\n\n\nO\n\\(\\text{O}\\)\n\\(\\text{o}\\)\n\n\nPi\n\\(\\Pi\\)\n\\(\\pi\\)\n\n\nRho\n\\(\\text{P}\\)\n\\(\\rho\\)\n\n\nSigma\n\\(\\Sigma\\)\n\\(\\sigma\\)\n\n\nTau\n\\(\\text{T}\\)\n\\(\\tau\\)\n\n\nUpsilon\n\\(\\Upsilon\\)\n\\(\\upsilon\\)\n\n\nPhi\n\\(\\Phi\\)\n\\(\\phi\\)\n\n\nChi\n\\(\\text{X}\\)\n\\(\\chi\\)\n\n\nPsi\n\\(\\Psi\\)\n\\(\\psi\\)\n\n\nOmega\n\\(\\Omega\\)\n\\(\\omega\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Greek Alphabet</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html",
    "href": "book/C-distributional-mind-map.html",
    "title": "Appendix C — Distributional Mind Map",
    "section": "",
    "text": "D Discrete Random Variables\nLet us recall what a discrete random variable is. This type of variable is defined to take on a set of countable possible values. In other words, these values belong to a finite set. Figure C.1 delves into the following specific probability distributions:\nTable D.1 outlines the parameter(s), support, mean, and variance for each discrete probability distribution utilized to model the target \\(Y\\) in a specific regression tool explained in this book.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-bernoulli-distribution",
    "href": "book/C-distributional-mind-map.html#sec-bernoulli-distribution",
    "title": "Appendix C — Distributional Mind Map",
    "section": "\nD.1 Bernoulli",
    "text": "D.1 Bernoulli\nLet \\(Y\\) be a discrete random variable that is part of a random process or system. \\(Y\\) can only take on the following values:\n\\[\ny =\n\\begin{cases}\n1 \\; \\; \\; \\; \\text{if there is a success},\\\\\n0 \\; \\; \\; \\; \\mbox{otherwise}.\n\\end{cases}\n\\tag{D.1}\\]\nNote that the support of \\(Y\\) in Equation D.1 makes it binary with these outcomes: \\(1\\) for success and \\(0\\) for failure. Then, \\(Y\\) is said to have a Bernoulli distribution with parameter \\(\\pi\\):\n\\[\nY \\sim \\text{Bern}(\\pi).\n\\]\n\nD.1.1 Probability Mass Function\nThe PMF of \\(Y\\) is the following:\n\\[\nP_Y \\left( Y = y \\mid \\pi \\right) = \\pi^y (1 - \\pi)^{1 - y} \\quad \\text{for $y \\in \\{ 0, 1 \\}$.}\n\\tag{D.2}\\]\nParameter \\(\\pi \\in [0, 1]\\) refers to the probability of success. We can verify Equation D.2 is a proper probability distribution (i.e., all the standalone probabilities over the support of \\(Y\\) add up to one) given that:\n\nProof. \\[\n\\begin{align*}\n\\sum_{y = 0}^1 P_Y \\left( Y = y \\mid \\pi \\right) &=  \\sum_{y = 0}^1 \\pi^y (1 - \\pi)^{1 - y}  \\\\\n&= \\underbrace{\\pi^0}_{1} (1 - \\pi) + \\pi \\underbrace{(1 - \\pi)^{0}}_{1} \\\\\n&= (1 - \\pi) + \\pi \\\\\n&= 1. \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\nIndeed, the Bernoulli PMF is a proper probability distribution!\n\n\n\nD.1.2 Expected Value\nVia Equation C.3, the expected value or mean of a Bernoulli-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\sum_{y = 0}^1 y P_Y \\left( Y = y \\mid \\pi \\right) \\\\\n&= \\sum_{y = 0}^1 y \\left[ \\pi^y (1 - \\pi)^{1 - y} \\right] \\\\\n&= \\underbrace{(0) \\left[ \\pi^0 (1 - \\pi) \\right]}_{0} + (1) \\left[ \\pi (1 - \\pi)^{0} \\right] \\\\\n&= 0 + \\pi \\\\\n&= \\pi. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\n\nD.1.3 Variance\nVia Equation C.5 and the Equation C.3 of a discrete expected value, the variance of a Bernoulli-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y)\\right]^2 \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - \\pi^2 \\qquad \\text{since $\\mathbb{E}(Y) = \\pi$} \\\\\n&= \\sum_{y = 0}^1 y^2 P_Y \\left( Y = y \\mid \\pi \\right) - \\pi^2 \\qquad \\text{by LOTUS} \\\\\n&= \\left\\{ \\underbrace{(0^2) \\left[ \\pi^0 (1 - \\pi) \\right]}_{0} + \\underbrace{(1^2) \\left[ \\pi (1 - \\pi)^{0} \\right]}_{\\pi} \\right\\} - \\pi^2 \\\\\n&= (0 + \\pi) - \\pi^2 \\\\\n&= \\pi - \\pi^2 \\\\\n&= \\pi (1 - \\pi). \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-binomial-distribution",
    "href": "book/C-distributional-mind-map.html#sec-binomial-distribution",
    "title": "Appendix C — Distributional Mind Map",
    "section": "\nD.2 Binomial",
    "text": "D.2 Binomial\nSuppose you execute \\(n\\) independent Bernoulli trials, each one with a probability of success \\(\\pi\\). Let \\(Y\\) be the number of successes obtained within these \\(n\\) Bernoulli trials. Then, \\(Y\\) is said to have a Binomial distribution with parameters \\(n\\) and \\(\\pi\\):\n\\[\nY \\sim \\text{Bin}(n, \\pi).\n\\]\n\nD.2.1 Probability Mass Function\nThe PMF of \\(Y\\) is the following:\n\\[\n\\begin{align*}\nP_Y \\left( Y = y \\mid n, \\pi \\right) &= {n \\choose y} \\pi^y (1 - \\pi)^{n - y} \\\\\n& \\qquad \\qquad \\qquad \\text{for $y \\in \\{ 0, 1, \\dots, n \\}$.}\n\\end{align*}\n\\tag{D.3}\\]\nParameter \\(\\pi \\in [0, 1]\\) refers to the probability of success of each Bernoulli trial and \\(n \\in \\mathbb{N}\\) to the number of trials. On the other hand, the term \\({n \\choose y}\\) indicates the total number of possible combinations for \\(y\\) successes out of our \\(n\\) trials:\n\\[\n{n \\choose y} = \\frac{n!}{y!(n - y)!}.\n\\tag{D.4}\\]\n\nHow can we verify that Equation D.3 is a proper PMF (i.e., all the standalone probabilities over the support of \\(Y\\) add up to one)?\n\nTo elaborate on this, we need to use a handy mathematical result called the binomial theorem.\n\nTheorem D.1 (Binomial Theorem) This theorem is associated to the Pascal’s identity, and it defines the pattern of coefficients in the expansion of a polynomial in the form \\((u + v)^m\\). More specifically, the binomial theorem indicates that if \\(m\\) is a non-negative integer, then the polynomial \\((u + v)^m\\) can be expanded via the following series:\n\\[\n\\begin{align*}\n(u + v)^m &= u^m + {m \\choose 1} u^{m - 1} v + {m \\choose 2} u^{m - 2} v^2 + \\dots + \\\\\n& \\qquad {m \\choose r} u^{m - r} v^r + \\dots + \\\\\n& \\qquad {m \\choose m - 1} u v^{m - 1} + v^m \\\\\n&= \\underbrace{{m \\choose 0}}_1 u^m + {m \\choose 1} u^{m - 1} v + {m \\choose 2} u^{m - 2} v^2 + \\dots + \\\\\n& \\qquad {m \\choose r} u^{m - r} v^r + \\dots + \\\\\n& \\qquad {m \\choose m - 1} u v^{m - 1} + \\underbrace{{m \\choose m}}_1 v^m \\\\\n&= \\sum_{i = 0}^m {m \\choose i} u^{m - i} v^i.\n\\end{align*}\n\\tag{D.5}\\]\n\n\n\nTip on the binomial theorem and Pascal’s identity\n\n\nLet us dig into the proof of the binomial theorem from Equation D.5. This proof will require another important result called the Pascal’s identity. This identity states that for any integers \\(m\\) and \\(k\\), with \\(k \\in \\{ 1, \\dots, m \\}\\), it follows that:\n\nProof. \\[\n\\begin{align*}\n{m \\choose k - 1} + {m \\choose k} &= \\left[ \\frac{m!}{(k - 1)! (m - k + 1)!} \\right] \\\\\n& \\qquad + \\left[ \\frac{m!}{k! (m - k)!} \\right] \\\\\n&= m! \\biggl\\{ \\left[ \\frac{1}{(k - 1)! (m - k + 1)!} \\right] + \\\\\n& \\qquad \\left[ \\frac{1}{k! (m - k)!} \\right] \\biggl\\} \\\\\n&= m! \\Biggl\\{ \\Biggr[ \\frac{k}{\\underbrace{k (k - 1)!}_{k!} (m - k + 1)!} \\Biggr] + \\\\\n& \\qquad \\Biggr[ \\frac{m - k + 1}{k! \\underbrace{(m - k + 1)(m - k)!}_{(m - k + 1)!}} \\Biggr] \\Biggl\\}  \\\\\n&= m! \\left[ \\frac{k + m - k + 1}{k! (m - k + 1)!} \\right] \\\\\n&= m! \\left[ \\frac{m + 1}{k! (m - k + 1)!} \\right] \\\\\n&= \\frac{(m + 1)!}{k! (m + 1 - k)!} \\\\\n&= {m + 1 \\choose k }. \\qquad \\qquad \\qquad \\qquad \\square\n\\end{align*}\n\\tag{D.6}\\]\n\n\nProof. Now, we will use mathematical induction to prove the binomial theorem from Equation D.5. Firstly, on the left-hand side of the theorem, note that when \\(m = 0\\) we have:\n\\[\n(u + v)^0 = 1.\n\\]\nNow, when \\(m = 0\\), for the right-hand side of this equation, we have that\n\\[\n\\sum_{i = 0}^m {m \\choose i} u^{m - i} v^i  = \\sum_{i = 0}^0 {0 \\choose i} u^i v^{i} = {0 \\choose 0} u^0 v^0 = 1.\n\\]\nHence, the binomial theorem holds when \\(m = 0\\). This is what we call the base case in mathematical induction.\nThat said, let us proceed with the inductive hypothesis. We aim to prove that the binomial theorem\n\\[\n\\begin{align*}\n(u + v)^j &= u^j + {j \\choose 1} u^{j - 1} v + {j \\choose 2} u^{j - 2} v^2 + \\dots + \\\\\n& \\qquad {j \\choose r} u^{j - r} v^r + \\dots + \\\\\n& \\qquad {j \\choose j - 1} u v^{j - 1} + v^j \\\\\n&= \\underbrace{{j \\choose 0}}_1 u^j + {j \\choose 1} u^{j - 1} v + {j \\choose 2} u^{j - 2} v^2 + \\dots + \\\\\n& \\qquad {j \\choose r} u^{j - r} v^r + \\dots + \\\\\n& \\qquad {j \\choose j - 1} u v^{j - 1} + \\underbrace{{j \\choose j}}_1 v^j \\\\\n&= \\sum_{i = 0}^j {j \\choose i} u^{j - i} v^i\n\\end{align*}\n\\tag{D.7}\\]\nholds when integer \\(j \\geq 1\\). This is our inductive hypothesis.\nThen, we pave the way to the inductive step. Let us consider the following expansion:\n\\[\n\\begin{align*}\n(u + v)^{j + 1} &= (u + v) (u + v)^j \\\\\n&= (u + v) \\times \\\\\n& \\qquad \\bigg[ u^j + {j \\choose 1} u^{j - 1} v + {j \\choose 2} u^{j - 2} v^2 + \\dots + \\\\\n& \\qquad {j \\choose r} u^{j - r} v^r + \\dots + {j \\choose j - 1} u v^{j - 1} + v^j \\bigg] \\\\\n&= \\bigg[u^{j + 1} + {j \\choose 1} u^j v + {j \\choose 2} u^{j - 1} v^2 + \\dots + \\\\\n& \\qquad {j \\choose r} u^{j - r + 1} v^r + \\dots + \\\\\n& \\qquad {j \\choose j - 1} u^2 v^{j - 1} + u v^j \\bigg] + \\\\\n& \\qquad \\bigg[ u^j v + {j \\choose 1} u^{j - 1} v^2 + {j \\choose 2} u^{j - 2} v^3 + \\dots + \\\\\n& \\qquad {j \\choose r} u^{j - r} v^{r + 1} + \\dots + \\\\\n& \\qquad {j \\choose j - 1} u v^j + {j \\choose j} v^{j + 1} \\bigg] \\\\\n&= u^{j + 1} + \\left[ {j \\choose 0} + {j \\choose 1} \\right] u^j v + \\\\\n& \\qquad \\left[ {j \\choose 1} + {j \\choose 2} \\right] u^{j - 1} v^2 + \\dots + \\\\\n& \\qquad \\left[ {j \\choose r - 1} + {j \\choose r} \\right] u^{j - r + 1} v^r + \\dots + \\\\\n& \\qquad \\left[ {j \\choose j - 1} + {j \\choose j} \\right] u v^j + v^{j + 1}.\n\\end{align*}\n\\tag{D.8}\\]\nLet us plug in the Pascal’s identity from Equation D.6 into Equation D.8:\n\\[\n\\begin{align*}\n(u + v)^{j + 1} &= u^{j + 1} + {j + 1 \\choose 1} u^j v + \\\\\n& \\qquad {j + 1 \\choose 2} u^{j - 1} v^2 + \\dots + \\\\\n& \\qquad {j + 1 \\choose r} u^{j - r + 1} v^r + \\dots + \\\\\n& \\qquad {j + 1 \\choose j} u v^j + v^{j + 1} \\\\\n&= \\underbrace{{j + 1 \\choose 0}}_1 u^{j + 1} + {j + 1 \\choose 1} u^j v + \\\\\n& \\qquad {j + 1 \\choose 2} u^{j - 1} v^2 + \\dots + \\\\\n& \\qquad {j + 1 \\choose r} u^{j - r + 1} v^r + \\dots + \\\\\n& \\qquad {j + 1 \\choose j} u v^j + \\underbrace{{j + 1 \\choose j + 1}}_1 v^{j + 1} \\\\\n&= \\sum_{i = 0}^{j + 1} {j + 1 \\choose i} u^{j + 1 - i} v^i. \\qquad \\quad \\square\n\\end{align*}\n\\tag{D.9}\\]\nNote that the result for \\(j\\) in Equation D.7 also holds for \\(j + 1\\) in Equation D.9. Therefore, by induction, the binomial theorem from Equation D.5 is true for all positive integers \\(m\\).\n\n\n\nAfter the above fruitful digression on the binomial theorem, let us use it to show that our Binomial PMF in Equation D.3 actually adds up to one all over the support of the random variable:\n\nProof. \\[\n\\begin{align*}\n\\sum_{y = 0}^n P_Y \\left( Y = y \\mid n, \\pi \\right) &= \\sum_{y = 0}^n {n \\choose y} \\pi^y (1 - \\pi)^{n - y} \\\\\n&= \\sum_{y = 0}^n {n \\choose y} (1 - \\pi)^{n - y} \\pi^y \\\\\n& \\quad \\qquad \\text{rearranging factors.}\n\\end{align*}\n\\]\nNow, by using the binomial theorem in Equation D.5, let:\n\\[\n\\begin{gather*}\nm  = n\\\\\ni = y \\\\\nu = 1 - \\pi \\\\\nv = \\pi.\n\\end{gather*}\n\\]\nThe above arrangement yields the following result:\n\\[\n\\begin{align*}\n\\sum_{y = 0}^n P_Y \\left( Y = y \\mid n, \\pi \\right) &= (1 - \\pi + \\pi)^n \\\\\n&= 1^n = 1. \\qquad \\square\n\\end{align*}\n\\tag{D.10}\\]\n\nIndeed, the Binomial PMF is a proper probability distribution!\n\n\n\nD.2.2 Expected Value\nVia Equation C.3, the expected value or mean of a Binomial-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\sum_{y = 0}^n y P_Y \\left( Y = y \\mid n, \\pi \\right) \\\\\n&= \\sum_{y = 1}^n y P_Y \\left( Y = y \\mid n, \\pi \\right) \\\\\n& \\quad \\qquad \\text{for $y = 0$, the addend is equal to zero} \\\\\n&= \\sum_{y = 1}^n y \\left[ {n \\choose y} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n&= \\sum_{y = 1}^n y \\left[ \\frac{n!}{y! (n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n&= \\sum_{y = 1}^n \\left[ \\frac{y n!}{y (y - 1)!(n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{in the denominator, $y! = y (y - 1)!$}\\\\\n&= \\sum_{y = 1}^n \\left[ \\frac{n (n - 1)!}{(y - 1)!(n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{in the numerator, $n! = n (n - 1)!$} \\\\\n&= \\sum_{y = 1}^n \\left[ \\frac{n (n - 1)!}{(y - 1)!(n - y)!} \\pi^{y + 1 - 1} (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{note $\\pi^y = \\pi^{y + 1 - 1}$} \\\\\n&= n \\sum_{y = 1}^n \\left[ \\frac{(n - 1)!}{(y - 1)!(n - y)!} \\pi \\pi^{y - 1} (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{rearranging terms} \\\\\n&= n \\pi \\sum_{y = 1}^n \\left[ \\frac{(n - 1)!}{(y - 1)!(n - y)!} \\pi^{y - 1} (1 - \\pi)^{n - y} \\right].\n\\end{align*}\n\\tag{D.11}\\]\nNow, let us make the following variable rearrangement:\n\\[\n\\begin{gather*}\nm = n - 1 \\\\\nz = y - 1 \\\\\nm - z = n - y.\n\\end{gather*}\n\\]\nGoing back to Equation D.11, and applying our above variable rearrangement within the summation, we have:\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= n \\pi \\sum_{z = 0}^m \\left[ \\frac{m!}{z!(m - z)!} \\pi^{z} (1 - \\pi)^{m - z} \\right] \\\\\n&= n \\pi \\sum_{z = 0}^m \\left[ {m \\choose z}\\pi^{z} (1 - \\pi)^{m - z} \\right].\n\\end{align*}\n\\tag{D.12}\\]\nNote that, in the summation of Equation D.12, we encounter the PMF of a random variable \\(Z\\) as follows:\n\\[\nZ \\sim \\text{Bin}(m, \\pi).\n\\]\nSince the summation, where this Binomial PMF of \\(Z\\) is depicted, goes from \\(z = 0\\) to \\(m\\), we can apply our result from Equation D.10:\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= n \\pi \\underbrace{\\sum_{z = 0}^m \\left[ {m \\choose z}\\pi^{z} (1 - \\pi)^{m - z} \\right]}_{1} \\\\\n&= n \\pi. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\n\nD.2.3 Variance\nVia Equation C.5 and the Equation C.3 of a discrete expected value, the variance of a Binomial-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y)\\right]^2 \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - (n \\pi)^2 \\qquad \\text{since $\\mathbb{E}(Y) = n \\pi$.}\n\\end{align*}\n\\tag{D.13}\\]\nUnlike the Bernoulli random variable, finding \\(\\mathbb{E} \\left( Y^2 \\right)\\) is not quite straightforward. We need to play around with the below expected value expression as follows:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\mathbb{E} \\left[ Y (Y - 1) \\right] + \\mathbb{E}(Y) \\\\\n&= \\mathbb{E} \\left[ Y (Y - 1) \\right] + n \\pi \\qquad \\text{since $\\mathbb{E}(Y) = n \\pi$.}\n\\end{align*}\n\\tag{D.14}\\]\nNow, to find \\(\\mathbb{E} \\left[ Y (Y - 1) \\right]\\), we make the following derivation via the LOTUS from Equation C.1 when \\(g(Y) = y (y - 1)\\):\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= \\sum_{y = 0}^n y (y - 1) P_Y \\left( Y = y \\mid n, \\pi \\right) \\\\\n&= \\sum_{y = 2}^n y (y - 1) P_Y \\left( Y = y \\mid n, \\pi \\right) \\\\\n& \\quad \\qquad \\text{for $y = \\{0, 1\\}$,} \\\\\n& \\quad \\qquad \\text{the addends are equal to zero} \\\\\n&= \\sum_{y = 2}^n y (y - 1) \\left[ {n \\choose y} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n&= \\sum_{y = 2}^n y (y - 1) \\left[ \\frac{n!}{y! (n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n&= \\sum_{y = 2}^n \\left[ \\frac{y (y - 1) n!}{y (y - 1) (y - 2)! (n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\\\\n&= \\sum_{y = 2}^n \\left[ \\frac{n (n - 1) (n - 2)!}{(y - 2)! (n - y)!} \\pi^y (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{in the numerator, $n! = n (n - 1) (n - 2)!$} \\\\\n&= \\sum_{y = 2}^n \\left[ \\frac{n (n - 1) (n - 2)!}{(y - 2)! (n - y)!} \\pi^{y + 2 - 2} (1 - \\pi)^{n - y} \\right] \\\\\n& \\quad \\qquad \\text{note $\\pi^y = \\pi^{y + 2 - 2}$} \\\\\n&= n (n - 1) \\times \\\\\n& \\qquad \\sum_{y = 2}^n \\left[ \\frac{(n - 2)!}{(y - 2)! (n - y)!} \\pi^2 \\pi^{y - 2} (1 - \\pi)^{n - y} \\right] \\\\\n& \\qquad \\qquad \\text{rearranging terms} \\\\\n&= n (n - 1) \\pi^2 \\times \\\\\n& \\qquad \\sum_{y = 2}^n \\left[ \\frac{(n - 2)!}{(y - 2)! (n - y)!} \\pi^{y - 2} (1 - \\pi)^{n - y} \\right] \\\\\n& \\qquad \\qquad \\text{rearranging terms.}\n\\end{align*}\n\\tag{D.15}\\]\nThen, we make the following variable rearrangement:\n\\[\n\\begin{gather*}\nm = n - 2 \\\\\nz = y - 2 \\\\\nm - z = n - y.\n\\end{gather*}\n\\]\nGoing back to Equation D.15, and applying our above variable rearrangement within the summation, we have:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= n (n - 1) \\pi^2 \\sum_{z = 0}^m \\left[ \\frac{m!}{z! (m - z)!} \\pi^{z} (1 - \\pi)^{m - z} \\right] \\\\\n&= n (n - 1) \\pi^2 \\sum_{z = 0}^m \\left[ {m \\choose z} \\pi^{z} (1 - \\pi)^{m - z} \\right].\n\\end{align*}\n\\tag{D.16}\\]\nNote that, in the summation of Equation D.16, we encounter the PMF of a random variable \\(Z\\) as follows:\n\\[\nZ \\sim \\text{Bin}(m, \\pi).\n\\]\nSince the summation, where this Binomial PMF of \\(Z\\) is depicted, goes from \\(z = 0\\) to \\(m,\\) we can apply our result from Equation D.10:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= n (n - 1) \\pi^2 \\underbrace{\\sum_{z = 0}^m \\left[ {m \\choose z} \\pi^{z} (1 - \\pi)^{m - z} \\right]}_{1} \\\\\n&= n (n - 1) \\pi^2.\n\\end{align*}\n\\]\nLet us go back to Equation D.14 and plug in the above result:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\mathbb{E} \\left[ Y (Y - 1) \\right] + n \\pi \\\\\n&= n (n - 1) \\pi^2 + n \\pi. \\\\\n\\end{align*}\n\\]\nFinally, we plug in \\(\\mathbb{E} \\left( Y^2 \\right)\\) in Equation D.13:\n\\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - (n \\pi)^2 \\\\\n&= n (n - 1) \\pi^2 + n \\pi - n^2 \\pi^2 \\\\\n&= n^2 \\pi^2 - n \\pi^2 + n \\pi - n^2 \\pi^2 \\\\\n&= n \\pi - n \\pi^2 \\\\\n&= n \\pi (1 - \\pi). \\qquad \\qquad \\qquad \\square\n\\end{align*}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-negative-binomial-distribution",
    "href": "book/C-distributional-mind-map.html#sec-negative-binomial-distribution",
    "title": "Appendix C — Distributional Mind Map",
    "section": "\nD.3 Negative Binomial",
    "text": "D.3 Negative Binomial\nSuppose you execute a series of independent Bernoulli trials, each one with a probability of success \\(\\pi\\). Let \\(Y\\) be the number of failures in this series of Bernoulli trials you obtain before experiencing \\(k\\) successes. Therefore, \\(Y\\) is said to have a Negative Binomial distribution with parameters \\(k\\) and \\(\\pi\\):\n\\[\nY \\sim \\text{NegBin}(k, \\pi).\n\\]\n\nD.3.1 Probability Mass Function\nThe PMF of \\(Y\\) is the following:\n\\[\n\\begin{align*}\nP_Y \\left( Y = y \\mid k, \\pi \\right) &= {k + y - 1 \\choose y} \\pi^k (1 - \\pi)^y \\\\\n& \\qquad \\qquad \\qquad \\quad \\text{for $y \\in \\{ 0, 1, \\dots \\}$.}\n\\end{align*}\n\\tag{D.17}\\]\nParameter \\(\\pi \\in [0, 1]\\) refers to the probability of success of each Bernoulli trial, whereas \\(k\\) refers to the number of successes.\n\n\nTip on an alternative Negative Binomial PMF!\n\n\nThere is an alternative parametrization to define a Negative Binomial distribution in which we have a random variable \\(Z\\) defined as the total number of Bernoulli trials (i.e., \\(k\\) successes plus the \\(Y\\) failures depicted in Equation D.17):\n\\[\nZ = Y + k.\n\\]\nThis alternative parametrization of the Negative Binomial distribution yields the following PMF:\n\\[\n\\begin{align*}\nP_Z \\left( Z = z \\mid k, \\pi \\right) &= {z - 1 \\choose k - 1} \\pi^k (1 - \\pi)^{z - k} \\\\\n& \\qquad \\qquad \\qquad \\text{for $z \\in \\{ k, k + 1, \\dots \\}$.}\n\\end{align*}\n\\]\nNevertheless, we will not dig into this version of the Negative Binomial distribution since Chapter 11 delves into a modelling estimation via a joint PMF of the training set involving Equation D.17.\n\n\n\nHow can we verify that Equation D.17 is a proper PMF (i.e., all the standalone probabilities over the support of \\(Y\\) add up to one)?\n\n\nProof. Let us manipulate the factor involving the number of combinations corresponding to how many different possible subsets of size \\(y\\) can be made from the larger set of size \\(k + y - 1\\):\n\\[\n\\begin{align*}\n{k + y - 1 \\choose y} &= \\frac{(k + y - 1)!}{(k + y - 1 - y)! y !} \\\\\n&= \\frac{(k + y - 1)!}{(k - 1)! y!} \\\\\n&= \\frac{(k + y - 1) (k + y - 2) \\cdots (k + 1) (k) (k - 1)!}{(k - 1)! y!} \\\\\n&= \\frac{(\\overbrace{k + y - 1) (k + y - 2) \\cdots (k + 1) k}^{\\text{we have $y$ factors}}}{y!} \\\\\n&= (- 1)^y \\frac{\\overbrace{(-k - y + 1) (-k - y + 2) \\cdots (-k - 1) (-k)}^{\\text{multiplying each factor times $-1$}}}{y!} \\\\\n&= (- 1)^y \\frac{\\overbrace{(-k) (-k - 1) \\cdots (-k - y + 2) (-k - y + 1)}^{\\text{rearranging factors}}}{y!} \\\\\n&= (- 1)^y \\frac{(-k) (-k - 1) \\cdots (-k - y + 2) (-k - y + 1)}{y!} \\times \\\\\n& \\qquad \\frac{(-k - y) (-k - y - 1) \\cdots (1)}{(-k - y) (-k - y - 1) \\cdots (1)} \\\\\n&= (- 1)^y \\frac{(-k) (-k - 1) \\cdots (-k - y + 2) (-k - y + 1)}{y!} \\times \\\\\n& \\qquad \\frac{(-k - y) (-k - y - 1) \\cdots (1)}{(-k - y)!}.\n\\end{align*}\n\\]\nIn the equation above, note that there are still several factors in the numerator, which can be summarized using a factorial as follows:\n\\[\n\\begin{align*}\n(-k)! &= (-k) (-k - 1) \\cdots (-k - y + 2) (-k - y + 1) \\times \\\\\n& \\quad \\qquad (-k - y) (-k - y - 1) \\cdots (1).\n\\end{align*}\n\\]\nTherefore:\n\\[\n\\begin{align*}\n{k + y - 1 \\choose y} &= (- 1)^y \\frac{(-k)!}{(-k - y)! y!}\\\\\n&= (- 1)^y {-k \\choose y}.\n\\end{align*}\n\\]\nNow, let us begin with the summation involving the Negative Binomial PMF depicted in Equation D.17 from \\(0\\) to \\(\\infty\\):\n\\[\n\\begin{align*}\n\\sum_{y = 0}^{\\infty} P_Y \\left( Y = y \\mid k, \\pi \\right) &= \\sum_{y = 0}^{\\infty} {k + y - 1 \\choose y} \\pi^k (1 - \\pi)^y \\\\\n&= \\sum_{y = 0}^{\\infty} (- 1)^y {-k \\choose y} \\pi^k (1 - \\pi)^y \\\\\n&= \\pi^k \\sum_{y = 0}^{\\infty} (- 1)^y {-k \\choose y} (1 - \\pi)^y \\\\\n&= \\pi^k \\sum_{y = 0}^{\\infty} {-k \\choose y} (-1 + \\pi)^y.\n\\end{align*}\n\\tag{D.18}\\]\nOn the right-hand side of Equation D.18 we will add the following factor:\n\\[\n(1)^{-k - y} = 1.\n\\]\nThus:\n\\[\n\\begin{align*}\n\\sum_{y = 0}^{\\infty} P_Y \\left( Y = y \\mid k, \\pi \\right) &= \\pi^k \\sum_{y = 0}^{\\infty} {-k \\choose y} (1)^{-k - y} (-1 + \\pi)^y.\n\\end{align*}\n\\tag{D.19}\\]\nNow, by using the binomial theorem in Equation D.5, let:\n\\[\n\\begin{gather*}\nm  = -k\\\\\ni = y \\\\\nu = 1 \\\\\nv = -1 + \\pi.\n\\end{gather*}\n\\]\nThe above arrangement yields the following result in Equation D.19:\n\\[\n\\begin{align*}\n\\sum_{y = 0}^{\\infty} P_Y \\left( Y = y \\mid k, \\pi \\right) &=  \\pi^k (1 - 1 + \\pi)^{-k} \\\\\n&= \\pi^k (\\pi) ^{-k} \\\\\n&= \\pi^0 \\\\\n&= 1. \\qquad \\qquad \\qquad \\square\n\\end{align*}\n\\tag{D.20}\\]\n\nIndeed, the Negative Binomial PMF is a proper probability distribution!\n\n\n\nD.3.2 Expected Value\nVia Equation C.3, the expected value or mean of a Negative Binomial-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\sum_{y = 0}^{\\infty} y P_Y \\left( Y = y \\mid k, \\pi \\right) \\\\\n&= \\sum_{y = 1}^{\\infty} y P_Y \\left( Y = y \\mid k, \\pi \\right) \\\\\n& \\quad \\qquad \\text{for $y = 0$, the addend is equal to zero} \\\\\n&= \\sum_{y = 1}^{\\infty} y \\left[ {k + y - 1 \\choose y} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= \\sum_{y = 1}^{\\infty} y \\left[ \\frac{(k + y - 1)!}{y! (k + y - 1 - y)!} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= \\sum_{y = 1}^{\\infty} y \\left[ \\frac{(k + y - 1)!}{y! (k - 1)!} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= \\sum_{y = 1}^{\\infty} y \\Bigg[ \\frac{(k + y - 1)!}{y (y - 1)! \\underbrace{\\left( \\frac{k!}{k} \\right)}_{(k - 1)!}} \\pi^k (1 - \\pi)^y \\Bigg] \\\\\n&= \\sum_{y = 1}^{\\infty} k \\left[ \\frac{(k + y - 1)!}{k! (y - 1)!} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= k \\sum_{y = 1}^{\\infty} \\left[ {k + y - 1 \\choose y - 1} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= k \\sum_{y = 1}^{\\infty} \\left[ {k + y - 1 \\choose y - 1} \\pi^{k + 1 - 1} (1 - \\pi)^{y + 1 - 1} \\right] \\\\\n& \\quad \\qquad \\text{note $\\pi^k = \\pi^{k + 1 - 1}$ and $(1 - \\pi)^y = (1 - \\pi)^{y + 1 - 1}$} \\\\\n&= \\frac{k (1 - \\pi)}{\\pi} \\sum_{y = 1}^{\\infty} \\left[ {k + y - 1 \\choose y - 1} \\pi^{k + 1} (1 - \\pi)^{y - 1} \\right].\n\\end{align*}\n\\tag{D.21}\\]\nNow, let us make the following variable rearrangement:\n\\[\n\\begin{gather*}\nm = k + 1 \\\\\nz = y - 1 \\\\\nm + z - 1  = k + y - 1.\n\\end{gather*}\n\\]\nGoing back to Equation D.21, and applying our above variable rearrangement within the summation, we have:\n\\[\n\\mathbb{E}(Y) = \\frac{k (1 - \\pi)}{\\pi} \\sum_{z = 0}^{\\infty} \\left[ {m + z - 1 \\choose z} \\pi^{m} (1 - \\pi)^{z} \\right].\n\\tag{D.22}\\]\nNote that, in the summation of Equation D.22, we encounter the PMF of a random variable \\(Z\\) as follows:\n\\[\nZ \\sim \\text{NegBin}(m, \\pi).\n\\]\nSince the summation, where this Negative Binomial PMF of \\(Z\\) is depicted, goes from \\(z = 0\\) to \\(\\infty\\), we can apply our result from Equation D.20:\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\frac{k (1 - \\pi)}{\\pi} \\underbrace{\\sum_{z = 0}^m \\left[ {m + z - 1 \\choose z} \\pi^{m} (1 - \\pi)^{z} \\right]}_{1} \\\\\n&= \\frac{k (1 - \\pi)}{\\pi}. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\n\nD.3.3 Variance\nVia Equation C.5 and the Equation C.3 of a discrete expected value, the variance of a Negative Binomial-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y)\\right]^2 \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\frac{k (1 - \\pi)}{\\pi} \\right]^2 \\quad \\text{since $\\mathbb{E}(Y) = \\frac{k (1 - \\pi)}{\\pi}$.}\n\\end{align*}\n\\tag{D.23}\\]\nNow, we need to play around with the below expected value expression as follows:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\mathbb{E} \\left[ Y (Y - 1) \\right] + \\mathbb{E}(Y) \\\\\n&= \\mathbb{E} \\left[ Y (Y - 1) \\right] + \\frac{k (1 - \\pi)}{\\pi}.\n\\end{align*}\n\\tag{D.24}\\]\nTo find \\(\\mathbb{E} \\left[ Y (Y - 1) \\right]\\), we make the following derivation via the LOTUS from Equation C.1 when \\(g(Y) = y (y - 1)\\):\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= \\sum_{y = 0}^{\\infty} y (y - 1) P_Y \\left( Y = y \\mid k, \\pi \\right) \\\\\n&= \\sum_{y = 2}^{\\infty} y (y - 1) P_Y \\left( Y = y \\mid k, \\pi \\right) \\\\\n& \\quad \\qquad \\text{for $y = \\{0, 1\\}$,} \\\\\n& \\quad \\qquad \\text{the addends are equal to zero} \\\\\n&= \\sum_{y = 2}^{\\infty} y (y - 1) \\left[ {k + y - 1 \\choose y} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= \\sum_{y = 2}^{\\infty} y (y - 1) \\left[ \\frac{(k + y - 1)!}{y! (k + y - 1 - y)!} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= \\sum_{y = 2}^{\\infty} y (y - 1) \\left[ \\frac{(k + y - 1)!}{y! (k - 1)!} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= \\sum_{y = 2}^{\\infty} \\frac{y (y - 1)}{y (y - 1)} \\left[ \\frac{(k + y - 1)!}{(y - 2)! (k - 1)!} \\pi^k (1 - \\pi)^y \\right] \\\\\n& \\quad \\qquad \\text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\\\\n&= \\sum_{y = 2}^{\\infty} \\Bigg[ \\frac{(k + y - 1)!}{(y - 2)! \\underbrace{\\frac{(k + 1)!}{k (k + 1)}}_{(k - 1)!}} \\pi^k (1 - \\pi)^y \\Bigg] \\\\\n&= \\sum_{y = 2}^{\\infty} \\left[ k (k + 1) \\frac{(k + y - 1)!}{(k + 1)! (y - 2)!} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= k (k + 1) \\sum_{y = 2}^{\\infty} \\left[ {k + y - 1 \\choose y - 2} \\pi^k (1 - \\pi)^y \\right] \\\\\n&= k (k + 1) \\sum_{y = 2}^{\\infty} \\left[ {k + y - 1 \\choose y - 2} \\pi^{k + 2 - 2} (1 - \\pi)^{y + 2 - 2} \\right] \\\\\n& \\quad \\qquad \\text{note $\\pi^k = \\pi^{k + 2 - 2}$ and} \\\\\n& \\quad \\qquad (1 - \\pi)^y = (1 - \\pi)^{y + 2 - 2} \\\\\n&= \\frac{k (k + 1) ( 1 - \\pi)^2}{\\pi^2} \\times \\\\\n& \\qquad \\sum_{y = 2}^{\\infty} \\left[ {k + y - 1 \\choose y - 2} \\pi^{k + 2} (1 - \\pi)^{y - 2} \\right].\n\\end{align*}\n\\tag{D.25}\\]\nThen, we make the following variable rearrangement:\n\\[\n\\begin{gather*}\nm = k + 2\\\\\nz = y - 2 \\\\\nm + z - 1  = k + y - 1.\n\\end{gather*}\n\\]\nGoing back to Equation D.25, and applying our above variable rearrangement within the summation, we have:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= \\frac{k (k + 1) ( 1 - \\pi)^2}{\\pi^2} \\times \\\\\n& \\qquad \\sum_{y = 2}^{\\infty} \\left[ {m + z - 1 \\choose z} \\pi^m (1 - \\pi)^z \\right].\n\\end{align*}\n\\tag{D.26}\\]\nNote that, in the summation of Equation D.26, we encounter the PMF of a random variable \\(Z\\) as follows:\n\\[\nZ \\sim \\text{NegBin}(m, \\pi).\n\\]\nSince the summation, where this Binomial PMF of \\(Z\\) is depicted, goes from \\(z = 0\\) to \\(\\infty\\), we can apply our result from Equation D.20:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= \\frac{k (k + 1) ( 1 - \\pi)^2}{\\pi^2} \\times \\\\\n& \\qquad \\underbrace{\\sum_{y = 2}^{\\infty} \\left[ {m + z - 1 \\choose z} \\pi^m (1 - \\pi)^z \\right]}_{1} \\\\\n&= \\frac{k (k + 1) ( 1 - \\pi)^2}{\\pi^2}.\n\\end{align*}\n\\]\nLet us go back to Equation D.24 and plug in the above result:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\mathbb{E} \\left[ Y (Y - 1) \\right] + \\frac{k ( 1 - \\pi)}{\\pi} \\\\\n&= \\frac{k (k + 1) ( 1 - \\pi)^2}{\\pi^2} + \\frac{k ( 1 - \\pi)}{\\pi}.\n\\end{align*}\n\\]\nFinally, we plug in \\(\\mathbb{E} \\left( Y^2 \\right)\\) in Equation D.23:\n\\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\frac{k (1 - \\pi)}{\\pi} \\right]^2 \\\\\n&= \\frac{k (k + 1) ( 1 - \\pi)^2}{\\pi^2} + \\frac{k ( 1 - \\pi)}{\\pi} - \\left[ \\frac{k (1 - \\pi)}{\\pi} \\right]^2 \\\\\n&= \\frac{k (1 - \\pi)}{\\pi} \\left[ \\frac{(k + 1) (1 - \\pi)}{\\pi} + 1 - \\frac{k (1 - \\pi)}{\\pi} \\right] \\\\\n&= \\frac{k (1 - \\pi)}{\\pi} \\left[ \\frac{(k + 1) (1 - \\pi) + \\pi - k (1 - \\pi)}{\\pi} \\right] \\\\\n&= \\frac{k (1 - \\pi)}{\\pi} \\left( \\frac{k - k \\pi + 1 - \\pi + \\pi - k + k \\pi}{\\pi} \\right) \\\\\n&= \\frac{k (1 - \\pi)}{\\pi} \\left( \\frac{1}{\\pi} \\right) \\\\\n&= \\frac{k (1 - \\pi)}{\\pi^2}. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-classical-poisson-distribution",
    "href": "book/C-distributional-mind-map.html#sec-classical-poisson-distribution",
    "title": "Appendix C — Distributional Mind Map",
    "section": "\nD.4 Classical Poisson",
    "text": "D.4 Classical Poisson\nSuppose you observe the count of events happening in a fixed interval of time or space. Let \\(Y\\) be the number of counts considered of integer type. Then, \\(Y\\) is said to have a classical Poisson distribution with a continuous parameter \\(\\lambda\\):\n\\[\nY \\sim \\text{Pois}(\\lambda).\n\\]\n\nD.4.1 Probability Mass Function\nThe PMF of this count-type \\(Y\\) is the following:\n\\[\nP_Y \\left( Y = y \\mid \\lambda \\right) = \\frac{\\lambda^y \\exp{(-\\lambda)}}{y!} \\quad \\text{for $y \\in \\{ 0, 1, 2, \\dots\\}$,}\n\\tag{D.27}\\]\nwhere \\(\\exp{(\\cdot)}\\) depicts the base \\(e\\) (i.e., Euler’s number, \\(e = 2.71828...\\)) and \\(y!\\) is the factorial\n\\[\ny! = y \\times (y - 1) \\times (y - 2) \\times (y - 3) \\times \\cdots \\times 3 \\times 2 \\times 1.  \n\\]\nwith\n\\[\n0! = 1.\n\\]\nThe continuous parameter \\(\\lambda \\in (0, \\infty)\\) represents the average rate at which these events happen (i.e., events per area unit or events per time unit). Curiously, even though the random variable \\(Y\\) is considered discrete in this case, \\(\\lambda\\) is modelled as continuous!\n\nHow can we verify that Equation D.27 is a proper PMF (i.e., all the standalone probabilities over the support of \\(Y\\) add up to one)?\n\nTo elaborate on this, we need to use some mathematical tools called the Taylor series expansions and a derived result called Maclaurin series expansions.\n\n\nHeads-up on the Taylor and Maclaurin series expansions!\n\n\nIn mathematics, there are helpful tools known as Taylor series expansions, which were officially published by English mathematician Brook Taylor in Methodus Incrementorum Directa & Inversa (Taylor 1715).\n\n\nPortrait of mathematician Brook Taylor (Earlom 1793).\n\nHowever, it is essential to note that Scottish mathematician James Gregory introduced the notion of these series expansions in his work Vera Circuli et Hyperbolae Quadratura (Gregory 1668).\n\n\nPortrait of mathematician James Gregory (Scotland, n.d.).\n\nThese series approximate complex mathematical functions through an infinite sum of polynomial terms. For example, in machine learning, the Taylor series expansions can be utilized in gradient-based optimization methods. Specifically, Newton’s method uses these expansions to find roots of equations that cannot be solved analytically, which is common in maximum likelihood-based parameter estimation for the varied regression models discussed throughout this book. Moreover, we can find these series in different engineering and scientific fields such as physics.\nSuppose we have real function \\(f(u)\\) around a point \\(u = a\\), then the one-dimensional infinite Taylor series expansion is given by the expression\n\\[\n\\begin{align*}\nf(u) &= f(a) + f'(a) (u - a) + \\frac{f''(a)}{2!} (u - a)^2 + \\\\\n& \\qquad \\frac{f^{(3)}(a)}{3!} (u - a)^3 + \\frac{f^{(4)}(a)}{4!} (u - a)^4 + \\\\\n& \\qquad \\frac{f^{(5)}(a)}{5!} (u - a)^5 + \\cdots \\\\\n&= \\sum_{j = 0}^{\\infty} \\frac{f^{(j)}(a)}{j!} (u - a)^j.\n\\end{align*}\n\\tag{D.28}\\]\nA complete mathematical derivation of Equation D.28 can be found in Weisstein (n.d.b). Moving along, specifically in the last line of this equation which shows an infinite summation, note the following:\n\n\n\\(f^{(j)}(a)\\) indicates the \\(j\\)th order derivative of \\(f(u)\\) and evaluated at point \\(a\\).\n\n\\(j!\\) implicates the factorial of \\(j\\) such that\n\n\\[\nj! = j \\times (j - 1) \\times (j - 2) \\times (j - 3) \\times \\cdots \\times 3 \\times 2 \\times 1.\n\\]\nwith\n\\[\n0! = 1.\n\\]\nIf we go even further with Equation D.28, we have a specific case when \\(a = 0\\) called the Maclaurin series expansions. This case was introduced by the Scottish mathematician Colin Maclaurin in his work A Treatise of Fluxions (Maclaurin 1742).\n\n\nPortrait of mathematician Colin Maclaurin (Harding 1798).\n\nHence, in a Mclaurin series, Equation D.28 becomes:\n\\[\n\\begin{align*}\nf(u) &= f(0) + f'(0) (u) + \\frac{f''(0)}{2!} u^2 + \\\\\n& \\qquad \\frac{f^{(3)}(0)}{3!} u^3 + \\frac{f^{(4)}(0)}{4!} u^4 + \\\\\n& \\qquad \\frac{f^{(5)}(0)}{5!} u^5 + \\cdots \\\\\n&= \\sum_{j = 0}^{\\infty} \\frac{f^{(j)}(0)}{j!} u^j.\n\\end{align*}\n\\tag{D.29}\\]\nDifferent statistical proofs make use of Taylor series expansions as well as the Mclaurin series, and the Poisson distribution is not an exception at all!\n\n\nThe above Mclaurin series in Equation D.29 will help us to show that our Poisson PMF in Equation D.27 actually adds up to one all over the support of the random variable:\n\nProof. \\[\n\\begin{align*}\n\\sum_{y = 0}^{\\infty} P_Y \\left( Y = y \\mid \\lambda \\right) &= \\sum_{y = 0}^{\\infty} \\frac{\\lambda^y \\exp{(-\\lambda)}}{y!} \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 0}^{\\infty} \\frac{\\lambda^y}{y!} \\\\\n& \\quad \\qquad \\text{factoring out $\\exp{(-\\lambda)}$,} \\\\\n& \\quad \\qquad \\text{since it does not depend on $y$.}\n\\end{align*}\n\\tag{D.30}\\]\nNow, we will focus on the above summation\n\\[\n\\sum_{y = 0}^{\\infty} \\frac{\\lambda^y}{y!}\n\\] and use the Mclaurin series from Equation D.29 by letting\n\\[\nf(u) = \\exp(u).\n\\tag{D.31}\\]\nWe know that all derivatives of the above function are equal\n\\[\nf'(u) = f''(u) = f^{(3)}(u) = f^{(4)}(u) = f^{(5)}(u) = \\cdots = \\exp{(u)},\n\\] which allows us to conclude that the \\(j\\)th derivative is\n\\[\nf^{(j)}(u) = \\exp(u).\n\\]\nThis \\(j\\)th derivative evaluated at \\(u = 0\\) becomes\n\\[\nf^{(j)}(0) = \\exp(0) = 1.\n\\]\nTherefore, the Mclaurin series for Equation D.31 is the following:\n\\[\n\\begin{align*}\nf(u) &= \\exp(u) \\\\\n&= \\sum_{j = 0}^{\\infty} \\frac{\\exp(0)}{j!} u^j \\\\\n&= \\sum_{j = 0}^{\\infty} \\frac{u^j }{j!}.\n\\end{align*}\n\\tag{D.32}\\]\nThat said, using Equation D.32, let:\n\\[\n\\begin{gather*}\n\\lambda = u \\\\\ny = j.\n\\end{gather*}\n\\]\nThus, we have the following:\n\\[\n\\sum_{y = 0}^{\\infty} \\frac{\\lambda^y}{y!} = \\exp{(\\lambda)}.\n\\]\nFinally, going back to Equation D.30:\n\\[\n\\begin{align*}\n\\sum_{y = 0}^{\\infty} P_Y \\left( Y = y \\mid \\lambda \\right) &= \\exp{(-\\lambda)} \\overbrace{\\sum_{y = 0}^{\\infty} \\frac{\\lambda^y}{y!}}^{\\exp{(\\lambda)}} \\\\\n&= \\exp{(-\\lambda)} \\times \\exp{(\\lambda)} \\\\\n&= \\exp{(-\\lambda + \\lambda)} \\\\\n&= \\exp{(0)} \\\\\n&= 1. \\qquad \\qquad \\qquad \\qquad \\square\n\\end{align*}\n\\tag{D.33}\\]\n\nIndeed, the Poisson PMF is a proper probability distribution!\n\n\n\nD.4.2 Expected Value\nVia Equation C.3, the expected value or mean of a Poisson-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\sum_{y = 0}^{\\infty} y P_Y \\left( Y = y \\mid \\lambda \\right) \\\\\n&= \\sum_{y = 1}^{\\infty} y P_Y \\left( Y = y \\mid \\lambda \\right) \\\\\n& \\quad \\qquad \\text{for $y = 0$, the addend is equal to zero} \\\\\n&= \\sum_{y = 1}^{\\infty} y \\left[ \\frac{\\lambda^y \\exp{(-\\lambda)}}{y!} \\right] \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 1}^{\\infty} \\frac{y \\lambda^y}{y!} \\\\\n& \\quad \\qquad \\text{factoring out $\\exp{(-\\lambda)}$,} \\\\\n& \\quad \\qquad \\text{since it does not depend on $y$} \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 1}^{\\infty} \\frac{y \\lambda^y}{y (y - 1)!} \\\\\n& \\quad \\qquad \\text{in the denominator, $y! = y (y - 1)!$}\\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 1}^{\\infty} \\frac{\\lambda^y}{(y - 1)!} \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 1}^{\\infty} \\frac{\\lambda^{y + 1 - 1}}{(y - 1)!} \\\\\n& \\quad \\qquad \\text{note $\\lambda^y = \\lambda^{y + 1 - 1}$} \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 1}^{\\infty} \\frac{\\lambda \\lambda^{y - 1}}{(y - 1)!} \\\\\n& \\quad \\qquad \\text{rearranging terms} \\\\\n&= \\lambda \\exp{(-\\lambda)} \\sum_{y = 1}^{\\infty} \\frac{\\lambda^{y - 1}}{(y - 1)!} \\\\\n& \\quad \\qquad \\text{factoring out $\\lambda$,} \\\\\n& \\quad \\qquad \\text{since it does not depend on $y$.}\n\\end{align*}\n\\tag{D.34}\\]\nThen, let us make the following variable rearrangement:\n\\[\nz = y - 1.\n\\]\nGoing back to Equation D.34, and applying our above variable rearrangement within the summation, we have:\n\\[\n\\mathbb{E}(Y) = \\lambda \\exp{(-\\lambda)} \\sum_{z = 0}^{\\infty} \\frac{\\lambda^z}{z!}\n\\tag{D.35}\\]\nUsing Equation D.32, let:\n\\[\n\\begin{gather*}\n\\lambda = u \\\\\nz = j.\n\\end{gather*}\n\\]\nHence, we have the following:\n\\[\n\\sum_{z = 0}^{\\infty} \\frac{\\lambda^z}{z!} = \\exp{(\\lambda)}.\n\\]\nFinally, going back to Equation D.35:\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\lambda \\exp{(-\\lambda)} \\overbrace{\\sum_{z = 0}^{\\infty} \\frac{\\lambda^z}{z!}}^{\\exp{(\\lambda)}} \\\\\n&= \\lambda \\exp{(-\\lambda)} \\times \\exp{(\\lambda)} \\\\\n&= \\lambda \\exp{(-\\lambda + \\lambda)} \\\\\n&= \\lambda \\exp{(0)} \\\\\n&= \\lambda. \\qquad \\qquad \\qquad \\qquad \\square\n\\end{align*}\n\\]\n\n\nD.4.3 Variance\nVia Equation C.5 and the Equation C.3 of a discrete expected value, the variance of a Poisson-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y)\\right]^2 \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - \\lambda^2 \\qquad \\text{since $\\mathbb{E}(Y) = \\lambda$.}\n\\end{align*}\n\\tag{D.36}\\]\nNow, we need to play around with the below expected value expression as follows:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\mathbb{E} \\left[ Y (Y - 1) \\right] + \\mathbb{E}(Y) \\\\\n&= \\mathbb{E} \\left[ Y (Y - 1) \\right] + \\lambda \\qquad \\text{since $\\mathbb{E}(Y) = \\lambda$.}\n\\end{align*}\n\\tag{D.37}\\]\nNow, to find \\(\\mathbb{E} \\left[ Y (Y - 1) \\right]\\), we make the following derivation via the LOTUS from Equation C.1 when \\(g(Y) = y (y - 1)\\):\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= \\sum_{y = 0}^{\\infty} y (y - 1) P_Y \\left( Y = y \\mid \\lambda \\right) \\\\\n&= \\sum_{y = 2}^{\\infty} y (y - 1) P_Y \\left( Y = y \\mid \\lambda \\right) \\\\\n& \\quad \\qquad \\text{for $y = \\{0, 1\\}$,} \\\\\n& \\quad \\qquad \\text{the addends are equal to zero} \\\\\n&= \\sum_{y = 2}^{\\infty} y (y - 1) \\left[ \\frac{\\lambda^y \\exp{(-\\lambda)}}{y!} \\right] \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 2}^{\\infty} \\left[ \\frac{y (y - 1) \\lambda^y}{y!} \\right] \\\\\n& \\quad \\qquad \\text{factoring out $\\exp{(-\\lambda)}$,} \\\\\n& \\quad \\qquad \\text{since it does not depend on $y$} \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 2}^{\\infty} \\left[ \\frac{y (y - 1) \\lambda^y}{y (y - 1) (y - 2)!} \\right] \\\\\n& \\quad \\qquad \\text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 2}^{\\infty} \\frac{\\lambda^y}{(y - 2)!} \\\\\n&= \\exp{(-\\lambda)} \\sum_{y = 2}^{\\infty} \\frac{\\lambda^{y + 2 - 2}}{(y - 2)!} \\\\\n& \\quad \\qquad \\text{note $\\lambda^y = \\lambda^{y + 2 - 2} $} \\\\\n&= \\lambda^2 \\exp{(-\\lambda)} \\sum_{y = 2}^{\\infty} \\frac{\\lambda^{y - 2}}{(y - 2)!} \\\\\n& \\quad \\qquad \\text{factoring out $\\lambda^2$,} \\\\\n& \\quad \\qquad \\text{since it does not depend on $y$.} \\\\\n\\end{align*}\n\\tag{D.38}\\]\nThen, we make the following variable rearrangement:\n\\[\nz = y - 2.\n\\]\nGoing back to Equation D.38, and applying our above variable rearrangement within the summation, we have:\n\\[\n\\mathbb{E} \\left[ Y (Y - 1) \\right] = \\lambda^2 \\exp{(-\\lambda)} \\sum_{z = 0}^{\\infty} \\frac{\\lambda^z}{z!}.\n\\tag{D.39}\\]\nUsing Equation D.32, let:\n\\[\n\\begin{gather*}\n\\lambda = u \\\\\nz = j.\n\\end{gather*}\n\\]\nThus, we have the following:\n\\[\n\\sum_{z = 0}^{\\infty} \\frac{\\lambda^z}{z!} = \\exp{(\\lambda)}.\n\\]\nGoing back to Equation D.39:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left[ Y (Y - 1) \\right] &= \\lambda^2 \\exp{(-\\lambda)} \\overbrace{\\sum_{z = 0}^{\\infty} \\frac{\\lambda^z}{z!}}^{\\exp{(\\lambda)}} \\\\\n&= \\lambda^2 \\exp{(-\\lambda)} \\times \\exp{\\lambda} \\\\\n&= \\lambda^2 \\exp{(-\\lambda + \\lambda)} \\\\\n&= \\lambda^2 \\exp{(0)} \\\\\n&= \\lambda^2.\n\\end{align*}\n\\tag{D.40}\\]\nLet us retake Equation D.37 and plug in the above result:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\mathbb{E} \\left[ Y (Y - 1) \\right] + \\lambda \\\\\n&= \\lambda^2 + \\lambda. \\\\\n\\end{align*}\n\\]\nFinally, we plug in \\(\\mathbb{E} \\left( Y^2 \\right)\\) in Equation D.36:\n\\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\lambda^2 \\\\\n&= \\lambda^2 + \\lambda - \\lambda^2 \\\\\n&= \\lambda. \\qquad \\qquad \\square\n\\end{align*}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-generalized-poisson-distribution",
    "href": "book/C-distributional-mind-map.html#sec-generalized-poisson-distribution",
    "title": "Appendix C — Distributional Mind Map",
    "section": "\nD.5 Generalized Poisson",
    "text": "D.5 Generalized Poisson\nThe generalized Poisson (GP) distribution is viewed as the general Poisson case. It was introduced by Consul and Jain (1973). Suppose you observe the count of events happening in a fixed interval of time or space. Let \\(Y\\) be the number of counts considered of integer type. Then, \\(Y\\) is said to have a GP distribution with continuous parameters \\(\\lambda\\) and \\(\\theta\\):\n\\[\nY \\sim \\text{GP}(\\lambda, \\theta).\n\\]\n\nD.5.1 Probability Mass Function\nThe PMF of this count-type \\(Y\\) is the following:\n\\[\n\\begin{align*}\nP_Y \\left( Y = y \\mid \\lambda, \\theta \\right) &= \\frac{\\lambda (\\lambda + y \\theta)^{y - 1} \\exp{\\left[ -(\\lambda + y \\theta) \\right]}}{y!} \\\\\n& \\qquad \\qquad \\qquad \\text{for $y \\in \\{ 0, 1, 2, \\dots\\}$,}\n\\end{align*}\n\\tag{D.41}\\]\nwhere \\(\\exp{(\\cdot)}\\) depicts the base \\(e\\) (i.e., Euler’s number, \\(e = 2.71828...\\)) and \\(y!\\) is the factorial\n\\[\ny! = y \\times (y - 1) \\times (y - 2) \\times (y - 3) \\times \\cdots \\times 3 \\times 2 \\times 1.  \n\\]\nwith\n\\[\n0! = 1.\n\\]\nThe continuous parameter \\(\\lambda \\in (0, \\infty)\\) represents the average rate at which these events happen (i.e., events per area unit or events per time unit). As in the case of the classical Poisson case, even though the GP random variable \\(Y\\) is considered discrete, \\(\\lambda\\) is modelled as continuous!\nOn the other hand, the continuous and bounded parameter \\(\\theta \\in (-1, 1)\\) controls for dispersion present in the GP random variable Y as follows:\n\nWhen \\(0 &lt; \\theta &lt; 1\\), the GP \\(Y\\) shows overdispersion which implies that \\[\\text{Var}(Y) &gt; \\mathbb{E}(Y).\\]\nWhen \\(-1 &lt; \\theta &lt; 0\\), the GP \\(Y\\) shows underdispersion which implies that \\[\\text{Var}(Y) &lt; \\mathbb{E}(Y).\\]\nWhen \\(\\theta = 0\\), the PMF of the GP \\(Y\\) in Equation D.41 becomes the classical Poisson PMF from Equation D.27: \\[\n\\begin{align*}\nP_Y \\left( Y = y \\mid \\lambda, \\theta = 0 \\right) &= \\frac{\\lambda (\\lambda + y \\theta)^{y - 1} \\exp{\\left[ -(\\lambda + y \\theta) \\right]}}{y!} \\\\\n&= \\frac{\\lambda (\\lambda)^{y - 1} \\exp{\\left( -\\lambda \\right)}}{y!} \\qquad \\text{setting $\\theta = 0$} \\\\\n&= \\frac{\\lambda^y \\exp{\\left( -\\lambda \\right)}}{y!} \\\\\n& \\qquad \\qquad \\qquad \\text{for $y \\in \\{ 0, 1, 2, \\dots\\}$.}\n\\end{align*}\n\\]\n\n\n\nHeads-up on equidispersion in a generalized Poisson random variable!\n\n\nIn a GP-distributed \\(Y\\), when \\(\\theta = 0\\) in its corresponding PMF, we have equidispersion which implies \\[\n\\mathbb{E}(Y \\mid \\theta = 0) = \\frac{\\lambda}{1 - \\theta} = \\lambda\n\\] \\[\n\\text{Var}(Y \\mid \\theta = 0) = \\frac{\\lambda}{(1 - \\theta)^2} = \\lambda\n\\] \\[\n\\mathbb{E}(Y \\mid \\theta = 0) = \\text{Var}(Y).\n\\]\n\n\n\nHow can we verify that Equation D.41 is a proper PMF (i.e., all the standalone probabilities over the support of \\(Y\\) add up to one)?\n\n\nD.5.2 Expected Value\n\nD.5.3 Variance",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-zero-inflated-poisson-distribution",
    "href": "book/C-distributional-mind-map.html#sec-zero-inflated-poisson-distribution",
    "title": "Appendix C — Distributional Mind Map",
    "section": "\nD.6 Zero-inflated Poisson",
    "text": "D.6 Zero-inflated Poisson",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-multinomial-distribution",
    "href": "book/C-distributional-mind-map.html#sec-multinomial-distribution",
    "title": "Appendix C — Distributional Mind Map",
    "section": "\nD.7 Multinomial",
    "text": "D.7 Multinomial",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-weibull-distribution",
    "href": "book/C-distributional-mind-map.html#sec-weibull-distribution",
    "title": "Appendix C — Distributional Mind Map",
    "section": "\nE.1 Weibull",
    "text": "E.1 Weibull\nSuppose you observe the waiting times for some event of interest to happen (i.e., survival times). Let random variable \\(Y\\) be considered continuous and nonnegative. Then, \\(Y\\) is said to have a Weibull distribution with the following scale continuous parameter \\(\\beta\\) and shape continuous parameter \\(\\gamma\\):\n\\[\nY \\sim \\text{Weibull}(\\beta, \\gamma).\n\\]\n\nE.1.1 Probability Density Function\nThe PDF of \\(Y\\) is the following:\n\\[\nf_Y \\left(y \\mid \\beta, \\gamma \\right) = \\frac{\\gamma}{\\beta} \\left( \\frac{y}{\\beta} \\right)^{\\gamma - 1} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\quad \\text{for $y \\in [0, \\infty )$.}\n\\tag{E.1}\\]\nParameters \\(\\beta \\in (0, \\infty)\\) and \\(\\gamma \\in (0, \\infty)\\) refer to the random process’ scale and shape, respectively. Figure E.1 shows nine members of the Weibull parametric family, i.e., nine different PDFs with all possible pairwise combinations for three different scale parameters \\(\\beta = 0.5, 1, 2\\) and shape parameters \\(\\gamma = 1.5, 3, 6\\). We can highlight the following:\n\nRegardless of the shape parameter \\(\\gamma\\), as we increase the scale parameter \\(\\beta\\), note that there is more spread in the corresponding distributions.\nRegardless of the scale parameter \\(\\beta\\), as we increase the shape parameter \\(\\gamma\\), note the peak of the distribution moves more to the right.\n\n\n\n\n\n\n\n\nFigure E.1: Some members of the Weibull parametric family.\n\n\n\n\n\n\nHeads-up on the Weibull and Exponential distributions in survival analysis!\n\n\nThe Weibull distribution extends its Exponential counterpart (as in Section E.3) by allowing the event rate (or hazard) to change over time, rather than staying constant. This makes it especially useful in survival analysis and reliability studies, where capturing how the risk of an event evolves is critical.\nAs a side note, the Weibull and Exponential PDFs are mathematically related. When \\(\\gamma = 1\\) in Equation E.1, the Weibull PDF is equal to the Exponential PDF under the scale parametrization as in Equation E.11:\n\\[\n\\begin{align*}\nf_Y \\left(y \\mid \\beta, \\gamma = 1 \\right) &= \\frac{\\gamma}{\\beta} \\left( \\frac{y}{\\beta} \\right)^{\\gamma - 1} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\\\\n&= \\frac{1}{\\beta} \\underbrace{\\left( \\frac{y}{\\beta} \\right)^0}_{1} \\exp{\\left( -\\frac{y}{\\beta} \\right)} \\\\\n&= \\frac{1}{\\beta} \\exp{\\left( -\\frac{y}{\\beta} \\right)} \\quad \\text{for $y \\in [0, \\infty )$}.\n\\end{align*}\n\\]\n\n\n\nHow can we verify that Equation E.1 is a proper PDF (i.e., Equation E.1 integrates to one over the support of \\(Y\\))?\n\n\nProof. \\[\n\\begin{align*}\n\\int_{y = 0}^{y = \\infty} f_Y \\left(y \\mid \\beta, \\gamma \\right) \\mathrm{d}y &= \\int_{y = 0}^{y = \\infty} \\frac{\\gamma}{\\beta} \\left( \\frac{y}{\\beta} \\right)^{\\gamma - 1} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y.\n\\end{align*}\n\\tag{E.2}\\]\nNow, let us make the variable substitution:\n\\[\n\\begin{align*}\nu &= \\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\\\\ny &= \\beta u^{\\frac{1}{\\gamma}} \\qquad \\Rightarrow \\qquad \\mathrm{d}y = \\frac{\\beta}{\\gamma} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u.\n\\end{align*}\n\\]\nThe above rearrangement yields the following in Equation E.2:\n\\[\n\\begin{align*}\n\\int_{y = 0}^{y = \\infty} f_Y \\left(y \\mid \\beta, \\gamma \\right) \\mathrm{d}y &= \\int_{u = 0}^{u = \\infty} \\frac{\\gamma}{\\beta} \\left( \\frac{\\beta u^{\\frac{1}{\\gamma}}}{\\beta} \\right)^{\\gamma - 1} \\exp{\\left( -u \\right)} \\frac{\\beta}{\\gamma} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\int_{u = 0}^{u = \\infty} \\left( u^{\\frac{1}{\\gamma}} \\right)^{\\gamma - 1} \\exp{\\left( -u \\right)} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\int_{u = 0}^{u = \\infty} u^{\\frac{\\gamma - 1}{\\gamma}} \\exp{\\left( -u \\right)} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\int_{u = 0}^{u = \\infty} u^{\\frac{\\gamma - 1}{\\gamma} + \\frac{1}{\\gamma} - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n&= \\int_{u = 0}^{u = \\infty} u^{1 - \\frac{1}{\\gamma} + \\frac{1}{\\gamma} - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n&= \\int_{u = 0}^{u = \\infty} u^0 \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n&= \\int_{u = 0}^{u = \\infty} \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n&= -\\exp{\\left( -u \\right)} \\Bigg|_{u = 0}^{u = \\infty} \\\\\n&= - \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\\\\n&= - \\left( 0 - 1 \\right) \\\\\n&= 1. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\square\n\\end{align*}\n\\]\n\nIndeed, the Weibull PDF is a proper probability distribution!\n\n\n\nE.1.2 Expected Value\nVia Equation C.4, the expected value or mean of a Weibull-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\int_{y = 0}^{y = \\infty} y f_Y \\left(y \\mid \\beta, \\gamma \\right) \\mathrm{d}y \\\\\n&= \\int_{y = 0}^{y = \\infty} y \\frac{\\gamma}{\\beta} \\left( \\frac{y}{\\beta} \\right)^{\\gamma - 1} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y \\\\\n&= \\frac{\\gamma}{\\beta} \\int_{y = 0}^{y = \\infty} y \\left( \\frac{y}{\\beta} \\right)^{\\gamma - 1} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y \\\\\n&= \\frac{\\gamma}{\\beta} \\int_{y = 0}^{y = \\infty} y \\frac{y^{\\gamma - 1}}{\\beta^{\\gamma - 1}} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y \\\\\n&= \\frac{\\gamma}{\\beta} \\int_{y = 0}^{y = \\infty} \\frac{y^{\\gamma}}{\\beta^{\\gamma - 1}} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y \\\\\n&= \\frac{\\gamma}{\\beta^{\\gamma}} \\int_{y = 0}^{y = \\infty} y^{\\gamma} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y.\n\\end{align*}\n\\tag{E.3}\\]\nThen, we make the following variable substitution:\n\\[\n\\begin{align*}\nu &= \\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\\\\ny &= \\beta u^{\\frac{1}{\\gamma}} \\qquad \\Rightarrow \\qquad \\mathrm{d}y = \\frac{\\beta}{\\gamma} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u.\n\\end{align*}\n\\]\nThe above rearrangement yields the following in Equation E.3:\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\frac{\\gamma}{\\beta^{\\gamma}} \\int_{u = 0}^{u = \\infty} \\left( \\beta u^{\\frac{1}{\\gamma}} \\right)^{\\gamma} \\exp{\\left( -u \\right)} \\frac{\\beta}{\\gamma} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\frac{\\gamma}{\\beta^{\\gamma}} \\int_{u = 0}^{u = \\infty} \\beta^{\\gamma} u \\exp{\\left( -u \\right)} \\frac{\\beta}{\\gamma} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\frac{\\gamma \\beta^{\\gamma} \\beta}{\\beta^{\\gamma} \\gamma} \\int_{u = 0}^{u = \\infty} u \\exp{\\left( -u \\right)} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\beta \\int_{u = 0}^{u = \\infty} u^{\\frac{1}{\\gamma}} \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n&= \\beta \\int_{u = 0}^{u = \\infty} u^{\\left( \\frac{1}{\\gamma} + 1 \\right) - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n& \\quad \\qquad \\text{note $\\frac{1}{\\gamma} = \\left( \\frac{1}{\\gamma} + 1 \\right) - 1$.}\n\\end{align*}\n\\tag{E.4}\\]\nThe integral on the right-hand side of Equation E.4 corresponds to the so-called Gamma function as described below.\n\n\nHeads-up on the Gamma function!\n\n\nThe Gamma function is a mathematical generalization of the factorial function, but applied to non-integer numbers. In many different probability distributions, this function appears as a normalizing constant. Moreover, it also appears as part of the expressions of expected values and variances.\nThat said, for a variable \\(z\\) in general, we can represent the Gamma function via the following integral:\n\\[\n\\Gamma(z) = \\int_{t = 0}^{t = \\infty} t^{z - 1} \\exp{\\left( -t \\right)} \\mathrm{d}t.\n\\tag{E.5}\\]\nWeisstein (n.d.a) provides further insights on this Gamma function along with some useful properties.\n\n\nThus, via the Gamma function from Equation E.5, we set the following:\n\\[\n\\begin{gather*}\nt = u \\\\\nz = \\frac{1}{\\gamma} + 1,\n\\end{gather*}\n\\]\nwhich yields\n\\[\n\\Gamma \\left( \\frac{1}{\\gamma} + 1 \\right) = \\int_{u = 0}^{u = \\infty} u^{\\left( \\frac{1}{\\gamma} + 1 \\right) - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u.\n\\]\nMoving along with Equation E.4, we have:\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\beta \\int_{u = 0}^{u = \\infty} u^{\\left( \\frac{1}{\\gamma} + 1 \\right) - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n&= \\beta \\Gamma \\left( \\frac{1}{\\gamma} + 1 \\right). \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\n\nE.1.3 Variance\nVia Equation C.5 and the Equation C.4 of a continuous expected value, the variance of a Weibull-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y)\\right]^2 \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - \\beta^2 \\Gamma^2 \\left( \\frac{1}{\\gamma} + 1 \\right) \\\\\n& \\quad \\qquad \\text{since $\\mathbb{E}(Y) = \\beta \\Gamma \\left( \\frac{1}{\\gamma} + 1 \\right)$}.\n\\end{align*}\n\\tag{E.6}\\]\nNow, we need to find \\(\\mathbb{E} \\left( Y^2 \\right)\\) from Equation E.6. Thus, we make the following derivation via the LOTUS from Equation C.2 when \\(g(Y) = y^2\\):\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\int_{y = 0}^{y = \\infty} y^2 f_Y \\left(y \\mid \\beta, \\gamma \\right) \\mathrm{d}y \\\\\n&= \\int_{y = 0}^{y = \\infty} y^2 \\frac{\\gamma}{\\beta} \\left( \\frac{y}{\\beta} \\right)^{\\gamma - 1} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y \\\\\n&= \\frac{\\gamma}{\\beta} \\int_{y = 0}^{y = \\infty} y^2 \\left( \\frac{y}{\\beta} \\right)^{\\gamma - 1} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y \\\\\n&= \\frac{\\gamma}{\\beta} \\int_{y = 0}^{y = \\infty} y^2 \\frac{y^{\\gamma - 1}}{\\beta^{\\gamma - 1}} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y \\\\\n&= \\frac{\\gamma}{\\beta^{\\gamma}} \\int_{y = 0}^{y = \\infty} y^{\\gamma + 1} \\exp{\\left[ -\\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\right]} \\mathrm{d}y.\n\\end{align*}\n\\tag{E.7}\\]\nThen, we make the following variable substitution:\n\\[\n\\begin{align*}\nu &= \\left( \\frac{y}{\\beta} \\right)^{\\gamma} \\\\\ny &= \\beta u^{\\frac{1}{\\gamma}} \\qquad \\Rightarrow \\qquad \\mathrm{d}y = \\frac{\\beta}{\\gamma} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u.\n\\end{align*}\n\\]\nThe above rearrangement yields the following in Equation E.7:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\frac{\\gamma}{\\beta^{\\gamma}} \\int_{u = 0}^{u = \\infty} \\left( \\beta u^{\\frac{1}{\\gamma}} \\right)^{\\gamma + 1} \\exp{\\left( -u \\right)} \\frac{\\beta}{\\gamma} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\frac{\\gamma}{\\beta^{\\gamma}} \\int_{u = 0}^{u = \\infty} \\beta^{\\gamma + 1} u^{1 + \\frac{1}{\\gamma}} \\exp{\\left( -u \\right)} \\frac{\\beta}{\\gamma} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\frac{\\gamma \\beta^{\\gamma + 1} \\beta}{\\beta^{\\gamma} \\gamma} \\int_{u = 0}^{u = \\infty} u^{1 + \\frac{1}{\\gamma}} \\exp{\\left( -u \\right)} u^{\\frac{1}{\\gamma} - 1} \\mathrm{d}u \\\\\n&= \\beta^2 \\int_{u = 0}^{u = \\infty} u^{1 + \\frac{1}{\\gamma} + \\frac{1}{\\gamma} - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n&= \\beta^2 \\int_{u = 0}^{u = \\infty} u^{\\left( \\frac{2}{\\gamma} + 1 \\right) - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u.\n\\end{align*}\n\\tag{E.8}\\]\nHence, via the Gamma function from Equation E.5, we set the following:\n\\[\n\\begin{gather*}\nt = u \\\\\nz = \\frac{2}{\\gamma} + 1,\n\\end{gather*}\n\\]\nwhich yields\n\\[\n\\Gamma \\left( \\frac{2}{\\gamma} + 1 \\right) = \\int_{u = 0}^{u = \\infty} u^{\\left( \\frac{2}{\\gamma} + 1 \\right) - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u.\n\\]\nMoving along with Equation E.8, we have:\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\beta^2 \\int_{u = 0}^{u = \\infty} u^{\\left( \\frac{2}{\\gamma} + 1 \\right) - 1} \\exp{\\left( -u \\right)} \\mathrm{d}u \\\\\n&= \\beta^2 \\Gamma \\left( \\frac{2}{\\gamma} + 1 \\right).\n\\end{align*}\n\\tag{E.9}\\]\nFinally, we plug Equation E.9 into Equation E.6:\n\\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E}\\left( Y^2 \\right) - \\beta^2 \\Gamma^2 \\left( \\frac{1}{\\gamma} + 1 \\right) \\\\\n&= \\beta^2 \\Gamma \\left( \\frac{2}{\\gamma} + 1 \\right) - \\beta^2 \\Gamma^2 \\left( \\frac{1}{\\gamma} + 1 \\right) \\\\\n&= \\beta^2 \\left[  \\Gamma \\left( \\frac{2}{\\gamma} + 1 \\right) - \\Gamma^2 \\left( \\frac{1}{\\gamma} + 1 \\right) \\right]. \\qquad \\qquad \\square\n\\end{align*}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-lognormal-distribution",
    "href": "book/C-distributional-mind-map.html#sec-lognormal-distribution",
    "title": "Appendix C — Distributional Mind Map",
    "section": "\nE.2 Lognormal",
    "text": "E.2 Lognormal",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-exponential-distribution",
    "href": "book/C-distributional-mind-map.html#sec-exponential-distribution",
    "title": "Appendix C — Distributional Mind Map",
    "section": "\nE.3 Exponential",
    "text": "E.3 Exponential\nSuppose you observe the waiting times for some event of interest to happen (i.e., survival times). Let random variable \\(Y\\) be considered continuous and nonnegative. Then, \\(Y\\) is said to have an Exponential distribution with the following rate continuous parameter \\(\\lambda\\):\n\\[\nY \\sim \\text{Exponential}(\\lambda).\n\\]\nWe can also model \\(Y\\) with the following scale continuous parameter \\(\\beta\\):\n\\[\nY \\sim \\text{Exponential}(\\beta).\n\\]\n\nE.3.1 Probability Density Functions\nGiven the two above parametrizations of the Exponential distribution, there are two possible PDFs as discussed below.\nRate Parametrization\nThe PDF of \\(Y\\) is the following:\n\\[\nf_Y \\left(y \\mid \\lambda \\right) = \\lambda \\exp \\left( -\\lambda y \\right) \\quad \\text{for $y \\in [0, \\infty )$.}\n\\tag{E.10}\\]\nParameter \\(\\lambda \\in (0, \\infty)\\) refers to the random process’ rate. Figure E.2 shows three members of the Exponential parametric family, i.e., three different PDFs with different rate parameters \\(\\lambda = 0.25, 0.5, 1\\). As we increase the rate parameter, note that smaller observed values \\(y\\) get more probable.\n\n\n\n\n\n\n\nFigure E.2: Some members of the Exponential parametric family with rate parametrization.\n\n\n\n\n\nHow can we verify that Equation E.10 is a proper PDF (i.e., Equation E.10 integrates to one over the support of \\(Y\\))?\n\n\nProof. \\[\n\\begin{align*}\n\\int_{y = 0}^{y = \\infty} f_Y \\left(y \\mid \\lambda \\right) \\mathrm{d}y &= \\int_{y = 0}^{y = \\infty} \\lambda \\exp \\left( -\\lambda y \\right) \\mathrm{d}y \\\\\n&= \\lambda \\int_{y = 0}^{y = \\infty} \\exp \\left( -\\lambda y \\right) \\mathrm{d}y \\\\\n&= - \\frac{\\lambda}{\\lambda} \\exp \\left( -\\lambda y \\right) \\Bigg|_{y = 0}^{y = \\infty} \\\\\n&= - \\exp \\left( -\\lambda y \\right) \\Bigg|_{y = 0}^{y = \\infty} \\\\\n&= - \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\\\\n&= - \\left( 0 - 1 \\right) \\\\\n&= 1. \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\nIndeed, the Exponential PDF, under a rate parametrization, is a proper probability distribution!\n\n\nScale Parametrization\nThe PDF of \\(Y\\) is the following:\n\\[\nf_Y \\left(y \\mid \\beta \\right) = \\frac{1}{\\beta} \\exp \\left( -\\frac{y}{\\beta} \\right) \\quad \\text{for $y \\in [0, \\infty )$.}\n\\tag{E.11}\\]\nParameter \\(\\beta \\in (0, \\infty)\\) refers to the random process’ scale. Figure E.3 shows three members of the Exponential parametric family, i.e., three different PDFs with different scale parameters \\(\\beta = 0.25, 0.5, 1\\). As we increase the scale parameter, note that larger observed values \\(y\\) get more probable.\n\n\n\n\n\n\n\nFigure E.3: Some members of the Exponential parametric family with scale parametrization.\n\n\n\n\n\nHow can we verify that Equation E.11 is a proper PDF (i.e., Equation E.11 integrates to one over the support of \\(Y\\))?\n\n\nProof. \\[\n\\begin{align*}\n\\int_{y = 0}^{y = \\infty} f_Y \\left(y \\mid \\beta \\right) \\mathrm{d}y &= \\int_{y = 0}^{y = \\infty} \\frac{1}{\\beta} \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\\\\n&= \\frac{1}{\\beta} \\int_{y = 0}^{y = \\infty} \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\\\\n&= - \\frac{\\beta}{\\beta} \\exp \\left( -\\frac{y}{\\beta} \\right) \\Bigg|_{y = 0}^{y = \\infty} \\\\\n&= - \\exp \\left( -\\frac{y}{\\beta} \\right) \\Bigg|_{y = 0}^{y = \\infty} \\\\\n&= - \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\\\\n&= - \\left( 0 - 1 \\right) \\\\\n&= 1. \\qquad \\qquad \\qquad \\qquad \\quad \\square\n\\end{align*}\n\\]\n\nIndeed, the Exponential PDF, under a scale parametrization, is a proper probability distribution!\n\n\n\nE.3.2 Expected Value\nAgain, given the two above parametrizations of the Exponential distribution, there are two possible mathematical expressions for the expected value as discussed below.\nRate Parametrization\nVia Equation C.4, the expected value or mean of an Exponential-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\int_{y = 0}^{y = \\infty} y f_Y \\left(y \\mid \\lambda \\right) \\mathrm{d}y \\\\\n&= \\int_{y = 0}^{y = \\infty} y \\lambda \\exp \\left( -\\lambda y \\right) \\mathrm{d}y \\\\\n&= \\lambda \\int_{y = 0}^{y = \\infty} y \\exp \\left( -\\lambda y \\right) \\mathrm{d}y. \\\\\n\\end{align*}\n\\tag{E.12}\\]\nEquation E.12 cannot be solved straightforwardly, we need to use integration by parts as follows:\n\\[\n\\begin{align*}\nu &= y & &\\Rightarrow & \\mathrm{d}u &= \\mathrm{d}y \\\\\n\\mathrm{d}v &= \\exp \\left( -\\lambda y \\right) \\mathrm{d}y & &\\Rightarrow & v &= -\\frac{1}{\\lambda} \\exp \\left( -\\lambda y \\right),\n\\end{align*}\n\\]\nwhich yields\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\lambda \\left[ u v \\Bigg|_{y = 0}^{y = \\infty} - \\int_{y = 0}^{y = \\infty} v \\mathrm{d}u \\right] \\\\\n&= \\lambda \\left\\{ \\left[ -\\frac{1}{\\lambda} y \\exp(-\\lambda y) \\right] \\Bigg|_{y = 0}^{y = \\infty} + \\frac{1}{\\lambda} \\int_{y = 0}^{y = \\infty} \\exp{\\left( -\\lambda y \\right)} \\mathrm{d}y \\right\\} \\\\\n&= \\lambda \\Bigg\\{ -\\frac{1}{\\lambda} \\Bigg[ \\underbrace{\\infty \\times \\exp(-\\infty)}_{0} - \\underbrace{0 \\times \\exp(0)}_{0} \\Bigg] - \\\\\n& \\qquad \\frac{1}{\\lambda^2} \\exp{\\left( -\\lambda y \\right)} \\Bigg|_{y = 0}^{y = \\infty} \\Bigg\\} \\\\\n&= \\lambda \\left\\{ -\\frac{1}{\\lambda} (0) - \\frac{1}{\\lambda^2} \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\right\\} \\\\\n&= \\lambda \\left[ 0 - \\frac{1}{\\lambda^2} (0 - 1) \\right] \\\\\n&= \\frac{\\lambda}{\\lambda^2} \\\\\n&= \\frac{1}{\\lambda}. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\qquad \\qquad \\quad \\qquad \\qquad \\square\n\\end{align*}\n\\]\n\nScale Parametrization\nVia Equation C.4, the expected value or mean of an Exponential-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\int_{y = 0}^{y = \\infty} y f_Y \\left(y \\mid \\beta \\right) \\mathrm{d}y \\\\\n&= \\int_{y = 0}^{y = \\infty} \\frac{y}{\\beta} \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\\\\n&= \\frac{1}{\\beta} \\int_{y = 0}^{y = \\infty} y \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y. \\\\\n\\end{align*}\n\\tag{E.13}\\]\nEquation E.13 cannot be solved straightforwardly, we need to use integration by parts as follows:\n\\[\n\\begin{align*}\nu &= y & &\\Rightarrow & \\mathrm{d}u &= \\mathrm{d}y \\\\\n\\mathrm{d}v &= \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y & &\\Rightarrow & v &= -\\beta \\exp \\left( -\\frac{y}{\\beta} \\right),\n\\end{align*}\n\\]\nwhich yields\n\\[\n\\begin{align*}\n\\mathbb{E}(Y) &= \\frac{1}{\\beta} \\left[ u v \\Bigg|_{y = 0}^{y = \\infty} - \\int_{y = 0}^{y = \\infty} v \\mathrm{d}u \\right] \\\\\n&= \\frac{1}{\\beta} \\left\\{ \\left[ -\\beta y \\exp \\left( -\\frac{y}{\\beta} \\right) \\right] \\Bigg|_{y = 0}^{y = \\infty} + \\beta \\int_{y = 0}^{y = \\infty} \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\right\\} \\\\\n&= \\frac{1}{\\beta} \\Bigg\\{ -\\beta \\Bigg[ \\underbrace{\\infty \\times \\exp(-\\infty)}_{0} - \\underbrace{0 \\times \\exp(0)}_{0} \\Bigg] - \\\\\n& \\qquad \\beta^2 \\exp \\left( -\\frac{y}{\\beta} \\right) \\Bigg|_{y = 0}^{y = \\infty} \\Bigg\\} \\\\\n&= \\frac{1}{\\beta} \\left\\{ -\\beta (0) - \\beta^2 \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\right\\} \\\\\n&= \\frac{1}{\\beta} \\left[ 0 - \\beta^2 (0 - 1) \\right] \\\\\n&= \\frac{\\beta^2}{\\beta} \\\\\n&= \\beta. \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\quad \\qquad \\qquad \\quad \\qquad \\qquad \\square\n\\end{align*}\n\\]\n\n\nE.3.3 Variance\nGiven the two above parametrizations of the Exponential distribution, there are two possible mathematical expressions for the variance as discussed below.\nRate Parametrization\nVia Equation C.5 and the Equation C.4 of a continuous expected value, the variance of an Exponential-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y)\\right]^2 \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - \\frac{1}{\\lambda^2} \\qquad \\text{since $\\mathbb{E}(Y) = \\frac{1}{\\lambda}$}.\n\\end{align*}\n\\tag{E.14}\\]\nNow, we need to find \\(\\mathbb{E} \\left( Y^2 \\right)\\) from Equation E.14. Hence, we make the following derivation via the LOTUS from Equation C.2 when \\(g(Y) = y^2\\):\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\int_{y = 0}^{y = \\infty} y^2 f_Y \\left(y \\mid \\lambda \\right) \\mathrm{d}y \\\\\n&= \\int_{y = 0}^{y = \\infty} y^2 \\lambda \\exp \\left( -\\lambda y \\right) \\mathrm{d}y \\\\\n&= \\lambda \\int_{y = 0}^{y = \\infty} y^2 \\exp \\left( -\\lambda y \\right) \\mathrm{d}y. \\\\\n\\end{align*}\n\\tag{E.15}\\]\nEquation E.15 cannot be solved straightforwardly, we need to use integration by parts as follows:\n\\[\n\\begin{align*}\nu &= y^2 & &\\Rightarrow & \\mathrm{d}u &= 2y \\mathrm{d}y \\\\\n\\mathrm{d}v &= \\exp \\left( -\\lambda y \\right) \\mathrm{d}y & &\\Rightarrow & v &= -\\frac{1}{\\lambda} \\exp \\left( -\\lambda y \\right),\n\\end{align*}\n\\]\nwhich yields\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\lambda \\left[ u v \\Bigg|_{y = 0}^{y = \\infty} - \\int_{y = 0}^{y = \\infty} v \\mathrm{d}u \\right] \\\\\n&= \\lambda \\bigg\\{ \\left[ -\\frac{1}{\\lambda} y^2 \\exp(-\\lambda y) \\right] \\Bigg|_{y = 0}^{y = \\infty} + \\\\\n& \\qquad \\frac{2}{\\lambda} \\int_{y = 0}^{y = \\infty} y \\exp{\\left( -\\lambda y \\right)} \\mathrm{d}y \\bigg\\} \\\\\n&= \\lambda \\Bigg\\{ -\\frac{1}{\\lambda} \\Bigg[ \\underbrace{\\infty \\times \\exp(-\\infty)}_{0} - \\underbrace{0 \\times \\exp(0)}_{0} \\Bigg] + \\\\\n& \\qquad \\frac{2}{\\lambda} \\int_{y = 0}^{y = \\infty} y \\exp{\\left( -\\lambda y \\right)} \\mathrm{d}y \\Bigg\\} \\\\\n&= \\lambda \\left\\{ -\\frac{1}{\\lambda} (0) + \\frac{2}{\\lambda} \\int_{y = 0}^{y = \\infty} y \\exp{\\left( -\\lambda y \\right)} \\mathrm{d}y \\right\\} \\\\\n&= \\lambda \\left\\{ 0 + \\frac{2}{\\lambda} \\int_{y = 0}^{y = \\infty} y \\exp{\\left( -\\lambda y \\right)} \\mathrm{d}y \\right\\} \\\\\n&= 2 \\int_{y = 0}^{y = \\infty} y \\exp{\\left( -\\lambda y \\right)} \\mathrm{d}y. \\\\\n\\end{align*}\n\\tag{E.16}\\]\nAgain, we need to apply integration by parts to solve Equation E.16:\n\\[\n\\begin{align*}\nu &= y & &\\Rightarrow & \\mathrm{d}u &= \\mathrm{d}y \\\\\n\\mathrm{d}v &= \\exp \\left( -\\lambda y \\right) \\mathrm{d}y & &\\Rightarrow & v &= -\\frac{1}{\\lambda} \\exp \\left( -\\lambda y \\right),\n\\end{align*}\n\\]\nwhich yields\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= 2 \\left[ u v \\Bigg|_{y = 0}^{y = \\infty} - \\int_{y = 0}^{y = \\infty} v \\mathrm{d}u \\right] \\\\\n&= 2 \\left\\{ \\left[ -\\frac{1}{\\lambda} y \\exp(-\\lambda y) \\right] \\Bigg|_{y = 0}^{y = \\infty} + \\frac{1}{\\lambda} \\int_{y = 0}^{y = \\infty} \\exp{\\left( -\\lambda y \\right)} \\mathrm{d}y \\right\\} \\\\\n&= 2 \\Bigg\\{ -\\frac{1}{\\lambda} \\Bigg[ \\underbrace{\\infty \\times \\exp(-\\infty)}_{0} - \\underbrace{0 \\times \\exp(0)}_{0} \\Bigg] - \\\\\n& \\qquad \\frac{1}{\\lambda^2} \\exp{\\left( -\\lambda y \\right)} \\Bigg|_{y = 0}^{y = \\infty} \\Bigg\\} \\\\\n&= 2 \\left\\{ -\\frac{1}{\\lambda} (0) - \\frac{1}{\\lambda^2} \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\right\\} \\\\\n&= 2 \\left[ 0 - \\frac{1}{\\lambda^2} (0 - 1) \\right] \\\\\n&= \\frac{2}{\\lambda^2}.\n\\end{align*}\n\\tag{E.17}\\]\nFinally, we plug Equation E.17 into Equation E.14:\n\\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\frac{1}{\\lambda^2} \\\\\n&= \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} \\\\\n&= \\frac{1}{\\lambda^2}. \\qquad \\qquad \\square\n\\end{align*}\n\\]\n\nScale Parametrization\nVia Equation C.5 and the Equation C.4 of a continuous expected value, the variance of an Exponential-distributed random variable \\(Y\\) can be found as follows:\n\nProof. \\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\left[ \\mathbb{E}(Y)\\right]^2 \\\\\n&= \\mathbb{E} \\left( Y^2 \\right) - \\beta^2 \\qquad \\text{since $\\mathbb{E}(Y) = \\beta$}.\n\\end{align*}\n\\tag{E.18}\\]\nNow, we need to find \\(\\mathbb{E} \\left( Y^2 \\right)\\) from Equation E.18. Hence, we make the following derivation via the LOTUS from Equation C.2 when \\(g(Y) = y^2\\):\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\int_{y = 0}^{y = \\infty} y^2 f_Y \\left(y \\mid \\beta \\right) \\mathrm{d}y \\\\\n&= \\int_{y = 0}^{y = \\infty} y^2 \\frac{1}{\\beta} \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\\\\n&= \\frac{1}{\\beta} \\int_{y = 0}^{y = \\infty} y^2 \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y. \\\\\n\\end{align*}\n\\tag{E.19}\\]\nEquation E.19 cannot be solved straightforwardly, we need to use integration by parts as follows:\n\\[\n\\begin{align*}\nu &= y^2 & &\\Rightarrow & \\mathrm{d}u &= 2y \\mathrm{d}y \\\\\n\\mathrm{d}v &= \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y & &\\Rightarrow & v &= -\\beta \\exp \\left( -\\frac{y}{\\beta} \\right),\n\\end{align*}\n\\]\nwhich yields\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= \\frac{1}{\\beta} \\left[ u v \\Bigg|_{y = 0}^{y = \\infty} - \\int_{y = 0}^{y = \\infty} v \\mathrm{d}u \\right] \\\\\n&= \\frac{1}{\\beta} \\Bigg\\{ \\left[ -\\beta y^2 \\exp \\left( -\\frac{y}{\\beta} \\right) \\right] \\Bigg|_{y = 0}^{y = \\infty} + \\\\\n& \\qquad 2 \\beta \\int_{y = 0}^{y = \\infty} y \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\Bigg\\} \\\\\n&= \\frac{1}{\\beta} \\Bigg\\{ -\\beta \\Bigg[ \\underbrace{\\infty \\times \\exp(-\\infty)}_{0} - \\underbrace{0 \\times \\exp(0)}_{0} \\Bigg] + \\\\\n& \\qquad 2 \\beta \\int_{y = 0}^{y = \\infty} y \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\Bigg\\} \\\\\n&= \\frac{1}{\\beta} \\left\\{ -\\beta (0) + 2 \\beta \\int_{y = 0}^{y = \\infty} y \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\right\\} \\\\\n&= \\frac{1}{\\beta} \\left\\{ 0 + 2 \\beta \\int_{y = 0}^{y = \\infty} y \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\right\\} \\\\\n&= 2 \\int_{y = 0}^{y = \\infty} y \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y. \\\\\n\\end{align*}\n\\tag{E.20}\\]\nAgain, we need to apply integration by parts to solve Equation E.20:\n\\[\n\\begin{align*}\nu &= y & &\\Rightarrow & \\mathrm{d}u &= \\mathrm{d}y \\\\\n\\mathrm{d}v &= \\exp \\left( -\\frac{y}{\\beta} \\right)\\mathrm{d}y & &\\Rightarrow & v &= -\\beta \\exp \\left( -\\frac{y}{\\beta} \\right),\n\\end{align*}\n\\]\nwhich yields\n\\[\n\\begin{align*}\n\\mathbb{E} \\left( Y^2 \\right) &= 2 \\left[ u v \\Bigg|_{y = 0}^{y = \\infty} - \\int_{y = 0}^{y = \\infty} v \\mathrm{d}u \\right] \\\\\n&= 2 \\left\\{ \\left[ -\\beta y \\exp \\left( -\\frac{y}{\\beta} \\right) \\right] \\Bigg|_{y = 0}^{y = \\infty} + \\beta \\int_{y = 0}^{y = \\infty} \\exp \\left( -\\frac{y}{\\beta} \\right) \\mathrm{d}y \\right\\} \\\\\n&= 2 \\Bigg\\{ -\\beta \\Bigg[ \\underbrace{\\infty \\times \\exp(-\\infty)}_{0} - \\underbrace{0 \\times \\exp(0)}_{0} \\Bigg] - \\\\\n& \\qquad \\beta^2 \\exp \\left( -\\frac{y}{\\beta} \\right) \\Bigg|_{y = 0}^{y = \\infty} \\Bigg\\} \\\\\n&= 2 \\left\\{ -\\beta (0) - \\beta^2 \\left[ \\exp \\left( -\\infty \\right) - \\exp \\left( 0 \\right) \\right] \\right\\} \\\\\n&= 2 \\left[ 0 - \\beta^2 (0 - 1) \\right] \\\\\n&= 2 \\beta^2.\n\\end{align*}\n\\tag{E.21}\\]\nFinally, we plug Equation E.21 into Equation E.18:\n\\[\n\\begin{align*}\n\\text{Var} (Y) &= \\mathbb{E} \\left( Y^2 \\right) - \\beta^2 \\\\\n&= 2 \\beta^2 - \\beta^2 \\\\\n&= \\beta^2. \\qquad \\qquad \\square\n\\end{align*}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-gamma-distribution",
    "href": "book/C-distributional-mind-map.html#sec-gamma-distribution",
    "title": "Appendix C — Distributional Mind Map",
    "section": "\nE.4 Gamma",
    "text": "E.4 Gamma",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-logistic-distribution",
    "href": "book/C-distributional-mind-map.html#sec-logistic-distribution",
    "title": "Appendix C — Distributional Mind Map",
    "section": "\nE.5 Logistic",
    "text": "E.5 Logistic",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-normal-distribution",
    "href": "book/C-distributional-mind-map.html#sec-normal-distribution",
    "title": "Appendix C — Distributional Mind Map",
    "section": "\nE.6 Normal",
    "text": "E.6 Normal\nThis is possibly one the most famous probability distributions, and it is also known as Gaussian. It appears in many different statistical tools in the literature where certain regression models are included. Let random variable \\(Y\\) be considered continuous and unbounded. Then, \\(Y\\) is said to have a Normal distribution with the following location continuous parameter \\(\\mu\\) and scale continuous parameter \\(\\sigma^2\\):\n\\[\nY \\sim \\text{Normal}(\\mu, \\sigma^2).\n\\]\n\nE.6.1 Probability Density Function\nThe PDF of \\(Y\\) is the following:\n\\[\nf_Y \\left(y \\mid \\mu, \\sigma^2 \\right) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{\\left[ - \\frac{(y - \\mu)^2}{2 \\sigma^2} \\right]} \\quad \\text{for $y \\in ( -\\infty, \\infty )$.}\n\\tag{E.22}\\]\n\n\nHeads-up on the use of \\(\\pi\\) in the Normal distribution!\n\n\nThe term \\(\\pi\\) in the Normal PDF depicted in Equation E.22 corresponds to the mathematical constant \\(3.141592...\\) Hence, this term does not correspond to another population parameter in this probability distribution.\n\n\nParameters \\(\\mu \\in (-\\infty, \\infty)\\) and \\(\\sigma \\in (0, \\infty)\\) refer to the random process’ location and scale, respectively. Figure E.4 shows nine members of the Normal parametric family, i.e., nine different PDFs with all possible pairwise combinations for three different scale parameters \\(\\mu = -3, 0, 3\\) and shape parameters \\(\\sigma^2 = 0.25, 1, 4\\). We can highlight the following:\n\nRegardless of the scale parameter \\(\\sigma^2\\), as we increase the location parameter \\(\\mu\\), note the center of the corresponding symmetric distribution moves more to the right.\nRegardless of the location parameter \\(\\mu\\), as we increase the scale parameter \\(\\sigma^2\\), note that there is more spread in the corresponding symmetric distribution.\n\n\n\n\n\n\n\n\nFigure E.4: Some members of the Normal or Gaussian parametric family.\n\n\n\n\n\nHow can we verify that Equation E.22 is a proper PDF (i.e., Equation E.22 integrates to one over the support of \\(Y\\))?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/C-distributional-mind-map.html#sec-beta-distribution",
    "href": "book/C-distributional-mind-map.html#sec-beta-distribution",
    "title": "Appendix C — Distributional Mind Map",
    "section": "\nE.7 Beta",
    "text": "E.7 Beta\n\n\n\n\nConsul, P. C., and G. C. Jain. 1973. “A Generalization of the Poisson Distribution.” Technometrics 15 (4): 791–99. http://www.jstor.org/stable/1267389.\n\n\nEarlom, Richard. 1793. “Brook Taylor - National Portrait Gallery.” NPG D6930; Brook Taylor - Portrait - National Portrait Gallery. National Portrait Gallery. https://www.npg.org.uk/collections/search/portrait/mw40921/Brook-Taylor.\n\n\nGregory, James. 1668. Vera circuli et hyperbolae quadratura cui accedit geometria pars vniuersalis inseruiens quantitatum curuarum transmutationi & mensurae. Authore Iacobo Gregorio Abredonensi. Padua, Italy: Patavii: typis heredum Pauli Frambotti bibliop. https://archive.org/details/ita-bnc-mag-00001357-001/page/n10/mode/2up.\n\n\nHarding, Edward. 1798. Portrait of Colin MacLaurin. Courtesy of the Smithsonian Libraries and Archives. https://library.si.edu/image-gallery/72863.\n\n\nMaclaurin, Colin. 1742. A Treatise of Fluxions. Edinburgh, Scotland: Printed for the Author by T.W.; T. Ruddimans. https://archive.org/details/treatiseonfluxio02macl/page/n5/mode/2up.\n\n\nScotland, National Galleries of. n.d. Professor James Gregory, 1638 - 1675 (1). Mathematician. Professor James Gregory, 1638 - 1675 (1). Mathematician | National Galleries. https://www.nationalgalleries.org/art-and-artists/31132/professor-james-gregory-1638-1675-mathematician.\n\n\nTaylor, Brook. 1715. Methodus incrementorum directa & inversa. Auctore Brook Taylor, LL. D. & Regiae Societatis Secretario. London, England: Typis Pearsonianis Prostant apud Gul. Innys ad Insignia Principis in Coemeterio Paulino MDCCXV. https://archive.org/details/bim_eighteenth-century_methodus-incrementorum-d_taylor-brook_1717.\n\n\nWeisstein, Eric W. n.d.a. “Gamma Function.” From MathWorld–A Wolfram Web Resource. https://mathworld.wolfram.com/GammaFunction.html.\n\n\n———. n.d.b. “Taylor Series.” From MathWorld–A Wolfram Web Resource. https://mathworld.wolfram.com/TaylorSeries.html.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Distributional Mind Map</span>"
    ]
  },
  {
    "objectID": "book/D-regression-mind-map.html",
    "href": "book/D-regression-mind-map.html",
    "title": "Appendix D — Regression Mind Map",
    "section": "",
    "text": "Image by Manfred Steger via Pixabay.\n\n\n\nThe regression mind map is a key component of the philosophy behind this book, besides the data science workflow from Section 1.2 and the ML-Stats dictionary found in Appendix A. Figure D.1 shows this regression mind map split in two zones by colour: discrete and continuous. This regression mind map is handy when executing the data modelling stage from the data science workflow, as explained in Section 1.2.4. Recall the first step in this stage is to choose a suitable regression model, and we made this decision in the function of the type of outcome \\(Y\\) we are dealing with. That said, the distributional mind map from Figure C.1 complements the regression mind map when identifying the correct type of outcome \\(Y\\).\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n      {{Binary &lt;br/&gt;Outcome Y}}\n        {{Ungrouped &lt;br/&gt;Data}}\n          )Chapter 8: &lt;br/&gt;Binary Logistic &lt;br/&gt;Regression(\n            (Bernoulli &lt;br/&gt;Outcome Y)\n        {{Grouped &lt;br/&gt;Data}}\n          )Chapter 9: &lt;br/&gt;Binomial Logistic &lt;br/&gt;Regression(\n            (Binomial &lt;br/&gt;Outcome Y)\n      {{Count &lt;br/&gt;Outcome Y}}\n        {{Equidispersed &lt;br/&gt;Data}}\n          )Chapter 10: &lt;br/&gt;Classical Poisson &lt;br/&gt;Regression(\n            (Poisson &lt;br/&gt;Outcome Y)\n        {{Overdispersed &lt;br/&gt;Data}}\n          )Chapter 11: &lt;br/&gt;Negative Binomial &lt;br/&gt;Regression(\n            (Negative Binomial &lt;br/&gt;Outcome Y)\n        {{Overdispersed or &lt;br/&gt;Underdispersed &lt;br/&gt;Data}}\n          )Chapter 13: &lt;br/&gt;Generalized &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Generalized &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n        {{Zero Inflated &lt;br/&gt;Data}}\n          )Chapter 12: &lt;br/&gt;Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Regression(\n            (Zero Inflated &lt;br/&gt;Poisson &lt;br/&gt;Outcome Y)\n      {{Categorical &lt;br/&gt;Outcome Y}}\n        {{Nominal &lt;br/&gt;Outcome Y}}\n          )Chapter 14: &lt;br/&gt;Multinomial &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Multinomial &lt;br/&gt;Outcome Y)\n        {{Ordinal &lt;br/&gt;Outcome Y}}\n          )Chapter 15: &lt;br/&gt;Ordinal &lt;br/&gt;Logistic &lt;br/&gt;Regression(\n            (Logistic &lt;br/&gt;Distributed &lt;br/&gt;Cumulative Outcome &lt;br/&gt;Probability)\n\n\n\n\n\n\n\n\nFigure D.1: Regression analysis mind map depicting all modelling techniques to be explored in this book. Depending on the type of outcome \\(Y\\), these techniques are split into two large zones: discrete and continuous.\n\n\n\nSuppose we start reading this regression map clockwise in the continuous zone. In that case, note this zone starts with the cloud corresponding to Chapter 3 on the regression model called Ordinary Least-squares (OLS). Moreover, we can see that OLS is meant for an unbounded outcome \\(Y \\in (-\\infty, \\infty)\\). Then, we can proceed to the distributional assumption on \\(Y\\) in OLS, which would be Normal. Following up with the cloud corresponding to Chapter 4 on the generalized linear model (GLM) called Gamma regression, we can see this model is meant for a nonnegative outcome \\(Y \\in [0, \\infty)\\) where we assume a Gamma distribution for \\(Y\\). This way of reading the continuous zone in the mind map will persist until the survival analysis models: Chapter 6 and Chapter 7.\nThen, we can proceed to the discrete zone with the cloud corresponding to Chapter 8 on the GLM called Binary Logistic regression which aims to model a binary outcome \\(Y \\in \\{0, 1 \\}\\). Note that the Binary Logistic regression model is meant for ungrouped data where each row in the training dataset contains unique feature values. Hence, in this modelling case, we assume the outcome \\(Y\\) as a Bernoulli trial where \\(1\\) indicates a success and \\(0\\) indicates a failure. Then, suppose we take another clockwise case such as Chapter 10 on the GLM called Classical Poisson regression, this model is suitable for count-type outcomes where equidispersion is present (i.e., the mean of the counts is equal to its corresponding variance). Finally, this model assumes that the outcome is Poisson-distributed. This way of reading the discrete zone in the mind map will persist until Chapter 15 on the GLM called Ordinal Logistic Regression.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Regression Mind Map</span>"
    ]
  },
  {
    "objectID": "book/continuous-zone.html",
    "href": "book/continuous-zone.html",
    "title": "Continuous Cuisine",
    "section": "",
    "text": "After the introduction to this textbook on dynamics in Chapter 1 and the refresher on the fundamentals of probability and statistical inference provided in Chapter 2, we have divided the subsequent thirteen core chapters into two major sections: continuous outcomes and discrete outcomes. This division is based on insights from 1.2.4 Data Modelling regarding the data modelling stage of the data science workflow. It indicates that we must choose an appropriate regression model considering the following factors: response type (i.e., the type of outcome), flexibility, and interpretability. Therefore, regarding response type, it is convenient for the efficient use of this textbook to group these two major types of outcomes. Note that the diagram in Figure 1 will continue to evolve as we progress through our regression journey in these areas.\n\n\n\n\n\n\n\n\nmindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 1: Initial regression analysis mind map where, depending on the type of outcome \\(Y\\), we will split out modelling techniques into two large zones: discrete and continuous.\n\n\n\nAt the beginning of each of the thirteen core chapters, we will include useful recommendations on when to use and when not to use each model. These guidelines will be beneficial during the data modelling stage, helping us choose the suitable regression model based on our inquiries and data conditions. It is essential to note that these guidelines will also address the modelling assumptions that each model makes during the estimation stage. These assumptions must always be verified through the corresponding goodness of fit stage, as discussed in 1.2.6 Goodness of Fit. Additionally, when outlining situations in which a specific model should not be used, we will provide alternative modelling approaches, drawing on both further chapters and external resources.\n\n\n\nImage by manfredsteger via Pixabay.\n\n\nLet us begin with what we refer to as “continuous cuisine” in this textbook. This category includes all regression models that have a continuous outcome denoted by \\(Y\\) (note the uppercase, as our estimation methods will treat it as a random variable). In real-life situations, we often deal with continuous outcomes of interest, which we aim to model using regression analysis conditioned on observed explanatory variables (the \\(x\\) regressors). In the statistical literature, specific regression techniques are tailored to the characteristics and distribution of the continuous outcome (or random variable) as outlined in 2.1.3 The Random Variables.\nChapter 3 begins with the classic Ordinary Least-squares (OLS) regression model, which is designed to formally model outcomes that are well-behaved and unbounded through linear relationships with respect to the regressors. It also assumes that the residuals are normally distributed with a common variance (i.e., homoscedasticity) to address inferential inquiries, a condition that extends to the \\(Y\\) responses. OLS is the foundational model in regression analysis, and its primary estimation method involves minimizing the squared errors between the observed outcomes in the training dataset and their corresponding in-sample predictions (calculated using the observed regressors and a mathematically determined set of estimated regression parameters).\n\n\n\nImage by Satheesh Sankaran via Pixabay.\n\n\nReal-life continuous outcomes are not always unbounded, which means that a regression model like OLS may not be the most suitable option for modelling a given outcome \\(Y\\) under these circumstances. When responses are strictly positive and skewed (for example, customer purchases in an online store) the assumptions underlying OLS are violated. This is because OLS assumes normality, and the probability distribution in this case is unbounded and asymmetrical (refer to E.6 Normal). In such situations, Gamma regression becomes a useful alternative, as it operates under the assumption that \\(Y\\) follows a Gamma distribution. The Gamma distribution is well-suited for modelling responses because its support accommodates nonnegativity and skewness (see E.4 Gamma). The details of the corresponding regression model are elaborated in Chapter 4, where it is classified as a generalized linear model (GLM).\n\n\nDefinition of generalized linear models\n\n\nAn umbrella of regression approaches that model the conditional expected value of the response variable \\(Y\\) based on a set of observed regressors \\(x\\). Unlike a traditional model such as the continuous OLS that relies solely on a Normal distribution to make inference, GLMs extend this distributional assumption, allowing for a variety of probability distributions for the response variable. Note that this umbrella encompasses approaches that accommodate continuous or discrete responses \\(Y\\). According to Casella and Berger (2024), a typical GLM consists of three key components:\n\nRandom Component: The response variables in a training dataset of size \\(n\\) (i.e., the random variables \\(Y_1, Y_2, \\ldots, Y_n\\)) are statistically independent but not identically distributed. Still, they do belong to the same family of probability distributions (e.g., Gamma, Beta, Poisson, Bernoulli, etc.).\nSystematic Component: For the \\(i\\)th observation, this component depicts how the \\(k\\) regressors \\(x_{i, j}\\) (for \\(j = 1, 2, \\ldots, k\\)) come into the GLM as a linear combination involving \\(k + 1\\) regression parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_k\\). This relationship is expressed as\n\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_{i, 1} + \\beta_2 x_{i, 2} + \\ldots + \\beta_k x_{i, k}.\n\\]\n\nLink Function: This component connects (or “links”) the systematic component \\(\\eta_i\\) with the mean of the random variable \\(Y_i\\), denoted as \\(\\mu_i\\). The link function is mathematically represented as\n\n\\[\ng(\\mu_i) = \\eta_i.\n\\]\nNelder and Wedderburn (1972) introduced this umbrella term called GLM in the statistical literature and identified a set of distinct statistical models that shared the above three components.\n\n\n\n\n\n\nImage by Megan Rexazin Conde via Pixabay.\n\n\nIn other situations, the outcome \\(Y\\) is bounded between \\(0\\) and \\(1\\), meaning it represents a proportion. For instance, we may need to model the fraction of a monetary budget allocated for a specific purpose or the share of time spent on a particular activity, all of which depend on a given set of observed regressors \\(x\\). In these cases, Beta regression (a model discussed in Chapter 5) is a suitable modelling approach. It effectively addresses the bounded nature of proportions via the Beta distribution (see E.7 Beta) and allows for the assumption of heteroscedasticity (that is, different variances across all our training data points).\n\n\n\nImage by Megan Rexazin Conde via Pixabay.\n\n\nTo wrap up this continuous zone, we encounter situations where we need to model the “time until an event of interest.” This includes scenarios such as the time \\(Y\\) until an equipment fails in a manufacturing facility, the time \\(Y\\) until a customer decides to churn, or the survival time \\(Y\\) of a patient. Each of these outcomes may depend on a specific set of observed variables \\(x\\). Similar to Gamma regression, we are still modelling a nonnegative continuous outcome. However, this type of regression extends beyond GLMs and falls under a statistical field known as survival analysis. Survival analysis utilizes additional probabilistic tools relevant to a random variable \\(Y\\), including the cumulative distribution function, survival function, and hazard function. This textbook approaches survival models in the following ways:\n\nParametric Survival Modelling: Chapter 6 focuses on this category of survival models, where we assume a specific distribution for the survival time \\(Y\\) that we are interested in. This includes distributions such as the Exponential (as in E.3 Exponential), Weibull (as in E.1 Weibull), and Lognormal (as in E.2 Lognormal).\nSemiparametric Survival Modelling: Chapter 7 examines this category of survival models, with a specific emphasis on the Cox Proportional-Hazards model. This model offers greater flexibility by not enforcing a specific form on a part of the hazard function for \\(Y\\).\n\n\n\nDefinition of semiparametric model\n\n\nA semiparametric model is a statistical model that incorporates both parametric and nonparametric parts. In the context of linear regression, these parts can be described as follows:\n\nThe parametric part includes the systematic component where \\(k\\) observed regressors \\(x\\) are modelled along with \\(k + 1\\) regression parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_k\\) in a linear combination.\nThe nonparametric part does not impose specific assumptions on one or more modelling components, allowing the observed training dataset to estimate these elements without requiring any probability distributions.\n\n\n\nAll the regression techniques discussed above make up what we refer to as the “continuous cuisine” of this textbook. The primary purpose of organizing this major section is to help you identify the most suitable technique for your continuous outcome variable \\(Y\\), while also examining how the training data behaves descriptively. This understanding will guide you in making informed decisions about your data modelling assumptions, particularly with respect to the characteristics of the previously mentioned probability distributions, especially in the context of parametric regression models. In the five chapters that follow, we will illustrate how each regression model incorporates its specific assumptions while also highlighting its limitations.\n\n\n\n\nCasella, G., and R. Berger. 2024. Statistical Inference. Chapman & Hall/CRC Texts in Statistical Science. CRC Press. https://www.taylorfrancis.com/books/mono/10.1201/9781003456285/statistical-inference-roger-berger-george-casella.\n\n\nNelder, J. A., and R. W. M. Wedderburn. 1972. “Generalized Linear Models.” Journal of the Royal Statistical Society, Series a (General) 135 (3): 370–84.",
    "crumbs": [
      "Continuous Cuisine"
    ]
  },
  {
    "objectID": "book/discrete-zone.html",
    "href": "book/discrete-zone.html",
    "title": "Discrete Cuisine",
    "section": "",
    "text": "mindmap\n  root((Regression \n  Analysis)\n    Continuous &lt;br/&gt;Outcome Y\n      {{Unbounded &lt;br/&gt;Outcome Y}}\n        )Chapter 3: &lt;br/&gt;Ordinary &lt;br/&gt;Least Squares &lt;br/&gt;Regression(\n          (Normal &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Outcome Y}}\n        )Chapter 4: &lt;br/&gt;Gamma Regression(\n          (Gamma &lt;br/&gt;Outcome Y)\n      {{Bounded &lt;br/&gt;Outcome Y &lt;br/&gt; between 0 and 1}}\n        )Chapter 5: Beta &lt;br/&gt;Regression(\n          (Beta &lt;br/&gt;Outcome Y)\n      {{Nonnegative &lt;br/&gt;Survival &lt;br/&gt;Time Y}}\n        )Chapter 6: &lt;br/&gt;Parametric &lt;br/&gt; Survival &lt;br/&gt;Regression(\n          (Exponential &lt;br/&gt;Outcome Y)\n          (Weibull &lt;br/&gt;Outcome Y)\n          (Lognormal &lt;br/&gt;Outcome Y)\n        )Chapter 7: &lt;br/&gt;Semiparametric &lt;br/&gt;Survival &lt;br/&gt;Regression(\n          (Cox Proportional &lt;br/&gt;Hazards Model)\n            (Hazard Function &lt;br/&gt;Outcome Y)\n    Discrete &lt;br/&gt;Outcome Y\n\n\n\n\n\n\n\n\nFigure 1",
    "crumbs": [
      "Discrete Cuisine"
    ]
  }
]