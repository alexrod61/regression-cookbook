# Getting Ready for Regression Cooking! {#sec-intro}

```{r}
#| include: false

colourize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

:::: columns
::: {.column width="45%"}
It is time to prepare for the different regression techniques we will use in each of the subsequent chapters of this book. That said, there is a strong message I want to convey across all this work:

> **Different modelling estimation techniques in regression analysis can be smoothly grasped when we develop a fair probabilistic and inferential intuition on our populations or systems of interest.**

:::
::: {.column width="55%"}
![Image by [*Lucas Israel*](https://pixabay.com/users/lucasjisrael-43158173/) via [*Pixabay*](https://pixabay.com/illustrations/flowchart-diagram-sketch-notepad-8860311/).](img/flowchart.png){width="480" fig-align="right"} 
:::
::::

The above statement has a key statistical foundation on how data is generated and can be modelled via different regression approaches. More details on the concepts and ideas associated with this foundation will be delivered in @sec-quick-review.

Then, once we have reviewed these statistical concepts and ideas, we will move on to the three big pillars I previously pointed out: 

1. The use of an ordered **data science workflow** in @sec-ds-workflow,
2. choosing the proper workflow flavour according to either an **inferential** or **predictive** paradigm as shown in @fig-ds-workflow, and
2. the correct use of an **appropriate regression model** based on the response or outcome of interest as shown in the mind map from @sec-regression-mindmap (analogous to a **regression toolbox**).

::: {.callout-tip icon=false}
# The Rationale Behind the Three Pillars

Each data science-related problem that uses regression analysis might have distinctive characteristics considering inferential (statistics!) or predictive (machine learning!) inquiries. Specific problems would implicate using outcomes (or responses) related to survival times (e.g., the time until one particular equipment of a given brand fails), categories (e.g., a preferred musical genre in the Canadian young population), counts (e.g., how many customers we would expect on a regular Monday morning in a national central bank), etc. Moreover, under this regression context, our analyses would be expanded to explore and assess how our outcome of interest is related to a further set of variables (the so-called features!). For instance, following up with the categorical outcome of the preferred musical genre in the Canadian young population, we might analyze how particular age groups prefer certain genres over others or even how preferred genres compare each other across different Canadian provinces in this young population. **The sky is the limit here!**\

Therefore, we might be tempted to say that each regression problem should have its own workflow, given that the regression model to use would implicate particular analysis phases. **However, it turns out that is not the case to a certain extent**, and we have a regression workflow in @fig-ds-workflow to support this bold statement as a proof of concept for thirteen different regression models (i.e, thirteen subsequent chapters in this book). The workflow aims to homogenize our data analyses and make our modelling process more transparent and smoother. We can deliver exactly concluding insights as data storytelling while addressing our initial main inquires. Of course, when depicting the workflow as a flowchart, there will be decision points that will turn it into **inferential** or **predictive** (the second pillar). Finally, where does the third pillar come into play in this workflow? This pillar is contained in the **data modelling stage**, where the mind map from @fig-regression-mindmap will come in handy.
:::

Now, before delving into probability and frequentist statistical inference, let us establish a convention on the use of admonitions beginning @sec-quick-review in this textbook:

::: {.callout-important}
# Definition

A formal statistical and/or machine learning definition. This admonition aims to untangle the significant amount of jargon and concepts that both fields have. Furthermore, alternative terminology will be brought up when necessary to indicate the same definition across both fields.
:::

::: {.callout-note}
# Heads-up!

An idea (or ideas!) of key relevance for any given modelling approach, specific workflow stage or data science-related terminology. This admonition also extends to crucial statistical or machine learning topics that the reader would be interested in exploring more in-depth. 
:::

::: {.callout-tip}
# Tip

An idea (or ideas!) that might be slightly out of the scope of the topic any specific section is discussing. Still, I will provide significant insights on the matter along with further literature references to look for.
:::

The core idea of the above admonition arrangement is to allow the reader to discern between ideas or concepts that are key to grasp from those whose understanding might not be highly essential (but still interesting to check out in external references!).

## The ML-Stats Dictionary {#sec-ml-stats-dictionary}

The above admonition for a **definition** will pave the way to a complimentary aspect of this textbook that I have had in mind since I started teaching statistics (and, more especially, regression analysis) in a data science context. Machine learning and statistics usually overlap across many subjects, and regression modelling is no exception. Topics we teach in an utterly regression-based course, under a purely statistical framework, also appear in machine learning-based courses such as fundamental supervised learning, but often with different terminology. On this basis, the Master of Data Science (MDS) program at the University of British Columbia (UBC) provides the [MDS Stat-ML dictionary](https://ubc-mds.github.io/resources_pages/terminology/) [@gelbart2017] under the following premises:

> *This document is intended to help students navigate the large amount of jargon, terminology, and acronyms encountered in the MDS program and beyond.*

> *This section covers terms that have different meanings in different contexts, specifically statistics vs. machine learning (ML).*

Indeed, both disciplines have a tremendous amount of jargon and terminology. Furthermore, as I previously emphasized in the **Preface**, machine learning and statistics construct a **substantial synergy** that is reflected in data science. Even with this, people in both fields could encounter miscommunication issues when working together. This should not happen if we build solid bridges between both disciplines. Hence, a comprehensive **ML-Stats dictionary** (*ML* stands for *Machine Learning*) is imperative, and this textbook offers a perfect opportunity to build this resource. Primarily, this dictionary clarifies any potential confusion between statistics and machine learning regarding terminology within supervised learning and regression analysis contexts.  

::: {#nte-term-highlight .callout-note}
# Heads-up on terminology highlights!

Following the spirit of the **ML-Stats dictionary**, throughout the book, all `r colourize("statistical", "magenta")` terms will be highlighted in `r colourize("magenta", "magenta")` whereas the `r colourize("machine learning", "orange")` terms will be highlighted in `r colourize("orange", "orange")`. This colour scheme strives to combine this terminology so we can switch from one field to another in an easier way. With practice and time, we should be able to jump back and forth when using these concepts.
:::

Finally, @sec-dictionary will be the section in this book where the reader can find all those `r colourize("statistical", "magenta")` and `r colourize("machine learning-related", "orange")` terms in alphabetical order. Notable terms (either statistical or machine learning-related) will include an admonition identifying which terms (again, either statistical or machine learning-related) are **equivalent** (**or maybe NOT equivalent!**). Take as an example the statistical term `r colourize("dependent variable", "magenta")`:

> In supervised learning, it is the main variable of interest we are trying to **learn** or **predict**, or equivalently, the variable we are trying **explain** in a statistical inference framework.

Then, the above definition will be followed by this admonition:

::: {.callout-warning}
## Equivalent to:

`r colourize("Response", "magenta")`, `r colourize("outcome", "orange")`, `r colourize("output", "orange")` or `r colourize("target", "orange")`.
:::

Note we have identified four equivalent terms for the term `r colourize("dependent variable", "magenta")`. Furthermore, according to our already defined colour scheme, these terms can be `r colourize("statistical", "magenta")` or `r colourize("machine learning-related", "orange")`.

::: {#nte-use-terminology .callout-note}
# Heads-up on the use of terminology!

Throughout this book, I will interchangeably use specific terms when explaining the different regression approaches in each chapter. Whenever confusion arises about using these interchangeable terms, it is highly recommended to consult their definitions and equivalences (or non-equivalences) in @sec-dictionary.
:::

Now, let us proceed to a quick review on probability and statistics in a frequentist framework. This review will be important to understanding the philosophy of modelling parameter estimation, mainly in relation to statistical inference.

## A Quick Review on Probability and Frequentist Statistical Inference {#sec-quick-review}

:::: columns
::: {.column width="45%"}
When I was an undergraduate student and took my very first course in probability and statistics (inference included!) in an industrial engineering context, I used to feel quite overwhelmed by the large amount of jargon and formulas one had to grasp and use regularly for primary engineering fields such as quality control in a manufacturing facility. *Population parameters*, *hypothesis testing*, *tests statistics*, *significance level*, *$p$-values*, and *confidence intervals* (**do not worry, our `r colourize("statistical", "magenta")`/`r colourize("machine learning", "orange")` scheme will come in later in this quick review**) were appearing here and there! And to my frustration, I could never find a statistical connection between all these inferential tools! Instead, I relied on mechanistic procedures when solving assignments or exam problems. 
:::
::: {.column width="55%"}
![Image by [*OpenClipart-Vectors*](https://pixabay.com/users/openclipart-vectors-30363/) via [*Pixabay*](https://pixabay.com/vectors/panda-cute-bear-blue-question-149818/).](img/panda.png){width="450" fig-align="right"} 
:::
::::

For instance, when performing hypothesis testing for a two-sample $t$-test, I never reflected on what the hypotheses were trying to indicate for the corresponding population parameters(s) nor how the test statistic was related to these hypotheses. Moreover, my interpretation of the resulting $p$-value and/or confidence interval was purely mechanical with the inherent claim: 

> *With a significance level $\alpha = 0.05$, we reject (**or fail to reject, if that is the case!**) the null hypothesis given that...* 

Honestly, I am not proud of this whole mechanical way of doing statistics now that I reflect on it after many years of practice, teaching, and research. Then, of course, the above situation should not happen when we learn key statistical topics for the very first time as undergraduate students. That is why we will dig into a more intuitive way of viewing probability and its crucial role in statistical inference. This matter will help us deliver more coherent storytelling when presenting our results in practice during any regression analysis to our peers or stakeholders. Note that the role of probability also extends to model training when it comes to supervised learning and not just in regard to statistical inference.

Having said all this, it is time to introduce a statement that I ended up reasoning the very first time I taught hypothesis testing in the introductory statistical inference course in the MDS program at UBC:

> **In statistical inference, everything always boils down to randomness and how we can control it!**

That is quite a bold statement! Nonetheless, once one starts teaching statistical topics to audiences not entirely familiar with the usual field jargon, the idea of randomness always persists across many different tools. And, of course, regression analysis is not an exception at all since it also involves inference on population parameters of interest! This is why I have allocated this section in the textbook to explain core probabilistic and inferential concepts to pave the way to its role in regression analysis.

::: {#nte-mechinical-clarification .callout-note}
# Heads-up on why we mean as a non-ideal mechanical analysis!

The reader might get a contradictory impression of why the above mechanical way of performing hypothesis testing is viewed as something **not ideal**, whereas the word **cookbook** appears in this book's title. Certainly, the **cookbook** idea refers to some class of recipe to perform data modelling (which is reflected in the workflow from @fig-ds-workflow). Still, there is a critical difference between a non-ideal mechanical way of performing hypothesis testing versus our data modelling workflow.\

On the one hand, the non-ideal mechanical way refers to **the use of a tool without understanding the rationale of what this tool stands for**, resulting in vacuous and standard statements that we would not be able to explain any way further, such as the statement I previously indicated:\

> *With a significance level $\alpha = 0.05$, we reject (**or fail to reject, if that is the case!**) the null hypothesis given that...*\

What if a stakeholder of our analysis asks us in plain words what a significance level means? Why are we phrasing our conclusion on the null hypothesis and not directly on the alternative one? As a data scientist, one should be able to deliver the corresponding explanations of why the whole inference process is yielding that statement without misleading the stakeholder's understanding. **For sure, this also implicates appropriate communication skills that cater to general audiences rather than just statistical.**\

On the other hand, as we will elaborate in further detail, the workflow in @fig-ds-workflow has stages that demand a **thorough and precise understanding of what exactly we are executing in our analysis**. Moving forward from one current stage in the workflow to the next (without a complete understanding of what is going on in the current one) would risk carrying over false insights that might become faulty data storytelling of the whole analysis.
:::

Finally, even though this book has suggested reviews related to the basics of probability via different distributions and the fundamentals of frequentist statistical inference as stated in **Audience and Scope**, we will retake essential concepts as follows:

- The role of *random variables* and *probability distributions* and the governance of *population (or system) parameters* (i.e., the so-called Greek letters we usually see in statistical inference and regression analysis). @sec-basics-prob will explore these topics more in detail while connecting them to the subsequent inferential terrain under a *frequentist context*.
- When delving into supervised learning and regression analysis, we might wonder how randomness is incorporated into *model fitting* (i.e., *parameter estimation*). That is quite a fascinating aspect, implemented via a crucial statistical tool known as *maximum likelihood estimation*. This tool is heavily related to the concept of *loss function* in supervised learning. @sec-mle will explore these matters in more detail and how the concept of *random sample* is connected to this estimation tool.
- @sec-basics-inf will explore the basics of *hypothesis testing* and its intrinsic components such as *null* and *alternative hypotheses*, *type I* and *type II* errors, *test statistic*, *standard error*, *$p$-value*, and *confidence interval*.
- Finally, @sec-sup-learning-regression will briefly discuss the connections between supervised learning and regression analysis regarding terminology.

Without further ado, let us start with reviewing core concepts in probability via quite a tasty example.

### Basics of Probability {#sec-basics-prob}

In terms of regression analysis and its supervised analysis counterpart (either on an **inferential** or **predictive** framework), `r colourize("probability", "magenta")` can be viewed as the solid foundation on which more complex tools, including estimation and `r colourize("hypothesis testing", "magenta")`, are built upon. Under this foundation, our data is coming from a given `r colourize("population", "magenta")` or system of interest. Moreover, the `r colourize("population", "magenta")` or system is assumed to be governed by `r colourize("parameters", "magenta")` which, as data scientists or researchers, they are of their best interest to study. That said, the terms `r colourize("population", "magenta")` and `r colourize("parameter", "magenta")` will pave the way to our first statistical definitions.

::::: {#imp-population .callout-important}
# Definition of population

It is a **whole collection of individuals or items** that share **distinctive attributes**. As data scientists or researchers, we are interested in studying these attributes, which we assume are **governed** by `r colourize("parameters", "magenta")`. In practice, we must be **as precise as possible** when defining our given `r colourize("population", "magenta")` such that we would frame our entire data modelling process since its very early stages. Examples of a `r colourize("population", "magenta")` could be the following:\

:::: columns
::: {.column width="45%"}

- *Children between the ages of 5 and 10 years old in states of the American West Coast.*
- *Customers of musical vinyl records in the Canadian provinces of British Columbia and Alberta.*
- *Avocado trees grown in the Mexican state of Michoacán.*
- *Adult giant pandas in the Southwestern Chinese province of Sichuan.*
- *Mature açaí palm trees from the Brazilian Amazonian jungle.*
:::
::: {.column width="55%"}
![Image by [*Eak K.*](https://pixabay.com/users/eak_kkk-907811/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1044891) via [*Pixabay*](https://pixabay.com/photos/lego-toys-figurines-crowd-many-1044891/).](img/lego.jpg){width="450" fig-align="right"} 
:::
::::

Note that the term `r colourize("population", "magenta")` could be exchanged for the term **system**, given that certain contexts do not particularly refer to individuals or items. Instead, these contexts could refer to **processes** whose attributes are also governed by `r colourize("parameters", "magenta")`. Examples of a **system** could be the following:

- *The production of cellular phones in a set of manufacturing facilities.*
- *The sale process in the Vancouver franchises of a well-known ice cream parlour.*
- *The transit cycle of the twelve lines of Mexico City's subway.*
:::::

::::: {#imp-parameter .callout-important}
# Definition of parameter

It is a characteristic (**numerical** or even **non-numerical**, such as a **distinctive category**) that **summarizes** the state of our `r colourize("population", "magenta")` or **system** of interest. Examples of a `r colourize("population parameter", "magenta")` can be described as follows:

:::: columns
::: {.column width="45%"}

- *The average weight of children between the ages of 5 and 10 years old in states of the American west coast.*
- *The variability in the height of the mature açaí palm trees from the Brazilian Amazonian jungle.*
- *The proportion of defective items in the production of cellular phones in a set of manufacturing facilities.*
- *The average customer waiting time to get their order in the Vancouver franchises of a well-known ice cream parlour.*
:::
::: {.column width="55%"}
![Image by [*meineresterampe*](https://pixabay.com/users/meineresterampe-26089/) via [*Pixabay*](https://pixabay.com/photos/typewriter-old-retro-office-1899760/).](img/typewriter.jpg){width="450" fig-align="right"} 
:::
::::

Note the **standard mathematical notation** for a `r colourize("population parameters", "magenta")` are **Greek letters**. Moreover, in practice, these `r colourize("population parameter(s)", "magenta")` of interest will be **unknown** to the data scientist or researcher. Instead, they would use formal statistical inference to **estimate** them.
:::::

The `r colourize("parameter", "magenta")` definition in @imp-parameter points out a crucial fact in investigating any given `r colourize("population", "magenta")` or system: 

> **Our `r colourize("parameter(s)", "magenta")` of interest are usually *unknown*!** 

Given this fact, it would be pretty unfortunate and inconvenient if we eventually wanted to discover any significant insights about the `r colourize("population", "magenta")` or system. Therefore, let us proceed to our so-called tasty example so we can dive into the need for statistical inference and why `r colourize("probability", "magenta")` is our perfect ally in this `r colourize("parameter", "magenta")` quest.

::::: columns
:::: {.column width="45%"}
Imagine you are the owner of a large fleet of ice cream carts, around 500, to be exact. These ice cream carts operate across different parks in the following Canadian cities: *Vancouver*, *Victoria*, *Edmonton*, *Calgary*, *Winnipeg*, *Ottawa*, *Toronto*, and *Montréal*. In the past, to optimize operational costs, you decided to limit ice cream cones to only two items: *vanilla* and *chocolate* flavours, as in @fig-ice-cream. 

Now, let us point this whole case onto a more statistical and probabilistic field; suppose you have a well-defined overall `r colourize("population", "magenta")` of interest for those above eight Canadian cities: **children between 4 and 11 years old attending these parks during the Summer weekends**. Of course, Summer time is coming this year, and you would like to know which ice cream cone flavour is the favourite one for this population (*and by how much*). As a business owner, knowing these facts would allow you to plan your summer restocks more carefully with your corresponding suppliers. Therefore, you decided to meet with all your regional managers to discuss how to tackle this *demand query*.
::::
:::: {.column width="55%"}
::: {#fig-ice-cream}
![](img/ice-cream.jpg){width="490" fig-align="right"}

The two flavours of the ice cream cone you sell across all your ice cream carts: *vanilla* and *chocolate*. Image by [*tomekwalecki*](https://pixabay.com/users/tomekwalecki-13027968/) via [*Pixabay*](https://pixabay.com/photos/ice-cream-flavor-chocolate-vanilla-4401300/).
:::
::::
:::::

During the meeting with the regional managers, it was decided to run a comprehensive market study on the `r colourize("population", "magenta")`  of interest across the eight Canadian cities right before next Summer (suppose we are currently in Spring). Surprisingly, when discussing study requirements for the marketing firm who would be in charge of it, one of the managers dares to state the following:

> *Since we're already planning to collect consumer data on these cities, let's mimic a census-type study to ensure we can have the **MOST** precise results on their preferences.*



::: {#imp-probability .callout-important}
# Definition of probability

:::

### What is Maximum Likelihood Estimation? {#sec-mle}

### Basics of Frequentist Statistical Inference {#sec-basics-inf}

### Supervised Learning and Regression Analysis {#sec-sup-learning-regression}

## The Data Science Workflow {#sec-ds-workflow}

It is time to review the so-called data science workflow. Each one of these three pillars is heavily connected since a general Data Science workflow is applied in each one of these regression models, which aims to help in our learning (i.e., we would be able to know what exact stage to expect in our data analysis regardless of the regression model we are being exposed to). Therefore, a crucial aspect of the practice of Regression Analysis is the need for this systematic Data Science workflow that will allow us to solve our respective inquiries in a reproducible way. @fig-ds-workflow shows this workflow which has the following general stages (I briefly define each one of them; note a broader delivery will be done in subsequent subsections):

1. **Study design:**
2. **Data collection and wrangling:**
3. **Exploratory data analysis:**
4. **Data modelling:**
5. **Estimation:**
6. **Goodness of fit:**
6. **Results:**
7. **Storytelling** 

::: {#note-no-formal .callout-note}
# What if there is no formal structure in our regression analysis?

Since very early learning stages in data analysis, it is crucial tp

Now, suppose we do not follow a predefined workflow in practice. In that case, we might be at stake in incorrectly addressing our inquiries, translating into meaningless results outside the context of the problem we aim to solve. This is why the formation of a Data Scientist must stress this workflow from the very introductory learning stages.
:::

::: {#fig-ds-workflow}
![](img/data-science-workflow.png){width="1000"} 

Data science workflow for *inferential* and *predictive* inquiries in regression analysis and supervised learning, respectively.
:::

### Study Design {#sec-ds-workflow-study-design}

The first stage of this workflow is heavily related to the *main statistical inquiries* we aim to address throughout the whole data analysis process. As a data scientist, it is your task to primarily translate these inquiries from the stakeholders of the problem as *inferential* or *predictive*. Roughly speaking, this primary classification can be explained as follows:

- **Inferential.** The main objective is to untangle relationships of *association* or *causation* between the regressors (i.e., explanatory variables) and the corresponding response in the context of the problem of interest. Firstly, we would assess whether there is a statistical relationship between them. Then, if significant, we would quantify by how much.
- **Predictive.** The main objective is to deliver response predictions on further observations of regressors, having estimated a given model via a current training dataset. Unlike inferential inquiries, assessing a statistically significant association or causation between our variables of interest is not a primary objective but *accurate predictions*. **This is one of the fundamental paradigms of machine learning.**

::: {#fig-ds-workflow-study-design}
![](img/study-design.png){width="670"}

*Study design* stage from the data science workflow in @fig-ds-workflow. This stage is directly followed by *data collection and wrangling*.
:::

### Data Collection and Wrangling {#sec-ds-workflow-data-collection}

Once we have defined our main statistical inquiries, it is time to collect our data. Note we have to be careful about the way we collect this data since it might have a particular impact on the quality of our statistical practice:

- Regarding inferential inquiries, recall we are approaching populations or systems of interest governed by *unknown and fixed distributional parameters*. Thus, via sampled data, we aim to *estimate* these distributional parameters. This is why **a proper sampling method** on this population or system of interest is critical to obtaining representative data for *appropriate hypothesis testing*.

::: {#tip-sampling .callout-tip}
# A Quick Debrief on Sampling!

Sampling topics are out of the scope of this book. Nevertheless, we still need to stress that a  proper sampling method is also key in inferential inquiries to assess association and/or causation between the regressors and your response of interest. That said, depending on the context of the problem, we could apply either one of the following methods of sampling:

- **Simple random sampling**.
- **Systematic sampling**.
- **Stratified sampling**.
- **Clustered sampling**.
- Etc.

As in the case of Regression Analysis, statistical sampling is a vast field, and we could spend a whole course on it. If you are more interested in these topics, [*Sampling: design and analysis*](https://webcat.library.ubc.ca/vwebv/holdingsInfo?bibId=2206157) by Lohr offers great foundations.
:::


- In practice, regarding predictive inquiries, we would likely have to deal with databases given that our trained models will not be used to make inference and parameter interpretations.

::: {#fig-ds-workflow-data-collection-wrangling}
![](img/data-collection-and-wrangling.png){width="670"} 

*Data collection and wrangling* stage from the data science workflow in @fig-ds-workflow. This stage is directly followed by *exploratory data analysis* and preceded by *study design*.
:::

### Exploratory Data Analysis {#sec-ds-workflow-eda}

::: {#fig-ds-workflow-eda}
![](img/eda.png){width="670"} 

*Exploratory data analysis* stage from the data science workflow in @fig-ds-workflow. This stage is directly followed by *data modelling* and preceded by *data collection and wrangling*.
:::

### Data Modelling {#sec-ds-workflow-modelling}

::: {#fig-ds-workflow-data-modelling}
![](img/data-modelling.png){width="670"} 

*Data modelling* stage from the data science workflow in @fig-ds-workflow. This stage is directly preceded by *exploratory data analysis*. On the other hand, it is directly followed by *estimation* but indirectly with *goodness of fit*. If necessary, the *goodness of fit* stage could retake the process to *data modelling*.
:::

### Estimation {#sec-ds-workflow-estimation}

::: {#fig-ds-workflow-estimation}
![](img/estimation.png){width="670"} 

*Estimation* stage from the data science workflow in @fig-ds-workflow. This stage is directly preceded by *data modelling* and followed by *goodness of fit*. If necessary, the *goodness of fit* stage could retake the process to *data modelling* and then to *estimation*.
:::

### Goodness of Fit {#sec-ds-workflow-goodness-of-it}

::: {#fig-ds-workflow-goodness-of-fit}
![](img/goodness-of-fit.png){width="670"} 

*Goodness of fit* stage from the data science workflow in @fig-ds-workflow. This stage is directly preceded by *estimation* and followed by *results*. If necessary, the *goodness of fit* stage could retake the process to *data modelling* and then to *estimation*.
:::

### Results {#sec-ds-workflow-results}

::: {#fig-ds-workflow-results}
![](img/results.png){width="670"}

*Results* stage from the data science workflow in @fig-ds-workflow. This stage is directly followed by *storytelling* and preceded by *goodness of fit*.
:::

### Storytelling {#sec-ds-workflow-storytelling}

::: {#fig-ds-workflow-storytelling}
![](img/storytelling.png){width="670"}

*Storytelling* stage from the data science workflow in @fig-ds-workflow. This stage preceded by *results*.
:::


## Mindmap of Regression Analysis {#sec-regression-mindmap}

Having defined the necessary statistical aspects to execute a proper supervised learning analysis, either *inferential* or *predictive* across its seven sequential phases, we must dig into the different approaches we might encounter in practice as regression models. The nature of our outcome of interest will dictate any given modelling approach to apply, depicted as clouds in @fig-regression-mindmap. Note these regression models can be split into two sets depending on whether the outcome of interest is *continuous* or *discrete*. Therefore, under a probabilistic view, identifying the nature of a given random variable is crucial in regression analysis.

::: {#fig-regression-mindmap}
[![](https://mermaid.ink/img/pako:eNqVVd9v2jAQ_lciP4FEOgolDdE0aaVTN2kr1VhfKl6u8REsJT56dugPxP8-J6SCAgHqp-R8933fnc_nhYhJoohEprTMYDbWnsdEtvEXE0ZjFGmvsHnedw3pq1GmufodkLZK55Sbr4_85dswtzFluNqLIhWTbkzAeBPwOU-Rq6hi3etHyrVEuTZ9DMCXeAo6QR9SuxFXrOaQpdLAryXpbwRj_dFTDk5qaVmrbqwDb0lrTMCqOdZRpqQTH5jpuSD1WSXTHeobyDLYYtnwuWOaEVtnrCOZIceoLSS4DX2F9gDyKOe5mkPq_VNZbQYxpKglsG-dk9kiWCzugCFDyypeLj_ulQLW26WMd8b6mq5QR5ip2WHkAb1slKaC_AlvwNJ4O9DXysSMFo_1lJ1uZHhVdkRdYaSK0bfPtFOSkVNuVkzXYGGv-quNZqNEGVsV6EBNXLf5J0JTpqqCHAcfuDtj63LMDfLumf94ypVUxrWdQXlYzCAFRxhXau5IGUP6WKbDOfKJ-LfV_Su9PmR-Kr5HXPrfu8lxKusNamRI1VvleGJeD8jk_9KTFOwxhl3XoxwD55oQF8WuO043IrZP89ZVTG9G7JlhroP8PG3ukfknT-3n2q1Yq1l7nJP2clbRn-B7X42deTGU0jRrnIv7tj9AtESGnIGS7n1bFOFjYafoJoqI3KfECbi6jMVYL50r5JZGrzoWkeUcWyKfSXdS1woSN99ENIHUOOsMtIgW4kVEficMzy4vgiDo94Ner93vXrTEq7OfBxdnl0Ev7AVhOwyCdrBsiTcih3F-1u2e98NOEIT9sNvudDstgVJZ4j-rR7h8i0uShzKgULL8DwFMR-8?type=png)](https://mermaid.live/edit#pako:eNqVVd9v2jAQ_lciP4FEOgolDdE0aaVTN2kr1VhfKl6u8REsJT56dugPxP8-J6SCAgHqp-R8933fnc_nhYhJoohEprTMYDbWnsdEtvEXE0ZjFGmvsHnedw3pq1GmufodkLZK55Sbr4_85dswtzFluNqLIhWTbkzAeBPwOU-Rq6hi3etHyrVEuTZ9DMCXeAo6QR9SuxFXrOaQpdLAryXpbwRj_dFTDk5qaVmrbqwDb0lrTMCqOdZRpqQTH5jpuSD1WSXTHeobyDLYYtnwuWOaEVtnrCOZIceoLSS4DX2F9gDyKOe5mkPq_VNZbQYxpKglsG-dk9kiWCzugCFDyypeLj_ulQLW26WMd8b6mq5QR5ip2WHkAb1slKaC_AlvwNJ4O9DXysSMFo_1lJ1uZHhVdkRdYaSK0bfPtFOSkVNuVkzXYGGv-quNZqNEGVsV6EBNXLf5J0JTpqqCHAcfuDtj63LMDfLumf94ypVUxrWdQXlYzCAFRxhXau5IGUP6WKbDOfKJ-LfV_Su9PmR-Kr5HXPrfu8lxKusNamRI1VvleGJeD8jk_9KTFOwxhl3XoxwD55oQF8WuO043IrZP89ZVTG9G7JlhroP8PG3ukfknT-3n2q1Yq1l7nJP2clbRn-B7X42deTGU0jRrnIv7tj9AtESGnIGS7n1bFOFjYafoJoqI3KfECbi6jMVYL50r5JZGrzoWkeUcWyKfSXdS1woSN99ENIHUOOsMtIgW4kVEficMzy4vgiDo94Ner93vXrTEq7OfBxdnl0Ev7AVhOwyCdrBsiTcih3F-1u2e98NOEIT9sNvudDstgVJZ4j-rR7h8i0uShzKgULL8DwFMR-8)

Regression analysis mindmap depicting all modelling techniques to be explored in this book. These techniques are split into two big sets: *continuous* and *discrete* outcomes.
:::

That said, we will go beyond OLS regression and explore further regression techniques. In practice, these techniques have been developed in the statistical literature to address practical cases where the OLS modelling framework and assumptions are not suitable anymore. Thus, throughout this block, we will cover (at least) one new regression model per lecture.

As we can see in the clouds of @fig-regression-mindmap, there are 13 regression models: 8 belonging to discrete outcomes and 5 to continuous outcomes. Each of these models is contained in a chapter of this book, beginning with the most basic regression tool known as ordinary least-squares in @sec-ols. We must clarify that the current statistical literature is not restricted to these 13 regression models. The field of regression analysis is vast, and one might encounter more complex models to target certain specific inquiries. Nonetheless, I consider these models the fundamental regression approaches that any data scientist must be familiar with in everyday practice.

Even though this book comprises 13 chapters, each depicting a different regression model, we have split these chapters into two major subsets: those with *continuous* outcomes and those with *discrete* outcomes. 

