<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7PRVEBE1EF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7PRVEBE1EF');
</script>

# Ordinal Logistic Regression {#sec-ordinal-logistic}

```{r}
#| include: false

colourize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

::: {#fig-regression-ordinal-logistic-regression}
```{mermaid}
mindmap
  root((Regression 
  Analysis)
    Continuous <br/>Outcome Y
      {{Unbounded <br/>Outcome Y}}
        )Chapter 3: <br/>Ordinary <br/>Least Squares <br/>Regression(
          (Normal <br/>Outcome Y)
      {{Nonnegative <br/>Outcome Y}}
        )Chapter 4: <br/>Gamma Regression(
          (Gamma <br/>Outcome Y)
      {{Bounded <br/>Outcome Y <br/> between 0 and 1}}
        )Chapter 5: Beta <br/>Regression(
          (Beta <br/>Outcome Y)
      {{Nonnegative <br/>Survival <br/>Time Y}}
        )Chapter 6: <br/>Parametric <br/> Survival <br/>Regression(
          (Exponential <br/>Outcome Y)
          (Weibull <br/>Outcome Y)
          (Lognormal <br/>Outcome Y)
        )Chapter 7: <br/>Semiparametric <br/>Survival <br/>Regression(
          (Cox Proportional <br/>Hazards Model)
            (Hazard Function <br/>Outcome Y)
    Discrete <br/>Outcome Y
      {{Binary <br/>Outcome Y}}
        {{Ungrouped <br/>Data}}
          )Chapter 8: <br/>Binary Logistic <br/>Regression(
            (Bernoulli <br/>Outcome Y)
        {{Grouped <br/>Data}}
          )Chapter 9: <br/>Binomial Logistic <br/>Regression(
            (Binomial <br/>Outcome Y)
      {{Count <br/>Outcome Y}}
        {{Equidispersed <br/>Data}}
          )Chapter 10: <br/>Classical Poisson <br/>Regression(
            (Poisson <br/>Outcome Y)
        {{Overdispersed <br/>Data}}
          )Chapter 11: <br/>Negative Binomial <br/>Regression(
            (Negative Binomial <br/>Outcome Y)
        {{Overdispersed or <br/>Underdispersed <br/>Data}}
          )Chapter 13: <br/>Generalized <br/>Poisson <br/>Regression(
            (Generalized <br/>Poisson <br/>Outcome Y)
        {{Zero Inflated <br/>Data}}
          )Chapter 12: <br/>Zero Inflated <br/>Poisson <br/>Regression(
            (Zero Inflated <br/>Poisson <br/>Outcome Y)
      {{Categorical <br/>Outcome Y}}
        {{Nominal <br/>Outcome Y}}
          )Chapter 14: <br/>Multinomial <br/>Logistic <br/>Regression(
            (Multinomial <br/>Outcome Y)
        {{Ordinal <br/>Outcome Y}}
          )Chapter 15: <br/>Ordinal <br/>Logistic <br/>Regression(
            (Logistic <br/>Distributed <br/>Cumulative Outcome <br/>Probability)
```
:::



```{r setup-r, include=FALSE}
# Load required libraries
library(tidyverse, quietly = TRUE, warn.conflicts = FALSE)
```



::: {.LO}
::::{.LO-header}
Learning Objectives
::::
::::{.LO-container}
By the end of this chapter, you will be able to:

- Describe why ordinary **linear models** and **multinomial logistic regression** are inappropriate for **ordered categorical outcomes**.

- Write down the ordinal regression likelihood and the cumulative logit link functions, express cumulative probabilities, and recover category probabilities.

- Understand the **proportional odds assumption** and how it constrains the model (common slope parameters across cutpoints).

- Understand how coefficient estimation via maximum likelihood works in this model.

- Interpret ordinal regression coefficients in real scenarios.

- Use **ordinal logistic regression** for prediction by computing cumulative and category-specific probabilities.

- Assess and diagnose the proportional odds assumption by the **Brant–Wald** test.

- Formulate and interpret **non-proportional odds models** when the proportional odds assumption is violated.

- Evaluate model performance and construct confidence intervals.

::::
:::


## Introduction

**Ordinal logistic regression** is a statistical modeling technique used to analyze relationships between an ordered multi-class categorical response variable and a set of explanatory variables. Unlike **multinomial logistic regression**, which treats categories as unordered, ordinal logistic regression take into account the natural ranking of response levels. Ignoring this ordering may lead to loss of valuable information in both **inference** and **prediction**.

In ordinal outcomes, the information lies **not only** in category membership **but also** in the relative ordering between categories. This structure is modeled through cumulative probabilities, which form the foundation of the cumulative logit (proportional odds) model. Instead of modeling the log-odds of each category relative to a baseline (as we do in multinomial regression), ordinal logistic regression models the **log-odds of being at or below a given category threshold**. That's right! We are looking at a cumulative probabbility now.

A key characteristic of the most commonly used ordinal models is the proportional odds model. It asserts that the effect of predictors is assumed to be constant across all cumulative splits of the response. This assumption plays a central role in the modeling framework and interpretation. To start this chpater let us see some of the assumptions in this model.


### Ordinal Logistic Regression Assumptions

The following are the assumptions that we need to be aware of before fitting this model:

- The response or dependent variable should be measured at the ordinal level with more than two ordered categories. This means there is a natural order between the categories in the response variable. For example likert scale: Strongly agree / Agree / Neutral / Disagree / Strongly Disagree

- There should be one or more independent variables that are continuous, ordinal, or nominal (including dichotomous variables).

- Logit linearity assumption: There must be a linear relationship between any continuous independent variables and the logit of the cumulative probabilities.

- Independent observations: The observations should be independent and the dependent variable should consist of mutually exclusive and exhaustive ordered categories.

- Proportional odds assumption: The relationship between each predictor and the log-odds of being at or below any category is constant across all category thresholds (parallel slopes assumption).

- When the proportional odds assumption is violated, a non-proportional odds model may be considered that allows for some predictors to vary across thresholds.


### Use Cases of Ordinal Logistic Regression

Ordinal logistic regression is particularly useful when the outcome has a meaningful ranking and collapsing categories would discard important structure. Some examples of this are:

1. Likert Scale Survey Responses: A classic example is agreement levels in surveys (Strongly disagree/ Disagree / Neutral / Agree / Strongly Agree). For example, you may study how predictors such as age, education, or exposure to misinformation influence the probability of showing extreme political ideas. 

2. Customer Satisfaction Levels: Marketing analysts may use ordinal regression to understand how service quality, wait time, or pricing influence the likelihood of higher satisfaction ratings.

3. Educational Achievement Levels: Educational researchers may study how study time, attendance in lectures and class activities, or prior preparation affects the probability of achieving higher results in exams.

4. Clinical Severity Scales: Ordinal models help quantify how treatments or patient characteristics influence the likelihood of more severe disease states. These status are often divided into categories such as mild, moderate, or severe.

In all these examples, the ordering is essential. Treating these outcomes as nominal would ignore the structured progression between categories, while treating them as numeric would impose unrealistic equal spacing. Ordinal logistic regression provides a principled framework that respects the ranking while maintaining probabilistic modeling rigor.

## Case Study

- TBD


## Data Collection and Wrangling

- TBD


## Exploratory Data Analysis

- TBD


## Data Modeling

We have been discussing the advantageous of using a different model in the presence of an **ordinal response variable**. In this section, we go over the details of data modeling for an **ordinal response variable** with a set of discrete or continuous explanatory variables using an ordinal logistic regression model. This type of modeling is the most frequent and popular model for ordinal responses. 

To present the details of the modeling framework, we begin with the concept of **cumulative logits**. We have already encountered the logit function in the context of binary logistic regression. Recall that in logistic regression, the logit function is applied to the odds of an event. Specifically, if 

$$
p = P(Y = 1 |X_1, X_2, ..., X_k)
$$

then the odds of the event $Y=1$ given the predictors is:

$$
\frac{P(Y = 1 \mid X_1, ..., X_k)}{P(Y = 0 \mid X_1, ..., X_k)} = \frac{p}{1-p}
$$

and the logit is defined as the logarithm of these odds, i.e. $log(\frac{p}{1-p})$. In logistic regression modeling we assume that this log-odds is a linear function of the predictors:

$$
log(\frac{p}{1-p}) = \beta_0 + \beta_1 X_1 + \beta_2X_2 + \ldots + \beta_k X_k
$$

This idea and formulation is the basis for extending logistic regression modeling for an outcome with two categories to ordinal outcomes where there are more than two categories and the order matters as well. In the ordinal case, instead of modeling the **log-odds of a single binary event**, we model **the cumulative logits** which compare the probability of being at or below a given category to the probability of being above that category. 

**Notation:** To avoid lengthy formulas, we introduce compact notation that will be used consistently throughout this chapter of book. In expressions such as $P(Y = 1 \mid X_1, ..., X_k)$ the conditioning is on all explanatory variables. Instead of repeatedly writing all predictors, we define:  

$$
\underline{X} = (X_1,...,X_k)
$$ 

and write;

$$
P(Y = 1 \mid \underline{X})
$$

This notation simplifies expressions and improves readability. Let's continue!

Imagine your ordered response variable have $J$ different categories where $J \ge 3$. We start with the category ordering by formally defining the **logits of cumulative probabilities** as follows. Let:

$$
P(Y \leq j \mid \underline{X}) = p_1(\underline{X}) + p_2(\underline{X}) + \ldots + p_j(\underline{X}) \quad \quad j = 1, 2, \ldots, J-1
$$

be the cumulative probability. The **logits of cumulative probabilities** are defined as:

$$
\begin{aligned}
&\text{logit}(P(Y \leq j \mid \underline{X})) = \log( \frac{P(Y \leq j \mid \underline{X})}{ 1 - P(Y \leq j \mid \underline{X})} ) \\
&= \log( \frac{P(Y = 1 \mid \underline{X}) + P(Y = 2 \mid \underline{X}) + \ldots + P(Y = j \mid \underline{X})}{ P(Y = j+1 \mid \underline{X}) + P(Y = j+2 \mid \underline{X}) + \ldots + P(Y = J \mid \underline{X}) } )
\end{aligned}
$$

Perfect! We have defined our **cumulative logit odds here**! At this point, an important modeling decision must be addressed. When working with cumulative logits in ordinal logistic regression, there are two primary modeling approaches based on different assumptions about the relationship between the predictors and the response categories. These two primary modeling approaches are 1) the proportional odds assumption and 2) the non-proportional odds assumption. 

The proportional odds model assumes that the effect of each explanatory variable is constant across all cumulative logits, meaning that the regression coefficients do not depend on the category $j$. In contrast, the non-proportional odds model allows the effects of the explanatory variables to vary across cumulative splits of the response variable. In the following sections, we introduce these two approaches in detail and discuss their interpretation and practical implications. Later in this chapter we explain how you can verify these assumptions while fitting the model.


### The Proportional Odds Modeling

Up until now we established that for an ordinal response variable with $J$ ordered categories, we defined the cumulative logits as:

$$
\text{logit}(P(Y \le j \mid \underline{X})) = \frac{ P(Y \le j \mid \underline{X}) }{ P(Y \ge j+1 \mid \underline{X}) }
$$

for all values of $j = 1, 2, \ldots, J-1$. This means that for an outcome with $J$ categories, we have $J-1$ cumulative logits. A natural question now arises: **how should these cumulative logits depend on the explanatory variables?** The proportional odds model assumes that each cumulative logit is a linear function of the predictors, with the important restriction that the regression coefficients are the same across all cumulative logits. The model is written as:

$$
\text{logit}( P(Y \le j \mid \underline{X}) ) = \alpha_j + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k
$$

for all values of $j = 1, 2, \ldots, J-1$. Note that in this formula:

- $\alpha_j$ is a category-specific intercept and depends on the category $j$,
- $\beta_1, ..., \beta_k$ are regression coefficients that do not depend on $j$. 

The essential assumption is that the effect of each explanatory variable on the log-odds is constant across all cumulative splits of the response variable. That is, the slope parameters remain the same whether we compare:

- category 1 versus higher categories  
- categories 1–2 versus higher categories  
- $\vdots$  
- categories 1 through $J-1$ versus higher categories

and in a more mathematical framework, that is:

- $P(Y \le 1 \mid \underline{X})$ versus $P(Y > 1 \mid \underline{X})$
- $P(Y \le 2 \mid \underline{X})$ versus $P(Y > 2 \mid \underline{X})$
- $\vdots$
- $P(Y \le J-1 \mid \underline{X})$ versus $P(Y > J-1 \mid \underline{X})$


This property of coefficients across cumulative logits is known as **the proportional odds assumption**.


### The Non-proportional Odds Modeling

While the proportional odds model assumes that the effect of each predictor is the same across all cumulative logits, this assumption may not always hold in practice. When the effect of one or more explanatory variables varies across the cumulative splits of the ordinal response, we use a non-proportional odds model.

Formally speaking, the non-proportional odds model allows the regression coefficients to depend on the cumulative category $j$ as shown below:

$$
\text{logit}( P(Y \le j \mid \underline{X}) ) = \alpha_j + \beta_{1j} X_1 + \beta_{2j} X_2 + \ldots + \beta_{kj} X_k
$$

In this model:

- $\alpha_j$ is the category-specific intercept (the same as before),
- $\beta_{1j}, \beta_{2j}, \ldots , \beta_{kj}$ are category-specific regression coefficients, meaning that the effect of each explanatory variable can differ across cumulative logits.

At the first glance this seems to be a great flexibility for modeling but this flexibility comes at a cost: **the number of parameters increases with the number of cumulative splits**, making the model more complex and potentially harder to interpret. However, it provides a more accurate representation when the proportional odds assumption is not valid. We will discuss how to assess these assumptions later in this chapter.


To help us understand the differences in assumption visually, we have the following plot. The figure below visualizes four cumulative probabilities $P(Y <= 1 \mid X)$, $P(Y <= 2 \mid X)$, $P(Y <= 3 \mid X)$, and $P(Y <= 4 \mid X)$ on the y-axis plotted as a logistic curve against one explanatory variable $X$. Notice that all curves have the same slope (same amount of steepness) but different intercepts. This parallel structure shows the proportional odds assumption: the effect of $X$ on the log-odds scale (which is the coefficient of $X$) is the same across all cumulative logits. In other words, increasing $X$ shifts each cumulative logit by the same amount regardless of which categorical group $j$ we consider. Note that the intercepts in this model are $\alpha_1 = 4$, $\alpha_2 = 5.5$, $\alpha_3 = 6.3$, $\alpha_4 = 10$. The coefficient which measures the effect of $X$ on log-odds is $\beta=12$ and the same for all cumulative probabilities. Thus, the visual parallelism in this plot provides an intuitive way to understand what proportional odds means geometrically


```{r echo=FALSE, message=FALSE, warning=FALSE}
# Borrowed from DSCI 562 lecture notes written by Alexi
x_grid <- seq(-0.5, 3, length.out = 1200)
a_vals <- c("k = 4" = 10, "k = 3" = 6.3, "k = 2" = 5.5, "k = 1" = 4)

df <- map_dfr(names(a_vals), \(nm) {
  a <- a_vals[[nm]]
  tibble(
    x = x_grid,
    y = plogis(a - 12 * x),
    curve = nm
  )
}) |>
  mutate(curve = factor(curve, levels = names(a_vals)))

eq_df <- tibble(
  curve = factor(names(a_vals), levels = names(a_vals)),
  x = 3.1,
  y = c(0.95, 0.70, 0.45, 0.15),
  lab = c(
    "P(Y <= 4 ~ '|' ~ X)==frac(e^{10-12*x},1+e^{10-12*x})",
    "P(Y <= 3 ~ '|' ~ X)==frac(e^{6.3-12*x},1+e^{6.3-12*x})",
    "P(Y <= 2 ~ '|' ~ X)==frac(e^{5.5-12*x},1+e^{5.5-12*x})",
    "P(Y <= 1 ~ '|' ~ X)==frac(e^{4-12*x},1+e^{4-12*x})"
  )
)

cb_pal <- c(
  "k = 4" = "#0072B2",
  "k = 3" = "#D55E00",
  "k = 2" = "#009E73",
  "k = 1" = "#CC79A7"
)

ggplot(df, aes(x, y, color = curve)) +
  geom_line(linewidth = 1.1) +
  geom_text(
    data = eq_df,
    aes(x = x, y = y, label = lab, color = curve),
    hjust = 1, vjust = 0.5, parse = TRUE,
    size = 7.5,         
    show.legend = FALSE
  ) +
  scale_color_manual(values = cb_pal) +
  coord_cartesian(xlim = c(-0.5, 3), ylim = c(0, 1.05)) +
  labs(x = "x", y = "Cumulative Probability") +
  theme_minimal(base_size = 13) +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 24),
    axis.text = element_text(size = 17),
    axis.title = element_text(size = 21)
  )
```


If the proportional odds assumption were violated, these curves would no longer be parallel on the logit scale; instead, their slopes would differ, indicating that the effect of $X$ changes depending on which cumulative split of the ordinal outcome is being modeled. The following figure is an example of proportional odds assumption being violated:

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Non-proportional odds visualization

x_grid <- seq(-0.5, 3, length.out = 1200)

# Different intercepts and coefficients
param_vals <- tribble(
  ~curve,   ~a,   ~b,
  "k = 4",  10,  -12,
  "k = 3",  6.3,  -9,
  "k = 2",  5.5,  -6,
  "k = 1",  4,    -3
)

df_np <- param_vals |>
  mutate(data = map2(a, b, ~tibble(
    x = x_grid,
    y = plogis(.x + .y * x_grid)
  ))) |>
  unnest(data) |>
  mutate(curve = factor(curve, levels = param_vals$curve))

eq_df_np <- param_vals |>
  mutate(
    x = 3.1,
    y = c(0.95, 0.70, 0.45, 0.15),
    lab = paste0(
      "P(Y <= ", 4:1, " ~ '|' ~ X)==",
      "frac(e^{", a, ifelse(b >= 0, "+", ""), b, "*x},",
      "1+e^{", a, ifelse(b >= 0, "+", ""), b, "*x})"
    )
  )

cb_pal <- c(
  "k = 4" = "#0072B2",
  "k = 3" = "#D55E00",
  "k = 2" = "#009E73",
  "k = 1" = "#CC79A7"
)

ggplot(df_np, aes(x, y, color = curve)) +
  geom_line(linewidth = 1.1) +
  geom_text(
    data = eq_df_np,
    aes(x = x, y = y, label = lab, color = curve),
    hjust = 1, vjust = 0.5, parse = TRUE,
    size = 7.5,
    show.legend = FALSE
  ) +
  scale_color_manual(values = cb_pal) +
  coord_cartesian(xlim = c(-0.5, 3), ylim = c(0, 1.05)) +
  labs(x = "x", y = "Cumulative Probability") +
  theme_minimal(base_size = 13) +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 24),
    axis.text = element_text(size = 17),
    axis.title = element_text(size = 21)
  )
```


## Estimation


## Goodness of Fit


## Inference


## Results


## Storytelling



