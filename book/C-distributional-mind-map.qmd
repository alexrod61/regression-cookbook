<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7PRVEBE1EF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7PRVEBE1EF');
</script>

# The Chocolified Distributional Mind Map {#sec-distributional-mind-map}

```{r}
#| include: false

colourize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

::: {.Warning}
::::{.Warning-header}
Fun fact!
::::
::::{.Warning-container}
**Chocolified!** Every inch of it is chocolate, probably topped with more chocolate.
::::
:::

A crucial part of the practice of `r colourize("regression analysis", "blue")` is a fair knowledge of all the different `r colourize("probability distributions", "blue")` that would allow us to identify **the most suitable regression model**. Let us delve into the distributional toolbox to be used in this book.

@fig-app-distributions shows all those `r colourize("probability distributions", "blue")` **depicted as clouds**, in the form of a **univariate** `r colourize("random variable", "blue")` $Y$, used to model our `r colourize("outcomes", "magenta")` of interest in the regression tools explored in each of the core thirteen chapters, i.e., we take a `r colourize("generative modelling", "blue")` approach. Note this mind map splits the `r colourize("outcomes", "magenta")` of interest into two large zones: `r colourize("discrete", "blue")` and `r colourize("continuous", "blue")`. Furthermore, this mind map briefly describes a given `r colourize("random variable", "blue")` $Y$ as a quick cheat sheet regarding its **support** (e.g., nonnegative, bounded, unbounded, etc.) or **distributional definition** (e.g., success or failure, successes in $n$ trials, etc.).

::: {#fig-app-distributions}
```{mermaid}
mindmap
  root((Univariate 
    Random
    Variable Y))
    Continuous
      {{Unbounded}}
        )Normal(
        )Logistic(
      {{Nonnegative Y}}
        )Lognormal(
        )Exponential(
        )Gamma(
        )Weibull(
      {{Y is between <br/>0 and 1}}
        )Beta(
    Discrete
      Binary
        {{Y is a success or <br/>failure}}
          )Bernoulli(
      Count
        {{Y succeses in <br/>n trials}}
          )Binomial(
        {{Y failures <br/>before experiencing <br/>k successes}}
          )Negative Binomial(
        {{Y events in <br/>a fixed interval <br/>of time or space}}
          )Classical <br/>Poisson(
          )Generalized <br/>Poisson(
          )Zero Inflated <br/>Poisson(
      Categorical
        {{Y successes of a given category, <br/>among a set of k categories, <br/>in n trials}}
          )Multinomial(
```

Probability distribution mind map depicting all univariate random variables to be used as outcomes of interest $Y$ in the modelling techniques to be explored in this book. These outcomes are split into two large zones: *discrete* and *continuous*.
:::

Since a given `r colourize("random variable", "blue")` (e.g., the `r colourize("outcome", "magenta")` $Y$ in any of the thirteen regression models in this book) will have a `r colourize("probability distribution", "blue")` associated with it, which will define the `r colourize("probability", "blue")` arrangement of each possible value or category $Y$ could take on, we also need a way to summarize all this information via key estimated metrics called `r colourize("measures of central tendency and uncertainty", "blue")`:

- **Measure of Central Tendency:** This metric refers to a **typical value or category** that a given `r colourize("random variable", "blue")` might take when we observe various realizations of this variable over a long period. 
- **Measure of Uncertainty:** This metric pertains to the **spread** of a `r colourize("random variable", "blue")` when we observe its different realizations in the long term. As a side note, a larger spread indicates more variability in these realizations.

![Image by [*Tobias Frick*](https://pixabay.com/users/pixounaut-2729335/) via [*Pixabay*](https://pixabay.com/photos/game-dice-board-game-craps-game-4695864/).](img/dice.jpg){width="400"} 

These metrics allow us to clearly communicate how the outcome $Y$ behaves in our case study, and this is heavily related to the **storytelling** stage from the **data science workflow**, as explained in @sec-ds-workflow-storytelling. More specifically, the `r colourize("measures of central tendency", "blue")` can be communicated along with our estimated regression `r colourize("parameters", "blue")`, given that these metrics are usually conditioned to our `r colourize("regressors", "blue")` of interest within our modelling framework.

::: {#Headsup-sample .Heads-up}
::::{.Heads-up-header}
Heads-up on parameter estimation!
::::
::::{.Heads-up-container}
Just as in the case of regression `r colourize("parameters", "blue")`, the `r colourize("measures of central tendency and uncertainty", "blue")` are also `r colourize("parameters", "blue")` (more specifically, belonging to a given `r colourize("probability distribution", "blue")`) that can be estimated via an observed `r colourize("random sample", "blue")` through methods such as `r colourize("maximum likelihood estimation (MLE)", "blue")`. You can check further details on the `r colourize("MLE", "blue")` fundamentals in @sec-mle.
::::
:::

There are different `r colourize("measures of central tendency and uncertainty", "blue")`. Nevertheless, we will only focus on the `r colourize("expected value (i.e., the mean)", "blue")` and the `r colourize("variance", "blue")`. Hence, let $Y$ be a `r colourize("random variable", "blue")` with possible values $y \in \mathcal{Y}$. Then, let $g(Y)$ be a general function of $Y$. By the `r colourize("law of the unconscious statistician (LOTUS)", "blue")`, we have that:

$$
\mathbb{E} \left[ g(Y) \right] = \displaystyle \sum_{\mathcal{Y}} g(Y) \cdot P(Y = y),
$${#eq-app-expected-value-discrete-function}

where $P(Y = y)$ is the `r colourize("probability mass function (PMF)", "blue")` of $Y$.

If $Y$ is a `r colourize("continuous random variable", "blue")`, by the `r colourize("LOTUS", "blue")`, the `r colourize("mean", "blue")` of function $g(Y)$ is defined as

$$
\mathbb{E} \left[ g(Y) \right] = \displaystyle \int_{\mathcal{Y}} g(Y) \cdot f_Y(y) \text{d}y,
$${#eq-app-expected-value-continuous-function}

where $f_Y(y)$ is the `r colourize("probability density function (PDF)", "blue")` of $Y$.

Note that when $g(Y) = y$ in the `r colourize("discrete", "blue")` case, @eq-app-expected-value-discrete-function becomes

$$
\mathbb{E} \left[ Y \right] = \displaystyle \sum_{\mathcal{Y}} y \cdot P(Y = y).
$${#eq-app-expected-value-discrete}

On the other hand, when $g(Y) = y$ in the `r colourize("continuous", "blue")` case, @eq-app-expected-value-continuous-function becomes

$$
\mathbb{E} \left[ Y \right] = \displaystyle \int_{\mathcal{Y}} y \cdot f_Y(y) \text{d}y.
$${#eq-app-expected-value-continuous}

Either for a `r colourize("discrete", "blue")` or `r colourize("continuous", "blue")` case, the `r colourize("variance", "blue")` is defined as 

$$
\text{Var}(Y) = \mathbb{E}\{[Y - \mathbb{E}(Y)]^2\}.
$$

After applying some algebraic rearrangements and `r colourize("expected value", "blue")` properties, the expression above is equivalent to:

::: {.proof}
\
$$
\begin{align*}
\text{Var}(Y) &= \mathbb{E} \left\{ \left[ Y - \mathbb{E}(Y) \right]^2 \right\} \\
&= \mathbb{E} \left\{ Y^2 - 2Y \mathbb{E}(Y) + \left[ \mathbb{E}(Y) \right]^2 \right\} \\
&= \mathbb{E}(Y^2) - \mathbb{E} \left[ 2 Y \mathbb{E}(Y) \right] + \mathbb{E} \left[ \mathbb{E}(Y) \right]^2 \\
& \qquad \text{distributing the expected value operator}\\
&= \mathbb{E}(Y^2) - 2 \mathbb{E} \left[ Y \mathbb{E}(Y) \right] + \mathbb{E} \left[ \mathbb{E}(Y) \right]^2 \\
& \qquad \text{since $2$ is a constant}\\
&= \mathbb{E}(Y^2) - 2 \mathbb{E}(Y) \mathbb{E} \left( Y \right) + \left[ \mathbb{E}(Y) \right]^2 \\
& \qquad \text{since $\mathbb{E}(Y)$ is a constant}\\
&= \mathbb{E}(Y^2) - 2 \left[ \mathbb{E}(Y) \right]^2 + \left[ \mathbb{E}(Y) \right]^2 \\
&= \mathbb{E}(Y^2) - [\mathbb{E}(Y)]^2,  \qquad \qquad \qquad \qquad \qquad \square
\end{align*}
$${#eq-app-variance}
:::

where $\mathbb{E}(Y^2)$ can be computed either via @eq-app-expected-value-discrete-function or @eq-app-expected-value-continuous-function depending on the nature of $Y$ with $g(Y) = y^2$.

Now, for each case depicted as a cloud in @fig-app-distributions, subsequent sections in this appendix will show elaborate on why each corresponding `r colourize("PMF", "blue")` or `r colourize("PDF", "blue")` (depending on the type of `r colourize("random variable", "blue")`, $Y$) is a proper probability distribution (i.e., all the standalone `r colourize("probabilities", "blue")` over the support of $Y$ add up to one) along with the respective proofs of their corresponding `r colourize("means", "blue")` and `r colourize("variances", "blue")`.

# Discrete Random Variables

Let us recall what a `r colourize("discrete random variable", "blue")` is. This type of variable is defined to take on a set of countable values. In other words, these values belong to a finite set. @fig-app-distributions delves into the following specific `r colourize("probability distributions", "blue")`:

- **Bernoulli.** A `r colourize("random variable", "blue")` $Y$ that can take on the values of $0$ (i.e., a failure) or $1$ (i.e., a success) where the distributional `r colourize("parameter", "blue")` is the `r colourize("probability", "blue")` of success $\pi \in [0, 1]$. Note $Y$ is said to be **binary** with a support of $y \in \{ 0, 1 \}$.
- **Binomial.** A `r colourize("random variable", "blue")` $Y$ that defines the number of `r colourize("independent", "blue")` **Bernoulli trials** in which we observe a success out of $n$ trials. Its distributional `r colourize("parameters", "blue")` are the `r colourize("probability", "blue")` of success $\pi \in [0, 1]$of each Bernoulli trial along with the total number of trials $n \in \mathbb{N}$. Note $Y$ is said to be of **count** type with a support of $y \in \{ 0, 1, \dots, n \}$ successes.
- **Negative Binomial.** A `r colourize("random variable", "blue")` $Y$ that defines the number of `r colourize("independent", "blue")` **Bernoulli trials** in which we observe a failure before experiencing $k$ successes. Its distributional `r colourize("parameters", "blue")` are the `r colourize("probability", "blue")` of success $\pi \in [0, 1]$ of each Bernoulli trial along with the number of $k \in \{ 0, 1, 2 \dots\}$ successes. Note $Y$ is said to be of **count** type with a support of $y \in \{ 0, 1, 2 \dots\}$ failures.
- **Classical Poisson.** A `r colourize("random variable", "blue")` $Y$ that defines the number of events occurring in a predetermined interval of time or space. Its distributional `r colourize("parameter", "blue")` is the `r colourize("average", "blue")` rate $\lambda \in (0, \infty)$ of events per this predetermined interval of time or space. Note $Y$ is said to be of **count** type with a support of $y \in \{ 0, 1, 2, \dots \}$ events.
- **Generalized Poisson.** As in the above **classical Poisson** case, it is `r colourize("random variable", "blue")` $Y$ that defines the number of events occurring in a predetermined interval of time or space. It has two distributional `r colourize("parameters", "blue")`: the `r colourize("average", "blue")` rate $\lambda \in (0, \infty)$ of events per this predetermined interval of time or space, and a `r colourize("dispersion parameter", "blue")` $\theta \in (-1, 1)$ that models the `r colourize("random variable's", "blue")` degree of `r colourize("underdispersion", "blue")` (when $-1 < \theta < 0$) and `r colourize("overdispersion", "blue")` (when $0 < \theta < 1$).

@tbl-distributions-discrete outlines the `r colourize("parameter(s)", "blue")`, support, `r colourize("mean", "blue")`, and `r colourize("variance", "blue")` for each `r colourize("discrete probability distribution", "blue")` utilized to model the `r colourize("target", "magenta")` $Y$ in a specific regression tool explained in this book.

| Distribution |  Parameter(s) | Support |  Mean  |  Variance  | 
|:----:|:------:|:-----:|:-----:|:-----:|
| Bernoulli | $Y \sim \text{Bern}(\pi)$ with probability of success $\pi \in [0, 1]$ | $y \in \{ 0, 1 \}$ | $\pi$ | $\pi (1 - \pi)$ | 
| Binomial | $Y \sim \text{Bin}(n, \pi)$ with number of trials $n \in \mathbb{N}$ and probability of success $\pi \in [0, 1]$ | $y \in \{ 0, 1, \dots, n \}$| $n \pi$ | $n \pi (1 - \pi)$ |
| Negative Binomial | $Y \sim \text{NegBin}(k, \pi)$ with number of successes $k \in \{ 0, 1, 2 \dots\}$ and probability of success $\pi \in [0, 1]$| $y \in \{ 0, 1, 2 \dots\}$ | $\frac{k (1 - \pi)}{\pi}$ | $\frac{k (1 - \pi)}{\pi^2}$ |
| Classical Poisson | $Y \sim \text{Pois}(\lambda)$ with continuous average rate $\lambda \in (0, \infty)$ | $y \in \{ 0, 1, 2, \dots\}$ | $\lambda$ | $\lambda$ |
| Generalized Poisson | $Y \sim \text{GP}(\lambda, \theta)$ with continuous average rate $\lambda \in (0, \infty)$ and dispersion parameter $\theta \in (-1, 1)$ | $y \in \{ 0, 1, 2, \dots\}$ | $\frac{\lambda}{1 - \theta}$ | $\frac{\lambda}{(1 - \theta)^2}$ | 

: Univariate discrete probability distributions for a random variable $Y$; including parameter(s), support, mean, and variance. {#tbl-distributions-discrete .striped .hover}

## Bernoulli

Let $Y$ be a `r colourize("discrete random variable", "blue")` that is part of a random process or system. $Y$ can only take on the following values:

$$
Y =
\begin{cases}
1 \; \; \; \; \text{if there is a success},\\
0 \; \; \; \; \mbox{otherwise}.
\end{cases}
$${#eq-app-bernoulli-support}

Note that the support of $Y$ in @eq-app-bernoulli-support makes it binary with these outcomes: $1$ for *success* and $0$ for *failure*.  Then, $Y$ is said to have a **Bernoulli distribution** with `r colourize("parameter", "blue")` $\pi$:

$$
Y \sim \text{Bern}(\pi).
$$

### Probability Mass Function

The `r colourize("probability mass function (PMF)", "blue")` of $Y$ is the following:

$$
P \left( Y = y \mid \pi \right) = \pi^y (1 - \pi)^{1 - y} \quad \text{for $y \in \{ 0, 1 \}$.}
$${#eq-app-bernoulli-pmf}

`r colourize("Parameter", "blue")` $\pi \in [0, 1]$ refers to the `r colourize("probability", "blue")` of success. We can verify @eq-app-bernoulli-pmf is a proper `r colourize("probability distribution", "blue")` (i.e., all the standalone `r colourize("probabilities", "blue")` over the support of $Y$ add up to one) given that:

::: {.proof}
$$
\begin{align*} 
\sum_{y = 0}^1 P \left( Y = y \mid \pi \right) &=  \sum_{y = 0}^1 \pi^y (1 - \pi)^{1 - y}  \\
&= \underbrace{\pi^0}_{1} (1 - \pi) + \pi \underbrace{(1 - \pi)^{0}}_{1} \\
&= (1 - \pi) + \pi \\
&= 1. \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$

> **Indeed, the Bernoulli `r colourize("PMF", "blue")` is a proper `r colourize("probability distribution", "blue")`!**

:::

### Expected Value

Via @eq-app-expected-value-discrete, the `r colourize("expected value", "blue")` or `r colourize("mean", "blue")` of a Bernoulli-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*} 
\mathbb{E}(Y) &= \sum_{y = 0}^1 y P \left( Y = y \mid \pi \right) \\
&= \sum_{y = 0}^1 y \left[ \pi^y (1 - \pi)^{1 - y} \right] \\
&= \underbrace{(0) \left[ \pi^0 (1 - \pi) \right]}_{0} + (1) \left[ \pi (1 - \pi)^{0} \right] \\
&= 0 + \pi \\
&= \pi. \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$
:::

### Variance

Via @eq-app-variance and the @eq-app-expected-value-discrete of a `r colourize("discrete expected value", "blue")`, the `r colourize("variance", "blue")` of a Bernoulli-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E}(Y^2) - \left[ \mathbb{E}(Y)\right]^2 \\
&= \mathbb{E}(Y^2) - \pi^2 \qquad \text{since $\mathbb{E}(Y) = \pi$} \\
&= \sum_{y = 0}^1 y^2 P \left( Y = y \mid \pi \right) - \pi^2 \\
&= \left\{ \underbrace{(0^2) \left[ \pi^0 (1 - \pi) \right]}_{0} + \underbrace{(1^2) \left[ \pi (1 - \pi)^{0} \right]}_{\pi} \right\} - \pi^2 \\
&= (0 + \pi) - \pi^2 \\
&= \pi - \pi^2 \\
&= \pi (1 - \pi). \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$
:::

## Binomial

Suppose you execute $n$ `r colourize("independent", "blue")` **Bernoulli trials**, each one with a `r colourize("probability", "blue")` of success $\pi$. Let $Y$ be the number of successes obtained within these $n$ Bernoulli trials. Then, $Y$ is said to have a **Binomial distribution** with `r colourize("parameters", "blue")` $n$ and $\pi$:

$$
Y \sim \text{Bin}(n, \pi).
$$

### Probability Mass Function

The `r colourize("PMF", "blue")` of $Y$ is the following:

$$
\begin{align*}
P \left( Y = y \mid n, \pi \right) &= {n \choose y} \pi^y (1 - \pi)^{n - y} \\
& \qquad \qquad \qquad \text{for $y \in \{ 0, 1, \dots, n \}$.}
\end{align*}
$${#eq-app-binomial-pmf}

`r colourize("Parameter", "blue")` $\pi \in [0, 1]$ refers to the `r colourize("probability", "blue")` of success of each Bernoulli trial and $n \in \mathbb{N}$ to the number of trials. On the other hand, the term ${n \choose y}$ indicates the total number of possible combinations for $y$ successes out of our $n$ trials:

$$
{n \choose y} = \frac{n!}{y!(n - y)!}.
$${#eq-app-combination}

> **How can we verify that @eq-app-binomial-pmf is a proper `r colourize("PMF", "blue")` (i.e., all the standalone `r colourize("probabilities", "blue")` over the support of $Y$ add up to one)?**

To elaborate on this, we need to use a handy mathematical result called the **binomial theorem**.

::: {#thm-binomial-theorem}

# Binomial Theorem

This theorem is associated to the **Pascal's identity**, and it defines the pattern of coefficients in the expansion of a polynomial in the form $(u + v)^m$. More specifically, the binomial theorem indicates that if $m$ is a non-negative integer, then the polynomial $(u + v)^m$ can be expanded via the following series:

$$
\begin{align*}
(u + v)^m &= u^m + {m \choose 1} u^{m - 1} v + {m \choose 2} u^{m - 2} v^2 + \dots + \\
& \qquad {m \choose r} u^{m - r} v^r + \dots + \\
& \qquad {m \choose m - 1} u v^{m - 1} + v^m \\
&= \underbrace{{m \choose 0}}_1 u^m + {m \choose 1} u^{m - 1} v + {m \choose 2} u^{m - 2} v^2 + \dots + \\
& \qquad {m \choose r} u^{m - r} v^r + \dots + \\
& \qquad {m \choose m - 1} u v^{m - 1} + \underbrace{{m \choose m}}_1 v^m \\
&= \sum_{i = 0}^m {m \choose i} u^{m - i} v^i.
\end{align*}
$${#eq-app-binomial-theorem}
:::

::: {#Tip-pascal-triangle .Tip}
::::{.Tip-header}
Tip on the binomial theorem and Pascal's identity
::::
::::{.Tip-container}
Let us dig into the proof of the binomial theorem from @eq-app-binomial-theorem. This proof will require another important result called the **Pascal's identity**. This identity states that for any integers $m$ and $k$, with $k \in \{ 1, \dots, m \}$, it follows that:\

::: {.proof}
$$
\begin{align*}
{m \choose k - 1} + {m \choose k} &= \left[ \frac{m!}{(k - 1)! (m - k + 1)!} \right] \\ 
& \qquad + \left[ \frac{m!}{k! (m - k)!} \right] \\
&= m! \biggl\{ \left[ \frac{1}{(k - 1)! (m - k + 1)!} \right] + \\
& \qquad \left[ \frac{1}{k! (m - k)!} \right] \biggl\} \\
&= m! \Biggl\{ \Biggr[ \frac{k}{\underbrace{k (k - 1)!}_{k!} (m - k + 1)!} \Biggr] + \\
& \qquad \Biggr[ \frac{m - k + 1}{k! \underbrace{(m - k + 1)(m - k)!}_{(m - k + 1)!}} \Biggr] \Biggl\}  \\
&= m! \left[ \frac{k + m - k + 1}{k! (m - k + 1)!} \right] \\
&= m! \left[ \frac{m + 1}{k! (m - k + 1)!} \right] \\
&= \frac{(m + 1)!}{k! (m + 1 - k)!} \\
&= {m + 1 \choose k }. \qquad \qquad \qquad \qquad \square
\end{align*}
$${#eq-pascal-dentity}
:::

::: {.proof}
\
Now, we will use **mathematical induction** to prove the binomial theorem from @eq-app-binomial-theorem. Firstly, on the left-hand side of the theorem, note that when $m = 0$ we have:\

$$
(u + v)^0 = 1.
$$

Now, when $m = 0$, for the right-hand side of this equation, we have that\

$$
\sum_{i = 0}^m {m \choose i} u^{m - i} v^i  = \sum_{i = 0}^0 {0 \choose i} u^i v^{i} = {0 \choose 0} u^0 v^0 = 1.
$$

Hence, the binomial theorem holds when $m = 0$. This is what we call the **base case** in mathematical induction.\

That said, let us proceed with the **inductive hypothesis**. We aim to prove that the binomial theorem\

$$
\begin{align*}
(u + v)^j &= u^j + {j \choose 1} u^{j - 1} v + {j \choose 2} u^{j - 2} v^2 + \dots + \\
& \qquad {j \choose r} u^{j - r} v^r + \dots + \\
& \qquad {j \choose j - 1} u v^{j - 1} + v^j \\
&= \underbrace{{j \choose 0}}_1 u^j + {j \choose 1} u^{j - 1} v + {j \choose 2} u^{j - 2} v^2 + \dots + \\
& \qquad {j \choose r} u^{j - r} v^r + \dots + \\
& \qquad {j \choose j - 1} u v^{j - 1} + \underbrace{{j \choose j}}_1 v^j \\
&= \sum_{i = 0}^j {j \choose i} u^{j - i} v^i
\end{align*}
$${#eq-inductive-hyp}

holds when integer $j \geq 1$. This is our inductive hypothesis.

Then, we pave the way to the **inductive step**. Let us consider the following expansion:\

$$
\begin{align*}
(u + v)^{j + 1} &= (u + v) (u + v)^j \\
&= (u + v) \times \\
& \qquad \bigg[ u^j + {j \choose 1} u^{j - 1} v + {j \choose 2} u^{j - 2} v^2 + \dots + \\
& \qquad {j \choose r} u^{j - r} v^r + \dots + {j \choose j - 1} u v^{j - 1} + v^j \bigg] \\
&= \bigg[u^{j + 1} + {j \choose 1} u^j v + {j \choose 2} u^{j - 1} v^2 + \dots + \\
& \qquad {j \choose r} u^{j - r + 1} v^r + \dots + \\
& \qquad {j \choose j - 1} u^2 v^{j - 1} + u v^j \bigg] + \\
& \qquad \bigg[ u^j v + {j \choose 1} u^{j - 1} v^2 + {j \choose 2} u^{j - 2} v^3 + \dots + \\
& \qquad {j \choose r} u^{j - r} v^{r + 1} + \dots + \\
& \qquad {j \choose j - 1} u v^j + {j \choose j} v^{j + 1} \bigg] \\
&= u^{j + 1} + \left[ {j \choose 0} + {j \choose 1} \right] u^j v + \\
& \qquad \left[ {j \choose 1} + {j \choose 2} \right] u^{j - 1} v^2 + \dots + \\
& \qquad \left[ {j \choose r - 1} + {j \choose r} \right] u^{j - r + 1} v^r + \dots + \\
& \qquad \left[ {j \choose j - 1} + {j \choose j} \right] u v^j + v^{j + 1}.
\end{align*} 
$${#eq-binomial-inductive-step}

Let us plug in the Pascal's identity from @eq-pascal-dentity into @eq-binomial-inductive-step:\

$$
\begin{align*}
(u + v)^{j + 1} &= u^{j + 1} + {j + 1 \choose 1} u^j v + \\
& \qquad {j + 1 \choose 2} u^{j - 1} v^2 + \dots + \\
& \qquad {j + 1 \choose r} u^{j - r + 1} v^r + \dots + \\
& \qquad {j + 1 \choose j} u v^j + v^{j + 1} \\
&= \underbrace{{j + 1 \choose 0}}_1 u^{j + 1} + {j + 1 \choose 1} u^j v + \\
& \qquad {j + 1 \choose 2} u^{j - 1} v^2 + \dots + \\ 
& \qquad {j + 1 \choose r} u^{j - r + 1} v^r + \dots + \\
& \qquad {j + 1 \choose j} u v^j + \underbrace{{j + 1 \choose j + 1}}_1 v^{j + 1} \\
&= \sum_{i = 0}^{j + 1} {j + 1 \choose i} u^{j + 1 - i} v^i. \qquad \quad \square
\end{align*}
$${#eq-proof-inductive-hyp}

Note that the result for $j$ in @eq-inductive-hyp also holds for $j + 1$ in @eq-proof-inductive-hyp. Therefore, by induction, the binomial theorem from @eq-app-binomial-theorem is true for all positive integers $m$.
:::

::::
:::

After the above fruitful digression on the binomial theorem, let us use it to show that our Binomial `r colourize("PMF", "blue")` in @eq-app-binomial-pmf actually adds up to one all over the support of the `r colourize("random variable", "blue")`:

::: {.proof}
$$
\begin{align*} 
\sum_{y = 0}^n P \left( Y = y \mid n, \pi \right) &= \sum_{y = 0}^n {n \choose y} \pi^y (1 - \pi)^{n - y} \\
&= \sum_{y = 0}^n {n \choose y} (1 - \pi)^{n - y} \pi^y \\
& \quad \qquad \text{rearranging factors.}
\end{align*}
$$

Now, by using the binomial theorem in @eq-app-binomial-theorem, let:

$$
\begin{gather*}
m  = n\\
i = y \\
u = 1 - \pi \\
v = \pi.
\end{gather*}
$$

The above arrangement yields the following result:

$$
\begin{align*}
\sum_{y = 0}^n P \left( Y = y \mid n, \pi \right) &= (1 - \pi + \pi)^n \\
&= 1^n = 1. \qquad \square
\end{align*}
$${#eq-proof-binomial-PMF-adds-to-1}

> **Indeed, the Binomial `r colourize("PMF", "blue")` is a proper `r colourize("probability distribution", "blue")`!**
:::

### Expected Value

Via @eq-app-expected-value-discrete, the `r colourize("expected value", "blue")` or `r colourize("mean", "blue")` of a Binomial-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*} 
\mathbb{E}(Y) &= \sum_{y = 0}^n y P \left( Y = y \mid n, \pi \right) \\
&= \sum_{y = 1}^n y P \left( Y = y \mid n, \pi \right) \\
& \quad \qquad \text{for $y = 0$, the addend is equal to zero} \\
&= \sum_{y = 1}^n y \left[ {n \choose y} \pi^y (1 - \pi)^{n - y} \right] \\
&= \sum_{y = 1}^n y \left[ \frac{n!}{y! (n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
&= \sum_{y = 1}^n \left[ \frac{y n!}{y (y - 1)!(n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
& \quad \qquad \text{in the denominator, $y! = y (y - 1)!$}\\
&= \sum_{y = 1}^n \left[ \frac{n (n - 1)!}{(y - 1)!(n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\ 
& \quad \qquad \text{in the numerator, $n! = n (n - 1)!$} \\
&= \sum_{y = 1}^n \left[ \frac{n (n - 1)!}{(y - 1)!(n - y)!} \pi^{y + 1 - 1} (1 - \pi)^{n - y} \right] \\
& \quad \qquad \text{note $\pi^y = \pi^{y + 1 - 1}$} \\
&= n \sum_{y = 1}^n \left[ \frac{(n - 1)!}{(y - 1)!(n - y)!} \pi \pi^{y - 1} (1 - \pi)^{n - y} \right] \\
& \quad \qquad \text{rearranging terms} \\
&= n \pi \sum_{y = 1}^n \left[ \frac{(n - 1)!}{(y - 1)!(n - y)!} \pi^{y - 1} (1 - \pi)^{n - y} \right].
\end{align*}
$${#eq-proof-binomial-mean-1}

Now, let us make the following variable rearrangement:

$$
\begin{gather*}
m = n - 1 \\
z = y - 1 \\
m - z = n - y.
\end{gather*}
$$

Going back to @eq-proof-binomial-mean-1, **and applying our above variable rearrangement within the summation**, we have:

$$
\begin{align*} 
\mathbb{E}(Y) &= n \pi \sum_{z = 0}^m \left[ \frac{m!}{z!(m - z)!} \pi^{z} (1 - \pi)^{m - z} \right] \\
&= n \pi \sum_{z = 0}^m \left[ {m \choose z}\pi^{z} (1 - \pi)^{m - z} \right].
\end{align*}
$${#eq-proof-binomial-mean-2}

Note that, in the summation of @eq-proof-binomial-mean-2, we encounter the `r colourize("PMF", "blue")` of a `r colourize("random variable", "blue")` $Z$ as follows:

$$
Z \sim \text{Bin}(m, \pi).
$$

Since the summation, where this Binomial `r colourize("PMF", "blue")` of $Z$ is depicted, goes from $z = 0$ to $m$, we can apply our result from @eq-proof-binomial-PMF-adds-to-1:

$$
\begin{align*} 
\mathbb{E}(Y) &= n \pi \underbrace{\sum_{z = 0}^m \left[ {m \choose z}\pi^{z} (1 - \pi)^{m - z} \right]}_{1} \\
&= n \pi. \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$
:::

### Variance

Via @eq-app-variance and the @eq-app-expected-value-discrete of a `r colourize("discrete expected value", "blue")`, the `r colourize("variance", "blue")` of a Binomial-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E}(Y^2) - \left[ \mathbb{E}(Y)\right]^2 \\
&= \mathbb{E}(Y^2) - (n \pi)^2 \qquad \text{since $\mathbb{E}(Y) = n \pi$.}
\end{align*}
$${#eq-proof-binomial-variance-1}

Unlike the Bernoulli `r colourize("random variable", "blue")`, finding $\mathbb{E}(Y^2)$ is not quite straightforward. We need to play around with the below `r colourize("expected value", "blue")` expression as follows:

$$
\begin{align*}
\mathbb{E}(Y^2) &= \mathbb{E} \left[ Y (Y - 1) \right] + \mathbb{E}(Y) \\
&= \mathbb{E} \left[ Y (Y - 1) \right] + n \pi \qquad \text{since $\mathbb{E}(Y) = n \pi$.}
\end{align*}
$${#eq-proof-binomial-variance-2}

Now, to find $\mathbb{E} \left[ Y (Y - 1) \right]$, we make the following derivation via @eq-app-expected-value-discrete-function when $g(Y) = y (y - 1)$:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= \sum_{y = 0}^n y (y - 1) P \left( Y = y \mid n, \pi \right) \\
&= \sum_{y = 2}^n y (y - 1) P \left( Y = y \mid n, \pi \right) \\
& \quad \qquad \text{for $y = \{0, 1\}$,} \\ 
& \quad \qquad \text{the addends are equal to zero} \\
&= \sum_{y = 2}^n y (y - 1) \left[ {n \choose y} \pi^y (1 - \pi)^{n - y} \right] \\
&= \sum_{y = 2}^n y (y - 1) \left[ \frac{n!}{y! (n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
&= \sum_{y = 2}^n \left[ \frac{y (y - 1) n!}{y (y - 1) (y - 2)! (n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
& \quad \qquad \text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\
&= \sum_{y = 2}^n \left[ \frac{n (n - 1) (n - 2)!}{(y - 2)! (n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\ 
& \quad \qquad \text{in the numerator, $n! = n (n - 1) (n - 2)!$} \\
&= \sum_{y = 2}^n \left[ \frac{n (n - 1) (n - 2)!}{(y - 2)! (n - y)!} \pi^{y + 2 - 2} (1 - \pi)^{n - y} \right] \\
& \quad \qquad \text{note $\pi^y = \pi^{y + 2 - 2}$} \\
&= n (n - 1) \times \\
& \qquad \sum_{y = 2}^n \left[ \frac{(n - 2)!}{(y - 2)! (n - y)!} \pi^2 \pi^{y - 2} (1 - \pi)^{n - y} \right] \\
& \qquad \qquad \text{rearranging terms} \\
&= n (n - 1) \pi^2 \times \\
& \qquad \sum_{y = 2}^n \left[ \frac{(n - 2)!}{(y - 2)! (n - y)!} \pi^{y - 2} (1 - \pi)^{n - y} \right] \\
& \qquad \qquad \text{rearranging terms.}
\end{align*}
$${#eq-proof-binomial-variance-3}

Then, we make the following variable rearrangement:

$$
\begin{gather*}
m = n - 2 \\
z = y - 2 \\
m - z = n - y.
\end{gather*}
$$

Going back to @eq-proof-binomial-variance-3, **and applying our above variable rearrangement within the summation**, we have:

$$
\begin{align*} 
\mathbb{E} \left[ Y (Y - 1) \right] &= n (n - 1) \pi^2 \sum_{z = 0}^m \left[ \frac{m!}{z! (m - z)!} \pi^{z} (1 - \pi)^{m - z} \right] \\
&= n (n - 1) \pi^2 \sum_{z = 0}^m \left[ {m \choose z} \pi^{z} (1 - \pi)^{m - z} \right].
\end{align*}
$${#eq-proof-binomial-variance-4}

Note that, in the summation of @eq-proof-binomial-variance-4, we encounter the `r colourize("PMF", "blue")` of a `r colourize("random variable", "blue")` $Z$ as follows:

$$
Z \sim \text{Bin}(m, \pi).
$$

Since the summation, where this Binomial `r colourize("PMF", "blue")` of $Z$ is depicted, goes from $z = 0$ to $m,$ we can apply our result from @eq-proof-binomial-PMF-adds-to-1:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= n (n - 1) \pi^2 \underbrace{\sum_{z = 0}^m \left[ {m \choose z} \pi^{z} (1 - \pi)^{m - z} \right]}_{1} \\
&= n (n - 1) \pi^2.
\end{align*}
$$

Let us go back to @eq-proof-binomial-variance-2 and plug in the above result:

$$
\begin{align*}
\mathbb{E}(Y^2) &= \mathbb{E} \left[ Y (Y - 1) \right] + n \pi \\
&= n (n - 1) \pi^2 + n \pi. \\
\end{align*}
$$

Finally, we plug in $\mathbb{E}(Y^2)$ in @eq-proof-binomial-variance-1:

$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E}(Y^2) - (n \pi)^2 \\
&= n (n - 1) \pi^2 + n \pi - n^2 \pi^2 \\
&= n^2 \pi^2 - n \pi^2 + n \pi - n^2 \pi^2 \\
&= n \pi - n \pi^2 \\
&= n \pi (1 - \pi). \qquad \qquad \qquad \square
\end{align*}
$$
:::

## Negative Binomial

Suppose you execute a series of `r colourize("independent", "blue")` **Bernoulli trials**, each one with a `r colourize("probability", "blue")` of success $\pi$. Let $Y$ be the number of failures in this series of Bernoulli trials you obtain before experiencing $k$ successes. Therefore, $Y$ is said to have a **Negative Binomial distribution** with `r colourize("parameters", "blue")` $k$ and $\pi$:

$$
Y \sim \text{NegBin}(k, \pi).
$$

### Probability Mass Function

The `r colourize("PMF", "blue")` of $Y$ is the following:

$$
\begin{align*}
P \left( Y = y \mid k, \pi \right) &= {k + y - 1 \choose y} \pi^k (1 - \pi)^y \\
& \qquad \qquad \qquad \quad \text{for $y \in \{ 0, 1, \dots \}$.}
\end{align*}
$${#eq-app-neg-binomial-pmf}

`r colourize("Parameter", "blue")` $\pi \in [0, 1]$ refers to the `r colourize("probability", "blue")` of success of each Bernoulli trial, whereas $k$ refers to the number of successes.

::: {#Headsup-sample .Heads-up}
::::{.Heads-up-header}
Heads-up on an alternative Negative Binomial PMF!
::::
::::{.Heads-up-container}
There is an alternative parametrization to define a Negative Binomial distribution in which we have a `r colourize("random variable", "blue")` $Z$ defined as the **total number of Bernoulli trials** (i.e., $k$ successes plus the $Y$ failures depicted in @eq-app-neg-binomial-pmf):

$$
Z = Y + k.
$$

This alternative parametrization of the Negative Binomial distribution yields the following `r colourize("PMF", "blue")`:

$$
\begin{align*}
P \left( Z = z \mid k, \pi \right) &= {z - 1 \choose k - 1} \pi^k (1 - \pi)^{z - k} \\
& \qquad \qquad \qquad \text{for $z \in \{ k, k + 1, \dots \}$.}
\end{align*}
$$

Nevertheless, we will not dig into this version of the Negative Binomial distribution since @sec-negative-binomial delves into a modelling estimation via a joint `r colourize("PMF", "blue")` of the `r colourize("training set", "magenta")` involving @eq-app-neg-binomial-pmf.
::::
:::

> **How can we verify that @eq-app-neg-binomial-pmf is a proper `r colourize("PMF", "blue")` (i.e., all the standalone `r colourize("probabilities", "blue")` over the support of $Y$ add up to one)?**

::: {.proof}
\
Let us manipulate the factor involving the number of combinations corresponding to how many different possible subsets of size $y$ can be made from the larger set of size $k + y - 1$:

$$
\begin{align*}
{k + y - 1 \choose y} &= \frac{(k + y - 1)!}{(k + y - 1 - y)! y !} \\
&= \frac{(k + y - 1)!}{(k - 1)! y!} \\
&= \frac{(k + y - 1) (k + y - 2) \cdots (k + 1) (k) (k - 1)!}{(k - 1)! y!} \\
&= \frac{(\overbrace{k + y - 1) (k + y - 2) \cdots (k + 1) k}^{\text{we have $y$ factors}}}{y!} \\
&= (- 1)^y \frac{\overbrace{(-k - y + 1) (-k - y + 2) \cdots (-k - 1) (-k)}^{\text{multiplying each factor times $-1$}}}{y!} \\
&= (- 1)^y \frac{\overbrace{(-k) (-k - 1) \cdots (-k - y + 2) (-k - y + 1)}^{\text{rearranging factors}}}{y!} \\
&= (- 1)^y \frac{(-k) (-k - 1) \cdots (-k - y + 2) (-k - y + 1)}{y!} \times \\
& \qquad \frac{(-k - y) (-k - y - 1) \cdots (1)}{(-k - y) (-k - y - 1) \cdots (1)} \\
&= (- 1)^y \frac{(-k) (-k - 1) \cdots (-k - y + 2) (-k - y + 1)}{y!} \times \\
& \qquad \frac{(-k - y) (-k - y - 1) \cdots (1)}{(-k - y)!}.
\end{align*}
$$


In the equation above, note that there are still several factors in the numerator, which can be summarized using a factorial as follows:

$$
\begin{align*}
(-k)! &= (-k) (-k - 1) \cdots (-k - y + 2) (-k - y + 1) \times \\
& \quad \qquad (-k - y) (-k - y - 1) \cdots (1).
\end{align*}
$$

Therefore:

$$
\begin{align*}
{k + y - 1 \choose y} &= (- 1)^y \frac{(-k)!}{(-k - y)! y!}\\
&= (- 1)^y {-k \choose y}.
\end{align*}
$$

Now, let us begin with the summation involving the Negative Binomial `r colourize("PMF", "blue")` depicted in  @eq-app-neg-binomial-pmf from $0$ to $\infty$:

$$
\begin{align*}
\sum_{y = 0}^{\infty} P \left( Y = y \mid k, \pi \right) &= \sum_{y = 0}^{\infty} {k + y - 1 \choose y} \pi^k (1 - \pi)^y \\
&= \sum_{y = 0}^{\infty} (- 1)^y {-k \choose y} \pi^k (1 - \pi)^y \\
&= \pi^k \sum_{y = 0}^{\infty} (- 1)^y {-k \choose y} (1 - \pi)^y \\
&= \pi^k \sum_{y = 0}^{\infty} {-k \choose y} (-1 + \pi)^y.
\end{align*}
$${#eq-proof-neg-binomial-PMF-adds-to-1-a}

On the right-hand side of @eq-proof-neg-binomial-PMF-adds-to-1-a we will add the following factor:

$$
(1)^{-k - y} = 1.
$$

Thus:

$$
\begin{align*}
\sum_{y = 0}^{\infty} P \left( Y = y \mid k, \pi \right) &= \pi^k \sum_{y = 0}^{\infty} {-k \choose y} (1)^{-k - y} (-1 + \pi)^y.
\end{align*}
$${#eq-proof-neg-binomial-PMF-adds-to-1-b}

Now, by using the **binomial theorem** in @eq-app-binomial-theorem, let:

$$
\begin{gather*}
m  = -k\\
i = y \\
u = 1 \\
v = -1 + \pi.
\end{gather*}
$$
The above arrangement yields the following result in @eq-proof-neg-binomial-PMF-adds-to-1-b:

$$
\begin{align*}
\sum_{y = 0}^{\infty} P \left( Y = y \mid k, \pi \right) &=  \pi^k (1 - 1 + \pi)^{-k} \\
&= \pi^k (\pi) ^{-k} \\
&= \pi^0 \\
&= 1. \qquad \qquad \qquad \square
\end{align*}
$${#eq-proof-neg-binomial-PMF-adds-to-1-c}

> **Indeed, the Negative Binomial `r colourize("PMF", "blue")` is a proper `r colourize("probability distribution", "blue")`!**
:::

### Expected Value

Via @eq-app-expected-value-discrete, the `r colourize("expected value", "blue")` or `r colourize("mean", "blue")` of a Negative Binomial-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*} 
\mathbb{E}(Y) &= \sum_{y = 0}^{\infty} y P \left( Y = y \mid k, \pi \right) \\
&= \sum_{y = 1}^{\infty} y P \left( Y = y \mid k, \pi \right) \\
& \quad \qquad \text{for $y = 0$, the addend is equal to zero} \\
&= \sum_{y = 1}^{\infty} y \left[ {k + y - 1 \choose y} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 1}^{\infty} y \left[ \frac{(k + y - 1)!}{y! (k + y - 1 - y)!} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 1}^{\infty} y \left[ \frac{(k + y - 1)!}{y! (k - 1)!} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 1}^{\infty} y \Bigg[ \frac{(k + y - 1)!}{y (y - 1)! \underbrace{\left( \frac{k!}{k} \right)}_{(k - 1)!}} \pi^k (1 - \pi)^y \Bigg] \\
&= \sum_{y = 1}^{\infty} k \left[ \frac{(k + y - 1)!}{k! (y - 1)!} \pi^k (1 - \pi)^y \right] \\
&= k \sum_{y = 1}^{\infty} \left[ {k + y - 1 \choose y - 1} \pi^k (1 - \pi)^y \right] \\
&= k \sum_{y = 1}^{\infty} \left[ {k + y - 1 \choose y - 1} \pi^{k + 1 - 1} (1 - \pi)^{y + 1 - 1} \right] \\
& \quad \qquad \text{note $\pi^k = \pi^{k + 1 - 1}$ and $(1 - \pi)^y = (1 - \pi)^{y + 1 - 1}$} \\
&= \frac{k (1 - \pi)}{\pi} \sum_{y = 1}^{\infty} \left[ {k + y - 1 \choose y - 1} \pi^{k + 1} (1 - \pi)^{y - 1} \right].
\end{align*}
$${#eq-proof-neg-binomial-mean-1}

Now, let us make the following variable rearrangement:

$$
\begin{gather*}
m = k + 1 \\
z = y - 1 \\
m + z - 1  = k + y - 1.
\end{gather*}
$$

Going back to @eq-proof-neg-binomial-mean-1, **and applying our above variable rearrangement within the summation**, we have:

$$
\mathbb{E}(Y) = \frac{k (1 - \pi)}{\pi} \sum_{z = 0}^{\infty} \left[ {m + z - 1 \choose z} \pi^{m} (1 - \pi)^{z} \right].
$${#eq-proof-neg-binomial-mean-2}

Note that, in the summation of @eq-proof-neg-binomial-mean-2, we encounter the `r colourize("PMF", "blue")` of a `r colourize("random variable", "blue")` $Z$ as follows:

$$
Z \sim \text{NegBin}(m, \pi).
$$

Since the summation, where this Negative Binomial `r colourize("PMF", "blue")` of $Z$ is depicted, goes from $z = 0$ to $\infty$, we can apply our result from @eq-proof-neg-binomial-PMF-adds-to-1-c:

$$
\begin{align*} 
\mathbb{E}(Y) &= \frac{k (1 - \pi)}{\pi} \underbrace{\sum_{z = 0}^m \left[ {m + z - 1 \choose z} \pi^{m} (1 - \pi)^{z} \right]}_{1} \\
&= \frac{k (1 - \pi)}{\pi}. \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$
:::

### Variance

Via @eq-app-variance and the @eq-app-expected-value-discrete of a `r colourize("discrete expected value", "blue")`, the `r colourize("variance", "blue")` of a Negative Binomial-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E}(Y^2) - \left[ \mathbb{E}(Y)\right]^2 \\
&= \mathbb{E}(Y^2) - \left[ \frac{k (1 - \pi)}{\pi} \right]^2 \quad \text{since $\mathbb{E}(Y) = \frac{k (1 - \pi)}{\pi}$.}
\end{align*}
$${#eq-proof-neg-binomial-variance-1}

Now, we need to play around with the below `r colourize("expected value", "blue")` expression as follows:

$$
\begin{align*}
\mathbb{E}(Y^2) &= \mathbb{E} \left[ Y (Y - 1) \right] + \mathbb{E}(Y) \\
&= \mathbb{E} \left[ Y (Y - 1) \right] + \frac{k (1 - \pi)}{\pi}.
\end{align*}
$${#eq-proof-neg-binomial-variance-2}

To find $\mathbb{E} \left[ Y (Y - 1) \right]$, we make the following derivation via @eq-app-expected-value-discrete-function when $g(Y) = y (y - 1)$:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= \sum_{y = 0}^{\infty} y (y - 1) P \left( Y = y \mid k, \pi \right) \\
&= \sum_{y = 2}^{\infty} y (y - 1) P \left( Y = y \mid k, \pi \right) \\
& \quad \qquad \text{for $y = \{0, 1\}$,} \\ 
& \quad \qquad \text{the addends are equal to zero} \\
&= \sum_{y = 2}^{\infty} y (y - 1) \left[ {k + y - 1 \choose y} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 2}^{\infty} y (y - 1) \left[ \frac{(k + y - 1)!}{y! (k + y - 1 - y)!} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 2}^{\infty} y (y - 1) \left[ \frac{(k + y - 1)!}{y! (k - 1)!} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 2}^{\infty} \frac{y (y - 1)}{y (y - 1)} \left[ \frac{(k + y - 1)!}{(y - 2)! (k - 1)!} \pi^k (1 - \pi)^y \right] \\
& \quad \qquad \text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\
&= \sum_{y = 2}^{\infty} \Bigg[ \frac{(k + y - 1)!}{(y - 2)! \underbrace{\frac{(k + 1)!}{k (k + 1)}}_{(k - 1)!}} \pi^k (1 - \pi)^y \Bigg] \\
&= \sum_{y = 2}^{\infty} \left[ k (k + 1) \frac{(k + y - 1)!}{(k + 1)! (y - 2)!} \pi^k (1 - \pi)^y \right] \\
&= k (k + 1) \sum_{y = 2}^{\infty} \left[ {k + y - 1 \choose y - 2} \pi^k (1 - \pi)^y \right] \\
&= k (k + 1) \sum_{y = 2}^{\infty} \left[ {k + y - 1 \choose y - 2} \pi^{k + 2 - 2} (1 - \pi)^{y + 2 - 2} \right] \\
& \quad \qquad \text{note $\pi^k = \pi^{k + 2 - 2}$ and} \\
& \quad \qquad (1 - \pi)^y = (1 - \pi)^{y + 2 - 2} \\
&= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} \times \\
& \qquad \sum_{y = 2}^{\infty} \left[ {k + y - 1 \choose y - 2} \pi^{k + 2} (1 - \pi)^{y - 2} \right].
\end{align*}
$${#eq-proof-neg-binomial-variance-3}

Then, we make the following variable rearrangement:

$$
\begin{gather*}
m = k + 2\\
z = y - 2 \\
m + z - 1  = k + y - 1.
\end{gather*}
$$

Going back to @eq-proof-neg-binomial-variance-3, **and applying our above variable rearrangement within the summation**, we have:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} \times \\
& \qquad \sum_{y = 2}^{\infty} \left[ {m + z - 1 \choose z} \pi^m (1 - \pi)^z \right].
\end{align*}
$${#eq-proof-neg-binomial-variance-4}

Note that, in the summation of @eq-proof-neg-binomial-variance-4, we encounter the `r colourize("PMF", "blue")` of a `r colourize("random variable", "blue")` $Z$ as follows:

$$
Z \sim \text{NegBin}(m, \pi).
$$

Since the summation, where this Binomial `r colourize("PMF", "blue")` of $Z$ is depicted, goes from $z = 0$ to $\infty$, we can apply our result from @eq-proof-neg-binomial-PMF-adds-to-1-c:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} \times \\
& \qquad \underbrace{\sum_{y = 2}^{\infty} \left[ {m + z - 1 \choose z} \pi^m (1 - \pi)^z \right]}_{1} \\
&= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2}.
\end{align*}
$$

Let us go back to @eq-proof-neg-binomial-variance-2 and plug in the above result:

$$
\begin{align*}
\mathbb{E}(Y^2) &= \mathbb{E} \left[ Y (Y - 1) \right] + \frac{k ( 1 - \pi)}{\pi} \\
&= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} + \frac{k ( 1 - \pi)}{\pi}.
\end{align*}
$$

Finally, we plug in $\mathbb{E}(Y^2)$ in @eq-proof-neg-binomial-variance-1:

$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E}(Y^2) - \left[ \frac{k (1 - \pi)}{\pi} \right]^2 \\
&= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} + \frac{k ( 1 - \pi)}{\pi} - \left[ \frac{k (1 - \pi)}{\pi} \right]^2 \\
&= \frac{k (1 - \pi)}{\pi} \left[ \frac{(k + 1) (1 - \pi)}{\pi} + 1 - \frac{k (1 - \pi)}{\pi} \right] \\
&= \frac{k (1 - \pi)}{\pi} \left[ \frac{(k + 1) (1 - \pi) + \pi - k (1 - \pi)}{\pi} \right] \\
&= \frac{k (1 - \pi)}{\pi} \left( \frac{k - k \pi + 1 - \pi + \pi - k + k \pi}{\pi} \right) \\
&= \frac{k (1 - \pi)}{\pi} \left( \frac{1}{\pi} \right) \\
&= \frac{k (1 - \pi)}{\pi^2}. \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$
:::

## Classical Poisson

Suppose you observe the count of events happening in a **fixed interval of time or space** Let $Y$ be the number of counts considered of integer type. Then, $Y$ is said to have a **classical Poisson distribution** with a `r colourize("continuous parameter", "blue")` $\lambda$:

$$
Y \sim \text{Pois}(\lambda).
$$

### Probability Mass Function

The `r colourize("PMF", "blue")` of this count-type $Y$ is the following:

$$
P \left( Y = y \mid \lambda \right) = \frac{\lambda^y \exp{(-\lambda)}}{y!} \quad \text{for $y \in \{ 0, 1, 2, \dots\}$,}
$${#eq-app-classical-poisson-pmf}

where $\exp{(\cdot)}$ depicts the base $e$ (i.e., **Euler's number**, $e = 2.71828...$) and $y!$ is the factorial

$$
y! = y \times (y - 1) \times (y - 2) \times (y - 3) \times \cdots \times 3 \times 2 \times 1.  
$$

with 

$$
0! = 1.
$$

The `r colourize("continuous parameter", "blue")` $\lambda \in (0, \infty)$ represents the average rate at which these events happen (i.e., events per area unit or events per time unit). Curiously, even though the `r colourize("random variable", "blue")` $Y$ is considered `r colourize("discrete", "blue")` in this case, $\lambda$ is modelled as `r colourize("continuous", "blue")`!

> **How can we verify that @eq-app-classical-poisson-pmf is a proper `r colourize("PMF", "blue")` (i.e., all the standalone `r colourize("probabilities", "blue")` over the support of $Y$ add up to one)?**

To elaborate on this, we need to use some mathematical tools called the **Taylor series expansions** and a derived result called **Maclaurin series expansions**.

::: {#Headsup-sample .Heads-up}
::::{.Heads-up-header}
Heads-up on the Taylor and Maclaurin series expansions!
::::
::::{.Heads-up-container}
In mathematics, there are helpful tools known as **Taylor series expansions**, which were officially published by English mathematician Brook Taylor in *Methodus Incrementorum Directa & Inversa* [@taylor1715]. 

![Portrait of mathematician Brook Taylor [@earlom1793].](img/brook-taylor.jpg){width="300"}

However, it is essential to note that Scottish mathematician James Gregory introduced the notion of these series expansions in his work *Vera Circuli et Hyperbolae Quadratura* [@gregory1668]. 

![Portrait of mathematician James Gregory [@watson1886].](img/james-gregory.jpg){width="300"}

These series approximate complex mathematical functions through an infinite sum of polynomial terms. For example, in **machine learning**, the Taylor series expansions can be utilized in **gradient-based optimization methods**. Specifically, **Newton's method** uses these expansions to find roots of equations that cannot be solved analytically, which is common in `r colourize("maximum likelihood-based parameter", "blue")` estimation for the varied regression models discussed throughout this book. Moreover, we can find these series in **different engineering and scientific fields such as physics**.

Suppose we have **real function** $f(u)$ around a point $u = a$, then the one-dimensional **infinite** Taylor series expansion is given by the expression 

$$
\begin{align*}
f(u) &= f(a) + f'(a) (u - a) + \frac{f''(a)}{2!} (u - a)^2 + \\
& \qquad \frac{f^{(3)}(a)}{3!} (u - a)^3 + \frac{f^{(4)}(a)}{4!} (u - a)^4 + \\
& \qquad \frac{f^{(5)}(a)}{5!} (u - a)^5 + \cdots \\
&= \sum_{j = 0}^{\infty} \frac{f^{(j)}(a)}{j!} (u - a)^j.
\end{align*}
$${#eq-taylor-series}

A complete mathematical derivation of @eq-taylor-series can be found in @weisstein. Moving along, specifically in the last line of this equation which shows an infinite summation, note the following:

- $f(a)$ indicates function $f(u)$ evaluated at point $a$.
- $f^{(j)}(a)$ indicates the $j$th order derivative of $f(u)$ and evaluated at point $a$.
- $j!$ implicates the factorial of $j$ such that

$$
j! = j \times (j - 1) \times (j - 2) \times (j - 3) \times \cdots \times 3 \times 2 \times 1.
$$

with 

$$
0! = 1.
$$

If we go even further with @eq-taylor-series, we have a specific case when $a = 0$ called the **Maclaurin series expansions**. This case was introduced by the Scottish mathematician Colin Maclaurin in his work *A Treatise of Fluxions* [@maclaurin1742].

![Portrait of mathematician Colin Maclaurin [@harding1798].](img/colin-maclaurin.jpg){width="300"}

Hence, in a Mclaurin series, @eq-taylor-series becomes:

$$
\begin{align*}
f(u) &= f(0) + f'(0) (u) + \frac{f''(0)}{2!} u^2 + \\
& \qquad \frac{f^{(3)}(0)}{3!} u^3 + \frac{f^{(4)}(0)}{4!} u^4 + \\
& \qquad \frac{f^{(5)}(0)}{5!} u^5 + \cdots \\
&= \sum_{j = 0}^{\infty} \frac{f^{(j)}(0)}{j!} u^j.
\end{align*}
$${#eq-maclaurin-series}

Different statistical proofs make use of Taylor series expansions as well as the Mclaurin series, and the Poisson distribution is not an exception at all! 
::::
:::

The above Mclaurin series in @eq-maclaurin-series will help us to show that our Poisson `r colourize("PMF", "blue")` in @eq-app-classical-poisson-pmf actually adds up to one all over the support of the `r colourize("random variable", "blue")`:

::: {.proof}
$$
\begin{align*}
\sum_{y = 0}^{\infty} P \left( Y = y \mid \lambda \right) &= \sum_{y = 0}^{\infty} \frac{\lambda^y \exp{(-\lambda)}}{y!} \\
&= \exp{(-\lambda)} \sum_{y = 0}^{\infty} \frac{\lambda^y}{y!} \\
& \quad \qquad \text{factoring out $\exp{(-\lambda)}$,} \\
& \quad \qquad \text{since it does not depend on $y$.}
\end{align*}
$${#eq-proof-poisson-PMF-sum}

Now, we will focus on the above summation 

$$
\sum_{y = 0}^{\infty} \frac{\lambda^y}{y!}
$$
and use the Mclaurin series from @eq-maclaurin-series by letting

$$
f(u) = \exp(u).
$${#eq-base-e}

We know that all derivatives of the above function are equal

$$
f'(u) = f''(u) = f^{(3)}(u) = f^{(4)}(u) = f^{(5)}(u) = \cdots = \exp{(u)},
$$
which allows us to conclude that the $j$th derivative is

$$
f^{(j)}(u) = \exp(u).
$$

This $j$th derivative evaluated at $u = 0$ becomes

$$
f^{(j)}(0) = \exp(0) = 1.
$$
Therefore, the Mclaurin series for @eq-base-e is the following:

$$
\begin{align*}
f(u) &= \exp(u) \\
&= \sum_{j = 0}^{\infty} \frac{\exp(0)}{j!} u^j \\
&= \sum_{j = 0}^{\infty} \frac{u^j }{j!}.
\end{align*}
$${#eq-maclaurin-series-base-e}

That said, using @eq-maclaurin-series-base-e, let:

$$
\begin{gather*}
\lambda = u \\
y = j.
\end{gather*}
$$

Thus, we have the following:

$$
\sum_{y = 0}^{\infty} \frac{\lambda^y}{y!} = \exp{(\lambda)}.
$$

Finally, going back to @eq-proof-poisson-PMF-sum:

$$
\begin{align*}
\sum_{y = 0}^{\infty} P \left( Y = y \mid \lambda \right) &= \exp{(-\lambda)} \overbrace{\sum_{y = 0}^{\infty} \frac{\lambda^y}{y!}}^{\exp{(\lambda)}} \\
&= \exp{(-\lambda)} \times \exp{(\lambda)} \\
&= \exp{(-\lambda + \lambda)} \\
&= \exp{(0)} \\
&= 1. \qquad \qquad \qquad \qquad \square
\end{align*}
$${#eq-proof-poisson-PMF-adds-to-1}

> **Indeed, the Poisson `r colourize("PMF", "blue")` is a proper `r colourize("probability distribution", "blue")`!**
:::

### Expected Value

Via @eq-app-expected-value-discrete, the `r colourize("expected value", "blue")` or `r colourize("mean", "blue")` of a Poisson-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*} 
\mathbb{E}(Y) &= \sum_{y = 0}^{\infty} y P \left( Y = y \mid \lambda \right) \\
&= \sum_{y = 1}^{\infty} y P \left( Y = y \mid \lambda \right) \\
& \quad \qquad \text{for $y = 0$, the addend is equal to zero} \\
&= \sum_{y = 1}^{\infty} y \left[ \frac{\lambda^y \exp{(-\lambda)}}{y!} \right] \\
&= \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{y \lambda^y}{y!} \\
& \quad \qquad \text{factoring out $\exp{(-\lambda)}$,} \\
& \quad \qquad \text{since it does not depend on $y$} \\
&= \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{y \lambda^y}{y (y - 1)!} \\
& \quad \qquad \text{in the denominator, $y! = y (y - 1)!$}\\
&= \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{\lambda^y}{(y - 1)!} \\
&= \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{\lambda^{y + 1 - 1}}{(y - 1)!} \\
& \quad \qquad \text{note $\lambda^y = \lambda^{y + 1 - 1}$} \\
&= \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{\lambda \lambda^{y - 1}}{(y - 1)!} \\
& \quad \qquad \text{rearranging terms} \\
&= \lambda \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{\lambda^{y - 1}}{(y - 1)!} \\
& \quad \qquad \text{factoring out $\lambda$,} \\
& \quad \qquad \text{since it does not depend on $y$.} \\
\end{align*}
$${#eq-proof-poisson-mean-1}

Then, let us make the following variable rearrangement:

$$
z = y - 1.
$$

Going back to @eq-proof-poisson-mean-1, **and applying our above variable rearrangement within the summation**, we have:

$$
\mathbb{E}(Y) = \lambda \exp{(-\lambda)} \sum_{z = 0}^{\infty} \frac{\lambda^z}{z!}
$${#eq-proof-poisson-mean-2}

Using @eq-maclaurin-series-base-e, let:

$$
\begin{gather*}
\lambda = u \\
z = j.
\end{gather*}
$$

Hence, we have the following:

$$
\sum_{z = 0}^{\infty} \frac{\lambda^z}{z!} = \exp{(\lambda)}.
$$

Finally, going back to @eq-proof-poisson-mean-2:

$$
\begin{align*}
\mathbb{E}(Y) &= \lambda \exp{(-\lambda)} \overbrace{\sum_{z = 0}^{\infty} \frac{\lambda^z}{z!}}^{\exp{(\lambda)}} \\
&= \lambda \exp{(-\lambda)} \times \exp{(\lambda)} \\
&= \lambda \exp{(-\lambda + \lambda)} \\
&= \lambda \exp{(0)} \\
&= \lambda. \qquad \qquad \qquad \qquad \square
\end{align*}
$$
:::

### Variance

Via @eq-app-variance and the @eq-app-expected-value-discrete of a `r colourize("discrete expected value", "blue")`, the `r colourize("variance", "blue")` of a Poisson-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E}(Y^2) - \left[ \mathbb{E}(Y)\right]^2 \\
&= \mathbb{E}(Y^2) - \lambda^2 \qquad \text{since $\mathbb{E}(Y) = \lambda$.}
\end{align*}
$${#eq-proof-poisson-variance-1}

Now, we need to play around with the below `r colourize("expected value", "blue")` expression as follows:

$$
\begin{align*}
\mathbb{E}(Y^2) &= \mathbb{E} \left[ Y (Y - 1) \right] + \mathbb{E}(Y) \\
&= \mathbb{E} \left[ Y (Y - 1) \right] + \lambda \qquad \text{since $\mathbb{E}(Y) = \lambda$.}
\end{align*}
$${#eq-proof-poisson-variance-2}

Now, to find $\mathbb{E} \left[ Y (Y - 1) \right]$, we make the following derivation via @eq-app-expected-value-discrete-function when $g(Y) = y (y - 1)$:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= \sum_{y = 0}^{\infty} y (y - 1) P \left( Y = y \mid \lambda \right) \\
&= \sum_{y = 2}^{\infty} y (y - 1) P \left( Y = y \mid \lambda \right) \\
& \quad \qquad \text{for $y = \{0, 1\}$,} \\ 
& \quad \qquad \text{the addends are equal to zero} \\
&= \sum_{y = 2}^{\infty} y (y - 1) \left[ \frac{\lambda^y \exp{(-\lambda)}}{y!} \right] \\
&= \exp{(-\lambda)} \sum_{y = 2}^{\infty} \left[ \frac{y (y - 1) \lambda^y}{y!} \right] \\
& \quad \qquad \text{factoring out $\exp{(-\lambda)}$,} \\
& \quad \qquad \text{since it does not depend on $y$} \\
&= \exp{(-\lambda)} \sum_{y = 2}^{\infty} \left[ \frac{y (y - 1) \lambda^y}{y (y - 1) (y - 2)!} \right] \\
& \quad \qquad \text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\
&= \exp{(-\lambda)} \sum_{y = 2}^{\infty} \frac{\lambda^y}{(y - 2)!} \\
&= \exp{(-\lambda)} \sum_{y = 2}^{\infty} \frac{\lambda^{y + 2 - 2}}{(y - 2)!} \\
& \quad \qquad \text{note $\lambda^y = \lambda^{y + 2 - 2} $} \\
&= \lambda^2 \exp{(-\lambda)} \sum_{y = 2}^{\infty} \frac{\lambda^{y - 2}}{(y - 2)!} \\
& \quad \qquad \text{factoring out $\lambda^2$,} \\
& \quad \qquad \text{since it does not depend on $y$.} \\
\end{align*}
$${#eq-proof-poisson-variance-3}

Then, we make the following variable rearrangement:

$$
z = y - 2.
$$

Going back to @eq-proof-poisson-variance-3, **and applying our above variable rearrangement within the summation**, we have:

$$
\mathbb{E} \left[ Y (Y - 1) \right] = \lambda^2 \exp{(-\lambda)} \sum_{z = 0}^{\infty} \frac{\lambda^z}{z!}.
$${#eq-proof-poisson-variance-4}

Using @eq-maclaurin-series-base-e, let:

$$
\begin{gather*}
\lambda = u \\
z = j.
\end{gather*}
$$

Thus, we have the following:

$$
\sum_{z = 0}^{\infty} \frac{\lambda^z}{z!} = \exp{(\lambda)}.
$$

Going back to @eq-proof-poisson-variance-4:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= \lambda^2 \exp{(-\lambda)} \overbrace{\sum_{z = 0}^{\infty} \frac{\lambda^z}{z!}}^{\exp{(\lambda)}} \\
&= \lambda^2 \exp{(-\lambda)} \times \exp{\lambda} \\
&= \lambda^2 \exp{(-\lambda + \lambda)} \\
&= \lambda^2 \exp{(0)} \\
&= \lambda^2.
\end{align*}
$${#eq-proof-poisson-variance-5}

Let us retake @eq-proof-poisson-variance-2 and plug in the above result:

$$
\begin{align*}
\mathbb{E}(Y^2) &= \mathbb{E} \left[ Y (Y - 1) \right] + \lambda \\
&= \lambda^2 + \lambda. \\
\end{align*}
$$

Finally, we plug in $\mathbb{E}(Y^2)$ in @eq-proof-poisson-variance-1:

$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E}(Y^2) - \lambda^2 \\
&= \lambda^2 + \lambda - \lambda^2 \\
&= \lambda. \qquad \qquad \square
\end{align*}
$$
:::

## Generalized Poisson

The generalized Poisson (GP) distribution is viewed as the general Poisson case. It was introduced by @consul1973. Suppose you observe the count of events happening in a fixed interval of time or space. Let $Y$ be the number of counts considered of integer type. Then, $Y$ is said to have a GP distribution with `r colourize("continuous parameters", "blue")` $\lambda$ and $\theta$:

$$
Y \sim \text{GP}(\lambda, \theta).
$$

### Probability Mass Function

The `r colourize("PMF", "blue")` of this count-type $Y$ is the following:

$$
\begin{align*}
P \left( Y = y \mid \lambda, \theta \right) &= \frac{\lambda (\lambda + y \theta)^{y - 1} \exp{\left[ -(\lambda + y \theta) \right]}}{y!} \\
& \qquad \qquad \qquad \text{for $y \in \{ 0, 1, 2, \dots\}$,}
\end{align*}
$${#eq-app-generalized-poisson-pmf}

where $\exp{(\cdot)}$ depicts the base $e$ (i.e., **Euler's number**, $e = 2.71828...$) and $y!$ is the factorial

$$
y! = y \times (y - 1) \times (y - 2) \times (y - 3) \times \cdots \times 3 \times 2 \times 1.  
$$

with 

$$
0! = 1.
$$

The `r colourize("continuous parameter", "blue")` $\lambda \in (0, \infty)$ represents the average rate at which these events happen (i.e., events per area unit or events per time unit). As in the case of the classical Poisson case, even though the GP `r colourize("random variable", "blue")` $Y$ is considered `r colourize("discrete", "blue")`, $\lambda$ is modelled as `r colourize("continuous", "blue")`! 

On the other hand, the `r colourize("continuous", "blue")` and bounded `r colourize("parameter", "blue")` $\theta \in (-1, 1)$ controls for `r colourize("dispersion", "blue")` present in the GP `r colourize("random variable", "blue")` Y as follows:

1. When $0 < \theta < 1$, the GP $Y$ shows `r colourize("overdispersion", "blue")` which implies that $$\text{Var}(Y) > \mathbb{E}(Y).$$

2. When $-1 < \theta < 0$, the GP $Y$ shows `r colourize("underdispersion", "blue")` which implies that 
$$\text{Var}(Y) < \mathbb{E}(Y).$$

3. When $\theta = 0$, the `r colourize("PMF", "blue")` of the GP $Y$ in @eq-app-generalized-poisson-pmf becomes the classical Poisson `r colourize("PMF", "blue")` from @eq-app-classical-poisson-pmf`:
$$
\begin{align*}
P \left( Y = y \mid \lambda, \theta = 0 \right) &= \frac{\lambda (\lambda + y \theta)^{y - 1} \exp{\left[ -(\lambda + y \theta) \right]}}{y!} \\
&= \frac{\lambda (\lambda)^{y - 1} \exp{\left( -\lambda \right)}}{y!} \qquad \text{setting $\theta = 0$} \\
&= \frac{\lambda^y \exp{\left( -\lambda \right)}}{y!}.
\end{align*}
$$


::: {#Headsup-poisson-equidispersion .Heads-up}
::::{.Heads-up-header}
Heads-up on equidispersion in a generalized Poisson random variable!
::::
::::{.Heads-up-container}
In a GP $Y$, when $\theta = 0$ in its corresponding `r colourize("PMF", "blue")`, we have `r colourize("equidispersion", "blue")` which implies
$$
\mathbb{E}(Y) = \frac{\lambda}{1 - \theta} = \lambda
$$
$$
\text{Var}(Y) = \frac{\lambda}{(1 - \theta)^2} = \lambda
$$
$$
\mathbb{E}(Y) = \text{Var}(Y).
$$
::::
:::

> **How can we verify that @eq-app-generalized-poisson-pmf is a proper `r colourize("PMF", "blue")` (i.e., all the standalone `r colourize("probabilities", "blue")` over the support of $Y$ add up to one)?**

### Expected Value

### Variance


## Zero-inflated Poisson

## Multinomial

# Continuous Random Variables

| Distribution |  Parameter(s) | Support |  Mean  |  Variance  | 
|:----:|:------:|:-----:|:-----:|:-----:|
| Exponential | $Y \sim \text{Exponential}(\lambda)$ with mean rate $\lambda >0$ <br> or $Y \sim \text{Exponential}(\beta)$ with mean wait time $\beta > 0$ | $y \geq 0$ | $\frac{1}{\lambda}$ <br> for mean rate <br> parametrization <br> or <br> $\beta$ <br> for mean <br> wait time <br> parametrization | $\frac{1}{\lambda^2}$ <br> for mean rate <br> parametrization <br> or <br> $\beta^2$ <br> for mean <br> wait time <br> parametrization | 

: Univariate continuous probability distributions for a random variable $Y$; including parameter(s), support, mean, and variance. {#tbl-distributions-continuous .striped .hover}

## Exponential
