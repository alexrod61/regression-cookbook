<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7PRVEBE1EF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7PRVEBE1EF');
</script>

# The Chocolified Distributional Mind Map {#sec-distributional-mind-map}

```{r}
#| include: false

library(tidyverse)

colourize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

::: {.Warning}
::::{.Warning-header}
Fun fact!
::::
::::{.Warning-container}
**Chocolified!** Every inch of it is chocolate, probably topped with more chocolate.
::::
:::

A crucial part of the practice of `r colourize("regression analysis", "blue")` is a fair knowledge of all the different `r colourize("probability distributions", "blue")` that would allow us to identify **the most suitable regression model**. Let us delve into the distributional toolbox to be used in this book.

@fig-app-distributions shows all those `r colourize("probability distributions", "blue")` **depicted as clouds**, in the form of a **univariate** `r colourize("random variable", "blue")` $Y$, used to model our `r colourize("outcomes", "magenta")` of interest in the regression tools explored in each of the core thirteen chapters, i.e., we take a `r colourize("generative modelling", "blue")` approach. Note this mind map splits the `r colourize("outcomes", "magenta")` of interest into two large zones: `r colourize("discrete", "blue")` and `r colourize("continuous", "blue")`. Furthermore, this mind map briefly describes a given `r colourize("random variable", "blue")` $Y$ as a quick cheat sheet regarding its **support** (e.g., nonnegative, bounded, unbounded, etc.) or **distributional definition** (e.g., success or failure, successes in $n$ trials, etc.).

::: {#fig-app-distributions}
```{mermaid}
mindmap
  root((Univariate 
    Random
    Variable Y))
    Continuous
      {{Unbounded}}
        )Normal(
        )Logistic(
      {{Nonnegative Y}}
        )Lognormal(
        )Exponential(
        )Gamma(
        )Weibull(
      {{Y is between <br/>0 and 1}}
        )Beta(
    Discrete
      Binary
        {{Y is a success or <br/>failure}}
          )Bernoulli(
      Count
        {{Y succeses in <br/>n trials}}
          )Binomial(
        {{Y failures <br/>before experiencing <br/>k successes}}
          )Negative Binomial(
        {{Y events in <br/>a fixed interval <br/>of time or space}}
          )Classical <br/>Poisson(
          )Generalized <br/>Poisson(
          )Zero Inflated <br/>Poisson(
      Categorical
        {{Y successes of a given category, <br/>among a set of k categories, <br/>in n trials}}
          )Multinomial(
```

Probability distribution mind map depicting all univariate random variables to be used as outcomes of interest $Y$ in the modelling techniques to be explored in this book. These outcomes are split into two large zones: *discrete* and *continuous*.
:::

Since a given `r colourize("random variable", "blue")` (e.g., the `r colourize("outcome", "magenta")` $Y$ in any of the thirteen regression models in this book) will have a `r colourize("probability distribution", "blue")` associated with it, which will define the `r colourize("probability", "blue")` arrangement of each possible value or category $Y$ could take on, we also need a way to summarize all this information via key estimated metrics called `r colourize("measures of central tendency and uncertainty", "blue")`:

- **Measure of Central Tendency:** This metric refers to a **central or typical value** that a given `r colourize("random variable", "blue")` might take when we observe various realizations of this variable over a long period. 
- **Measure of Uncertainty:** This metric pertains to the **spread** of a `r colourize("random variable", "blue")` when we observe its different realizations in the long term. As a side note, a larger spread indicates more variability in these realizations.

![Image by [*Tobias Frick*](https://pixabay.com/users/pixounaut-2729335/) via [*Pixabay*](https://pixabay.com/photos/game-dice-board-game-craps-game-4695864/).](img/dice.jpg){width="400"} 

These metrics allow us to clearly communicate how the outcome $Y$ behaves in our case study, and this is heavily related to the **storytelling** stage from the **data science workflow**, as explained in @sec-ds-workflow-storytelling. More specifically, the `r colourize("measures of central tendency", "blue")` can be communicated along with our estimated regression `r colourize("parameters", "blue")`, given that these metrics are usually conditioned to our `r colourize("regressors", "blue")` of interest within our modelling framework.

::: {.Heads-up}
::::{.Heads-up-header}
Heads-up on parameter estimation!
::::
::::{.Heads-up-container}
Just as in the case of regression `r colourize("parameters", "blue")`, the `r colourize("measures of central tendency and uncertainty", "blue")` are also `r colourize("parameters", "blue")` (more specifically, belonging to a given `r colourize("probability distribution", "blue")`) that can be estimated via an observed `r colourize("random sample", "blue")` through methods such as `r colourize("maximum likelihood estimation (MLE)", "blue")`. You can check further details on the `r colourize("MLE", "blue")` fundamentals in @sec-mle.
::::
:::

There are different `r colourize("measures of central tendency and uncertainty", "blue")`. Nevertheless, we will only focus on the `r colourize("expected value", "blue")` and the `r colourize("variance", "blue")`. Now, suppose $Y$ is a `r colourize("random variable", "blue")` whose support is $\mathcal{Y}$. Furthermore, let $g(Y)$ be a general function of $Y$. 

By the **law of the unconscious statistician (LOTUS)**, when $Y$ is a `r colourize("discrete random variable", "blue")`, we have that:

$$
\mathbb{E} \left[ g(Y) \right] = \displaystyle \sum_{y \in \mathcal{Y}} g(Y) \cdot P_Y(Y = y),
$${#eq-app-expected-value-discrete-function}

where $P_Y(Y = y)$ is the `r colourize("probability mass function (PMF)", "blue")` of $Y$.

If $Y$ is a `r colourize("continuous random variable", "blue")`, by the **LOTUS**, the `r colourize("mean", "blue")` of function $g(Y)$ is defined as

$$
\mathbb{E} \left[ g(Y) \right] = \displaystyle \int_{\mathcal{Y}} g(Y) \cdot f_Y(y) \text{d}y,
$${#eq-app-expected-value-continuous-function}

where $f_Y(y)$ is the `r colourize("probability density function (PDF)", "blue")` of $Y$.

Note that when $g(Y) = y$ in the `r colourize("discrete", "blue")` case, @eq-app-expected-value-discrete-function becomes

$$
\mathbb{E} \left( Y \right) = \displaystyle \sum_{y \in \mathcal{Y}} y \cdot P_Y(Y = y).
$${#eq-app-expected-value-discrete}

On the other hand, when $g(Y) = y$ in the `r colourize("continuous", "blue")` case, @eq-app-expected-value-continuous-function becomes

$$
\mathbb{E} \left( Y \right) = \displaystyle \int_{\mathcal{Y}} y \cdot f_Y(y) \mathrm{d}y.
$${#eq-app-expected-value-continuous}

Either for a `r colourize("discrete", "blue")` or `r colourize("continuous", "blue")` case, the `r colourize("variance", "blue")` is defined as 

$$
\text{Var}(Y) = \mathbb{E}\{[Y - \mathbb{E}(Y)]^2\}.
$$

After applying some algebraic rearrangements and `r colourize("expected value", "blue")` properties, the expression above is equivalent to:

$$
\text{Var}(Y) = \mathbb{E} \left( Y^2 \right) - [\mathbb{E}(Y)]^2.
$${#eq-app-variance}

where $\mathbb{E} \left( Y^2 \right)$ can be computed either via @eq-app-expected-value-discrete-function or @eq-app-expected-value-continuous-function depending on the nature of $Y$ with $g(Y) = y^2$.

Now, for each case depicted as a cloud in @fig-app-distributions, subsequent sections in this appendix will show elaborate on why each corresponding `r colourize("PMF", "blue")` or `r colourize("PDF", "blue")` (depending on the type of `r colourize("random variable", "blue")`, $Y$) is a proper probability distribution (i.e., all the standalone `r colourize("probabilities", "blue")` over the support of $Y$ add up to one) along with the respective proofs of their corresponding `r colourize("means", "blue")` and `r colourize("variances", "blue")`.

# Discrete Random Variables

Let us recall what a `r colourize("discrete random variable", "blue")` is. This type of variable is defined to take on a set of countable possible values. In other words, these values belong to a finite set. @fig-app-distributions delves into the following specific `r colourize("probability distributions", "blue")`:

- **Bernoulli.** A `r colourize("random variable", "blue")` $Y$ that can take on the values of $0$ (i.e., a failure) or $1$ (i.e., a success) where the distributional `r colourize("parameter", "blue")` is the `r colourize("probability", "blue")` of success $\pi \in [0, 1]$. Note $Y$ is said to be **binary** with a support of $y \in \{ 0, 1 \}$.
- **Binomial.** A `r colourize("random variable", "blue")` $Y$ that defines the number of `r colourize("independent", "blue")` **Bernoulli trials** in which we observe a success out of $n$ trials. Its distributional `r colourize("parameters", "blue")` are the `r colourize("probability", "blue")` of success $\pi \in [0, 1]$of each Bernoulli trial along with the total number of trials $n \in \mathbb{N}$. Note $Y$ is said to be of **count** type with a support of $y \in \{ 0, 1, \dots, n \}$ successes.
- **Negative Binomial.** A `r colourize("random variable", "blue")` $Y$ that defines the number of `r colourize("independent", "blue")` **Bernoulli trials** in which we observe a failure before experiencing $k$ successes. Its distributional `r colourize("parameters", "blue")` are the `r colourize("probability", "blue")` of success $\pi \in [0, 1]$ of each Bernoulli trial along with the number of $k \in \{ 0, 1, 2 \dots\}$ successes. Note $Y$ is said to be of **count** type with a support of $y \in \{ 0, 1, 2 \dots\}$ failures.
- **Classical Poisson.** A `r colourize("random variable", "blue")` $Y$ that defines the number of events occurring in a predetermined interval of time or space. Its distributional `r colourize("parameter", "blue")` is the `r colourize("average", "blue")` rate $\lambda \in (0, \infty)$ of events per this predetermined interval of time or space. Note $Y$ is said to be of **count** type with a support of $y \in \{ 0, 1, 2, \dots \}$ events.
- **Generalized Poisson.** As in the above **classical Poisson** case, it is `r colourize("random variable", "blue")` $Y$ that defines the number of events occurring in a predetermined interval of time or space. It has two distributional `r colourize("parameters", "blue")`: the `r colourize("average", "blue")` rate $\lambda \in (0, \infty)$ of events per this predetermined interval of time or space, and a `r colourize("dispersion parameter", "blue")` $\theta \in (-1, 1)$ that models the `r colourize("random variable's", "blue")` degree of `r colourize("underdispersion", "blue")` (when $-1 < \theta < 0$) and `r colourize("overdispersion", "blue")` (when $0 < \theta < 1$). Note $Y$ is said to be of **count** type with a support of $y \in \{ 0, 1, 2, \dots \}$ events.

@tbl-distributions-discrete outlines the `r colourize("parameter(s)", "blue")`, support, `r colourize("mean", "blue")`, and `r colourize("variance", "blue")` for each `r colourize("discrete probability distribution", "blue")` utilized to model the `r colourize("target", "magenta")` $Y$ in a specific regression tool explained in this book.

|  Distribution and <br> Parametrization | Support |  Mean  |  Variance  | 
|:------:|:-----:|:-----:|:-----:|
| **Bernoulli** as in <br> $Y \sim \text{Bern}(\pi)$ with probability of success $\pi \in [0, 1]$ | $$y \in \{ 0, 1 \}$$ | $$\pi$$ | $$\pi (1 - \pi)$$ | 
| **Binomial** as in <br> $Y \sim \text{Bin}(n, \pi)$ with number of trials $n \in \mathbb{N}$ and probability of success $\pi \in [0, 1]$ | $$y \in \{ 0, 1, \dots, n \}$$ | $$n \pi$$ | $$n \pi (1 - \pi)$$ |
| **Negative Binomial** as in <br> $Y \sim \text{NegBin}(k, \pi)$ with number of successes $k \in \{ 0, 1, 2 \dots\}$ and probability of success $\pi \in [0, 1]$| $$y \in \{ 0, 1, 2 \dots\}$$ | $$\frac{k (1 - \pi)}{\pi}$$ | $$\frac{k (1 - \pi)}{\pi^2}$$ |
| **Poisson** as in <br> $Y \sim \text{Pois}(\lambda)$ with continuous average rate $\lambda \in (0, \infty)$ | $$y \in \{ 0, 1, 2, \dots\}$$ | $$\lambda$$ | $$\lambda$$ |
| **Genealized Poisson** as in <br> $Y \sim \text{GP}(\lambda, \theta)$ with continuous average rate $\lambda \in (0, \infty)$ and dispersion parameter $\theta \in (-1, 1)$ | $$y \in \{ 0, 1, 2, \dots\}$$ | $$\frac{\lambda}{1 - \theta}$$ | $$\frac{\lambda}{(1 - \theta)^2}$$ | 

: Univariate discrete probability distributions for a random variable $Y$; including parameter(s), support, mean, and variance. {#tbl-distributions-discrete .striped .hover}

## Bernoulli {#sec-bernoulli-distribution}

Let $Y$ be a `r colourize("discrete random variable", "blue")` that is part of a random process or system. $Y$ can only take on the following values:

$$
Y =
\begin{cases}
1 \; \; \; \; \text{if there is a success},\\
0 \; \; \; \; \mbox{otherwise}.
\end{cases}
$${#eq-app-bernoulli-support}

Note that the support of $Y$ in @eq-app-bernoulli-support makes it binary with these outcomes: $1$ for *success* and $0$ for *failure*.  Then, $Y$ is said to have a **Bernoulli distribution** with `r colourize("parameter", "blue")` $\pi$:

$$
Y \sim \text{Bern}(\pi).
$$

### Probability Mass Function

The `r colourize("PMF", "blue")` of $Y$ is the following:

$$
P_Y \left( Y = y \mid \pi \right) = \pi^y (1 - \pi)^{1 - y} \quad \text{for $y \in \{ 0, 1 \}$.}
$${#eq-app-bernoulli-pmf}

`r colourize("Parameter", "blue")` $\pi \in [0, 1]$ refers to the `r colourize("probability", "blue")` of success. We can verify @eq-app-bernoulli-pmf is a proper `r colourize("probability distribution", "blue")` (i.e., all the standalone `r colourize("probabilities", "blue")` over the support of $Y$ add up to one) given that:

::: {.proof}
$$
\begin{align*} 
\sum_{y = 0}^1 P_Y \left( Y = y \mid \pi \right) &=  \sum_{y = 0}^1 \pi^y (1 - \pi)^{1 - y}  \\
&= \underbrace{\pi^0}_{1} (1 - \pi) + \pi \underbrace{(1 - \pi)^{0}}_{1} \\
&= (1 - \pi) + \pi \\
&= 1. \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$

> **Indeed, the Bernoulli `r colourize("PMF", "blue")` is a proper `r colourize("probability distribution", "blue")`!**
:::

### Expected Value

Via @eq-app-expected-value-discrete, the `r colourize("expected value", "blue")` or `r colourize("mean", "blue")` of a Bernoulli-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*} 
\mathbb{E}(Y) &= \sum_{y = 0}^1 y P_Y \left( Y = y \mid \pi \right) \\
&= \sum_{y = 0}^1 y \left[ \pi^y (1 - \pi)^{1 - y} \right] \\
&= \underbrace{(0) \left[ \pi^0 (1 - \pi) \right]}_{0} + (1) \left[ \pi (1 - \pi)^{0} \right] \\
&= 0 + \pi \\
&= \pi. \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$
:::

### Variance

Via @eq-app-variance and the @eq-app-expected-value-discrete of a `r colourize("discrete expected value", "blue")`, the `r colourize("variance", "blue")` of a Bernoulli-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E} \left( Y^2 \right) - \left[ \mathbb{E}(Y)\right]^2 \\
&= \mathbb{E} \left( Y^2 \right) - \pi^2 \qquad \text{since $\mathbb{E}(Y) = \pi$} \\
&= \sum_{y = 0}^1 y^2 P_Y \left( Y = y \mid \pi \right) - \pi^2 \\
&= \left\{ \underbrace{(0^2) \left[ \pi^0 (1 - \pi) \right]}_{0} + \underbrace{(1^2) \left[ \pi (1 - \pi)^{0} \right]}_{\pi} \right\} - \pi^2 \\
&= (0 + \pi) - \pi^2 \\
&= \pi - \pi^2 \\
&= \pi (1 - \pi). \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$
:::

## Binomial {#sec-binomial-distribution}

Suppose you execute $n$ `r colourize("independent", "blue")` **Bernoulli trials**, each one with a `r colourize("probability", "blue")` of success $\pi$. Let $Y$ be the number of successes obtained within these $n$ Bernoulli trials. Then, $Y$ is said to have a **Binomial distribution** with `r colourize("parameters", "blue")` $n$ and $\pi$:

$$
Y \sim \text{Bin}(n, \pi).
$$

### Probability Mass Function

The `r colourize("PMF", "blue")` of $Y$ is the following:

$$
\begin{align*}
P_Y \left( Y = y \mid n, \pi \right) &= {n \choose y} \pi^y (1 - \pi)^{n - y} \\
& \qquad \qquad \qquad \text{for $y \in \{ 0, 1, \dots, n \}$.}
\end{align*}
$${#eq-app-binomial-pmf}

`r colourize("Parameter", "blue")` $\pi \in [0, 1]$ refers to the `r colourize("probability", "blue")` of success of each Bernoulli trial and $n \in \mathbb{N}$ to the number of trials. On the other hand, the term ${n \choose y}$ indicates the total number of possible combinations for $y$ successes out of our $n$ trials:

$$
{n \choose y} = \frac{n!}{y!(n - y)!}.
$${#eq-app-combination}

> **How can we verify that @eq-app-binomial-pmf is a proper `r colourize("PMF", "blue")` (i.e., all the standalone `r colourize("probabilities", "blue")` over the support of $Y$ add up to one)?**

To elaborate on this, we need to use a handy mathematical result called the **binomial theorem**.

::: {#thm-binomial-theorem}
# Binomial Theorem

This theorem is associated to the **Pascal's identity**, and it defines the pattern of coefficients in the expansion of a polynomial in the form $(u + v)^m$. More specifically, the binomial theorem indicates that if $m$ is a non-negative integer, then the polynomial $(u + v)^m$ can be expanded via the following series:

$$
\begin{align*}
(u + v)^m &= u^m + {m \choose 1} u^{m - 1} v + {m \choose 2} u^{m - 2} v^2 + \dots + \\
& \qquad {m \choose r} u^{m - r} v^r + \dots + \\
& \qquad {m \choose m - 1} u v^{m - 1} + v^m \\
&= \underbrace{{m \choose 0}}_1 u^m + {m \choose 1} u^{m - 1} v + {m \choose 2} u^{m - 2} v^2 + \dots + \\
& \qquad {m \choose r} u^{m - r} v^r + \dots + \\
& \qquad {m \choose m - 1} u v^{m - 1} + \underbrace{{m \choose m}}_1 v^m \\
&= \sum_{i = 0}^m {m \choose i} u^{m - i} v^i.
\end{align*}
$${#eq-app-binomial-theorem}
:::

::: {.Tip}
::::{.Tip-header}
Tip on the binomial theorem and Pascal's identity
::::
::::{.Tip-container}
Let us dig into the proof of the binomial theorem from @eq-app-binomial-theorem. This proof will require another important result called the **Pascal's identity**. This identity states that for any integers $m$ and $k$, with $k \in \{ 1, \dots, m \}$, it follows that:

::: {.proof}
$$
\begin{align*}
{m \choose k - 1} + {m \choose k} &= \left[ \frac{m!}{(k - 1)! (m - k + 1)!} \right] \\ 
& \qquad + \left[ \frac{m!}{k! (m - k)!} \right] \\
&= m! \biggl\{ \left[ \frac{1}{(k - 1)! (m - k + 1)!} \right] + \\
& \qquad \left[ \frac{1}{k! (m - k)!} \right] \biggl\} \\
&= m! \Biggl\{ \Biggr[ \frac{k}{\underbrace{k (k - 1)!}_{k!} (m - k + 1)!} \Biggr] + \\
& \qquad \Biggr[ \frac{m - k + 1}{k! \underbrace{(m - k + 1)(m - k)!}_{(m - k + 1)!}} \Biggr] \Biggl\}  \\
&= m! \left[ \frac{k + m - k + 1}{k! (m - k + 1)!} \right] \\
&= m! \left[ \frac{m + 1}{k! (m - k + 1)!} \right] \\
&= \frac{(m + 1)!}{k! (m + 1 - k)!} \\
&= {m + 1 \choose k }. \qquad \qquad \qquad \qquad \square
\end{align*}
$${#eq-pascal-dentity}
:::

::: {.proof}
Now, we will use **mathematical induction** to prove the binomial theorem from @eq-app-binomial-theorem. Firstly, on the left-hand side of the theorem, note that when $m = 0$ we have:\

$$
(u + v)^0 = 1.
$$

Now, when $m = 0$, for the right-hand side of this equation, we have that\

$$
\sum_{i = 0}^m {m \choose i} u^{m - i} v^i  = \sum_{i = 0}^0 {0 \choose i} u^i v^{i} = {0 \choose 0} u^0 v^0 = 1.
$$

Hence, the binomial theorem holds when $m = 0$. This is what we call the **base case** in mathematical induction.\

That said, let us proceed with the **inductive hypothesis**. We aim to prove that the binomial theorem\

$$
\begin{align*}
(u + v)^j &= u^j + {j \choose 1} u^{j - 1} v + {j \choose 2} u^{j - 2} v^2 + \dots + \\
& \qquad {j \choose r} u^{j - r} v^r + \dots + \\
& \qquad {j \choose j - 1} u v^{j - 1} + v^j \\
&= \underbrace{{j \choose 0}}_1 u^j + {j \choose 1} u^{j - 1} v + {j \choose 2} u^{j - 2} v^2 + \dots + \\
& \qquad {j \choose r} u^{j - r} v^r + \dots + \\
& \qquad {j \choose j - 1} u v^{j - 1} + \underbrace{{j \choose j}}_1 v^j \\
&= \sum_{i = 0}^j {j \choose i} u^{j - i} v^i
\end{align*}
$${#eq-inductive-hyp}

holds when integer $j \geq 1$. This is our inductive hypothesis.

Then, we pave the way to the **inductive step**. Let us consider the following expansion:\

$$
\begin{align*}
(u + v)^{j + 1} &= (u + v) (u + v)^j \\
&= (u + v) \times \\
& \qquad \bigg[ u^j + {j \choose 1} u^{j - 1} v + {j \choose 2} u^{j - 2} v^2 + \dots + \\
& \qquad {j \choose r} u^{j - r} v^r + \dots + {j \choose j - 1} u v^{j - 1} + v^j \bigg] \\
&= \bigg[u^{j + 1} + {j \choose 1} u^j v + {j \choose 2} u^{j - 1} v^2 + \dots + \\
& \qquad {j \choose r} u^{j - r + 1} v^r + \dots + \\
& \qquad {j \choose j - 1} u^2 v^{j - 1} + u v^j \bigg] + \\
& \qquad \bigg[ u^j v + {j \choose 1} u^{j - 1} v^2 + {j \choose 2} u^{j - 2} v^3 + \dots + \\
& \qquad {j \choose r} u^{j - r} v^{r + 1} + \dots + \\
& \qquad {j \choose j - 1} u v^j + {j \choose j} v^{j + 1} \bigg] \\
&= u^{j + 1} + \left[ {j \choose 0} + {j \choose 1} \right] u^j v + \\
& \qquad \left[ {j \choose 1} + {j \choose 2} \right] u^{j - 1} v^2 + \dots + \\
& \qquad \left[ {j \choose r - 1} + {j \choose r} \right] u^{j - r + 1} v^r + \dots + \\
& \qquad \left[ {j \choose j - 1} + {j \choose j} \right] u v^j + v^{j + 1}.
\end{align*} 
$${#eq-binomial-inductive-step}

Let us plug in the Pascal's identity from @eq-pascal-dentity into @eq-binomial-inductive-step:\

$$
\begin{align*}
(u + v)^{j + 1} &= u^{j + 1} + {j + 1 \choose 1} u^j v + \\
& \qquad {j + 1 \choose 2} u^{j - 1} v^2 + \dots + \\
& \qquad {j + 1 \choose r} u^{j - r + 1} v^r + \dots + \\
& \qquad {j + 1 \choose j} u v^j + v^{j + 1} \\
&= \underbrace{{j + 1 \choose 0}}_1 u^{j + 1} + {j + 1 \choose 1} u^j v + \\
& \qquad {j + 1 \choose 2} u^{j - 1} v^2 + \dots + \\ 
& \qquad {j + 1 \choose r} u^{j - r + 1} v^r + \dots + \\
& \qquad {j + 1 \choose j} u v^j + \underbrace{{j + 1 \choose j + 1}}_1 v^{j + 1} \\
&= \sum_{i = 0}^{j + 1} {j + 1 \choose i} u^{j + 1 - i} v^i. \qquad \quad \square
\end{align*}
$${#eq-proof-inductive-hyp}

Note that the result for $j$ in @eq-inductive-hyp also holds for $j + 1$ in @eq-proof-inductive-hyp. Therefore, by induction, the binomial theorem from @eq-app-binomial-theorem is true for all positive integers $m$.
:::

::::
:::

After the above fruitful digression on the binomial theorem, let us use it to show that our Binomial `r colourize("PMF", "blue")` in @eq-app-binomial-pmf actually adds up to one all over the support of the `r colourize("random variable", "blue")`:

::: {.proof}
$$
\begin{align*} 
\sum_{y = 0}^n P_Y \left( Y = y \mid n, \pi \right) &= \sum_{y = 0}^n {n \choose y} \pi^y (1 - \pi)^{n - y} \\
&= \sum_{y = 0}^n {n \choose y} (1 - \pi)^{n - y} \pi^y \\
& \quad \qquad \text{rearranging factors.}
\end{align*}
$$

Now, by using the binomial theorem in @eq-app-binomial-theorem, let:

$$
\begin{gather*}
m  = n\\
i = y \\
u = 1 - \pi \\
v = \pi.
\end{gather*}
$$

The above arrangement yields the following result:

$$
\begin{align*}
\sum_{y = 0}^n P_Y \left( Y = y \mid n, \pi \right) &= (1 - \pi + \pi)^n \\
&= 1^n = 1. \qquad \square
\end{align*}
$${#eq-proof-binomial-PMF-adds-to-1}

> **Indeed, the Binomial `r colourize("PMF", "blue")` is a proper `r colourize("probability distribution", "blue")`!**
:::

### Expected Value

Via @eq-app-expected-value-discrete, the `r colourize("expected value", "blue")` or `r colourize("mean", "blue")` of a Binomial-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*} 
\mathbb{E}(Y) &= \sum_{y = 0}^n y P_Y \left( Y = y \mid n, \pi \right) \\
&= \sum_{y = 1}^n y P_Y \left( Y = y \mid n, \pi \right) \\
& \quad \qquad \text{for $y = 0$, the addend is equal to zero} \\
&= \sum_{y = 1}^n y \left[ {n \choose y} \pi^y (1 - \pi)^{n - y} \right] \\
&= \sum_{y = 1}^n y \left[ \frac{n!}{y! (n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
&= \sum_{y = 1}^n \left[ \frac{y n!}{y (y - 1)!(n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
& \quad \qquad \text{in the denominator, $y! = y (y - 1)!$}\\
&= \sum_{y = 1}^n \left[ \frac{n (n - 1)!}{(y - 1)!(n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\ 
& \quad \qquad \text{in the numerator, $n! = n (n - 1)!$} \\
&= \sum_{y = 1}^n \left[ \frac{n (n - 1)!}{(y - 1)!(n - y)!} \pi^{y + 1 - 1} (1 - \pi)^{n - y} \right] \\
& \quad \qquad \text{note $\pi^y = \pi^{y + 1 - 1}$} \\
&= n \sum_{y = 1}^n \left[ \frac{(n - 1)!}{(y - 1)!(n - y)!} \pi \pi^{y - 1} (1 - \pi)^{n - y} \right] \\
& \quad \qquad \text{rearranging terms} \\
&= n \pi \sum_{y = 1}^n \left[ \frac{(n - 1)!}{(y - 1)!(n - y)!} \pi^{y - 1} (1 - \pi)^{n - y} \right].
\end{align*}
$${#eq-proof-binomial-mean-1}

Now, let us make the following variable rearrangement:

$$
\begin{gather*}
m = n - 1 \\
z = y - 1 \\
m - z = n - y.
\end{gather*}
$$

Going back to @eq-proof-binomial-mean-1, **and applying our above variable rearrangement within the summation**, we have:

$$
\begin{align*} 
\mathbb{E}(Y) &= n \pi \sum_{z = 0}^m \left[ \frac{m!}{z!(m - z)!} \pi^{z} (1 - \pi)^{m - z} \right] \\
&= n \pi \sum_{z = 0}^m \left[ {m \choose z}\pi^{z} (1 - \pi)^{m - z} \right].
\end{align*}
$${#eq-proof-binomial-mean-2}

Note that, in the summation of @eq-proof-binomial-mean-2, we encounter the `r colourize("PMF", "blue")` of a `r colourize("random variable", "blue")` $Z$ as follows:

$$
Z \sim \text{Bin}(m, \pi).
$$

Since the summation, where this Binomial `r colourize("PMF", "blue")` of $Z$ is depicted, goes from $z = 0$ to $m$, we can apply our result from @eq-proof-binomial-PMF-adds-to-1:

$$
\begin{align*} 
\mathbb{E}(Y) &= n \pi \underbrace{\sum_{z = 0}^m \left[ {m \choose z}\pi^{z} (1 - \pi)^{m - z} \right]}_{1} \\
&= n \pi. \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$
:::

### Variance

Via @eq-app-variance and the @eq-app-expected-value-discrete of a `r colourize("discrete expected value", "blue")`, the `r colourize("variance", "blue")` of a Binomial-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E} \left( Y^2 \right) - \left[ \mathbb{E}(Y)\right]^2 \\
&= \mathbb{E} \left( Y^2 \right) - (n \pi)^2 \qquad \text{since $\mathbb{E}(Y) = n \pi$.}
\end{align*}
$${#eq-proof-binomial-variance-1}

Unlike the Bernoulli `r colourize("random variable", "blue")`, finding $\mathbb{E} \left( Y^2 \right)$ is not quite straightforward. We need to play around with the below `r colourize("expected value", "blue")` expression as follows:

$$
\begin{align*}
\mathbb{E} \left( Y^2 \right) &= \mathbb{E} \left[ Y (Y - 1) \right] + \mathbb{E}(Y) \\
&= \mathbb{E} \left[ Y (Y - 1) \right] + n \pi \qquad \text{since $\mathbb{E}(Y) = n \pi$.}
\end{align*}
$${#eq-proof-binomial-variance-2}

Now, to find $\mathbb{E} \left[ Y (Y - 1) \right]$, we make the following derivation via the LOTUS from @eq-app-expected-value-discrete-function when $g(Y) = y (y - 1)$:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= \sum_{y = 0}^n y (y - 1) P_Y \left( Y = y \mid n, \pi \right) \\
&= \sum_{y = 2}^n y (y - 1) P_Y \left( Y = y \mid n, \pi \right) \\
& \quad \qquad \text{for $y = \{0, 1\}$,} \\ 
& \quad \qquad \text{the addends are equal to zero} \\
&= \sum_{y = 2}^n y (y - 1) \left[ {n \choose y} \pi^y (1 - \pi)^{n - y} \right] \\
&= \sum_{y = 2}^n y (y - 1) \left[ \frac{n!}{y! (n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
&= \sum_{y = 2}^n \left[ \frac{y (y - 1) n!}{y (y - 1) (y - 2)! (n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
& \quad \qquad \text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\
&= \sum_{y = 2}^n \left[ \frac{n (n - 1) (n - 2)!}{(y - 2)! (n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\ 
& \quad \qquad \text{in the numerator, $n! = n (n - 1) (n - 2)!$} \\
&= \sum_{y = 2}^n \left[ \frac{n (n - 1) (n - 2)!}{(y - 2)! (n - y)!} \pi^{y + 2 - 2} (1 - \pi)^{n - y} \right] \\
& \quad \qquad \text{note $\pi^y = \pi^{y + 2 - 2}$} \\
&= n (n - 1) \times \\
& \qquad \sum_{y = 2}^n \left[ \frac{(n - 2)!}{(y - 2)! (n - y)!} \pi^2 \pi^{y - 2} (1 - \pi)^{n - y} \right] \\
& \qquad \qquad \text{rearranging terms} \\
&= n (n - 1) \pi^2 \times \\
& \qquad \sum_{y = 2}^n \left[ \frac{(n - 2)!}{(y - 2)! (n - y)!} \pi^{y - 2} (1 - \pi)^{n - y} \right] \\
& \qquad \qquad \text{rearranging terms.}
\end{align*}
$${#eq-proof-binomial-variance-3}

Then, we make the following variable rearrangement:

$$
\begin{gather*}
m = n - 2 \\
z = y - 2 \\
m - z = n - y.
\end{gather*}
$$

Going back to @eq-proof-binomial-variance-3, **and applying our above variable rearrangement within the summation**, we have:

$$
\begin{align*} 
\mathbb{E} \left[ Y (Y - 1) \right] &= n (n - 1) \pi^2 \sum_{z = 0}^m \left[ \frac{m!}{z! (m - z)!} \pi^{z} (1 - \pi)^{m - z} \right] \\
&= n (n - 1) \pi^2 \sum_{z = 0}^m \left[ {m \choose z} \pi^{z} (1 - \pi)^{m - z} \right].
\end{align*}
$${#eq-proof-binomial-variance-4}

Note that, in the summation of @eq-proof-binomial-variance-4, we encounter the `r colourize("PMF", "blue")` of a `r colourize("random variable", "blue")` $Z$ as follows:

$$
Z \sim \text{Bin}(m, \pi).
$$

Since the summation, where this Binomial `r colourize("PMF", "blue")` of $Z$ is depicted, goes from $z = 0$ to $m,$ we can apply our result from @eq-proof-binomial-PMF-adds-to-1:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= n (n - 1) \pi^2 \underbrace{\sum_{z = 0}^m \left[ {m \choose z} \pi^{z} (1 - \pi)^{m - z} \right]}_{1} \\
&= n (n - 1) \pi^2.
\end{align*}
$$

Let us go back to @eq-proof-binomial-variance-2 and plug in the above result:

$$
\begin{align*}
\mathbb{E} \left( Y^2 \right) &= \mathbb{E} \left[ Y (Y - 1) \right] + n \pi \\
&= n (n - 1) \pi^2 + n \pi. \\
\end{align*}
$$

Finally, we plug in $\mathbb{E} \left( Y^2 \right)$ in @eq-proof-binomial-variance-1:

$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E} \left( Y^2 \right) - (n \pi)^2 \\
&= n (n - 1) \pi^2 + n \pi - n^2 \pi^2 \\
&= n^2 \pi^2 - n \pi^2 + n \pi - n^2 \pi^2 \\
&= n \pi - n \pi^2 \\
&= n \pi (1 - \pi). \qquad \qquad \qquad \square
\end{align*}
$$
:::

## Negative Binomial {#sec-negative-binomial-distribution}

Suppose you execute a series of `r colourize("independent", "blue")` **Bernoulli trials**, each one with a `r colourize("probability", "blue")` of success $\pi$. Let $Y$ be the number of failures in this series of Bernoulli trials you obtain before experiencing $k$ successes. Therefore, $Y$ is said to have a **Negative Binomial distribution** with `r colourize("parameters", "blue")` $k$ and $\pi$:

$$
Y \sim \text{NegBin}(k, \pi).
$$

### Probability Mass Function

The `r colourize("PMF", "blue")` of $Y$ is the following:

$$
\begin{align*}
P_Y \left( Y = y \mid k, \pi \right) &= {k + y - 1 \choose y} \pi^k (1 - \pi)^y \\
& \qquad \qquad \qquad \quad \text{for $y \in \{ 0, 1, \dots \}$.}
\end{align*}
$${#eq-app-neg-binomial-pmf}

`r colourize("Parameter", "blue")` $\pi \in [0, 1]$ refers to the `r colourize("probability", "blue")` of success of each Bernoulli trial, whereas $k$ refers to the number of successes.

::: {.Tip}
::::{.Tip-header}
Tip on an alternative Negative Binomial PMF!
::::
::::{.Tip-container}
There is an alternative parametrization to define a Negative Binomial distribution in which we have a `r colourize("random variable", "blue")` $Z$ defined as the **total number of Bernoulli trials** (i.e., $k$ successes plus the $Y$ failures depicted in @eq-app-neg-binomial-pmf):

$$
Z = Y + k.
$$

This alternative parametrization of the Negative Binomial distribution yields the following `r colourize("PMF", "blue")`:

$$
\begin{align*}
P_Z \left( Z = z \mid k, \pi \right) &= {z - 1 \choose k - 1} \pi^k (1 - \pi)^{z - k} \\
& \qquad \qquad \qquad \text{for $z \in \{ k, k + 1, \dots \}$.}
\end{align*}
$$

Nevertheless, we will not dig into this version of the Negative Binomial distribution since @sec-negative-binomial delves into a modelling estimation via a joint `r colourize("PMF", "blue")` of the `r colourize("training set", "magenta")` involving @eq-app-neg-binomial-pmf.
::::
:::

> **How can we verify that @eq-app-neg-binomial-pmf is a proper `r colourize("PMF", "blue")` (i.e., all the standalone `r colourize("probabilities", "blue")` over the support of $Y$ add up to one)?**

::: {.proof}
Let us manipulate the factor involving the number of combinations corresponding to how many different possible subsets of size $y$ can be made from the larger set of size $k + y - 1$:

$$
\begin{align*}
{k + y - 1 \choose y} &= \frac{(k + y - 1)!}{(k + y - 1 - y)! y !} \\
&= \frac{(k + y - 1)!}{(k - 1)! y!} \\
&= \frac{(k + y - 1) (k + y - 2) \cdots (k + 1) (k) (k - 1)!}{(k - 1)! y!} \\
&= \frac{(\overbrace{k + y - 1) (k + y - 2) \cdots (k + 1) k}^{\text{we have $y$ factors}}}{y!} \\
&= (- 1)^y \frac{\overbrace{(-k - y + 1) (-k - y + 2) \cdots (-k - 1) (-k)}^{\text{multiplying each factor times $-1$}}}{y!} \\
&= (- 1)^y \frac{\overbrace{(-k) (-k - 1) \cdots (-k - y + 2) (-k - y + 1)}^{\text{rearranging factors}}}{y!} \\
&= (- 1)^y \frac{(-k) (-k - 1) \cdots (-k - y + 2) (-k - y + 1)}{y!} \times \\
& \qquad \frac{(-k - y) (-k - y - 1) \cdots (1)}{(-k - y) (-k - y - 1) \cdots (1)} \\
&= (- 1)^y \frac{(-k) (-k - 1) \cdots (-k - y + 2) (-k - y + 1)}{y!} \times \\
& \qquad \frac{(-k - y) (-k - y - 1) \cdots (1)}{(-k - y)!}.
\end{align*}
$$


In the equation above, note that there are still several factors in the numerator, which can be summarized using a factorial as follows:

$$
\begin{align*}
(-k)! &= (-k) (-k - 1) \cdots (-k - y + 2) (-k - y + 1) \times \\
& \quad \qquad (-k - y) (-k - y - 1) \cdots (1).
\end{align*}
$$

Therefore:

$$
\begin{align*}
{k + y - 1 \choose y} &= (- 1)^y \frac{(-k)!}{(-k - y)! y!}\\
&= (- 1)^y {-k \choose y}.
\end{align*}
$$

Now, let us begin with the summation involving the Negative Binomial `r colourize("PMF", "blue")` depicted in  @eq-app-neg-binomial-pmf from $0$ to $\infty$:

$$
\begin{align*}
\sum_{y = 0}^{\infty} P_Y \left( Y = y \mid k, \pi \right) &= \sum_{y = 0}^{\infty} {k + y - 1 \choose y} \pi^k (1 - \pi)^y \\
&= \sum_{y = 0}^{\infty} (- 1)^y {-k \choose y} \pi^k (1 - \pi)^y \\
&= \pi^k \sum_{y = 0}^{\infty} (- 1)^y {-k \choose y} (1 - \pi)^y \\
&= \pi^k \sum_{y = 0}^{\infty} {-k \choose y} (-1 + \pi)^y.
\end{align*}
$${#eq-proof-neg-binomial-PMF-adds-to-1-a}

On the right-hand side of @eq-proof-neg-binomial-PMF-adds-to-1-a we will add the following factor:

$$
(1)^{-k - y} = 1.
$$

Thus:

$$
\begin{align*}
\sum_{y = 0}^{\infty} P_Y \left( Y = y \mid k, \pi \right) &= \pi^k \sum_{y = 0}^{\infty} {-k \choose y} (1)^{-k - y} (-1 + \pi)^y.
\end{align*}
$${#eq-proof-neg-binomial-PMF-adds-to-1-b}

Now, by using the **binomial theorem** in @eq-app-binomial-theorem, let:

$$
\begin{gather*}
m  = -k\\
i = y \\
u = 1 \\
v = -1 + \pi.
\end{gather*}
$$

The above arrangement yields the following result in @eq-proof-neg-binomial-PMF-adds-to-1-b:

$$
\begin{align*}
\sum_{y = 0}^{\infty} P_Y \left( Y = y \mid k, \pi \right) &=  \pi^k (1 - 1 + \pi)^{-k} \\
&= \pi^k (\pi) ^{-k} \\
&= \pi^0 \\
&= 1. \qquad \qquad \qquad \square
\end{align*}
$${#eq-proof-neg-binomial-PMF-adds-to-1-c}

> **Indeed, the Negative Binomial `r colourize("PMF", "blue")` is a proper `r colourize("probability distribution", "blue")`!**
:::

### Expected Value

Via @eq-app-expected-value-discrete, the `r colourize("expected value", "blue")` or `r colourize("mean", "blue")` of a Negative Binomial-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*} 
\mathbb{E}(Y) &= \sum_{y = 0}^{\infty} y P_Y \left( Y = y \mid k, \pi \right) \\
&= \sum_{y = 1}^{\infty} y P_Y \left( Y = y \mid k, \pi \right) \\
& \quad \qquad \text{for $y = 0$, the addend is equal to zero} \\
&= \sum_{y = 1}^{\infty} y \left[ {k + y - 1 \choose y} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 1}^{\infty} y \left[ \frac{(k + y - 1)!}{y! (k + y - 1 - y)!} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 1}^{\infty} y \left[ \frac{(k + y - 1)!}{y! (k - 1)!} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 1}^{\infty} y \Bigg[ \frac{(k + y - 1)!}{y (y - 1)! \underbrace{\left( \frac{k!}{k} \right)}_{(k - 1)!}} \pi^k (1 - \pi)^y \Bigg] \\
&= \sum_{y = 1}^{\infty} k \left[ \frac{(k + y - 1)!}{k! (y - 1)!} \pi^k (1 - \pi)^y \right] \\
&= k \sum_{y = 1}^{\infty} \left[ {k + y - 1 \choose y - 1} \pi^k (1 - \pi)^y \right] \\
&= k \sum_{y = 1}^{\infty} \left[ {k + y - 1 \choose y - 1} \pi^{k + 1 - 1} (1 - \pi)^{y + 1 - 1} \right] \\
& \quad \qquad \text{note $\pi^k = \pi^{k + 1 - 1}$ and $(1 - \pi)^y = (1 - \pi)^{y + 1 - 1}$} \\
&= \frac{k (1 - \pi)}{\pi} \sum_{y = 1}^{\infty} \left[ {k + y - 1 \choose y - 1} \pi^{k + 1} (1 - \pi)^{y - 1} \right].
\end{align*}
$${#eq-proof-neg-binomial-mean-1}

Now, let us make the following variable rearrangement:

$$
\begin{gather*}
m = k + 1 \\
z = y - 1 \\
m + z - 1  = k + y - 1.
\end{gather*}
$$

Going back to @eq-proof-neg-binomial-mean-1, **and applying our above variable rearrangement within the summation**, we have:

$$
\mathbb{E}(Y) = \frac{k (1 - \pi)}{\pi} \sum_{z = 0}^{\infty} \left[ {m + z - 1 \choose z} \pi^{m} (1 - \pi)^{z} \right].
$${#eq-proof-neg-binomial-mean-2}

Note that, in the summation of @eq-proof-neg-binomial-mean-2, we encounter the `r colourize("PMF", "blue")` of a `r colourize("random variable", "blue")` $Z$ as follows:

$$
Z \sim \text{NegBin}(m, \pi).
$$

Since the summation, where this Negative Binomial `r colourize("PMF", "blue")` of $Z$ is depicted, goes from $z = 0$ to $\infty$, we can apply our result from @eq-proof-neg-binomial-PMF-adds-to-1-c:

$$
\begin{align*} 
\mathbb{E}(Y) &= \frac{k (1 - \pi)}{\pi} \underbrace{\sum_{z = 0}^m \left[ {m + z - 1 \choose z} \pi^{m} (1 - \pi)^{z} \right]}_{1} \\
&= \frac{k (1 - \pi)}{\pi}. \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$
:::

### Variance

Via @eq-app-variance and the @eq-app-expected-value-discrete of a `r colourize("discrete expected value", "blue")`, the `r colourize("variance", "blue")` of a Negative Binomial-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E} \left( Y^2 \right) - \left[ \mathbb{E}(Y)\right]^2 \\
&= \mathbb{E} \left( Y^2 \right) - \left[ \frac{k (1 - \pi)}{\pi} \right]^2 \quad \text{since $\mathbb{E}(Y) = \frac{k (1 - \pi)}{\pi}$.}
\end{align*}
$${#eq-proof-neg-binomial-variance-1}

Now, we need to play around with the below `r colourize("expected value", "blue")` expression as follows:

$$
\begin{align*}
\mathbb{E} \left( Y^2 \right) &= \mathbb{E} \left[ Y (Y - 1) \right] + \mathbb{E}(Y) \\
&= \mathbb{E} \left[ Y (Y - 1) \right] + \frac{k (1 - \pi)}{\pi}.
\end{align*}
$${#eq-proof-neg-binomial-variance-2}

To find $\mathbb{E} \left[ Y (Y - 1) \right]$, we make the following derivation via the LOTUS from @eq-app-expected-value-discrete-function when $g(Y) = y (y - 1)$:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= \sum_{y = 0}^{\infty} y (y - 1) P_Y \left( Y = y \mid k, \pi \right) \\
&= \sum_{y = 2}^{\infty} y (y - 1) P_Y \left( Y = y \mid k, \pi \right) \\
& \quad \qquad \text{for $y = \{0, 1\}$,} \\ 
& \quad \qquad \text{the addends are equal to zero} \\
&= \sum_{y = 2}^{\infty} y (y - 1) \left[ {k + y - 1 \choose y} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 2}^{\infty} y (y - 1) \left[ \frac{(k + y - 1)!}{y! (k + y - 1 - y)!} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 2}^{\infty} y (y - 1) \left[ \frac{(k + y - 1)!}{y! (k - 1)!} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 2}^{\infty} \frac{y (y - 1)}{y (y - 1)} \left[ \frac{(k + y - 1)!}{(y - 2)! (k - 1)!} \pi^k (1 - \pi)^y \right] \\
& \quad \qquad \text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\
&= \sum_{y = 2}^{\infty} \Bigg[ \frac{(k + y - 1)!}{(y - 2)! \underbrace{\frac{(k + 1)!}{k (k + 1)}}_{(k - 1)!}} \pi^k (1 - \pi)^y \Bigg] \\
&= \sum_{y = 2}^{\infty} \left[ k (k + 1) \frac{(k + y - 1)!}{(k + 1)! (y - 2)!} \pi^k (1 - \pi)^y \right] \\
&= k (k + 1) \sum_{y = 2}^{\infty} \left[ {k + y - 1 \choose y - 2} \pi^k (1 - \pi)^y \right] \\
&= k (k + 1) \sum_{y = 2}^{\infty} \left[ {k + y - 1 \choose y - 2} \pi^{k + 2 - 2} (1 - \pi)^{y + 2 - 2} \right] \\
& \quad \qquad \text{note $\pi^k = \pi^{k + 2 - 2}$ and} \\
& \quad \qquad (1 - \pi)^y = (1 - \pi)^{y + 2 - 2} \\
&= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} \times \\
& \qquad \sum_{y = 2}^{\infty} \left[ {k + y - 1 \choose y - 2} \pi^{k + 2} (1 - \pi)^{y - 2} \right].
\end{align*}
$${#eq-proof-neg-binomial-variance-3}

Then, we make the following variable rearrangement:

$$
\begin{gather*}
m = k + 2\\
z = y - 2 \\
m + z - 1  = k + y - 1.
\end{gather*}
$$

Going back to @eq-proof-neg-binomial-variance-3, **and applying our above variable rearrangement within the summation**, we have:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} \times \\
& \qquad \sum_{y = 2}^{\infty} \left[ {m + z - 1 \choose z} \pi^m (1 - \pi)^z \right].
\end{align*}
$${#eq-proof-neg-binomial-variance-4}

Note that, in the summation of @eq-proof-neg-binomial-variance-4, we encounter the `r colourize("PMF", "blue")` of a `r colourize("random variable", "blue")` $Z$ as follows:

$$
Z \sim \text{NegBin}(m, \pi).
$$

Since the summation, where this Binomial `r colourize("PMF", "blue")` of $Z$ is depicted, goes from $z = 0$ to $\infty$, we can apply our result from @eq-proof-neg-binomial-PMF-adds-to-1-c:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} \times \\
& \qquad \underbrace{\sum_{y = 2}^{\infty} \left[ {m + z - 1 \choose z} \pi^m (1 - \pi)^z \right]}_{1} \\
&= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2}.
\end{align*}
$$

Let us go back to @eq-proof-neg-binomial-variance-2 and plug in the above result:

$$
\begin{align*}
\mathbb{E} \left( Y^2 \right) &= \mathbb{E} \left[ Y (Y - 1) \right] + \frac{k ( 1 - \pi)}{\pi} \\
&= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} + \frac{k ( 1 - \pi)}{\pi}.
\end{align*}
$$

Finally, we plug in $\mathbb{E} \left( Y^2 \right)$ in @eq-proof-neg-binomial-variance-1:

$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E} \left( Y^2 \right) - \left[ \frac{k (1 - \pi)}{\pi} \right]^2 \\
&= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} + \frac{k ( 1 - \pi)}{\pi} - \left[ \frac{k (1 - \pi)}{\pi} \right]^2 \\
&= \frac{k (1 - \pi)}{\pi} \left[ \frac{(k + 1) (1 - \pi)}{\pi} + 1 - \frac{k (1 - \pi)}{\pi} \right] \\
&= \frac{k (1 - \pi)}{\pi} \left[ \frac{(k + 1) (1 - \pi) + \pi - k (1 - \pi)}{\pi} \right] \\
&= \frac{k (1 - \pi)}{\pi} \left( \frac{k - k \pi + 1 - \pi + \pi - k + k \pi}{\pi} \right) \\
&= \frac{k (1 - \pi)}{\pi} \left( \frac{1}{\pi} \right) \\
&= \frac{k (1 - \pi)}{\pi^2}. \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$
:::

## Classical Poisson {#sec-classical-poisson-distribution}

Suppose you observe the count of events happening in a **fixed interval of time or space**. Let $Y$ be the number of counts considered of integer type. Then, $Y$ is said to have a **classical Poisson distribution** with a `r colourize("continuous parameter", "blue")` $\lambda$:

$$
Y \sim \text{Pois}(\lambda).
$$

### Probability Mass Function

The `r colourize("PMF", "blue")` of this count-type $Y$ is the following:

$$
P_Y \left( Y = y \mid \lambda \right) = \frac{\lambda^y \exp{(-\lambda)}}{y!} \quad \text{for $y \in \{ 0, 1, 2, \dots\}$,}
$${#eq-app-classical-poisson-pmf}

where $\exp{(\cdot)}$ depicts the base $e$ (i.e., **Euler's number**, $e = 2.71828...$) and $y!$ is the factorial

$$
y! = y \times (y - 1) \times (y - 2) \times (y - 3) \times \cdots \times 3 \times 2 \times 1.  
$$

with 

$$
0! = 1.
$$

The `r colourize("continuous parameter", "blue")` $\lambda \in (0, \infty)$ represents the average rate at which these events happen (i.e., events per area unit or events per time unit). Curiously, even though the `r colourize("random variable", "blue")` $Y$ is considered `r colourize("discrete", "blue")` in this case, $\lambda$ is modelled as `r colourize("continuous", "blue")`!

> **How can we verify that @eq-app-classical-poisson-pmf is a proper `r colourize("PMF", "blue")` (i.e., all the standalone `r colourize("probabilities", "blue")` over the support of $Y$ add up to one)?**

To elaborate on this, we need to use some mathematical tools called the **Taylor series expansions** and a derived result called **Maclaurin series expansions**.

::: {.Heads-up}
::::{.Heads-up-header}
Heads-up on the Taylor and Maclaurin series expansions!
::::
::::{.Heads-up-container}
In mathematics, there are helpful tools known as **Taylor series expansions**, which were officially published by English mathematician Brook Taylor in *Methodus Incrementorum Directa & Inversa* [@taylor1715]. 

![Portrait of mathematician Brook Taylor [@earlom1793].](img/brook-taylor.jpg){width="300"}

However, it is essential to note that Scottish mathematician James Gregory introduced the notion of these series expansions in his work *Vera Circuli et Hyperbolae Quadratura* [@gregory1668]. 

![Portrait of mathematician James Gregory [@watson1886].](img/james-gregory.jpg){width="300"}

These series approximate complex mathematical functions through an infinite sum of polynomial terms. For example, in **machine learning**, the Taylor series expansions can be utilized in **gradient-based optimization methods**. Specifically, **Newton's method** uses these expansions to find roots of equations that cannot be solved analytically, which is common in `r colourize("maximum likelihood-based parameter", "blue")` estimation for the varied regression models discussed throughout this book. Moreover, we can find these series in **different engineering and scientific fields such as physics**.

Suppose we have **real function** $f(u)$ around a point $u = a$, then the one-dimensional **infinite** Taylor series expansion is given by the expression 

$$
\begin{align*}
f(u) &= f(a) + f'(a) (u - a) + \frac{f''(a)}{2!} (u - a)^2 + \\
& \qquad \frac{f^{(3)}(a)}{3!} (u - a)^3 + \frac{f^{(4)}(a)}{4!} (u - a)^4 + \\
& \qquad \frac{f^{(5)}(a)}{5!} (u - a)^5 + \cdots \\
&= \sum_{j = 0}^{\infty} \frac{f^{(j)}(a)}{j!} (u - a)^j.
\end{align*}
$${#eq-taylor-series}

A complete mathematical derivation of @eq-taylor-series can be found in @weisstein. Moving along, specifically in the last line of this equation which shows an infinite summation, note the following:

- $f^{(j)}(a)$ indicates the $j$th order derivative of $f(u)$ and evaluated at point $a$.
- $j!$ implicates the factorial of $j$ such that

$$
j! = j \times (j - 1) \times (j - 2) \times (j - 3) \times \cdots \times 3 \times 2 \times 1.
$$

with 

$$
0! = 1.
$$

If we go even further with @eq-taylor-series, we have a specific case when $a = 0$ called the **Maclaurin series expansions**. This case was introduced by the Scottish mathematician Colin Maclaurin in his work *A Treatise of Fluxions* [@maclaurin1742].

![Portrait of mathematician Colin Maclaurin [@harding1798].](img/colin-maclaurin.jpg){width="300"}

Hence, in a Mclaurin series, @eq-taylor-series becomes:

$$
\begin{align*}
f(u) &= f(0) + f'(0) (u) + \frac{f''(0)}{2!} u^2 + \\
& \qquad \frac{f^{(3)}(0)}{3!} u^3 + \frac{f^{(4)}(0)}{4!} u^4 + \\
& \qquad \frac{f^{(5)}(0)}{5!} u^5 + \cdots \\
&= \sum_{j = 0}^{\infty} \frac{f^{(j)}(0)}{j!} u^j.
\end{align*}
$${#eq-maclaurin-series}

Different statistical proofs make use of Taylor series expansions as well as the Mclaurin series, and the Poisson distribution is not an exception at all! 
::::
:::

The above Mclaurin series in @eq-maclaurin-series will help us to show that our Poisson `r colourize("PMF", "blue")` in @eq-app-classical-poisson-pmf actually adds up to one all over the support of the `r colourize("random variable", "blue")`:

::: {.proof}
$$
\begin{align*}
\sum_{y = 0}^{\infty} P_Y \left( Y = y \mid \lambda \right) &= \sum_{y = 0}^{\infty} \frac{\lambda^y \exp{(-\lambda)}}{y!} \\
&= \exp{(-\lambda)} \sum_{y = 0}^{\infty} \frac{\lambda^y}{y!} \\
& \quad \qquad \text{factoring out $\exp{(-\lambda)}$,} \\
& \quad \qquad \text{since it does not depend on $y$.}
\end{align*}
$${#eq-proof-poisson-PMF-sum}

Now, we will focus on the above summation 

$$
\sum_{y = 0}^{\infty} \frac{\lambda^y}{y!}
$$
and use the Mclaurin series from @eq-maclaurin-series by letting

$$
f(u) = \exp(u).
$${#eq-base-e}

We know that all derivatives of the above function are equal

$$
f'(u) = f''(u) = f^{(3)}(u) = f^{(4)}(u) = f^{(5)}(u) = \cdots = \exp{(u)},
$$
which allows us to conclude that the $j$th derivative is

$$
f^{(j)}(u) = \exp(u).
$$

This $j$th derivative evaluated at $u = 0$ becomes

$$
f^{(j)}(0) = \exp(0) = 1.
$$

Therefore, the Mclaurin series for @eq-base-e is the following:

$$
\begin{align*}
f(u) &= \exp(u) \\
&= \sum_{j = 0}^{\infty} \frac{\exp(0)}{j!} u^j \\
&= \sum_{j = 0}^{\infty} \frac{u^j }{j!}.
\end{align*}
$${#eq-maclaurin-series-base-e}

That said, using @eq-maclaurin-series-base-e, let:

$$
\begin{gather*}
\lambda = u \\
y = j.
\end{gather*}
$$

Thus, we have the following:

$$
\sum_{y = 0}^{\infty} \frac{\lambda^y}{y!} = \exp{(\lambda)}.
$$

Finally, going back to @eq-proof-poisson-PMF-sum:

$$
\begin{align*}
\sum_{y = 0}^{\infty} P_Y \left( Y = y \mid \lambda \right) &= \exp{(-\lambda)} \overbrace{\sum_{y = 0}^{\infty} \frac{\lambda^y}{y!}}^{\exp{(\lambda)}} \\
&= \exp{(-\lambda)} \times \exp{(\lambda)} \\
&= \exp{(-\lambda + \lambda)} \\
&= \exp{(0)} \\
&= 1. \qquad \qquad \qquad \qquad \square
\end{align*}
$${#eq-proof-poisson-PMF-adds-to-1}

> **Indeed, the Poisson `r colourize("PMF", "blue")` is a proper `r colourize("probability distribution", "blue")`!**
:::

### Expected Value

Via @eq-app-expected-value-discrete, the `r colourize("expected value", "blue")` or `r colourize("mean", "blue")` of a Poisson-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*} 
\mathbb{E}(Y) &= \sum_{y = 0}^{\infty} y P_Y \left( Y = y \mid \lambda \right) \\
&= \sum_{y = 1}^{\infty} y P_Y \left( Y = y \mid \lambda \right) \\
& \quad \qquad \text{for $y = 0$, the addend is equal to zero} \\
&= \sum_{y = 1}^{\infty} y \left[ \frac{\lambda^y \exp{(-\lambda)}}{y!} \right] \\
&= \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{y \lambda^y}{y!} \\
& \quad \qquad \text{factoring out $\exp{(-\lambda)}$,} \\
& \quad \qquad \text{since it does not depend on $y$} \\
&= \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{y \lambda^y}{y (y - 1)!} \\
& \quad \qquad \text{in the denominator, $y! = y (y - 1)!$}\\
&= \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{\lambda^y}{(y - 1)!} \\
&= \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{\lambda^{y + 1 - 1}}{(y - 1)!} \\
& \quad \qquad \text{note $\lambda^y = \lambda^{y + 1 - 1}$} \\
&= \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{\lambda \lambda^{y - 1}}{(y - 1)!} \\
& \quad \qquad \text{rearranging terms} \\
&= \lambda \exp{(-\lambda)} \sum_{y = 1}^{\infty} \frac{\lambda^{y - 1}}{(y - 1)!} \\
& \quad \qquad \text{factoring out $\lambda$,} \\
& \quad \qquad \text{since it does not depend on $y$.}
\end{align*}
$${#eq-proof-poisson-mean-1}

Then, let us make the following variable rearrangement:

$$
z = y - 1.
$$

Going back to @eq-proof-poisson-mean-1, **and applying our above variable rearrangement within the summation**, we have:

$$
\mathbb{E}(Y) = \lambda \exp{(-\lambda)} \sum_{z = 0}^{\infty} \frac{\lambda^z}{z!}
$${#eq-proof-poisson-mean-2}

Using @eq-maclaurin-series-base-e, let:

$$
\begin{gather*}
\lambda = u \\
z = j.
\end{gather*}
$$

Hence, we have the following:

$$
\sum_{z = 0}^{\infty} \frac{\lambda^z}{z!} = \exp{(\lambda)}.
$$

Finally, going back to @eq-proof-poisson-mean-2:

$$
\begin{align*}
\mathbb{E}(Y) &= \lambda \exp{(-\lambda)} \overbrace{\sum_{z = 0}^{\infty} \frac{\lambda^z}{z!}}^{\exp{(\lambda)}} \\
&= \lambda \exp{(-\lambda)} \times \exp{(\lambda)} \\
&= \lambda \exp{(-\lambda + \lambda)} \\
&= \lambda \exp{(0)} \\
&= \lambda. \qquad \qquad \qquad \qquad \square
\end{align*}
$$
:::

### Variance

Via @eq-app-variance and the @eq-app-expected-value-discrete of a `r colourize("discrete expected value", "blue")`, the `r colourize("variance", "blue")` of a Poisson-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E} \left( Y^2 \right) - \left[ \mathbb{E}(Y)\right]^2 \\
&= \mathbb{E} \left( Y^2 \right) - \lambda^2 \qquad \text{since $\mathbb{E}(Y) = \lambda$.}
\end{align*}
$${#eq-proof-poisson-variance-1}

Now, we need to play around with the below `r colourize("expected value", "blue")` expression as follows:

$$
\begin{align*}
\mathbb{E} \left( Y^2 \right) &= \mathbb{E} \left[ Y (Y - 1) \right] + \mathbb{E}(Y) \\
&= \mathbb{E} \left[ Y (Y - 1) \right] + \lambda \qquad \text{since $\mathbb{E}(Y) = \lambda$.}
\end{align*}
$${#eq-proof-poisson-variance-2}

Now, to find $\mathbb{E} \left[ Y (Y - 1) \right]$, we make the following derivation via the LOTUS from @eq-app-expected-value-discrete-function when $g(Y) = y (y - 1)$:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= \sum_{y = 0}^{\infty} y (y - 1) P_Y \left( Y = y \mid \lambda \right) \\
&= \sum_{y = 2}^{\infty} y (y - 1) P_Y \left( Y = y \mid \lambda \right) \\
& \quad \qquad \text{for $y = \{0, 1\}$,} \\ 
& \quad \qquad \text{the addends are equal to zero} \\
&= \sum_{y = 2}^{\infty} y (y - 1) \left[ \frac{\lambda^y \exp{(-\lambda)}}{y!} \right] \\
&= \exp{(-\lambda)} \sum_{y = 2}^{\infty} \left[ \frac{y (y - 1) \lambda^y}{y!} \right] \\
& \quad \qquad \text{factoring out $\exp{(-\lambda)}$,} \\
& \quad \qquad \text{since it does not depend on $y$} \\
&= \exp{(-\lambda)} \sum_{y = 2}^{\infty} \left[ \frac{y (y - 1) \lambda^y}{y (y - 1) (y - 2)!} \right] \\
& \quad \qquad \text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\
&= \exp{(-\lambda)} \sum_{y = 2}^{\infty} \frac{\lambda^y}{(y - 2)!} \\
&= \exp{(-\lambda)} \sum_{y = 2}^{\infty} \frac{\lambda^{y + 2 - 2}}{(y - 2)!} \\
& \quad \qquad \text{note $\lambda^y = \lambda^{y + 2 - 2} $} \\
&= \lambda^2 \exp{(-\lambda)} \sum_{y = 2}^{\infty} \frac{\lambda^{y - 2}}{(y - 2)!} \\
& \quad \qquad \text{factoring out $\lambda^2$,} \\
& \quad \qquad \text{since it does not depend on $y$.} \\
\end{align*}
$${#eq-proof-poisson-variance-3}

Then, we make the following variable rearrangement:

$$
z = y - 2.
$$

Going back to @eq-proof-poisson-variance-3, **and applying our above variable rearrangement within the summation**, we have:

$$
\mathbb{E} \left[ Y (Y - 1) \right] = \lambda^2 \exp{(-\lambda)} \sum_{z = 0}^{\infty} \frac{\lambda^z}{z!}.
$${#eq-proof-poisson-variance-4}

Using @eq-maclaurin-series-base-e, let:

$$
\begin{gather*}
\lambda = u \\
z = j.
\end{gather*}
$$

Thus, we have the following:

$$
\sum_{z = 0}^{\infty} \frac{\lambda^z}{z!} = \exp{(\lambda)}.
$$

Going back to @eq-proof-poisson-variance-4:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= \lambda^2 \exp{(-\lambda)} \overbrace{\sum_{z = 0}^{\infty} \frac{\lambda^z}{z!}}^{\exp{(\lambda)}} \\
&= \lambda^2 \exp{(-\lambda)} \times \exp{\lambda} \\
&= \lambda^2 \exp{(-\lambda + \lambda)} \\
&= \lambda^2 \exp{(0)} \\
&= \lambda^2.
\end{align*}
$${#eq-proof-poisson-variance-5}

Let us retake @eq-proof-poisson-variance-2 and plug in the above result:

$$
\begin{align*}
\mathbb{E} \left( Y^2 \right) &= \mathbb{E} \left[ Y (Y - 1) \right] + \lambda \\
&= \lambda^2 + \lambda. \\
\end{align*}
$$

Finally, we plug in $\mathbb{E} \left( Y^2 \right)$ in @eq-proof-poisson-variance-1:

$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E} \left( Y^2 \right) - \lambda^2 \\
&= \lambda^2 + \lambda - \lambda^2 \\
&= \lambda. \qquad \qquad \square
\end{align*}
$$
:::

## Generalized Poisson {#sec-generalized-poisson-distribution}

The generalized Poisson (GP) distribution is viewed as the general Poisson case. It was introduced by @consul1973. Suppose you observe the count of events happening in a fixed interval of time or space. Let $Y$ be the number of counts considered of integer type. Then, $Y$ is said to have a GP distribution with `r colourize("continuous parameters", "blue")` $\lambda$ and $\theta$:

$$
Y \sim \text{GP}(\lambda, \theta).
$$

### Probability Mass Function

The `r colourize("PMF", "blue")` of this count-type $Y$ is the following:

$$
\begin{align*}
P_Y \left( Y = y \mid \lambda, \theta \right) &= \frac{\lambda (\lambda + y \theta)^{y - 1} \exp{\left[ -(\lambda + y \theta) \right]}}{y!} \\
& \qquad \qquad \qquad \text{for $y \in \{ 0, 1, 2, \dots\}$,}
\end{align*}
$${#eq-app-generalized-poisson-pmf}

where $\exp{(\cdot)}$ depicts the base $e$ (i.e., **Euler's number**, $e = 2.71828...$) and $y!$ is the factorial

$$
y! = y \times (y - 1) \times (y - 2) \times (y - 3) \times \cdots \times 3 \times 2 \times 1.  
$$

with 

$$
0! = 1.
$$

The `r colourize("continuous parameter", "blue")` $\lambda \in (0, \infty)$ represents the average rate at which these events happen (i.e., events per area unit or events per time unit). As in the case of the classical Poisson case, even though the GP `r colourize("random variable", "blue")` $Y$ is considered `r colourize("discrete", "blue")`, $\lambda$ is modelled as `r colourize("continuous", "blue")`! 

On the other hand, the `r colourize("continuous", "blue")` and bounded `r colourize("parameter", "blue")` $\theta \in (-1, 1)$ controls for `r colourize("dispersion", "blue")` present in the GP `r colourize("random variable", "blue")` Y as follows:

1. When $0 < \theta < 1$, the GP $Y$ shows `r colourize("overdispersion", "blue")` which implies that $$\text{Var}(Y) > \mathbb{E}(Y).$$

2. When $-1 < \theta < 0$, the GP $Y$ shows `r colourize("underdispersion", "blue")` which implies that 
$$\text{Var}(Y) < \mathbb{E}(Y).$$

3. When $\theta = 0$, the `r colourize("PMF", "blue")` of the GP $Y$ in @eq-app-generalized-poisson-pmf becomes the classical Poisson `r colourize("PMF", "blue")` from @eq-app-classical-poisson-pmf:
$$
\begin{align*}
P_Y \left( Y = y \mid \lambda, \theta = 0 \right) &= \frac{\lambda (\lambda + y \theta)^{y - 1} \exp{\left[ -(\lambda + y \theta) \right]}}{y!} \\
&= \frac{\lambda (\lambda)^{y - 1} \exp{\left( -\lambda \right)}}{y!} \qquad \text{setting $\theta = 0$} \\
&= \frac{\lambda^y \exp{\left( -\lambda \right)}}{y!} \\
& \qquad \qquad \qquad \text{for $y \in \{ 0, 1, 2, \dots\}$.}
\end{align*}
$$


::: {.Heads-up}
::::{.Heads-up-header}
Heads-up on equidispersion in a generalized Poisson random variable!
::::
::::{.Heads-up-container}
In a GP-distributed $Y$, when $\theta = 0$ in its corresponding `r colourize("PMF", "blue")`, we have `r colourize("equidispersion", "blue")` which implies
$$
\mathbb{E}(Y \mid \theta = 0) = \frac{\lambda}{1 - \theta} = \lambda
$$
$$
\text{Var}(Y \mid \theta = 0) = \frac{\lambda}{(1 - \theta)^2} = \lambda
$$
$$
\mathbb{E}(Y \mid \theta = 0) = \text{Var}(Y).
$$
::::
:::

> **How can we verify that @eq-app-generalized-poisson-pmf is a proper `r colourize("PMF", "blue")` (i.e., all the standalone `r colourize("probabilities", "blue")` over the support of $Y$ add up to one)?**

### Expected Value

### Variance

## Zero-inflated Poisson {#sec-zero-inflated-poisson-distribution}

## Multinomial {#sec-multinomial-distribution}

# Continuous Random Variables

Let us recall what a `r colourize("continuous random variable", "blue")` is. This type of variable is defined to take on a set of uncountable possible values. In other words, these values belong to an infinite set. @fig-app-distributions delves into the following specific `r colourize("probability distributions", "blue")`:

- **Weibull.** A `r colourize("random variable", "blue")` $Y$ with a support of $y \in [0, \infty)$. It is a generalization of the **Exponential distribution** and is used in waiting time modelling for some event of interest to happen (i.e., **survival times**). Note its distributional `r colourize("parameters", "blue")` are the **scale** `r colourize("continuous parameter", "blue")` $\beta \in (0, \infty)$ and **shape** `r colourize("continuous parameter", "blue")` $\gamma \in (0, \infty)$.
- **Exponential.** A `r colourize("random variable", "blue")` $Y$ with a support of $y \in [0, \infty)$, which is also often used to model waiting times for some event of interest to happen (i.e., **survival times**). Its single distributional `r colourize("parameter", "blue")` can come in either one of the following forms:
  + As a **rate** $\lambda \in (0, \infty)$, which generally defines the number of events of interest per time interval or space unit.
  + As a **scale** $\beta \in (0, \infty)$, which generally defines the `r colourize("mean", "blue")` number of events of interest per time interval or space unit.
- **Normal.** A `r colourize("random variable", "blue")` $Y$ with a support of $y \in (-\infty, \infty)$. It is well-known for its **bell shape**. Note its distributional `r colourize("parameters", "blue")` are the **location** `r colourize("continuous parameter", "blue")` $\mu \in (-\infty, \infty)$ and **scale** `r colourize("continuous parameter", "blue")` $\sigma^2 \in (0, \infty)$.

| Distribution and <br> Parametrization | Support |  Mean  |  Variance  | 
|:------:|:-----:|:-----:|:-----:|
| **Weibull** as in <br> $Y \sim \text{Weibull}(\beta, \gamma)$ with scale <br> $\beta \in (0, \infty)$ <br> and shape <br> $\gamma \in (0, \infty)$ | $$y \in [0, \infty)$$ | $$\beta \Gamma \left( \frac{1}{\gamma} + 1 \right)$$ | $$\beta^2 \Bigg[ \Gamma \left( \frac{2}{\gamma} + 1 \right) - $$ $$ \quad \Gamma^2 \left( \frac{1}{\gamma} + 1 \right) \Bigg]$$ |
| **Exponential** as in <br> $Y \sim \text{Exponential}(\lambda)$ with rate <br> $\lambda \in (0, \infty)$ <br> or $Y \sim \text{Exponential}(\beta)$ with scale <br> $\beta \in (0, \infty)$ | $$y \in [0, \infty)$$ | $\frac{1}{\lambda}$ <br> for rate <br> parametrization <br> or <br> $\beta$ <br> for scale <br> parametrization | $\frac{1}{\lambda^2}$ <br> for rate <br> parametrization <br> or <br> $\beta^2$ <br> for scale <br> parametrization | 
| **Normal** as in <br> $Y \sim \text{Normal}(\mu, \sigma^2)$ with location <br> $\mu \in (-\infty, \infty)$ <br> and scale <br> $\sigma^2 \in (0, \infty)$ | $$y \in (-\infty, \infty)$$ | $$\mu$$ | $$\sigma^2$$ |

: Univariate continuous probability distributions for a random variable $Y$; including parameter(s), support, mean, and variance. {#tbl-distributions-continuous .striped .hover}

## Weibull {#sec-weibull-distribution}

Suppose you observe the waiting times for some event of interest to happen (i.e., **survival times**). Let `r colourize("random variable", "blue")` $Y$ be considered `r colourize("continuous", "blue")` and nonnegative. Then, $Y$ is said to have a **Weibull distribution** with the following **scale** `r colourize("continuous parameter", "blue")` $\beta$ and **shape** `r colourize("continuous parameter", "blue")` $\gamma$:

$$
Y \sim \text{Weibull}(\beta, \gamma).
$$

### Probability Density Function

The `r colourize("PDF", "blue")` of $Y$ is the following:

$$
f_Y \left(y \mid \beta, \gamma \right) = \frac{\gamma}{\beta} \left( \frac{y}{\beta} \right)^{\gamma - 1} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \quad \text{for $y \in [0, \infty )$.}
$${#eq-app-weibull-pdf}

`r colourize("Parameters", "blue")` $\beta \in (0, \infty)$ and $\gamma \in (0, \infty)$ refer to the random process' scale and shape, respectively. @fig-weibull-family shows nine members of the Weibull family, i.e., nine different `r colourize("PDFs", "blue")` with all possible pairwise combinations for three different scale parameters $\beta = 0.5, 1, 2$ and shape parameters $\gamma = 1.5, 3, 6$. We can highlight the following:

- Regardless of the shape parameter $\gamma$, **as we increase the scale parameter $\beta$**, note that there is more spread in the corresponding distributions.
- Regardless of the scale parameter $\beta$, **as we increase the shape parameter $\gamma$**, note the peak of the distribution moves more to the right.

```{r}
#| label: fig-weibull-family
#| fig-cap: "Some members of the Weibull family."
#| warning: false
#| echo: false
#| fig.height: 13
#| fig.width: 14

weibull_family_plot <- expand_grid(
  gamma = c(1.5, 3, 6),
  beta = c(0.5, 1, 2)
) |>
  mutate(f = map2(
    gamma, beta,
    ~ tibble(
      x = seq(0, 3, length.out = 1000),
      density = dweibull(x, shape = .x, scale = .y)
    )
  )) |>
  unnest(f) |>
  mutate(
    gamma = str_c('gamma*" = ', gamma, '"'),
    beta = str_c('beta*" = ', beta, '"')
  ) |>
  ggplot(aes(x, density)) +
  facet_grid(gamma ~ beta, labeller = label_parsed) +
  geom_line() +
  theme_bw() +
  theme(
    axis.text = element_text(size = 15.5),
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20, angle = 0, vjust = 0.5),
    strip.text = element_text(size = 20)
  ) +
  labs(
    x = "y",
    y = expression(paste(f[Y], "(y | ", beta, ", ", gamma, ") "))
  ) +
  scale_x_continuous(breaks = seq(0, 3, 0.5)) +
  geom_line(color = "seagreen", linewidth = 1)

weibull_family_plot
```

::: {.Heads-up}
::::{.Heads-up-header}
Heads-up on the Weibull and Exponential distributions in survival analysis!
::::
::::{.Heads-up-container}
The Weibull distribution extends its Exponential counterpart (as in @sec-exponential-distribution) by allowing the event rate (or hazard) to change over time, rather than staying constant. This makes it especially useful in `r colourize("survival analysis", "blue")` and reliability studies, where capturing how the risk of an event evolves is critical.

As a side note, the Weibull and Exponential `r colourize("PDFs", "blue")` are mathematically related. When $\gamma = 1$ in @eq-app-weibull-pdf, the Weibull `r colourize("PDF", "blue")` is equal to the Exponential `r colourize("PDF", "blue")` under the **scale parametrization** as in @eq-app-exponential-pdf-scale:

$$
\begin{align*}
f_Y \left(y \mid \beta, \gamma = 1 \right) &= \frac{\gamma}{\beta} \left( \frac{y}{\beta} \right)^{\gamma - 1} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \\
&= \frac{1}{\beta} \underbrace{\left( \frac{y}{\beta} \right)^0}_{1} \exp{\left( -\frac{y}{\beta} \right)} \\
&= \frac{1}{\beta} \exp{\left( -\frac{y}{\beta} \right)} \quad \text{for $y \in [0, \infty )$}.
\end{align*}
$$
::::
:::

> **How can we verify that @eq-app-weibull-pdf is a proper `r colourize("PDF", "blue")` (i.e., @eq-app-weibull-pdf integrates to one over the support of $Y$)?**

::: {.proof}
$$
\begin{align*}
\int_{y = 0}^{y = \infty} f_Y \left(y \mid \beta, \gamma \right) \mathrm{d}y &= \int_{y = 0}^{y = \infty} \frac{\gamma}{\beta} \left( \frac{y}{\beta} \right)^{\gamma - 1} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y.
\end{align*}
$${#eq-proof-weibull-PMF-adds-to-1-1}

Now, let us make the variable substitution:

$$
\begin{gather*}
u = \left( \frac{y}{\beta} \right)^{\gamma} \\
y = \beta u^{\frac{1}{\gamma}} \\
\mathrm{d}y = \frac{\beta}{\gamma} u^{\frac{1}{\gamma} - 1} \mathrm{d}u.
\end{gather*}
$$

The above rearrangemet yields the following in @eq-proof-weibull-PMF-adds-to-1-1:

$$
\begin{align*}
\int_{y = 0}^{y = \infty} f_Y \left(y \mid \beta, \gamma \right) \mathrm{d}y &= \int_{u = 0}^{u = \infty} \frac{\gamma}{\beta} \left( \frac{\beta u^{\frac{1}{\gamma}}}{\beta} \right)^{\gamma - 1} \exp{\left( -u \right)} \frac{\beta}{\gamma} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&= \int_{u = 0}^{u = \infty} \left( u^{\frac{1}{\gamma}} \right)^{\gamma - 1} \exp{\left( -u \right)} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&= \int_{u = 0}^{u = \infty} u^{\frac{\gamma - 1}{\gamma}} \exp{\left( -u \right)} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&= \int_{u = 0}^{u = \infty} u^{\frac{\gamma - 1}{\gamma} + \frac{1}{\gamma} - 1} \exp{\left( -u \right)} \mathrm{d}u \\
&= \int_{u = 0}^{u = \infty} u^{1 - \frac{1}{\gamma} + \frac{1}{\gamma} - 1} \exp{\left( -u \right)} \mathrm{d}u \\
&= \int_{u = 0}^{u = \infty} u^0 \exp{\left( -u \right)} \mathrm{d}u \\
&= \int_{u = 0}^{u = \infty} \exp{\left( -u \right)} \mathrm{d}u \\
&= -\exp{\left( -u \right)} \Bigg|_{u = 0}^{u = \infty} \\
&= - \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \\
&= - \left( 0 - 1 \right) \\
&= 1. \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \square
\end{align*}
$$

> **Indeed, the Weibull `r colourize("PDF", "blue")` is a proper `r colourize("probability distribution", "blue")`!**
:::

### Expected Value

Via @eq-app-expected-value-continuous, the `r colourize("expected value", "blue")` or `r colourize("mean", "blue")` of a Weibull-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*} 
\mathbb{E}(Y) &= \int_{y = 0}^{y = \infty} y f_Y \left(y \mid \beta, \gamma \right) \mathrm{d}y \\
&= \int_{y = 0}^{y = \infty} y \frac{\gamma}{\beta} \left( \frac{y}{\beta} \right)^{\gamma - 1} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y \\
&= \frac{\gamma}{\beta} \int_{y = 0}^{y = \infty} y \left( \frac{y}{\beta} \right)^{\gamma - 1} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y \\
&= \frac{\gamma}{\beta} \int_{y = 0}^{y = \infty} y \frac{y^{\gamma - 1}}{\beta^{\gamma - 1}} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y \\
&= \frac{\gamma}{\beta} \int_{y = 0}^{y = \infty} \frac{y^{\gamma}}{\beta^{\gamma - 1}} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y \\
&= \frac{\gamma}{\beta^{\gamma}} \int_{y = 0}^{y = \infty} y^{\gamma} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y.
\end{align*}
$${#eq-proof-weibull-mean-1}

Then, we make the following variable substitution:

$$
\begin{gather*}
u = \left( \frac{y}{\beta} \right)^{\gamma} \\
y = \beta u^{\frac{1}{\gamma}} \\
\mathrm{d}y = \frac{\beta}{\gamma} u^{\frac{1}{\gamma} - 1} \mathrm{d}u.
\end{gather*}
$$

The above rearrangemet yields the following in @eq-proof-weibull-mean-1:

$$
\begin{align*}
\mathbb{E}(Y) &= \frac{\gamma}{\beta^{\gamma}} \int_{u = 0}^{u = \infty} \left( \beta u^{\frac{1}{\gamma}} \right)^{\gamma} \exp{\left( -u \right)} \frac{\beta}{\gamma} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&= \frac{\gamma}{\beta^{\gamma}} \int_{u = 0}^{u = \infty} \beta^{\gamma} u \exp{\left( -u \right)} \frac{\beta}{\gamma} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&= \frac{\gamma \beta^{\gamma} \beta}{\beta^{\gamma} \gamma} \int_{u = 0}^{u = \infty} u \exp{\left( -u \right)} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&= \beta \int_{u = 0}^{u = \infty} u^{\frac{1}{\gamma}} \exp{\left( -u \right)} \mathrm{d}u \\
&= \beta \int_{u = 0}^{u = \infty} u^{\left( \frac{1}{\gamma} + 1 \right) - 1} \exp{\left( -u \right)} \mathrm{d}u \\
& \quad \qquad \text{note $\frac{1}{\gamma} = \left( \frac{1}{\gamma} + 1 \right) - 1$.}
\end{align*}
$${#eq-proof-weibull-mean-2}

The integral on the right-hand side of @eq-proof-weibull-mean-2 corresponds to the so-called **Gamma function** as described below.

::: {.Heads-up}
::::{.Heads-up-header}
Heads-up on the Gamma function!
::::
::::{.Heads-up-container}
The Gamma function is a mathematical generalization of the **factorial function**, but applied to non-integer numbers. In many different `r colourize("probability distributions", "blue")`, this function appears as a **normalizing constant**. Moreover, it also appears as part of the expressions of `r colourize("expected values", "blue")` and `r colourize("variances", "blue")`.

That said, for a variable $z$ in general, we can represent the Gamma function via the following integral:

$$
\Gamma(z) = \int_{t = 0}^{t = \infty} t^{z - 1} \exp{\left( -t \right)} \mathrm{d}t.
$${#eq-app-gamma-function}

@weisstein2 provides further insights on this Gamma function along with some useful properties.
::::
:::

Thus, via the Gamma function from @eq-app-gamma-function, we set the following:

$$
\begin{gather*}
t = u \\
z = \frac{1}{\gamma} + 1,
\end{gather*}
$$

which yields

$$
\Gamma \left( \frac{1}{\gamma} + 1 \right) = \int_{u = 0}^{u = \infty} u^{\left( \frac{1}{\gamma} + 1 \right) - 1} \exp{\left( -u \right)} \mathrm{d}u.
$$

Moving along with @eq-proof-weibull-mean-2, we have:

$$
\begin{align*}
\mathbb{E}(Y) &= \beta \int_{u = 0}^{u = \infty} u^{\left( \frac{1}{\gamma} + 1 \right) - 1} \exp{\left( -u \right)} \mathrm{d}u \\
&= \beta \Gamma \left( \frac{1}{\gamma} + 1 \right). \qquad \qquad \qquad \quad \square
\end{align*}
$$
:::

### Variance

Via @eq-app-variance and the @eq-app-expected-value-continuous of a `r colourize("continuous expected value", "blue")`, the `r colourize("variance", "blue")` of a Weibull-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E} \left( Y^2 \right) - \left[ \mathbb{E}(Y)\right]^2 \\
&= \mathbb{E} \left( Y^2 \right) - \beta^2 \Gamma^2 \left( \frac{1}{\gamma} + 1 \right) \\
& \quad \qquad \text{since $\mathbb{E}(Y) = \beta \Gamma \left( \frac{1}{\gamma} + 1 \right)$}.
\end{align*}
$${#eq-proof-weibull-variance-1}

Now, we need to find $\mathbb{E} \left( Y^2 \right)$ from @eq-proof-weibull-variance-1. Thus, we make the following derivation via the LOTUS from @eq-app-expected-value-continuous-function when $g(Y) = y^2$:

$$
\begin{align*}
\mathbb{E} \left( Y^2 \right) &= \int_{y = 0}^{y = \infty} y^2 f_Y \left(y \mid \beta, \gamma \right) \mathrm{d}y \\
&= \int_{y = 0}^{y = \infty} y^2 \frac{\gamma}{\beta} \left( \frac{y}{\beta} \right)^{\gamma - 1} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y \\
&= \frac{\gamma}{\beta} \int_{y = 0}^{y = \infty} y^2 \left( \frac{y}{\beta} \right)^{\gamma - 1} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y \\
&= \frac{\gamma}{\beta} \int_{y = 0}^{y = \infty} y^2 \frac{y^{\gamma - 1}}{\beta^{\gamma - 1}} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y \\
&= \frac{\gamma}{\beta^{\gamma}} \int_{y = 0}^{y = \infty} y^{\gamma + 1} \exp{\left[ -\left( \frac{y}{\beta} \right)^{\gamma} \right]} \mathrm{d}y.
\end{align*}
$${#eq-proof-weibull-variance-2} 

Then, we make the following variable substitution:

$$
\begin{gather*}
u = \left( \frac{y}{\beta} \right)^{\gamma} \\
y = \beta u^{\frac{1}{\gamma}} \\
\mathrm{d}y = \frac{\beta}{\gamma} u^{\frac{1}{\gamma} - 1} \mathrm{d}u.
\end{gather*}
$$

The above rearrangemet yields the following in @eq-proof-weibull-variance-2:

$$
\begin{align*}
\mathbb{E} \left( Y^2 \right) &= \frac{\gamma}{\beta^{\gamma}} \int_{u = 0}^{u = \infty} \left( \beta u^{\frac{1}{\gamma}} \right)^{\gamma + 1} \exp{\left( -u \right)} \frac{\beta}{\gamma} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&= \frac{\gamma}{\beta^{\gamma}} \int_{u = 0}^{u = \infty} \beta^{\gamma + 1} u^{1 + \frac{1}{\gamma}} \exp{\left( -u \right)} \frac{\beta}{\gamma} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&= \frac{\gamma \beta^{\gamma + 1} \beta}{\beta^{\gamma} \gamma} \int_{u = 0}^{u = \infty} u^{1 + \frac{1}{\gamma}} \exp{\left( -u \right)} u^{\frac{1}{\gamma} - 1} \mathrm{d}u \\
&= \beta^2 \int_{u = 0}^{u = \infty} u^{1 + \frac{1}{\gamma} + \frac{1}{\gamma} - 1} \exp{\left( -u \right)} \mathrm{d}u \\
&= \beta^2 \int_{u = 0}^{u = \infty} u^{\left( \frac{2}{\gamma} + 1 \right) - 1} \exp{\left( -u \right)} \mathrm{d}u.
\end{align*}
$${#eq-proof-weibull-variance-3}

Hence, via the Gamma function from @eq-app-gamma-function, we set the following:

$$
\begin{gather*}
t = u \\
z = \frac{2}{\gamma} + 1,
\end{gather*}
$$

which yields

$$
\Gamma \left( \frac{2}{\gamma} + 1 \right) = \int_{u = 0}^{u = \infty} u^{\left( \frac{2}{\gamma} + 1 \right) - 1} \exp{\left( -u \right)} \mathrm{d}u.
$$

Moving along with @eq-proof-weibull-variance-3, we have:

$$
\begin{align*}
\mathbb{E} \left( Y^2 \right) &= \beta^2 \int_{u = 0}^{u = \infty} u^{\left( \frac{2}{\gamma} + 1 \right) - 1} \exp{\left( -u \right)} \mathrm{d}u \\
&= \beta^2 \Gamma \left( \frac{2}{\gamma} + 1 \right).
\end{align*}
$${#eq-proof-weibull-variance-4}

Finally, we plug @eq-proof-weibull-variance-4 into @eq-proof-weibull-variance-1:

$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E}\left( Y^2 \right) - \beta^2 \Gamma^2 \left( \frac{1}{\gamma} + 1 \right) \\
&= \beta^2 \Gamma \left( \frac{2}{\gamma} + 1 \right) - \beta^2 \Gamma^2 \left( \frac{1}{\gamma} + 1 \right) \\
&= \beta^2 \left[  \Gamma \left( \frac{2}{\gamma} + 1 \right) - \Gamma^2 \left( \frac{1}{\gamma} + 1 \right) \right]. \qquad \qquad \square
\end{align*}
$$
:::

## Lognormal {#sec-lognormal-distribution}

## Exponential {#sec-exponential-distribution}

Suppose you observe the waiting times for some event of interest to happen (i.e., **survival times**). Let `r colourize("random variable", "blue")` $Y$ be considered `r colourize("continuous", "blue")` and nonnegative. Then, $Y$ is said to have an **Exponential distribution** with the following **rate** `r colourize("continuous parameter", "blue")` $\lambda$:

$$
Y \sim \text{Exponential}(\lambda).
$$

We can also model $Y$ with the following **scale** `r colourize("continuous parameter", "blue")` $\beta$:

$$
Y \sim \text{Exponential}(\beta).
$$

### Probability Density Functions

Given the two above parametrizations of the Exponential distribution, there are two possible `r colourize("PDFs", "blue")` as discussed below.

#### Rate Parametrization

The `r colourize("PDF", "blue")` of $Y$ is the following:

$$
f_Y \left(y \mid \lambda \right) = \lambda \exp \left( -\lambda y \right) \quad \text{for $y \in [0, \infty )$.}
$${#eq-app-exponential-pdf-rate}

`r colourize("Parameter", "blue")` $\lambda \in (0, \infty)$ refers to the random process' rate. @fig-exponential-family-rate shows three members of the Exponential family, i.e., three different `r colourize("PDFs", "blue")` with different rate parameters $\lambda = 0.25, 0.5, 1$. **As we increase the rate parameter**, note that smaller observed values $y$ get more probable.

```{r}
#| label: fig-exponential-family-rate
#| fig-cap: "Some members of the Exponential family with rate parametrization."
#| warning: false
#| echo: false
#| fig.height: 6.5
#| fig.width: 14

exponential_family_plot_rate <- tibble(lambda = c(1, 0.5, 0.25)) |>
  mutate(f = map(
    lambda,
    ~ tibble(
      x = seq(0, 10, length.out = 1000),
      density = dexp(x, rate = .x)
    )
  )) |>
  unnest(f) |>
  mutate(lambda = str_c('lambda*" = ', lambda, '"')) |>
  ggplot(aes(x, density)) +
  facet_wrap(~lambda, labeller = label_parsed) +
  geom_line() +
  theme_bw() +
  theme(
    axis.text = element_text(size = 15.5),
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20, angle = 0, vjust = 0.5),
    strip.text = element_text(size = 20)
  ) +
  labs(
    x = "y",
    y = expression(paste(f[Y], "(y | ", lambda, ") "))
  ) +
  geom_line(color = "royalblue3", linewidth = 1)

exponential_family_plot_rate
```

> **How can we verify that @eq-app-exponential-pdf-rate is a proper `r colourize("PDF", "blue")` (i.e., @eq-app-exponential-pdf-rate integrates to one over the support of $Y$)?**

::: {.proof}
$$
\begin{align*} 
\int_{y = 0}^{y = \infty} f_Y \left(y \mid \lambda \right) \mathrm{d}y &= \int_{y = 0}^{y = \infty} \lambda \exp \left( -\lambda y \right) \mathrm{d}y \\
&= \lambda \int_{y = 0}^{y = \infty} \exp \left( -\lambda y \right) \mathrm{d}y \\
&= - \frac{\lambda}{\lambda} \exp \left( -\lambda y \right) \Bigg|_{y = 0}^{y = \infty} \\
&= - \exp \left( -\lambda y \right) \Bigg|_{y = 0}^{y = \infty} \\
&= - \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \\
&= - \left( 0 - 1 \right) \\
&= 1. \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$

> **Indeed, the Exponential `r colourize("PDF", "blue")`, under a rate parametrization, is a proper `r colourize("probability distribution", "blue")`!**
:::

#### Scale Parametrization

The `r colourize("PDF", "blue")` of $Y$ is the following:

$$
f_Y \left(y \mid \beta \right) = \frac{1}{\beta} \exp \left( -\frac{y}{\beta} \right) \quad \text{for $y \in [0, \infty )$.}
$${#eq-app-exponential-pdf-scale}

`r colourize("Parameter", "blue")` $\beta \in (0, \infty)$ refers to the random process' scale. @fig-exponential-family-scale shows three members of the Exponential family, i.e., three different `r colourize("PDFs", "blue")` with different scale parameters $\beta = 0.25, 0.5, 1$. **As we increase the scale parameter**, note that larger observed values $y$ get more probable.

```{r}
#| label: fig-exponential-family-scale
#| fig-cap: "Some members of the Exponential family with scale parametrization."
#| warning: false
#| echo: false
#| fig.height: 6.5
#| fig.width: 14

exponential_family_plot_scale <- tibble(lambda = c(1, 2, 4)) |>
  mutate(f = map(
    lambda,
    ~ tibble(
      x       = seq(0, 10, length.out = 1000),
      density = dexp(x, rate = .x)
    )
  )) |>
  unnest(f) |>
  mutate(lambda = str_c('beta*" = ', 1 / lambda, '"')) %>%
  ggplot(aes(x, density)) +
  facet_wrap(~lambda, labeller = label_parsed) +
  geom_line() +
  theme_bw() +
  theme(
    axis.text = element_text(size = 15.5),
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20, angle = 0, vjust = 0.5),
    strip.text = element_text(size = 20)
  ) +
  labs(
    x = "y",
    y = expression(paste(f[Y], "(y | ", beta, ") "))
  ) +
  geom_line(color = "tomato", linewidth = 1)

exponential_family_plot_scale
```

> **How can we verify that @eq-app-exponential-pdf-scale is a proper `r colourize("PDF", "blue")` (i.e., @eq-app-exponential-pdf-scale integrates to one over the support of $Y$)?**

::: {.proof}
$$
\begin{align*} 
\int_{y = 0}^{y = \infty} f_Y \left(y \mid \beta \right) \mathrm{d}y &= \int_{y = 0}^{y = \infty} \frac{1}{\beta} \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \\
&= \frac{1}{\beta} \int_{y = 0}^{y = \infty} \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \\
&= - \frac{\beta}{\beta} \exp \left( -\frac{y}{\beta} \right) \Bigg|_{y = 0}^{y = \infty} \\
&= - \exp \left( -\frac{y}{\beta} \right) \Bigg|_{y = 0}^{y = \infty} \\
&= - \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \\
&= - \left( 0 - 1 \right) \\
&= 1. \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$

> **Indeed, the Exponential `r colourize("PDF", "blue")`, under a scale parametrization, is a proper `r colourize("probability distribution", "blue")`!**
:::

### Expected Value

Again, given the two above parametrizations of the Exponential distribution, there are two possible mathematical expressions for the `r colourize("expected value", "blue")` as discussed below.

#### Rate Parametrization

Via @eq-app-expected-value-continuous, the `r colourize("expected value", "blue")` or `r colourize("mean", "blue")` of an Exponential-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*}
\mathbb{E}(Y) &= \int_{y = 0}^{y = \infty} y f_Y \left(y \mid \lambda \right) \mathrm{d}y \\
&= \int_{y = 0}^{y = \infty} y \lambda \exp \left( -\lambda y \right) \mathrm{d}y \\
&= \lambda \int_{y = 0}^{y = \infty} y \exp \left( -\lambda y \right) \mathrm{d}y. \\
\end{align*}
$${#eq-app-exponential-rate-mean}

@eq-app-exponential-rate-mean cannot be solved straightforwardly, we need to use **integration by parts** as follows:

$$
\begin{equation}
  \begin{split}
    u &= y \\
    \mathrm{d}u &= \mathrm{d}y
  \end{split}
\qquad \qquad
  \begin{split}
    \mathrm{d}v &= \exp \left( -\lambda y \right) \mathrm{d}y \\
    v &= -\frac{1}{\lambda} \exp \left( -\lambda y \right),
  \end{split}
\end{equation}
$$

which yields

$$
\begin{align*} 
\mathbb{E}(Y) &= \lambda \left[ u v \Bigg|_{y = 0}^{y = \infty} - \int_{y = 0}^{y = \infty} v \mathrm{d}u \right] \\
&= \lambda \left\{ \left[ -\frac{1}{\lambda} y \exp(-\lambda y) \right] \Bigg|_{y = 0}^{y = \infty} + \frac{1}{\lambda} \int_{y = 0}^{y = \infty} \exp{\left( -\lambda y \right)} \mathrm{d}y \right\} \\
&= \lambda \Bigg\{ -\frac{1}{\lambda} \Bigg[ \underbrace{\infty \times \exp(-\infty)}_{0} - \underbrace{0 \times \exp(0)}_{0} \Bigg] - \\
& \qquad \frac{1}{\lambda^2} \exp{\left( -\lambda y \right)} \Bigg|_{y = 0}^{y = \infty} \Bigg\} \\
&= \lambda \left\{ -\frac{1}{\lambda} (0) - \frac{1}{\lambda^2} \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \right\} \\
&= \lambda \left[ 0 - \frac{1}{\lambda^2} (0 - 1) \right] \\
&= \frac{\lambda}{\lambda^2} \\
&= \frac{1}{\lambda}. \qquad \qquad \qquad \qquad \qquad \qquad \quad \qquad \qquad \quad \qquad \qquad \square
\end{align*}
$$
:::

#### Scale Parametrization

Via @eq-app-expected-value-continuous, the `r colourize("expected value", "blue")` or `r colourize("mean", "blue")` of an Exponential-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*}
\mathbb{E}(Y) &= \int_{y = 0}^{y = \infty} y f_Y \left(y \mid \beta \right) \mathrm{d}y \\
&= \int_{y = 0}^{y = \infty} \frac{y}{\beta} \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \\
&= \frac{1}{\beta} \int_{y = 0}^{y = \infty} y \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y. \\
\end{align*}
$${#eq-app-exponential-scale-mean}

@eq-app-exponential-scale-mean cannot be solved straightforwardly, we need to use **integration by parts** as follows:

$$
\begin{equation}
  \begin{split}
    u &= y \\
    \mathrm{d}u &= \mathrm{d}y
  \end{split}
\qquad \qquad
  \begin{split}
    \mathrm{d}v &= \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \\
    v &= -\beta \exp \left( -\frac{y}{\beta} \right),
  \end{split}
\end{equation}
$$

which yields

$$
\begin{align*} 
\mathbb{E}(Y) &= \frac{1}{\beta} \left[ u v \Bigg|_{y = 0}^{y = \infty} - \int_{y = 0}^{y = \infty} v \mathrm{d}u \right] \\
&= \frac{1}{\beta} \left\{ \left[ -\beta y \exp \left( -\frac{y}{\beta} \right) \right] \Bigg|_{y = 0}^{y = \infty} + \beta \int_{y = 0}^{y = \infty} \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \right\} \\
&= \frac{1}{\beta} \Bigg\{ -\beta \Bigg[ \underbrace{\infty \times \exp(-\infty)}_{0} - \underbrace{0 \times \exp(0)}_{0} \Bigg] - \\
& \qquad \beta^2 \exp \left( -\frac{y}{\beta} \right) \Bigg|_{y = 0}^{y = \infty} \Bigg\} \\
&= \frac{1}{\beta} \left\{ -\beta (0) - \beta^2 \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \right\} \\
&= \frac{1}{\beta} \left[ 0 - \beta^2 (0 - 1) \right] \\
&= \frac{\beta^2}{\beta} \\
&= \beta. \qquad \qquad \qquad \qquad \qquad \qquad \quad \qquad \qquad \quad \qquad \qquad \square
\end{align*}
$$
:::

### Variance

Given the two above parametrizations of the Exponential distribution, there are two possible mathematical expressions for the `r colourize("variance", "blue")` as discussed below.

#### Rate Parametrization

Via @eq-app-variance and the @eq-app-expected-value-continuous of a `r colourize("continuous expected value", "blue")`, the `r colourize("variance", "blue")` of an Exponential-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E} \left( Y^2 \right) - \left[ \mathbb{E}(Y)\right]^2 \\
&= \mathbb{E} \left( Y^2 \right) - \frac{1}{\lambda^2} \qquad \text{since $\mathbb{E}(Y) = \frac{1}{\lambda}$}.
\end{align*}
$${#eq-proof-exponential-rate-variance-1}

Now, we need to find $\mathbb{E} \left( Y^2 \right)$ from @eq-proof-exponential-rate-variance-1. Hence, we make the following derivation via the LOTUS from @eq-app-expected-value-continuous-function when $g(Y) = y^2$:

$$
\begin{align*}
\mathbb{E} \left( Y^2 \right) &= \int_{y = 0}^{y = \infty} y^2 f_Y \left(y \mid \lambda \right) \mathrm{d}y \\
&= \int_{y = 0}^{y = \infty} y^2 \lambda \exp \left( -\lambda y \right) \mathrm{d}y \\
&= \lambda \int_{y = 0}^{y = \infty} y^2 \exp \left( -\lambda y \right) \mathrm{d}y. \\
\end{align*}
$${#eq-proof-exponential-rate-variance-2} 

@eq-proof-exponential-rate-variance-2 cannot be solved straightforwardly, we need to use **integration by parts** as follows:

$$
\begin{equation}
  \begin{split}
    u &= y^2 \\
    \mathrm{d}u &= 2y \mathrm{d}y
  \end{split}
\qquad \qquad
  \begin{split}
    \mathrm{d}v &= \exp \left( -\lambda y \right) \mathrm{d}y \\
    v &= -\frac{1}{\lambda} \exp \left( -\lambda y \right),
  \end{split}
\end{equation}
$$

which yields

$$
\begin{align*} 
\mathbb{E} \left( Y^2 \right) &= \lambda \left[ u v \Bigg|_{y = 0}^{y = \infty} - \int_{y = 0}^{y = \infty} v \mathrm{d}u \right] \\
&= \lambda \bigg\{ \left[ -\frac{1}{\lambda} y^2 \exp(-\lambda y) \right] \Bigg|_{y = 0}^{y = \infty} + \\
& \qquad \frac{2}{\lambda} \int_{y = 0}^{y = \infty} y \exp{\left( -\lambda y \right)} \mathrm{d}y \bigg\} \\
&= \lambda \Bigg\{ -\frac{1}{\lambda} \Bigg[ \underbrace{\infty \times \exp(-\infty)}_{0} - \underbrace{0 \times \exp(0)}_{0} \Bigg] + \\
& \qquad \frac{2}{\lambda} \int_{y = 0}^{y = \infty} y \exp{\left( -\lambda y \right)} \mathrm{d}y \Bigg\} \\ 
&= \lambda \left\{ -\frac{1}{\lambda} (0) + \frac{2}{\lambda} \int_{y = 0}^{y = \infty} y \exp{\left( -\lambda y \right)} \mathrm{d}y \right\} \\
&= \lambda \left\{ 0 + \frac{2}{\lambda} \int_{y = 0}^{y = \infty} y \exp{\left( -\lambda y \right)} \mathrm{d}y \right\} \\
&= 2 \int_{y = 0}^{y = \infty} y \exp{\left( -\lambda y \right)} \mathrm{d}y. \\
\end{align*}
$${#eq-proof-exponential-rate-variance-3}

Again, we need to apply **integration by parts** to solve @eq-proof-exponential-rate-variance-3:

$$
\begin{equation}
  \begin{split}
    u &= y \\
    \mathrm{d}u &= \mathrm{d}y
  \end{split}
\qquad \qquad
  \begin{split}
    \mathrm{d}v &= \exp \left( -\lambda y \right) \mathrm{d}y \\
    v &= -\frac{1}{\lambda} \exp \left( -\lambda y \right),
  \end{split}
\end{equation}
$$

which yields

$$
\begin{align*} 
\mathbb{E} \left( Y^2 \right) &= 2 \left[ u v \Bigg|_{y = 0}^{y = \infty} - \int_{y = 0}^{y = \infty} v \mathrm{d}u \right] \\
&= 2 \left\{ \left[ -\frac{1}{\lambda} y \exp(-\lambda y) \right] \Bigg|_{y = 0}^{y = \infty} + \frac{1}{\lambda} \int_{y = 0}^{y = \infty} \exp{\left( -\lambda y \right)} \mathrm{d}y \right\} \\
&= 2 \Bigg\{ -\frac{1}{\lambda} \Bigg[ \underbrace{\infty \times \exp(-\infty)}_{0} - \underbrace{0 \times \exp(0)}_{0} \Bigg] - \\
& \qquad \frac{1}{\lambda^2} \exp{\left( -\lambda y \right)} \Bigg|_{y = 0}^{y = \infty} \Bigg\} \\
&= 2 \left\{ -\frac{1}{\lambda} (0) - \frac{1}{\lambda^2} \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \right\} \\
&= 2 \left[ 0 - \frac{1}{\lambda^2} (0 - 1) \right] \\
&= \frac{2}{\lambda^2}.
\end{align*}
$${#eq-proof-exponential-rate-variance-4}

Finally, we plug @eq-proof-exponential-rate-variance-4 into @eq-proof-exponential-rate-variance-1:

$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E} \left( Y^2 \right) - \frac{1}{\lambda^2} \\
&= \frac{2}{\lambda^2} - \frac{1}{\lambda^2} \\
&= \frac{1}{\lambda^2}. \qquad \qquad \square
\end{align*}
$$
:::

#### Scale Parametrization

Via @eq-app-variance and the @eq-app-expected-value-continuous of a `r colourize("continuous expected value", "blue")`, the `r colourize("variance", "blue")` of an Exponential-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E} \left( Y^2 \right) - \left[ \mathbb{E}(Y)\right]^2 \\
&= \mathbb{E} \left( Y^2 \right) - \beta^2 \qquad \text{since $\mathbb{E}(Y) = \beta$}.
\end{align*}
$${#eq-proof-exponential-scale-variance-1}

Now, we need to find $\mathbb{E} \left( Y^2 \right)$ from @eq-proof-exponential-scale-variance-1. Hence, we make the following derivation via the LOTUS from @eq-app-expected-value-continuous-function when $g(Y) = y^2$:

$$
\begin{align*}
\mathbb{E} \left( Y^2 \right) &= \int_{y = 0}^{y = \infty} y^2 f_Y \left(y \mid \beta \right) \mathrm{d}y \\
&= \int_{y = 0}^{y = \infty} y^2 \frac{1}{\beta} \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \\
&= \frac{1}{\beta} \int_{y = 0}^{y = \infty} y^2 \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y. \\
\end{align*}
$${#eq-proof-exponential-scale-variance-2} 

@eq-proof-exponential-scale-variance-2 cannot be solved straightforwardly, we need to use **integration by parts** as follows:

$$
\begin{equation}
  \begin{split}
    u &= y^2 \\
    \mathrm{d}u &= 2y \mathrm{d}y
  \end{split}
\qquad \qquad
  \begin{split}
    \mathrm{d}v &= \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \\
    v &= -\beta \exp \left( -\frac{y}{\beta} \right),
  \end{split}
\end{equation}
$$

which yields

$$
\begin{align*} 
\mathbb{E} \left( Y^2 \right) &= \frac{1}{\beta} \left[ u v \Bigg|_{y = 0}^{y = \infty} - \int_{y = 0}^{y = \infty} v \mathrm{d}u \right] \\
&= \frac{1}{\beta} \Bigg\{ \left[ -\beta y^2 \exp \left( -\frac{y}{\beta} \right) \right] \Bigg|_{y = 0}^{y = \infty} + \\
& \qquad 2 \beta \int_{y = 0}^{y = \infty} y \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \Bigg\} \\
&= \frac{1}{\beta} \Bigg\{ -\beta \Bigg[ \underbrace{\infty \times \exp(-\infty)}_{0} - \underbrace{0 \times \exp(0)}_{0} \Bigg] + \\
& \qquad 2 \beta \int_{y = 0}^{y = \infty} y \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \Bigg\} \\ 
&= \frac{1}{\beta} \left\{ -\beta (0) + 2 \beta \int_{y = 0}^{y = \infty} y \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \right\} \\
&= \frac{1}{\beta} \left\{ 0 + 2 \beta \int_{y = 0}^{y = \infty} y \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \right\} \\
&= 2 \int_{y = 0}^{y = \infty} y \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y. \\
\end{align*}
$${#eq-proof-exponential-scale-variance-3}

Again, we need to apply **integration by parts** to solve @eq-proof-exponential-scale-variance-3:

$$
\begin{equation}
  \begin{split}
    u &= y \\
    \mathrm{d}u &= \mathrm{d}y
  \end{split}
\qquad \qquad
  \begin{split}
    \mathrm{d}v &= \exp \left( -\frac{y}{\beta} \right)\mathrm{d}y \\
    v &= -\beta \exp \left( -\frac{y}{\beta} \right),
  \end{split}
\end{equation}
$$

which yields

$$
\begin{align*} 
\mathbb{E} \left( Y^2 \right) &= 2 \left[ u v \Bigg|_{y = 0}^{y = \infty} - \int_{y = 0}^{y = \infty} v \mathrm{d}u \right] \\
&= 2 \left\{ \left[ -\beta y \exp \left( -\frac{y}{\beta} \right) \right] \Bigg|_{y = 0}^{y = \infty} + \beta \int_{y = 0}^{y = \infty} \exp \left( -\frac{y}{\beta} \right) \mathrm{d}y \right\} \\
&= 2 \Bigg\{ -\beta \Bigg[ \underbrace{\infty \times \exp(-\infty)}_{0} - \underbrace{0 \times \exp(0)}_{0} \Bigg] - \\
& \qquad \beta^2 \exp \left( -\frac{y}{\beta} \right) \Bigg|_{y = 0}^{y = \infty} \Bigg\} \\
&= 2 \left\{ -\beta (0) - \beta^2 \left[ \exp \left( -\infty \right) - \exp \left( 0 \right) \right] \right\} \\
&= 2 \left[ 0 - \beta^2 (0 - 1) \right] \\
&= 2 \beta^2.
\end{align*}
$${#eq-proof-exponential-scale-variance-4}

Finally, we plug @eq-proof-exponential-scale-variance-4 into @eq-proof-exponential-scale-variance-1:

$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E} \left( Y^2 \right) - \beta^2 \\
&= 2 \beta^2 - \beta^2 \\
&= \beta^2. \qquad \qquad \square
\end{align*}
$$
:::

## Gamma {#sec-gamma-distribution}

## Logistic {#sec-logistic-distribution}

## Normal {#sec-normal-distribution}

This is possibly one the most famous `r colourize("probability distributions", "blue")`, and it is also known as **Gaussian**. It appears in many different statistical tools in the literature where certain regression models are included. Let `r colourize("random variable", "blue")` $Y$ be considered `r colourize("continuous", "blue")` and unbounded. Then, $Y$ is said to have a **Normal distribution** with the following **location** `r colourize("continuous parameter", "blue")` $\mu$ and **scale** `r colourize("continuous parameter", "blue")` $\sigma^2$:

$$
Y \sim \text{Normal}(\mu, \sigma^2).
$$

### Probability Density Function

The `r colourize("PDF", "blue")` of $Y$ is the following:

$$
f_Y \left(y \mid \mu, \sigma^2 \right) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{\left[ - \frac{(y - \mu)^2}{2 \sigma^2} \right]} \quad \text{for $y \in ( -\infty, \infty )$.}
$${#eq-app-normal-pdf}

::: {.Heads-up}
::::{.Heads-up-header}
Heads-up on the use $\pi$ in the Normal distribution!
::::
::::{.Heads-up-container}
The term $\pi$ in the Normal `r colourize("PDF", "blue")` depicted in @eq-app-normal-pdf corresponds to the mathematical constant $3.141592...$ Hence, this term **does not** correspond to another `r colourize("population parameter", "blue")` in this `r colourize("probability distribution", "blue")`.
::::
:::

`r colourize("Parameters", "blue")` $\mu \in (-\infty, \infty)$ and $\sigma \in (0, \infty)$ refer to the random process' location and scale, respectively. @fig-normal-family shows nine members of the Normal family, i.e., nine different `r colourize("PDFs", "blue")` with all possible pairwise combinations for three different scale parameters $\mu = -3, 0, 3$ and shape parameters $\sigma^2 = 0.25, 1, 4$. We can highlight the following:

- Regardless of the scale parameter $\sigma^2$, **as we increase the location parameter $\mu$**, note the center of the corresponding symmetric distribution moves more to the right.
- Regardless of the location parameter $\mu$, **as we increase the scale parameter $\sigma^2$**, note that there is more spread in the corresponding symmetric distribution.

```{r}
#| label: fig-normal-family
#| fig-cap: "Some members of the Normal or Gaussian family."
#| warning: false
#| echo: false
#| fig.height: 13
#| fig.width: 14

normal_family_plot <- expand_grid(
  mu = c(-3, 0, 3),
  sd = c(0.5, 1, 2)
) |>
  mutate(f = map2(
    mu, sd,
    ~ tibble(
      x = seq(-8, 8, length.out = 1000),
      density = dnorm(x, mean = .x, sd = .y)
    )
  )) |>
  unnest(f) |>
  mutate(
    mu = str_c('mu*" = ', mu, '"'),
    var = str_c('sigma^2*" = ', sd^2, '"')
  ) |>
  ggplot(aes(x, density)) +
  facet_grid(mu ~ var, labeller = label_parsed) +
  geom_line() +
  theme_bw() +
  theme(
    axis.text = element_text(size = 15.5),
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20, angle = 0, vjust = 0.5),
    strip.text = element_text(size = 20)
  ) +
  labs(
    x = "y",
    y = expression(paste(f[Y], "(y | ", mu, ", ", sigma^2, ") "))
  ) +
  scale_x_continuous(breaks = seq(-6, 6, 2)) +
  geom_line(color = "darkslateblue", linewidth = 1)

normal_family_plot
```

> **How can we verify that @eq-app-normal-pdf is a proper `r colourize("PDF", "blue")` (i.e., @eq-app-normal-pdf integrates to one over the support of $Y$)?**


## Beta {#sec-beta-distribution}

