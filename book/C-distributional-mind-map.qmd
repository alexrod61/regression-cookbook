<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7PRVEBE1EF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7PRVEBE1EF');
</script>

# The Chocolified Distributional Mind Map {#sec-distributional-mind-map}

```{r}
#| include: false

colourize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

::: {.Warning}
::::{.Warning-header}
Fun fact!
::::
::::{.Warning-container}
**Chocolified!** Every inch of it is chocolate, probably topped with more chocolate.
::::
:::

A crucial part of the practice of `r colourize("regression analysis", "blue")` is a fair knowledge of all the different `r colourize("probability distributions", "blue")` that would allow us to identify **the most suitable regression model**. Let us delve into the distributional toolbox to be used in this book.

@fig-app-distributions shows all those `r colourize("probability distributions", "blue")` **depicted as clouds**, in the form of a **univariate** `r colourize("random variable", "blue")` $Y$, used to model our `r colourize("outcomes", "magenta")` of interest in the regression tools explored in each of the core thirteen chapters, i.e., we take a `r colourize("generative modelling", "blue")` approach. Note this mind map splits the `r colourize("outcomes", "magenta")` of interest into two large zones: `r colourize("discrete", "blue")` and `r colourize("continuous", "blue")`. Furthermore, this mind map briefly describes a given `r colourize("random variable", "blue")` $Y$ as a quick cheat sheet regarding its **support** (e.g., nonnegative, bounded, unbounded, etc.) or **distributional definition** (e.g., success or failure, successes in $n$ trials, etc.).

::: {#fig-app-distributions}
```{mermaid}
mindmap
  root((Univariate 
    Random
    Variable Y))
    Continuous
      {{Unbounded}}
        )Normal(
        )Logistic(
      {{Nonnegative Y}}
        )Lognormal(
        )Exponential(
        )Gamma(
        )Weibull(
      {{Y is between <br/>0 and 1}}
        )Beta(
    Discrete
      Binary
        {{Y is a success or <br/>failure}}
          )Bernoulli(
      Count
        {{Y succeses in <br/>n trials}}
          )Binomial(
        {{Y failures <br/>before experiencing <br/>k successes}}
          )Negative Binomial(
        {{Y events in <br/>a fixed interval <br/>of time or space}}
          )Classical <br/>Poisson(
          )Generalized <br/>Poisson(
          )Zero Inflated <br/>Poisson(
      Categorical
        {{Y successes of a given category, <br/>among a set of k categories, <br/>in n trials}}
          )Multinomial(
```

Probability distribution mind map depicting all univariate random variables to be used as outcomes of interest $Y$ in the modelling techniques to be explored in this book. These outcomes are split into two large zones: *discrete* and *continuous*.
:::

Since a given `r colourize("random variable", "blue")` (e.g., the `r colourize("outcome", "magenta")` $Y$ in any of the thirteen regression models in this book) will have a `r colourize("probability distribution", "blue")` associated with it, which will define the `r colourize("probability", "blue")` arrangement of each possible value or category $Y$ could take on, we also need a way to summarize all this information via key estimated metrics called `r colourize("measures of central tendency and uncertainty", "blue")`:

- **Measure of Central Tendency:** This metric refers to a **typical value or category** that a given `r colourize("random variable", "blue")` might take when we observe various realizations of this variable over a long period. 
- **Measure of Uncertainty:** This metric pertains to the **spread** of a `r colourize("random variable", "blue")` when we observe its different realizations in the long term. As a side note, a larger spread indicates more variability in these realizations.

![Image by [*Tobias Frick*](https://pixabay.com/users/pixounaut-2729335/) via [*Pixabay*](https://pixabay.com/photos/game-dice-board-game-craps-game-4695864/).](img/dice.jpg){width="400"} 

These metrics allow us to clearly communicate how the outcome $Y$ behaves in our case study, and this is heavily related to the **storytelling** stage from the **data science workflow**, as explained in @sec-ds-workflow-storytelling. More specifically, the `r colourize("measures of central tendency", "blue")` can be communicated along with our estimated regression `r colourize("parameters", "blue")`, given that these metrics are usually conditioned to our `r colourize("regressors", "blue")` of interest within our modelling framework.

::: {#Headsup-sample .Heads-up}
::::{.Heads-up-header}
Heads-up on parameter estimation!
::::
::::{.Heads-up-container}
Just as in the case of regression `r colourize("parameters", "blue")`, the `r colourize("measures of central tendency and uncertainty", "blue")` are also `r colourize("parameters", "blue")` (more specifically, belonging to a given `r colourize("probability distribution", "blue")`) that can be estimated via an observed `r colourize("random sample", "blue")` through methods such as `r colourize("maximum likelihood estimation (MLE)", "blue")`. You can check further details on the `r colourize("MLE", "blue")` fundamentals in @sec-mle.
::::
:::

There are different `r colourize("measures of central tendency and uncertainty", "blue")`. Nevertheless, we will only focus on the `r colourize("expected value (i.e., the mean)", "blue")` and the `r colourize("variance", "blue")`. Hence, let $Y$ be a `r colourize("random variable", "blue")` with possible values $y \in \mathcal{Y}$. Then, let $g(Y)$ be a general function of $Y$. By the `r colourize("law of the unconscious statistician (LOTUS)", "blue")`, we have that:

$$
\mathbb{E} \left[ g(Y) \right] = \displaystyle \sum_{\mathcal{Y}} g(Y) \cdot P(Y = y),
$${#eq-app-expected-value-discrete-function}

where $P(Y = y)$ is the `r colourize("probability mass function (PMF)", "blue")` of $Y$.

If $Y$ is a `r colourize("continuous random variable", "blue")`, by the `r colourize("LOTUS", "blue")`, the `r colourize("mean", "blue")` of function $g(Y)$ is defined as

$$
\mathbb{E} \left[ g(Y) \right] = \displaystyle \int_{\mathcal{Y}} g(Y) \cdot f_Y(y) \text{d}y,
$${#eq-app-expected-value-continuous-function}

where $f_Y(y)$ is the `r colourize("probability density function (PDF)", "blue")` of $Y$.

Note that when $g(Y) = y$ in the `r colourize("discrete", "blue")` case, @eq-app-expected-value-discrete-function becomes

$$
\mathbb{E} \left[ Y \right] = \displaystyle \sum_{\mathcal{Y}} y \cdot P(Y = y).
$${#eq-app-expected-value-discrete}

On the other hand, when $g(Y) = y$ in the `r colourize("continuous", "blue")` case, @eq-app-expected-value-continuous-function becomes

$$
\mathbb{E} \left[ Y \right] = \displaystyle \int_{\mathcal{Y}} y \cdot f_Y(y) \text{d}y.
$${#eq-app-expected-value-continuous}

Either for a `r colourize("discrete", "blue")` or `r colourize("continuous", "blue")` case, the `r colourize("variance", "blue")` is defined as 

$$
\text{Var}(Y) = \mathbb{E}\{[Y - \mathbb{E}(Y)]^2\}.
$$

After applying some algebraic rearrangements and `r colourize("expected value", "blue")` properties, the expression above is equivalent to:

::: {.proof}
\
$$
\begin{align*}
\text{Var}(Y) &= \mathbb{E} \left\{ \left[ Y - \mathbb{E}(Y) \right]^2 \right\} \\
&= \mathbb{E} \left\{ Y^2 - 2Y \mathbb{E}(Y) + \left[ \mathbb{E}(Y) \right]^2 \right\} \\
&= \mathbb{E}(Y^2) - \mathbb{E} \left[ 2 Y \mathbb{E}(Y) \right] + \mathbb{E} \left[ \mathbb{E}(Y) \right]^2 \\
& \qquad \text{distributing the expected value operator}\\
&= \mathbb{E}(Y^2) - 2 \mathbb{E} \left[ Y \mathbb{E}(Y) \right] + \mathbb{E} \left[ \mathbb{E}(Y) \right]^2 \\
& \qquad \text{since $2$ is a constant}\\
&= \mathbb{E}(Y^2) - 2 \mathbb{E}(Y) \mathbb{E} \left( Y \right) + \left[ \mathbb{E}(Y) \right]^2 \\
& \qquad \text{since $\mathbb{E}(Y)$ is a constant}\\
&= \mathbb{E}(Y^2) - 2 \left[ \mathbb{E}(Y) \right]^2 + \left[ \mathbb{E}(Y) \right]^2 \\
&= \mathbb{E}(Y^2) - [\mathbb{E}(Y)]^2,  \qquad \qquad \qquad \qquad \qquad \square
\end{align*}
$${#eq-app-variance}
:::

where $\mathbb{E}(Y^2)$ can be computed either via @eq-app-expected-value-discrete-function or @eq-app-expected-value-continuous-function depending on the nature of $Y$ with $g(Y) = y^2$.

Now, for each case depicted as a cloud in @fig-app-distributions, subsequent sections in this appendix will show elaborate on why each corresponding `r colourize("PMF", "blue")` or `r colourize("PDF", "blue")` (depending on the type of `r colourize("random variable", "blue")`, $Y$) is a proper probability distribution (i.e., all the standalone `r colourize("probabilities", "blue")` over the support of $Y$ add up to one) along with the respective proofs of their corresponding `r colourize("means", "blue")` and `r colourize("variances", "blue")`.

# Discrete Random Variables

Let us recall what a `r colourize("discrete random variable", "blue")` is. This type of variable is defined to take on a set of countable values. In other words, these values belong to a finite set. @fig-app-distributions delves into the following specific `r colourize("probability distributions", "blue")`:

- **Bernoulli.** A `r colourize("random variable", "blue")` $Y$ that can take on the values of $0$ (i.e., a failure) or $1$ (i.e., a success) where the distributional `r colourize("parameter", "blue")` is the `r colourize("probability", "blue")` of success $\pi \in [0, 1]$. Note $Y$ is said to be **binary** with a support of $y \in \{ 0, 1 \}$.
- **Binomial.** A `r colourize("random variable", "blue")` $Y$ that defines the number of `r colourize("independent", "blue")` **Bernoulli trials** in which we observe a success out of $n$ trials. Its distributional `r colourize("parameters", "blue")` are the `r colourize("probability", "blue")` of success $\pi \in [0, 1]$of each Bernoulli trial along with the total number of trials $n \in \mathbb{N}$. Note $Y$ is said to be of **count** type with a support of $y \in \{ 0, 1, \dots, n \}$ successes.
- **Negative Binomial.** A `r colourize("random variable", "blue")` $Y$ that defines the number of `r colourize("independent", "blue")` **Bernoulli trials** in which we observe a failure before experiencing $k$ successes. Its distributional `r colourize("parameters", "blue")` are the `r colourize("probability", "blue")` of success $\pi \in [0, 1]$ of each Bernoulli trial along with the number of $k \in \{ 0, 1, 2 \dots\}$ successes. Note $Y$ is said to be of **count** type with a support of $y \in \{ 0, 1, 2 \dots\}$ failures.
- **Poisson.** A `r colourize("random variable", "blue")` $Y$ that defines the number of events occurring in a predetermined interval of time or space. Its distributional `r colourize("parameter", "blue")` is the `r colourize("average", "blue")` rate $\lambda \in (0, \infty)$ of events per this predetermined interval of time or space. Note $Y$ is said to be of **count** type with a support of $y \in \{ 0, 1, 2, \dots \}$ events.

@tbl-distributions-discrete outlines the `r colourize("parameter(s)", "blue")`, support, `r colourize("mean", "blue")`, and `r colourize("variance", "blue")` for each `r colourize("discrete probability distribution", "blue")` utilized to model the `r colourize("target", "magenta")` $Y$ in a specific regression tool explained in this book.

| Distribution |  Parameter(s) | Support |  Mean  |  Variance  | 
|:----:|:------:|:-----:|:-----:|:-----:|
| Bernoulli | $Y \sim \text{Bern}(\pi)$ with probability of success $\pi \in [0, 1]$ | $y \in \{ 0, 1 \}$ | $\pi$ | $\pi (1 - \pi)$ | 
| Binomial | $Y \sim \text{Bin}(n, \pi)$ with number of trials $n \in \mathbb{N}$ and probability of success $\pi \in [0, 1]$ | $y \in \{ 0, 1, \dots, n \}$| $n \pi$ | $n \pi (1 - \pi)$ |
| Negative Binomial | $Y \sim \text{NegBin}(k, \pi)$ with number of successes $k \in \{ 0, 1, 2 \dots\}$ and probability of success $\pi \in [0, 1]$| $y \in \{ 0, 1, 2 \dots\}$ | $\frac{k (1 - \pi)}{\pi}$ | $\frac{k (1 - \pi)}{\pi^2}$ |
| Poisson | $Y \sim \text{Pois}(\lambda)$ with continuous average rate $\lambda \in (0, \infty)$ | $y \in \{ 0, 1, 2 \dots\}$ | $\lambda$ | $\lambda$ | 

: Univariate discrete probability distributions for a random variable $Y$; including parameter(s), support, mean, and variance. {#tbl-distributions-discrete .striped .hover}

## Bernoulli

Let $Y$ be a `r colourize("discrete random variable", "blue")` that is part of a random process or system. $Y$ can only take on the following values:

$$
Y =
\begin{cases}
1 \; \; \; \; \text{if there is a success},\\
0 \; \; \; \; \mbox{otherwise}.
\end{cases}
$${#eq-app-bernoulli-support}

Note that the support of $Y$ in @eq-app-bernoulli-support makes it binary with these outcomes: $1$ for *success* and $0$ for *failure*.  Then, $Y$ is said to have a **Bernoulli distribution** with `r colourize("parameter", "blue")` $\pi$:

$$
Y \sim \text{Bern}(\pi).
$$

### Probability Mass Function

The `r colourize("probability mass function (PMF)", "blue")` of $Y$ is the following:

$$
P \left( Y = y \mid \pi \right) = \pi^y (1 - \pi)^{1 - y} \quad \text{for $y \in \{ 0, 1 \}$.}
$${#eq-app-bernoulli-pmf}

`r colourize("Parameter", "blue")` $\pi \in [0, 1]$ refers to the `r colourize("probability", "blue")` of success. We can verify @eq-app-bernoulli-pmf is a proper `r colourize("probability distribution", "blue")` (i.e., all the standalone `r colourize("probabilities", "blue")` over the support of $Y$ add up to one) given that:

::: {.proof}
$$
\begin{align*} 
\sum_{y = 0}^1 P \left( Y = y \mid \pi \right) &=  \sum_{y = 0}^1 \pi^y (1 - \pi)^{1 - y}  \\
&= \underbrace{\pi^0}_{1} (1 - \pi) + \pi \underbrace{(1 - \pi)^{0}}_{1} \\
&= (1 - \pi) + \pi \\
&= 1. \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$

> **Indeed, the Bernoulli `r colourize("PMF", "blue")` is a proper `r colourize("probability distribution", "blue")`!**

:::

### Expected Value

Via @eq-app-expected-value-discrete, the `r colourize("expected value", "blue")` or `r colourize("mean", "blue")` of a Bernoulli-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*} 
\mathbb{E}(Y) &= \sum_{y = 0}^1 y P \left( Y = y \mid \pi \right) \\
&= \sum_{y = 0}^1 y \left[ \pi^y (1 - \pi)^{1 - y} \right] \\
&= \underbrace{(0) \left[ \pi^0 (1 - \pi) \right]}_{0} + (1) \left[ \pi (1 - \pi)^{0} \right] \\
&= 0 + \pi \\
&= \pi. \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$
:::

### Variance

Via @eq-app-variance and the @eq-app-expected-value-discrete of a `r colourize("discrete expected value", "blue")`, the `r colourize("variance", "blue")` of a Bernoulli-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E}(Y^2) - \left[ \mathbb{E}(Y)\right]^2 \\
&= \mathbb{E}(Y^2) - \pi^2 \qquad \text{since $\mathbb{E}(Y) = \pi$} \\
&= \sum_{y = 0}^1 y^2 P \left( Y = y \mid \pi \right) - \pi^2 \\
&= \left\{ \underbrace{(0^2) \left[ \pi^0 (1 - \pi) \right]}_{0} + \underbrace{(1^2) \left[ \pi (1 - \pi)^{0} \right]}_{\pi} \right\} - \pi^2 \\
&= (0 + \pi) - \pi^2 \\
&= \pi - \pi^2 \\
&= \pi (1 - \pi). \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$
:::

## Binomial

Suppose you execute $n$ `r colourize("independent", "blue")` **Bernoulli trials**, each one with a `r colourize("probability", "blue")` of success $\pi$. Let $Y$ be the number of successes obtained within these $n$ Bernoulli trials. Then, $Y$ is said to have a **Binomial distribution** with `r colourize("parameters", "blue")` $n$ and $\pi$:

$$
Y \sim \text{Bin}(n, \pi).
$$

### Probability Mass Function

The `r colourize("PMF", "blue")` of $Y$ is the following:

$$
\begin{align*}
P \left( Y = y \mid n, \pi \right) &= {n \choose y} \pi^y (1 - \pi)^{n - y} \\
& \qquad \qquad \qquad \text{for $y \in \{ 0, 1, \dots, n \}$.}
\end{align*}
$${#eq-app-binomial-pmf}

`r colourize("Parameter", "blue")` $\pi \in [0, 1]$ refers to the `r colourize("probability", "blue")` of success of each Bernoulli trial and $n \in \mathbb{N}$ to the number of trials. On the other hand, the term ${n \choose y}$ indicates the total number of possible combinations for $y$ successes out of our $n$ trials:

$$
{n \choose y} = \frac{n!}{y!(n - y)!}.
$${#eq-app-combination}

> **How can we verify that @eq-app-binomial-pmf is a proper `r colourize("PMF", "blue")` (i.e., all the standalone `r colourize("probabilities", "blue")` over the support of $Y$ add up to one)?**

To elaborate on this, we need to use a handy mathematical result called the **binomial theorem**.

::: {#thm-binomial-theorem}

# Binomial Theorem

This theorem is associated to the **Pascal's identity**, and it defines the pattern of coefficients in the expansion of a polynomial in the form $(u + v)^m$. More specifically, the binomial theorem indicates that if $m$ is a non-negative integer, then the polynomial $(u + v)^m$ can be expanded via the following series:

$$
\begin{align*}
(u + v)^m &= u^m + {m \choose 1} u^{m - 1} v + {m \choose 2} u^{m - 2} v^2 + \dots + \\
& \qquad {m \choose r} u^{m - r} v^r + \dots + \\
& \qquad {m \choose m - 1} u v^{m - 1} + v^m \\
&= \underbrace{{m \choose 0}}_1 u^m + {m \choose 1} u^{m - 1} v + {m \choose 2} u^{m - 2} v^2 + \dots + \\
& \qquad {m \choose r} u^{m - r} v^r + \dots + \\
& \qquad {m \choose m - 1} u v^{m - 1} + \underbrace{{m \choose m}}_1 v^m \\
&= \sum_{i = 0}^m {m \choose i} u^{m - i} v^i.
\end{align*}
$${#eq-app-binomial-theorem}
:::

::: {#Tip-pascal-triangle .Tip}
::::{.Tip-header}
Tip on the binomial theorem and Pascal's identity
::::
::::{.Tip-container}
Let us dig into the proof of the binomial theorem from @eq-app-binomial-theorem. This proof will require another important result called the **Pascal's identity**. This identity states that for any integers $m$ and $k$, with $k \in \{ 1, \dots, m \}$, it follows that:\

::: {.proof}
$$
\begin{align*}
{m \choose k - 1} + {m \choose k} &= \left[ \frac{m!}{(k - 1)! (m - k + 1)!} \right] \\ 
& \qquad + \left[ \frac{m!}{k! (m - k)!} \right] \\
&= m! \biggl\{ \left[ \frac{1}{(k - 1)! (m - k + 1)!} \right] + \\
& \qquad \left[ \frac{1}{k! (m - k)!} \right] \biggl\} \\
&= m! \Biggl\{ \Biggr[ \frac{k}{\underbrace{k (k - 1)!}_{k!} (m - k + 1)!} \Biggr] + \\
& \qquad \Biggr[ \frac{m - k + 1}{k! \underbrace{(m - k + 1)(m - k)!}_{(m - k + 1)!}} \Biggr] \Biggl\}  \\
&= m! \left[ \frac{k + m - k + 1}{k! (m - k + 1)!} \right] \\
&= m! \left[ \frac{m + 1}{k! (m - k + 1)!} \right] \\
&= \frac{(m + 1)!}{k! (m + 1 - k)!} \\
&= {m + 1 \choose k }. \qquad \qquad \qquad \qquad \square
\end{align*}
$${#eq-pascal-dentity}
:::

::: {.proof}
\
Now, we will use **mathematical induction** to prove the binomial theorem from @eq-app-binomial-theorem. Firstly, on the left-hand side of the theorem, note that when $m = 0$ we have:\

$$
(u + v)^0 = 1.
$$

Now, when $m = 0$, for the right-hand side of this equation, we have that\

$$
\sum_{i = 0}^m {m \choose i} u^{m - i} v^i  = \sum_{i = 0}^0 {0 \choose i} u^i v^{i} = {0 \choose 0} u^0 v^0 = 1.
$$

Hence, the binomial theorem holds when $m = 0$. This is what we call the **base case** in mathematical induction.\

That said, let us proceed with the **inductive hypothesis**. We aim to prove that the binomial theorem\

$$
\begin{align*}
(u + v)^j &= u^j + {j \choose 1} u^{j - 1} v + {j \choose 2} u^{j - 2} v^2 + \dots + \\
& \qquad {j \choose r} u^{j - r} v^r + \dots + \\
& \qquad {j \choose j - 1} u v^{j - 1} + v^j \\
&= \underbrace{{j \choose 0}}_1 u^j + {j \choose 1} u^{j - 1} v + {j \choose 2} u^{j - 2} v^2 + \dots + \\
& \qquad {j \choose r} u^{j - r} v^r + \dots + \\
& \qquad {j \choose j - 1} u v^{j - 1} + \underbrace{{j \choose j}}_1 v^j \\
&= \sum_{i = 0}^j {j \choose i} u^{j - i} v^i
\end{align*}
$${#eq-inductive-hyp}

holds when integer $j \geq 1$. This is our inductive hypothesis.

Then, we pave the way to the **inductive step**. Let us consider the following expansion:\

$$
\begin{align*}
(u + v)^{j + 1} &= (u + v) (u + v)^j \\
&= (u + v) \times \\
& \qquad \bigg[ u^j + {j \choose 1} u^{j - 1} v + {j \choose 2} u^{j - 2} v^2 + \dots + \\
& \qquad {j \choose r} u^{j - r} v^r + \dots + {j \choose j - 1} u v^{j - 1} + v^j \bigg] \\
&= \bigg[u^{j + 1} + {j \choose 1} u^j v + {j \choose 2} u^{j - 1} v^2 + \dots + \\
& \qquad {j \choose r} u^{j - r + 1} v^r + \dots + \\
& \qquad {j \choose j - 1} u^2 v^{j - 1} + u v^j \bigg] + \\
& \qquad \bigg[ u^j v + {j \choose 1} u^{j - 1} v^2 + {j \choose 2} u^{j - 2} v^3 + \dots + \\
& \qquad {j \choose r} u^{j - r} v^{r + 1} + \dots + \\
& \qquad {j \choose j - 1} u v^j + {j \choose j} v^{j + 1} \bigg] \\
&= u^{j + 1} + \left[ {j \choose 0} + {j \choose 1} \right] u^j v + \\
& \qquad \left[ {j \choose 1} + {j \choose 2} \right] u^{j - 1} v^2 + \dots + \\
& \qquad \left[ {j \choose r - 1} + {j \choose r} \right] u^{j - r + 1} v^r + \dots + \\
& \qquad \left[ {j \choose j - 1} + {j \choose j} \right] u v^j + v^{j + 1}.
\end{align*} 
$${#eq-binomial-inductive-step}

Let us plug in the Pascal's identity from @eq-pascal-dentity into @eq-binomial-inductive-step:\

$$
\begin{align*}
(u + v)^{j + 1} &= u^{j + 1} + {j + 1 \choose 1} u^j v + \\
& \qquad {j + 1 \choose 2} u^{j - 1} v^2 + \dots + \\
& \qquad {j + 1 \choose r} u^{j - r + 1} v^r + \dots + \\
& \qquad {j + 1 \choose j} u v^j + v^{j + 1} \\
&= \underbrace{{j + 1 \choose 0}}_1 u^{j + 1} + {j + 1 \choose 1} u^j v + \\
& \qquad {j + 1 \choose 2} u^{j - 1} v^2 + \dots + \\ 
& \qquad {j + 1 \choose r} u^{j - r + 1} v^r + \dots + \\
& \qquad {j + 1 \choose j} u v^j + \underbrace{{j + 1 \choose j + 1}}_1 v^{j + 1} \\
&= \sum_{i = 0}^{j + 1} {j + 1 \choose i} u^{j + 1 - i} v^i. \qquad \quad \square
\end{align*}
$${#eq-proof-inductive-hyp}

Note that the result for $j$ in @eq-inductive-hyp also holds for $j + 1$ in @eq-proof-inductive-hyp. Therefore, by induction, the binomial theorem from @eq-app-binomial-theorem is true for all positive integers $m$.
:::

::::
:::

After the above fruitful digression on the binomial theorem, let us use it to show that our Binomial `r colourize("PMF", "blue")` in @eq-app-binomial-pmf actually adds up to one all over the support of the `r colourize("random variable", "blue")`:

::: {.proof}
$$
\begin{align*} 
\sum_{y = 0}^n P \left( Y = y \mid n, \pi \right) &= \sum_{y = 0}^n {n \choose y} \pi^y (1 - \pi)^{n - y} \\
&= \sum_{y = 0}^n {n \choose y} (1 - \pi)^{n - y} \pi^y \\
& \quad \qquad \text{rearranging factors.}
\end{align*}
$$

Now, by using the binomial theorem in @eq-app-binomial-theorem, let:

$$
\begin{gather*}
m  = n\\
i = y \\
u = 1 - \pi \\
v = \pi.
\end{gather*}
$$

The above arrangement yields the following result:

$$
\begin{align*}
\sum_{y = 0}^n P \left( Y = y \mid n, \pi \right) &= (1 - \pi + \pi)^n \\
&= 1^n = 1. \qquad \square
\end{align*}
$${#eq-proof-binomial-PMF-adds-to-1}

> **Indeed, the Binomial `r colourize("PMF", "blue")` is a proper `r colourize("probability distribution", "blue")`!**
:::

### Expected Value

Via @eq-app-expected-value-discrete, the `r colourize("expected value", "blue")` or `r colourize("mean", "blue")` of a Binomial-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*} 
\mathbb{E}(Y) &= \sum_{y = 0}^n y P \left( Y = y \mid n, \pi \right) \\
&= \sum_{y = 1}^n y P \left( Y = y \mid n, \pi \right) \\
& \quad \qquad \text{for $y = 0$, the addend is equal to zero} \\
&= \sum_{y = 1}^n y \left[ {n \choose y} \pi^y (1 - \pi)^{n - y} \right] \\
&= \sum_{y = 1}^n y \left[ \frac{n!}{y! (n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
&= \sum_{y = 1}^n \left[ \frac{y n!}{y (y - 1)!(n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
& \quad \qquad \text{in the denominator, $y! = y (y - 1)!$}\\
&= \sum_{y = 1}^n \left[ \frac{n (n - 1)!}{(y - 1)!(n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\ 
& \quad \qquad \text{in the numerator, $n! = n (n - 1)!$} \\
&= \sum_{y = 1}^n \left[ \frac{n (n - 1)!}{(y - 1)!(n - y)!} \pi^{y + 1 - 1} (1 - \pi)^{n - y} \right] \\
& \quad \qquad \text{note $\pi^y = \pi^{y + 1 - 1}$} \\
&= n \sum_{y = 1}^n \left[ \frac{(n - 1)!}{(y - 1)!(n - y)!} \pi \pi^{y - 1} (1 - \pi)^{n - y} \right] \\
& \quad \qquad \text{rearranging terms} \\
&= n \pi \sum_{y = 1}^n \left[ \frac{(n - 1)!}{(y - 1)!(n - y)!} \pi^{y - 1} (1 - \pi)^{n - y} \right].
\end{align*}
$${#eq-proof-binomial-mean-1}

Now, let us make the following variable rearrangement:

$$
\begin{gather*}
m = n - 1 \\
z = y - 1 \\
m - z = n - y.
\end{gather*}
$$

Going back to @eq-proof-binomial-mean-1, **and applying our above variable rearrangement within the summation**, we have:

$$
\begin{align*} 
\mathbb{E}(Y) &= n \pi \sum_{z = 0}^m \left[ \frac{m!}{z!(m - z)!} \pi^{z} (1 - \pi)^{m - z} \right] \\
&= n \pi \sum_{z = 0}^m \left[ {m \choose z}\pi^{z} (1 - \pi)^{m - z} \right].
\end{align*}
$${#eq-proof-binomial-mean-2}

Note that, in the summation of @eq-proof-binomial-mean-2, we encounter the `r colourize("PMF", "blue")` of a `r colourize("random variable", "blue")` $Z$ as follows:

$$
Z \sim \text{Bin}(m, \pi).
$$

Since the summation, where this Binomial `r colourize("PMF", "blue")` of $Z$ is depicted, goes from $z = 0$ to $m$, we can apply our result from @eq-proof-binomial-PMF-adds-to-1:

$$
\begin{align*} 
\mathbb{E}(Y) &= n \pi \underbrace{\sum_{z = 0}^m \left[ {m \choose z}\pi^{z} (1 - \pi)^{m - z} \right]}_{1} \\
&= n \pi. \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$
:::

### Variance

Via @eq-app-variance and the @eq-app-expected-value-discrete of a `r colourize("discrete expected value", "blue")`, the `r colourize("variance", "blue")` of a Binomial-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E}(Y^2) - \left[ \mathbb{E}(Y)\right]^2 \\
&= \mathbb{E}(Y^2) - (n \pi)^2 \qquad \text{since $\mathbb{E}(Y) = n \pi$.}
\end{align*}
$${#eq-proof-binomial-variance-1}

Unlike the Bernoulli `r colourize("random variable", "blue")`, finding $\mathbb{E}(Y^2)$ is not quite straightforward. We need to play around with its `r colourize("expected value", "blue")` expression as follows:

$$
\begin{align*}
\mathbb{E}(Y^2) &= \mathbb{E} \left[ Y (Y - 1) \right] + \mathbb{E}(Y) \\
&= \mathbb{E} \left[ Y (Y - 1) \right] + n \pi \qquad \text{since $\mathbb{E}(Y) = n \pi$.}
\end{align*}
$${#eq-proof-binomial-variance-2}

Now, to find $\mathbb{E} \left[ Y (Y - 1) \right]$, we make the following derivation via @eq-app-expected-value-discrete-function when $g(Y) = y (y - 1)$:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= \sum_{y = 0}^n y (y - 1) P \left( Y = y \mid n, \pi \right) \\
&= \sum_{y = 2}^n y (y - 1) P \left( Y = y \mid n, \pi \right) \\
& \quad \qquad \text{for $y = \{0, 1\}$,} \\ 
& \quad \qquad \text{the addends are equal to zero} \\
&= \sum_{y = 2}^n y (y - 1) \left[ {n \choose y} \pi^y (1 - \pi)^{n - y} \right] \\
&= \sum_{y = 2}^n y (y - 1) \left[ \frac{n!}{y! (n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
&= \sum_{y = 2}^n \left[ \frac{y (y - 1) n!}{y (y - 1) (y - 2)! (n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\
& \quad \qquad \text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\
&= \sum_{y = 2}^n \left[ \frac{n (n - 1) (n - 2)!}{(y - 2)! (n - y)!} \pi^y (1 - \pi)^{n - y} \right] \\ 
& \quad \qquad \text{in the numerator, $n! = n (n - 1) (n - 2)!$} \\
&= \sum_{y = 2}^n \left[ \frac{n (n - 1) (n - 2)!}{(y - 2)! (n - y)!} \pi^{y + 2 - 2} (1 - \pi)^{n - y} \right] \\
& \quad \qquad \text{note $\pi^y = \pi^{y + 2 - 2}$} \\
&= n (n - 1) \times \\
& \qquad \sum_{y = 2}^n \left[ \frac{(n - 2)!}{(y - 2)! (n - y)!} \pi^2 \pi^{y - 2} (1 - \pi)^{n - y} \right] \\
& \qquad \qquad \text{rearranging terms} \\
&= n (n - 1) \pi^2 \times \\
& \qquad \sum_{y = 2}^n \left[ \frac{(n - 2)!}{(y - 2)! (n - y)!} \pi^{y - 2} (1 - \pi)^{n - y} \right] \\
& \qquad \qquad \text{rearranging terms.}
\end{align*}
$${#eq-proof-binomial-variance-3}

Then, we make the following variable rearrangement:

$$
\begin{gather*}
m = n - 2 \\
z = y - 2 \\
m - z = n - y.
\end{gather*}
$$

Going back to @eq-proof-binomial-variance-3, **and applying our above variable rearrangement within the summation**, we have:

$$
\begin{align*} 
\mathbb{E} \left[ Y (Y - 1) \right] &= n (n - 1) \pi^2 \sum_{z = 0}^m \left[ \frac{m!}{z! (m - z)!} \pi^{z} (1 - \pi)^{m - z} \right] \\
&= n (n - 1) \pi^2 \sum_{z = 0}^m \left[ {m \choose z} \pi^{z} (1 - \pi)^{m - z} \right].
\end{align*}
$${#eq-proof-binomial-variance-4}

Note that, in the summation of @eq-proof-binomial-variance-4, we encounter the `r colourize("PMF", "blue")` of a `r colourize("random variable", "blue")` $Z$ as follows:

$$
Z \sim \text{Bin}(m, \pi).
$$

Since the summation, where this Binomial `r colourize("PMF", "blue")` of $Z$ is depicted, goes from $z = 0$ to $m,$ we can apply our result from @eq-proof-binomial-PMF-adds-to-1:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= n (n - 1) \pi^2 \underbrace{\sum_{z = 0}^m \left[ {m \choose z} \pi^{z} (1 - \pi)^{m - z} \right]}_{1} \\
&= n (n - 1) \pi^2.
\end{align*}
$$

Let us go back to @eq-proof-binomial-variance-2 and plug in the above result:

$$
\begin{align*}
\mathbb{E}(Y^2) &= \mathbb{E} \left[ Y (Y - 1) \right] + n \pi \\
&= n (n - 1) \pi^2 + n \pi. \\
\end{align*}
$$

Finally, we plug in $\mathbb{E}(Y^2)$ in @eq-proof-binomial-variance-1:

$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E}(Y^2) - (n \pi)^2 \\
&= n (n - 1) \pi^2 + n \pi - n^2 \pi^2 \\
&= n^2 \pi^2 - n \pi^2 + n \pi - n^2 \pi^2 \\
&= n \pi - n \pi^2 \\
&= n \pi (1 - \pi). \qquad \qquad \qquad \square
\end{align*}
$$
:::

## Negative Binomial

Suppose you execute a series of `r colourize("independent", "blue")` **Bernoulli trials**, each one with a `r colourize("probability", "blue")` of success $\pi$. Let $Y$ be the number of failures in this series of Bernoulli trials you obtain before experiencing $k$ successes. Therefore, $Y$ is said to have a **Negative Binomial distribution** with `r colourize("parameters", "blue")` $k$ and $\pi$:

$$
Y \sim \text{NegBin}(k, \pi).
$$

### Probability Mass Function

The `r colourize("PMF", "blue")` of $Y$ is the following:

$$
\begin{align*}
P \left( Y = y \mid k, \pi \right) &= {k + y - 1 \choose y} \pi^k (1 - \pi)^y \\
& \qquad \qquad \qquad \quad \text{for $y \in \{ 0, 1, \dots \}$.}
\end{align*}
$${#eq-app-neg-binomial-pmf}

`r colourize("Parameter", "blue")` $\pi \in [0, 1]$ refers to the `r colourize("probability", "blue")` of success of each Bernoulli trial, whereas $k$ refers to the number of successes.

::: {#Headsup-sample .Heads-up}
::::{.Heads-up-header}
Heads-up on an alternative Negative Binomial PMF!
::::
::::{.Heads-up-container}
There is an alternative parametrization to define a Negative Binomial distribution in which we have a `r colourize("random variable", "blue")` $Z$ defined as the **total number of Bernoulli trials** (i.e., $k$ successes plus the $Y$ failures depicted in @eq-app-neg-binomial-pmf):

$$
Z = Y + k.
$$

This alternative parametrization of the Negative Binomial distribution yields the following `r colourize("PMF", "blue")`:

$$
\begin{align*}
P \left( Z = z \mid k, \pi \right) &= {z - 1 \choose k - 1} \pi^k (1 - \pi)^{z - k} \\
& \qquad \qquad \qquad \text{for $z \in \{ k, k + 1, \dots \}$.}
\end{align*}
$$

Nevertheless, we will not dig into this version of the Negative Binomial distribution since @sec-negative-binomial delves into a modelling estimation via a joint `r colourize("PMF", "blue")` of the `r colourize("training set", "magenta")` involving @eq-app-neg-binomial-pmf.
::::
:::

> **How can we verify that @eq-app-neg-binomial-pmf is a proper `r colourize("PMF", "blue")` (i.e., all the standalone `r colourize("probabilities", "blue")` over the support of $Y$ add up to one)?**

::: {.proof}
\
Let us manipulate the factor involving the number of combinations corresponding to how many different possible subsets of size $y$ can be made from the larger set of size $k + y - 1$:

$$
\begin{align*}
{k + y - 1 \choose y} &= \frac{(k + y - 1)!}{(k + y - 1 - y)! y !} \\
&= \frac{(k + y - 1)!}{(k - 1)! y!} \\
&= \frac{(k + y - 1) (k + y - 2) \cdots (k + 1) (k) (k - 1)!}{(k - 1)! y!} \\
&= \frac{(\overbrace{k + y - 1) (k + y - 2) \cdots (k + 1) k}^{\text{we have $y$ factors}}}{y!} \\
&= (- 1)^y \frac{\overbrace{(-k - y + 1) (-k - y + 2) \cdots (-k - 1) (-k)}^{\text{multiplying each factor times $-1$}}}{y!} \\
&= (- 1)^y \frac{\overbrace{(-k) (-k - 1) \cdots (-k - y + 2) (-k - y + 1)}^{\text{rearranging factors}}}{y!} \\
&= (- 1)^y \frac{(-k) (-k - 1) \cdots (-k - y + 2) (-k - y + 1)}{y!} \times \\
& \qquad \frac{(-k - y) (-k - y - 1) \cdots (1)}{(-k - y) (-k - y - 1) \cdots (1)} \\
&= (- 1)^y \frac{(-k) (-k - 1) \cdots (-k - y + 2) (-k - y + 1)}{y!} \times \\
& \qquad \frac{(-k - y) (-k - y - 1) \cdots (1)}{(-k - y)!}.
\end{align*}
$$


In the equation above, note that there are still several factors in the numerator, which can be summarized using a factorial as follows:

$$
\begin{align*}
(-k)! &= (-k) (-k - 1) \cdots (-k - y + 2) (-k - y + 1) \times \\
& \quad \qquad (-k - y) (-k - y - 1) \cdots (1).
\end{align*}
$$

Therefore:

$$
\begin{align*}
{k + y - 1 \choose y} &= (- 1)^y \frac{(-k)!}{(-k - y)! y!}\\
&= (- 1)^y {-k \choose y}.
\end{align*}
$$

Now, let us begin with the summation involving the Negative Binomial `r colourize("PMF", "blue")` depicted in  @eq-app-neg-binomial-pmf from $0$ to $\infty$:

$$
\begin{align*}
\sum_{y = 0}^{\infty} P \left( Y = y \mid k, \pi \right) &= \sum_{y = 0}^{\infty} {k + y - 1 \choose y} \pi^k (1 - \pi)^y \\
&= \sum_{y = 0}^{\infty} (- 1)^y {-k \choose y} \pi^k (1 - \pi)^y \\
&= \pi^k \sum_{y = 0}^{\infty} (- 1)^y {-k \choose y} (1 - \pi)^y \\
&= \pi^k \sum_{y = 0}^{\infty} {-k \choose y} (-1 + \pi)^y.
\end{align*}
$${#eq-proof-neg-binomial-PMF-adds-to-1-a}

On the right-hand side of @eq-proof-neg-binomial-PMF-adds-to-1-a we will add the following factor:

$$
(1)^{-k - y} = 1.
$$

Thus:

$$
\begin{align*}
\sum_{y = 0}^{\infty} P \left( Y = y \mid k, \pi \right) &= \pi^k \sum_{y = 0}^{\infty} {-k \choose y} (1)^{-k - y} (-1 + \pi)^y.
\end{align*}
$${#eq-proof-neg-binomial-PMF-adds-to-1-b}

Now, by using the **binomial theorem** in @eq-app-binomial-theorem, let:

$$
\begin{gather*}
m  = -k\\
i = y \\
u = 1 \\
v = -1 + \pi.
\end{gather*}
$$
The above arrangement yields the following result in @eq-proof-neg-binomial-PMF-adds-to-1-b:

$$
\begin{align*}
\sum_{y = 0}^{\infty} P \left( Y = y \mid k, \pi \right) &=  \pi^k (1 - 1 + \pi)^{-k} \\
&= \pi^k (\pi) ^{-k} \\
&= \pi^0 \\
&= 1. \qquad \qquad \qquad \square
\end{align*}
$${#eq-proof-neg-binomial-PMF-adds-to-1-c}

> **Indeed, the Negative Binomial `r colourize("PMF", "blue")` is a proper `r colourize("probability distribution", "blue")`!**
:::

### Expected Value

Via @eq-app-expected-value-discrete, the `r colourize("expected value", "blue")` or `r colourize("mean", "blue")` of a Negative Binomial-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*} 
\mathbb{E}(Y) &= \sum_{y = 0}^{\infty} y P \left( Y = y \mid k, \pi \right) \\
&= \sum_{y = 1}^{\infty} y P \left( Y = y \mid k, \pi \right) \\
& \quad \qquad \text{for $y = 0$, the addend is equal to zero} \\
&= \sum_{y = 1}^{\infty} y \left[ {k + y - 1 \choose y} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 1}^{\infty} y \left[ \frac{(k + y - 1)!}{y! (k + y - 1 - y)!} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 1}^{\infty} y \left[ \frac{(k + y - 1)!}{y! (k - 1)!} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 1}^{\infty} y \Bigg[ \frac{(k + y - 1)!}{y (y - 1)! \underbrace{\left( \frac{k!}{k} \right)}_{(k - 1)!}} \pi^k (1 - \pi)^y \Bigg] \\
&= \sum_{y = 1}^{\infty} k \left[ \frac{(k + y - 1)!}{k! (y - 1)!} \pi^k (1 - \pi)^y \right] \\
&= k \sum_{y = 1}^{\infty} \left[ {k + y - 1 \choose y - 1} \pi^k (1 - \pi)^y \right] \\
&= k \sum_{y = 1}^{\infty} \left[ {k + y - 1 \choose y - 1} \pi^{k + 1 - 1} (1 - \pi)^{y + 1 - 1} \right] \\
& \quad \qquad \text{note $\pi^k = \pi^{k + 1 - 1}$ and $(1 - \pi)^y = (1 - \pi)^{y + 1 - 1}$} \\
&= \frac{k (1 - \pi)}{\pi} \sum_{y = 1}^{\infty} \left[ {k + y - 1 \choose y - 1} \pi^{k + 1} (1 - \pi)^{y - 1} \right].
\end{align*}
$${#eq-proof-neg-binomial-mean-1}

Now, let us make the following variable rearrangement:

$$
\begin{gather*}
m = k + 1 \\
z = y - 1 \\
m + z - 1  = k + y - 1.
\end{gather*}
$$

Going back to @eq-proof-neg-binomial-mean-1, **and applying our above variable rearrangement within the summation**, we have:

$$
\mathbb{E}(Y) = \frac{k (1 - \pi)}{\pi} \sum_{z = 0}^{\infty} \left[ {m + z - 1 \choose z} \pi^{m} (1 - \pi)^{z} \right].
$${#eq-proof-neg-binomial-mean-2}

Note that, in the summation of @eq-proof-neg-binomial-mean-2, we encounter the `r colourize("PMF", "blue")` of a `r colourize("random variable", "blue")` $Z$ as follows:

$$
Z \sim \text{NegBin}(m, \pi).
$$

Since the summation, where this Negative Binomial `r colourize("PMF", "blue")` of $Z$ is depicted, goes from $z = 0$ to $\infty$, we can apply our result from @eq-proof-neg-binomial-PMF-adds-to-1-c:

$$
\begin{align*} 
\mathbb{E}(Y) &= \frac{k (1 - \pi)}{\pi} \underbrace{\sum_{z = 0}^m \left[ {m + z - 1 \choose z} \pi^{m} (1 - \pi)^{z} \right]}_{1} \\
&= \frac{k (1 - \pi)}{\pi}. \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$
:::

### Variance

Via @eq-app-variance and the @eq-app-expected-value-discrete of a `r colourize("discrete expected value", "blue")`, the `r colourize("variance", "blue")` of a Negative Binomial-distributed `r colourize("random variable", "blue")` $Y$ can be found as follows:

::: {.proof}
$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E}(Y^2) - \left[ \mathbb{E}(Y)\right]^2 \\
&= \mathbb{E}(Y^2) - \left[ \frac{k (1 - \pi)}{\pi} \right]^2 \quad \text{since $\mathbb{E}(Y) = \frac{k (1 - \pi)}{\pi}$.}
\end{align*}
$${#eq-proof-neg-binomial-variance-1}

Now, we need to play around with its `r colourize("expected value", "blue")` expression as follows:

$$
\begin{align*}
\mathbb{E}(Y^2) &= \mathbb{E} \left[ Y (Y - 1) \right] + \mathbb{E}(Y) \\
&= \mathbb{E} \left[ Y (Y - 1) \right] + \frac{k (1 - \pi)}{\pi}.
\end{align*}
$${#eq-proof-neg-binomial-variance-2}

To find $\mathbb{E} \left[ Y (Y - 1) \right]$, we make the following derivation via @eq-app-expected-value-discrete-function when $g(Y) = y (y - 1)$:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= \sum_{y = 0}^{\infty} y (y - 1) P \left( Y = y \mid k, \pi \right) \\
&= \sum_{y = 2}^{\infty} y (y - 1) P \left( Y = y \mid k, \pi \right) \\
& \quad \qquad \text{for $y = \{0, 1\}$,} \\ 
& \quad \qquad \text{the addends are equal to zero} \\
&= \sum_{y = 2}^{\infty} y (y - 1) \left[ {k + y - 1 \choose y} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 2}^{\infty} y (y - 1) \left[ \frac{(k + y - 1)!}{y! (k + y - 1 - y)!} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 2}^{\infty} y (y - 1) \left[ \frac{(k + y - 1)!}{y! (k - 1)!} \pi^k (1 - \pi)^y \right] \\
&= \sum_{y = 2}^{\infty} \frac{y (y - 1)}{y (y - 1)} \left[ \frac{(k + y - 1)!}{(y - 2)! (k - 1)!} \pi^k (1 - \pi)^y \right] \\
& \quad \qquad \text{in the denominator, $y! = y (y - 1) (y - 2)!$} \\
&= \sum_{y = 2}^{\infty} \Bigg[ \frac{(k + y - 1)!}{(y - 2)! \underbrace{\frac{(k + 1)!}{k (k + 1)}}_{(k - 1)!}} \pi^k (1 - \pi)^y \Bigg] \\
&= \sum_{y = 2}^{\infty} \left[ k (k + 1) \frac{(k + y - 1)!}{(k + 1)! (y - 2)!} \pi^k (1 - \pi)^y \right] \\
&= k (k + 1) \sum_{y = 2}^{\infty} \left[ {k + y - 1 \choose y - 2} \pi^k (1 - \pi)^y \right] \\
&= k (k + 1) \sum_{y = 2}^{\infty} \left[ {k + y - 1 \choose y - 2} \pi^{k + 2 - 2} (1 - \pi)^{y + 2 - 2} \right] \\
& \quad \qquad \text{note $\pi^k = \pi^{k + 2 - 2}$ and} \\
& \quad \qquad (1 - \pi)^y = (1 - \pi)^{y + 2 - 2} \\
&= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} \times \\
& \qquad \sum_{y = 2}^{\infty} \left[ {k + y - 1 \choose y - 2} \pi^{k + 2} (1 - \pi)^{y - 2} \right].
\end{align*}
$${#eq-proof-neg-binomial-variance-3}

Then, we make the following variable rearrangement:

$$
\begin{gather*}
m = k + 2\\
z = y - 2 \\
m + z - 1  = k + y - 1.
\end{gather*}
$$

Going back to @eq-proof-neg-binomial-variance-3, **and applying our above variable rearrangement within the summation**, we have:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} \times \\
& \qquad \sum_{y = 2}^{\infty} \left[ {m + z - 1 \choose z} \pi^m (1 - \pi)^z \right].
\end{align*}
$${#eq-proof-neg-binomial-variance-4}

Note that, in the summation of @eq-proof-neg-binomial-variance-4, we encounter the `r colourize("PMF", "blue")` of a `r colourize("random variable", "blue")` $Z$ as follows:

$$
Z \sim \text{NegBin}(m, \pi).
$$

Since the summation, where this Binomial `r colourize("PMF", "blue")` of $Z$ is depicted, goes from $z = 0$ to $\infty$, we can apply our result from @eq-proof-neg-binomial-PMF-adds-to-1-c:

$$
\begin{align*}
\mathbb{E} \left[ Y (Y - 1) \right] &= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} \times \\
& \qquad \underbrace{\sum_{y = 2}^{\infty} \left[ {m + z - 1 \choose z} \pi^m (1 - \pi)^z \right]}_{1} \\
&= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2}.
\end{align*}
$$

Let us go back to @eq-proof-neg-binomial-variance-2 and plug in the above result:

$$
\begin{align*}
\mathbb{E}(Y^2) &= \mathbb{E} \left[ Y (Y - 1) \right] + \frac{k ( 1 - \pi)}{\pi} \\
&= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} + \frac{k ( 1 - \pi)}{\pi}.
\end{align*}
$$

Finally, we plug in $\mathbb{E}(Y^2)$ in @eq-proof-neg-binomial-variance-1:

$$
\begin{align*}
\text{Var} (Y) &= \mathbb{E}(Y^2) - \left[ \frac{k (1 - \pi)}{\pi} \right]^2 \\
&= \frac{k (k + 1) ( 1 - \pi)^2}{\pi^2} + \frac{k ( 1 - \pi)}{\pi} - \left[ \frac{k (1 - \pi)}{\pi} \right]^2 \\
&= \frac{k (1 - \pi)}{\pi} \left[ \frac{(k + 1) (1 - \pi)}{\pi} + 1 - \frac{k (1 - \pi)}{\pi} \right] \\
&= \frac{k (1 - \pi)}{\pi} \left[ \frac{(k + 1) (1 - \pi) + \pi - k (1 - \pi)}{\pi} \right] \\
&= \frac{k (1 - \pi)}{\pi} \left( \frac{k - k \pi + 1 - \pi + \pi - k + k \pi}{\pi} \right) \\
&= \frac{k (1 - \pi)}{\pi} \left( \frac{1}{\pi} \right) \\
&= \frac{k (1 - \pi)}{\pi^2}. \qquad \qquad \qquad \qquad \qquad \qquad \qquad \quad \square
\end{align*}
$$
:::

## Classical Poisson


::: {#Headsup-sample .Heads-up}
::::{.Heads-up-header}
Heads-up on the Taylor and Maclaurin series expansions!
::::
::::{.Heads-up-container}
The Poisson distribution...

@taylor1715

@maclaurin1742

::::
:::

## Generalized Poisson

## Zero-inflated Poisson

## Multinomial

# Continuous Random Variables

| Distribution |  Parameter(s) | Support |  Mean  |  Variance  | 
|:----:|:------:|:-----:|:-----:|:-----:|
| Exponential | $Y \sim \text{Exponential}(\lambda)$ with mean rate $\lambda >0$ <br> or $Y \sim \text{Exponential}(\beta)$ with mean wait time $\beta > 0$ | $y \geq 0$ | $\frac{1}{\lambda}$ <br> for mean rate <br> parametrization <br> or <br> $\beta$ <br> for mean <br> wait time <br> parametrization | $\frac{1}{\lambda^2}$ <br> for mean rate <br> parametrization <br> or <br> $\beta^2$ <br> for mean <br> wait time <br> parametrization | 

: Univariate continuous probability distributions for a random variable $Y$; including parameter(s), support, mean, and variance. {#tbl-distributions-continuous .striped .hover}

## Exponential
