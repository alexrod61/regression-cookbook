<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7PRVEBE1EF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7PRVEBE1EF');
</script>

# The ML-Stats Dictionary {#sec-dictionary}

```{r}
#| include: false

colourize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

Machine learning and statistics comprise a **substantial synergy** that is reflected in data science. Thus, it is imperative to construct solid bridges between both disciplines to ensure everything is clear regarding their tremendous amount of jargon and terminology. This **ML-Stats dictionary** (*ML* stands for *Machine Learning*) aims to be one of these bridges in this textbook, especially within supervised learning and regression analysis contexts.

![Image by [*Gerd Altmann*](https://pixabay.com/users/geralt-9301/) via [*Pixabay*](https://pixabay.com/photos/definition-books-library-bookshelf-4255486/).](img/definition.jpg){width="550"} 

Below, you will find definitions either highlighted in `r colourize("blue", "blue")` if they correspond to `r colourize("statistical", "blue")` terminology or `r colourize("magenta", "magenta")` if the terminology is `r colourize("machine learning-related", "magenta")`. These definitions come from all **definition** admonitions, such as in @imp-example. This colour scheme strives to combine all terminology to switch from one field to another easily. With practice and time, we should be able to jump back and forth when using these concepts.

::: {.callout-warning}
## Attention!

Noteworthy terms (either `r colourize("statistical", "blue")` or `r colourize("machine learning-related", "magenta")`) will include a particular admonition identifying which terms (again, either `r colourize("statistical", "blue")` or `r colourize("machine learning-related", "magenta")`) are **equivalent** (**or NOT equivalent if that is the case!**).
:::

## B {.unnumbered}

### `r colourize("Bayesian statistics", "blue")` {.unnumbered}

This statistical school of thinking also relies on the **frequency of events** to estimate specific `r colourize("parameters", "blue")` of interest in a `r colourize("population", "blue")` or **system**.  Nevertheless, unlike `r colourize("frequentist", "blue")` statistics, `r colourize("Bayesian", "blue")` statisticians use **prior knowledge** on the `r colourize("population parameters", "blue")` to update their estimations on them along with the **current evidence** they can gather. This evidence is in the form of the repetition of $n$ experiments involving a random phenomenon. All these ingredients allow `r colourize("Bayesian", "blue")` statisticians to make inference by conducting  appropriate `r colourize("hypothesis testings", "blue")`, which are designed differently from their mainstream `r colourize("frequentist", "blue")` counterpart.

Under the umbrella of this approach, we assume that our governing `r colourize("parameters", "blue")` are **random**; i.e., they have their own `r colourize("sample space", "blue")` and `r colourize("probabilities", "blue")` associated to their corresponding outcomes. The statistical process of inference is heavily backed up by **probability theory** mostly in the form of the **Bayes theorem** (named after Reverend **Thomas Bayes**, an English statistician from the 18th century). This theorem uses our **current evidence** along with our **prior beliefs** to deliver a **posterior distribution** of our **random** `r colourize("parameter(s)", "blue")` of interest.

## D {.unnumbered}
 
### `r colourize("Dependent variable", "blue")` {.unnumbered}

In supervised learning, it is the main variable of interest we are trying to **learn** or **predict**, or equivalently, the variable we are trying **explain** in a statistical inference framework.

::: {.callout-warning}
## Equivalent to:

`r colourize("Response", "blue")`, `r colourize("outcome", "magenta")`, `r colourize("output", "magenta")` or `r colourize("target", "magenta")`.
:::

## F {.unnumbered}

### `r colourize("Frequentist statistics", "blue")` {.unnumbered}

This statistical school of thinking heavily relies on the **frequency of events** to estimate specific `r colourize("parameters", "blue")` of interest in a `r colourize("population", "blue")` or **system**. This frequency of events is reflected in the repetition of $n$ experiments involving a random phenomenon within this `r colourize("population", "blue")` or **system**.\

Under the umbrella of this approach, we assume that our governing `r colourize("parameters", "blue")` are **fixed**. Note that, within the philosophy of this school of thinking, we can only make **precise** and **accurate** predictions as long as we repeat our $n$ experiments as many times as possible, i.e., 

$$
n \rightarrow \infty.
$$

## O {.unnumbered}

### `r colourize("Outcome", "magenta")` {.unnumbered}

In supervised learning, it is the main variable of interest we are trying to **learn** or **predict**, or equivalently, the variable we are trying **explain** in a statistical inference framework.

::: {.callout-warning}
## Equivalent to:

`r colourize("Dependent variable", "blue")`, `r colourize("response", "blue")`, `r colourize("output", "magenta")` or `r colourize("target", "magenta")`.
:::

### `r colourize("Output", "magenta")` {.unnumbered}

In supervised learning, it is the main variable of interest we are trying to **learn** or **predict**, or equivalently, the variable we are trying **explain** in a statistical inference framework.

::: {.callout-warning}
## Equivalent to:

`r colourize("Dependent variable", "blue")`, `r colourize("response", "blue")`, `r colourize("outcome", "magenta")` or `r colourize("target", "magenta")`.
:::

## P {.unnumbered}

### `r colourize("Parameter", "blue")` {.unnumbered}

It is a characteristic (**numerical** or even **non-numerical**, such as a **distinctive category**) that **summarizes** the state of our `r colourize("population", "blue")` or **system** of interest. Examples of a `r colourize("population parameter", "blue")` can be described as follows:

- *The average weight of children between the ages of 5 and 10 years old in states of the American west coast (**numerical**).*
- *The variability in the height of the mature açaí palm trees from the Brazilian Amazonian jungle (**numerical**).*
- *The proportion of defective items in the production of cellular phones in a set of manufacturing facilities (**numerical**).*
- *The average customer waiting time to get their order in the Vancouver franchises of a well-known ice cream parlour (**numerical**).*
- *The most favourite pizza topping of vegetarian adults between the ages of 30 and 40 years old in Edmonton (**non-numerical**).*

Note the **standard mathematical notation** for `r colourize("population parameters", "blue")` are **Greek letters**. Moreover, in practice, these `r colourize("population parameter(s)", "blue")` of interest will be **unknown** to the data scientist or researcher. Instead, they would use formal statistical inference to **estimate** them.

### `r colourize("Population", "blue")` {.unnumbered}

It is a **whole collection of individuals or items** that share **distinctive attributes**. As data scientists or researchers, we are interested in studying these attributes, which we assume are **governed** by `r colourize("parameters", "blue")`. In practice, we must be **as precise as possible** when defining our given `r colourize("population", "blue")` such that we would frame our entire data modelling process since its very early stages. Examples of a `r colourize("population", "blue")` could be the following:

- *Children between the ages of 5 and 10 years old in states of the American west coast.*
- *Customers of musical vinyl records in the Canadian provinces of British Columbia and Alberta.*
- *Avocado trees grown in the Mexican state of Michoacán.*
- *Adult giant pandas in the Southwestern Chinese province of Sichuan.*
- *Mature açaí palm trees from the Brazilian Amazonian jungle.*

Note that the term `r colourize("population", "blue")` could be exchanged for the term **system**, given that certain contexts do not specifically refer to individuals or items. Instead, these contexts could refer to **processes** whose attributes are also governed by `r colourize("parameters", "blue")`. Examples of a **system** could be the following:

- *The production of cellular phones in a set of manufacturing facilities.*
- *The sale process in the Vancouver franchises of a well-known ice cream parlour.*
- *The transit cycle of the twelve lines of Mexico City's subway.*

### `r colourize("Probability", "blue")` {.unnumbered}

Let $A$ be an event of interest in a random phenomenon, in a `r colourize("population", "blue")` or **system** of interest, whose all possible outcomes belong to a given `r colourize("sample space", "blue")` $S$. Generally, the `r colourize("probability", "blue")` for this event $A$ happening can be mathematically depicted as $P(A)$. Moreover, **suppose we observe the random phenomenon $n$ times** such as we were running some class of experiment, then $P(A)$ is defined as the following ratio:

$$
P(A) = \frac{\text{Number of times event $A$ is observed}}{n},
$${#eq-probability-dictionary}

**as the $n$ times we observe the random phenomenon goes to infinity.**

@eq-probability-dictionary will always put $P(A)$ in the following numerical range:

$$
0 \leq P(A) \leq 1.
$$

## R {.unnumbered}

### `r colourize("Response", "blue")` {.unnumbered}

In supervised learning, it is the main variable of interest we are trying to **learn** or **predict**, or equivalently, the variable we are trying **explain** in a statistical inference framework.

::: {.callout-warning}
## Equivalent to:

`r colourize("Dependent variable", "blue")`, `r colourize("outcome", "magenta")`, `r colourize("output", "magenta")` or `r colourize("target", "magenta")`.
:::

## S {.unnumbered}

### `r colourize("Sample space", "blue")` {.unnumbered}

Let $A$ be an event of interest in a random phenomenon in a `r colourize("population", "blue")` or **system** of interest. The `r colourize("sample space", "blue")` $S$ of event $A$ denotes the set of all the possible **random outcomes** we might encounter every time we randomly observe $A$ such as we were running some class of experiment.

Note each of these outcomes has a determined probability associated with them. If we add up all these probabilities, the probability of the sample $S$ will be one, i.e.,

$$
P(S) = 1.
$${#eq-sample-space}

## T {.unnumbered}

### `r colourize("Target", "magenta")` {.unnumbered}

In supervised learning, it is the main variable of interest we are trying to **learn** or **predict**, or equivalently, the variable we are trying **explain** in a statistical inference framework.

::: {.callout-warning}
## Equivalent to:

`r colourize("Dependent variable", "blue")`, `r colourize("response", "blue")`, `r colourize("outcome", "magenta")` or `r colourize("output", "magenta")`.
:::
