<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7PRVEBE1EF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7PRVEBE1EF');
</script>

# The Fusionified ML-Stats Dictionary {#sec-dictionary}

```{r}
#| include: false

colourize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

::: {.Warning}
::::{.Warning-header}
Fun fact!
::::
::::{.Warning-container}
**Fusionified!** A mix of flavors from various cuisines that somehow (or miraculously) works.
::::
:::

Machine learning and statistics comprise a **substantial synergy** that is reflected in data science. Thus, it is imperative to construct solid bridges between both disciplines to ensure everything is clear regarding their tremendous amount of jargon and terminology. This **ML-Stats dictionary** (*ML* stands for *Machine Learning*) aims to be one of these bridges in this textbook, especially within supervised learning and regression analysis contexts.

![Image by [*Gerd Altmann*](https://pixabay.com/users/geralt-9301/) via [*Pixabay*](https://pixabay.com/photos/definition-books-library-bookshelf-4255486/).](img/definition.jpg){width="550"} 

Below, you will find definitions either highlighted in `r colourize("blue", "blue")` if they correspond to `r colourize("statistical", "blue")` terminology or `r colourize("magenta", "magenta")` if the terminology is `r colourize("machine learning-related", "magenta")`. These definitions come from all **definition** admonitions, such as in @Definition-sample. This colour scheme strives to combine all terminology to switch from one field to another easily. With practice and time, we should be able to jump back and forth when using these concepts.

::: {.Equivalence}
::::{.Equivalence-header}
Attention!
::::
::::{.Equivalence-container}
Noteworthy terms (either `r colourize("statistical", "blue")` or `r colourize("machine learning-related", "magenta")`) will include a particular admonition identifying which terms (again, either `r colourize("statistical", "blue")` or `r colourize("machine learning-related", "magenta")`) are **equivalent** or **somewhat equivalent** (**or even NOT equivalent if that is the case!**).
::::
:::

## A {.unnumbered}

### `r colourize("Alternative hypothesis", "blue")` {.unnumbered}

### `r colourize("Attribute", "magenta")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Covariate", "blue")`, `r colourize("exogeneous variable", "magenta")`, `r colourize("explanatory variable", "blue")`, `r colourize("feature", "magenta")`, `r colourize("independent variable", "blue")`, `r colourize("input", "magenta")`, `r colourize("predictor", "magenta")` or `r colourize("regressor", "blue")`.
::::
:::

### `r colourize("Average", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("random variable", "blue")` whose support is $\mathcal{Y}$. In general, the `r colourize("expected  value", "blue")` or `r colourize("mean", "blue")` $\mathbb{E}(Y)$ of this `r colourize("random variable", "blue")` is defined as a **weighted** `r colourize("average", "blue")` according to its corresponding `r colourize("probability distribution", "blue")`. In other words, this `r colourize("measure of central tendency", "blue")` $\mathbb{E}(Y)$ aims to find the middle value of this `r colourize("random variable", "blue")` by weighting all its possible values in its support $\mathcal{Y}$ as dictated by its `r colourize("probability distribution", "blue")`.

Given the above definition, when $Y$ is a `r colourize("discrete random variable", "blue")` whose `r colourize("probability mass function (PMF)", "blue")` is $P_Y(Y = y)$, then its **weighted** `r colourize("average", "blue")` is mathematically defined as

$$
\mathbb{E}(Y) = \sum_{y \in \mathcal{Y}} y \cdot P_Y(Y = y).
$$

When $Y$ is a `r colourize("continuous random variable", "blue")` whose `r colourize("probability density function (PDF)", "blue")` is $f_Y(y)$, its **weighted** `r colourize("average", "blue")` is mathematically defined as

$$
\mathbb{E}(Y) = \int_{\mathcal{Y}} y \cdot f_Y(y) \mathrm{d}y.
$$

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Expected value", "blue")` or `r colourize("mean", "blue")`.
::::
:::

## B {.unnumbered}

### `r colourize("Bayesian statistics", "blue")` {.unnumbered}

This statistical school of thinking also relies on the **frequency of events** to estimate specific `r colourize("parameters", "blue")` of interest in a `r colourize("population", "blue")` or **system**.  Nevertheless, unlike `r colourize("frequentist", "blue")` statistics, `r colourize("Bayesian", "blue")` statisticians use **prior knowledge** on the `r colourize("population parameters", "blue")` to update their estimations on them along with the **current evidence** they can gather. This evidence is in the form of the repetition of $n$ experiments involving a random phenomenon. All these ingredients allow `r colourize("Bayesian", "blue")` statisticians to make inference by conducting  appropriate `r colourize("hypothesis testings", "blue")`, which are designed differently from their mainstream `r colourize("frequentist", "blue")` counterpart.

Under the umbrella of this approach, we assume that our governing `r colourize("parameters", "blue")` are **random**; i.e., they have their own `r colourize("sample space", "blue")` and `r colourize("probabilities", "blue")` associated to their corresponding outcomes. The statistical process of inference is heavily backed up by **probability theory** mostly in the form of the **Bayes theorem** (named after Reverend **Thomas Bayes**, an English statistician from the 18th century). This theorem uses our **current evidence** along with our **prior beliefs** to deliver a **posterior distribution** of our **random** `r colourize("parameter(s)", "blue")` of interest.

### `r colourize("Bayes' rule", "blue")` {.unnumbered}

Suppose you have two events of interest, $A$ and $B$, in a random phenomenon of a `r colourize("population", "blue")` or **system** of interest. From @eq-conditional-probability-app, we can state the following expression for the `r colourize("conditional probability", "blue")` of $A$ **given** $B$:

$$
P(A | B) = \frac{P(A \cap B)}{P(B)} \quad \text{if $P(B) > 0$.}
$${#eq-cond-prob-A-given-B-app}

Note the `r colourize("conditional probability", "blue")` of $B$ **given** $A$ can be stated as:

$$
\begin{align*}
P(B | A) &= \frac{P(B \cap A)}{P(A)} \quad \text{if $P(A) > 0$} \\
&= \frac{P(A \cap B)}{P(A)} \quad \text{since $P(B \cap A) = P(A \cap B)$.}
\end{align*}
$${#eq-cond-prob-B-given-A-app}

Then, we can manipulate @eq-cond-prob-B-given-A-app as follows:

$$
P(A \cap B) = P(B | A) \times P(A).
$$

The above result can be plugged into @eq-cond-prob-A-given-B-app:

$$
\begin{align*}
P(A | B) &= \frac{P(A \cap B)}{P(B)} \\
&= \frac{P(B | A) \times P(A)}{P(B)}.
\end{align*}
$${#eq-bayes-rule-app}

@eq-bayes-rule-app is called the `r colourize("Bayes' rule", "blue")`. We are basically flipping around `r colourize("conditional probabilities", "blue")`.

## C {.unnumbered}

### `r colourize("Critical value", "blue")` {.unnumbered}

### `r colourize("Conditional probability", "blue")` {.unnumbered}

Suppose you have two events of interest, $A$ and $B$, in a random phenomenon, in a `r colourize("population", "blue")` or **system** of interest. These two events belong to the  `r colourize("sample space", "blue")` $S$. Moreover, assume that the  `r colourize("probability", "blue")` of event $B$ is such that

$$
P(B) > 0,
$$

which is considered the **conditioning event**.

Hence, the `r colourize("conditional probability", "blue")` event $A$ **given** event $B$ is defined as

$$
P(A | B) = \frac{P(A \cap B)}{P(B)},
$${#eq-conditional-probability-app}

where $P(A \cap B)$ is read as **the `r colourize("probability", "blue")` of the intersection of events $A$ and $B$**.

### `r colourize("Confidence interval", "blue")` {.unnumbered}

### `r colourize("Continuous random variable", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("random variable", "blue")` whose support is $\mathcal{Y}$. If this support $\mathcal{Y}$ corresponds to an uncountably infinite set of possible values, then $Y$ is considered a `r colourize("continuous random variable", "blue")`.

Note a `r colourize("continuous random variable", "blue")` could be 

- **completely unbounded** (i.e., its set of possible values goes from $-\infty$ to $\infty$ as in $-\infty < y < \infty$), 
- **positively unbounded** (i.e., its set of possible values goes from $0$ to $\infty$ as in $0 \leq y < \infty$), 
- **negatively unbounded** (i.e., its set of possible values goes from $-\infty$ to $0$ as in $-\infty < y \leq 0$), or
- **bounded** between two values $a$ and $b$ (i.e., its set of possible values goes from $a$ to $b$ as in $a \leq y \leq b$).

### `r colourize("Covariate", "blue")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Attribute", "magenta")`, `r colourize("exogeneous variable", "magenta")`, `r colourize("explanatory variable", "blue")`, `r colourize("feature", "magenta")`, `r colourize("independent variable", "blue")`, `r colourize("input", "magenta")`, `r colourize("predictor", "magenta")` or `r colourize("regressor", "blue")`.
::::
:::

### `r colourize("Cumulative distribution function", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("random variable", "blue")` either `r colourize("discrete", "blue")` or `r colourize("continuous", "blue")`. Its `r colourize("cumulative distribution function (CDF)", "blue")` $F_Y(y)  : \mathbb{R} \rightarrow [0, 1]$ refers to the `r colourize("probability", "blue")` that $Y$ is less or equal than an observed value $y$:

$$
F_Y(y) = P(Y \leq y).
$$

Then, we have the following by type of  `r colourize("random variable", "blue")`:

- When $Y$ is `r colourize("discrete", "blue")`, whose support is $\mathcal{Y}$, suppose it has a `r colourize("probability mass function (PMF)", "blue")` $P_Y(Y = y)$. Then, the `r colourize("CDF", "blue")` is mathematically represented as:

$$
F_Y(y) = \sum_{\substack{t \in \mathcal{Y} \\ t \leq y}} P_Y(Y = t).
$${#eq-cdf-discrete-app}

- When $Y$ is `r colourize("continuous", "blue")`, whose support is $\mathcal{Y}$, suppose it has a `r colourize("probability density function (PDF)", "blue")` $f_Y(y)$. Then, the `r colourize("CDF", "blue")` is mathematically represented as:

$$
F_Y(y) = \int_{-\infty}^y f_Y(t) \mathrm{d}t.
$${#eq-cdf-continuous-app}

Note that in @eq-cdf-discrete-app and @eq-cdf-continuous-app, we use the auxiliary variable $t$ since we do not compute the summation or integral over the observed $y$ given its role on either the `r colourize("PMF", "blue")` or `r colourize("PDF", "blue")`. Therefore, we use this auxiliary variable $t$.

## D {.unnumbered}
 
### `r colourize("Dependent variable", "blue")` {.unnumbered}

In supervised learning, it is the main variable of interest we are trying to **learn** or **predict**, or equivalently, the variable we are trying **explain** in a statistical inference framework.

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Endogeneous variable", "magenta")`, `r colourize("response variable", "blue")`, `r colourize("outcome", "magenta")`, `r colourize("output", "magenta")` or `r colourize("target", "magenta")`.
::::
:::

### `r colourize("Discrete random variable", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("random variable", "blue")` whose support is $\mathcal{Y}$. If this support $\mathcal{Y}$ corresponds to a finite set or a countably infinite set of possible values, then $Y$ is considered a `r colourize("discrete random variable", "blue")`. 

For instance, we can encounter `r colourize("discrete random variables", "blue")` which could be classified as

- **binary** (i.e., a finite set of two possible values),
- **categorical** (either **nominal** or **ordinal**, which have a finite set of three or more possible values), or 
- **counts** (which might have a finite set or a countably infinite set of possible values as integers).

### `r colourize("Dispersion", "blue")` {.unnumbered}

## E {.unnumbered}

### `r colourize("Endogeneous variable", "magenta")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Dependent variable", "blue")`, `r colourize("outcome", "magenta")`, `r colourize("output", "magenta")`, `r colourize("response variable", "blue")` or `r colourize("target", "magenta")`.
::::
:::

### `r colourize("Equidispersion", "blue")` {.unnumbered}

### `r colourize("Expected value", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("random variable", "blue")` whose support is $\mathcal{Y}$. In general, the `r colourize("expected  value", "blue")` or `r colourize("mean", "blue")` $\mathbb{E}(Y)$ of this `r colourize("random variable", "blue")` is defined as a **weighted** `r colourize("average", "blue")` according to its corresponding `r colourize("probability distribution", "blue")`. In other words, this `r colourize("measure of central tendency", "blue")` $\mathbb{E}(Y)$ aims to find the middle value of this `r colourize("random variable", "blue")` by weighting all its possible values in its support $\mathcal{Y}$ as dictated by its `r colourize("probability distribution", "blue")`.

Given the above definition, when $Y$ is a `r colourize("discrete random variable", "blue")` whose `r colourize("probability mass function (PMF)", "blue")` is $P_Y(Y = y)$, then its `r colourize("expected  value", "blue")` is mathematically defined as

$$
\mathbb{E}(Y) = \sum_{y \in \mathcal{Y}} y \cdot P_Y(Y = y).
$${#eq-expected-value-discrete-app}

When $Y$ is a `r colourize("continuous random variable", "blue")` whose `r colourize("probability density function (PDF)", "blue")` is $f_Y(y)$, its `r colourize("expected  value", "blue")` is mathematically defined as

$$
\mathbb{E}(Y) = \int_{\mathcal{Y}} y \cdot f_Y(y) \mathrm{d}y.
$${#eq-expected-value-continuous-app}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Average", "blue")` or `r colourize("mean", "blue")`.
::::
:::

### `r colourize("Exogeneous variable", "magenta")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Attribute", "magenta")`, `r colourize("covariate", "blue")`, `r colourize("explanatory variable", "blue")`, `r colourize("feature", "magenta")`, `r colourize("independent variable", "blue")`, `r colourize("input", "magenta")`, `r colourize("predictor", "magenta")` or `r colourize("regressor", "blue")`.
::::
:::

### `r colourize("Explanatory variable", "blue")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Attribute", "magenta")`, `r colourize("covariate", "blue")`, `r colourize("exogeneous variable", "magenta")`, `r colourize("feature", "magenta")`, `r colourize("independent variable", "blue")`, `r colourize("input", "magenta")`, `r colourize("predictor", "magenta")` or `r colourize("regressor", "blue")`.
::::
:::

## F {.unnumbered}

### `r colourize("False negative", "blue")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Type II error", "blue")`.
::::
:::

### `r colourize("False positive", "blue")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Type I error", "blue")`.
::::
:::

### `r colourize("Feature", "magenta")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Attribute", "magenta")`, `r colourize("covariate", "blue")`, `r colourize("exogeneous variable", "magenta")`, `r colourize("explanatory variable", "blue")`, `r colourize("independent variable", "blue")`, `r colourize("input", "magenta")`, `r colourize("predictor", "magenta")` or `r colourize("regressor", "blue")`.
::::
:::

### `r colourize("Frequentist statistics", "blue")` {.unnumbered}

This statistical school of thinking heavily relies on the **frequency of events** to estimate specific `r colourize("parameters", "blue")` of interest in a `r colourize("population", "blue")` or **system**. This frequency of events is reflected in the repetition of $n$ experiments involving a random phenomenon within this `r colourize("population", "blue")` or **system**.\

Under the umbrella of this approach, we assume that our governing `r colourize("parameters", "blue")` are **fixed**. Note that, within the philosophy of this school of thinking, we can only make **precise** and **accurate** predictions as long as we repeat our $n$ experiments as many times as possible, i.e., 

$$
n \rightarrow \infty.
$$

## G {.unnumbered}

### `r colourize("Generalized linear model", "blue")` {.unnumbered}

### `r colourize("Generative model", "blue")` {.unnumbered}

Suppose you observe some data $y$ from a `r colourize("population", "blue")` or system of interest. Moreover, let us assume this `r colourize("population", "blue")` or system is governed by $k$ `r colourize("parameters", "blue")` contained in the following vector:

$$
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T.
$$

If we state that our observed data $y$ follows certain `r colourize("probability distribution", "blue")` $\mathcal{D}(\cdot)$, then we will have a `r colourize("generative model", "blue")` $m$ such that

$$
m: y \sim \mathcal{D}(\boldsymbol{\theta}).
$$

## H {.unnumbered}

### `r colourize("Hypothesis testing", "blue")` {.unnumbered}

## I {.unnumbered}

### `r colourize("Independence", "blue")` {.unnumbered}

Suppose you have two events of interest, $A$ and $B$, in a random phenomenon of a `r colourize("population", "blue")` or **system** of interest. These two events are **statistically** `r colourize("independent", "blue")` if event $B$ does not affect event $A$ and vice versa. Therefore, the `r colourize("probability", "blue")` of their corresponding intersection is given by:

$$
P(A \cap B) = P(A) \times P(B).
$$

Let us expand the above definition to a `r colourize("random variable", "blue")` framework:

- Suppose you have a set of $n$ `r colourize("discrete random variables", "blue")` $Y_1, \dots, Y_n$ whose supports are $\mathcal{Y_1}, \dots, \mathcal{Y_n}$ with `r colourize("probability mass functions (PMFs)", "blue")` $P_{Y_1}(Y_1 = y_1), \dots, P_{Y_n}(Y_n = y_n)$ respectively. That said, the joint `r colourize("PMF", "blue")` of these $n$ `r colourize("random variables", "blue")` is the multiplication of their corresponding standalone `r colourize("PMFs", "blue")`:

$$
\begin{align*}
P_{Y_1, \dots, Y_n}(Y_1 = y_1, \dots, Y_n = y_n) &= \prod_{i = 1}^n P_{Y_i}(Y_i = y_i) \\
& \qquad \text{for all} \\
& \qquad \quad y_i \in \mathcal{Y}_i, i = 1, \dots, n.
\end{align*}
$${#eq-joint-PMF-ind-random-variables-app}

- Suppose you have a set of $n$ `r colourize("continuous random variables", "blue")` $Y_1, \dots, Y_n$ whose supports are $\mathcal{Y_1}, \dots, \mathcal{Y_n}$ with `r colourize("probability density functions (PDFs)", "blue")` $f_{Y_1}(y_1), \dots, f_{Y_n}(y_n)$ respectively. That said, the joint `r colourize("PDF", "blue")` of these $n$ `r colourize("random variables", "blue")` is the multiplication of their corresponding standalone `r colourize("PDFs", "blue")`:

$$
\begin{align*}
f_{Y_1, \dots, Y_n}(y_1, \dots, y_n) &= \prod_{i = 1}^n f_{Y_i}(y_i) \\
& \qquad \text{for all} \\
& \qquad \quad y_i \in \mathcal{Y}_i, i = 1, \dots, n.
\end{align*}
$${#eq-joint-PDF-ind-random-variables-app}

### `r colourize("Independent variable", "blue")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Attribute", "magenta")`, `r colourize("covariate", "blue")`, `r colourize("exogeneous variable", "magenta")`, `r colourize("explanatory variable", "blue")`, `r colourize("feature", "magenta")`, `r colourize("input", "magenta")`, `r colourize("predictor", "magenta")` or `r colourize("regressor", "blue")`.
::::
:::

### `r colourize("Input", "magenta")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Attribute", "magenta")`, `r colourize("covariate", "blue")`, `r colourize("exogeneous variable", "magenta")`, `r colourize("explanatory variable", "blue")`, `r colourize("feature", "magenta")`, `r colourize("independent variable", "blue")`, `r colourize("predictor", "magenta")` or `r colourize("regressor", "blue")`.
::::
:::

## M {.unnumbered}

### `r colourize("Mean", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("random variable", "blue")` whose support is $\mathcal{Y}$. In general, the `r colourize("expected  value", "blue")` or `r colourize("mean", "blue")` $\mathbb{E}(Y)$ of this `r colourize("random variable", "blue")` is defined as a **weighted** `r colourize("average", "blue")` according to its corresponding `r colourize("probability distribution", "blue")`. In other words, this `r colourize("measure of central tendency", "blue")` $\mathbb{E}(Y)$ aims to find the middle value of this `r colourize("random variable", "blue")` by weighting all its possible values in its support $\mathcal{Y}$ as dictated by its `r colourize("probability distribution", "blue")`.

Given the above definition, when $Y$ is a `r colourize("discrete random variable", "blue")` whose `r colourize("probability mass function (PMF)", "blue")` is $P_Y(Y = y)$, then its `r colourize("mean", "blue")` is mathematically defined as

$$
\mathbb{E}(Y) = \sum_{y \in \mathcal{Y}} y \cdot P_Y(Y = y).
$$

When $Y$ is a `r colourize("continuous random variable", "blue")` whose `r colourize("probability density function (PDF)", "blue")` is $f_Y(y)$, its `r colourize("mean", "blue")` is mathematically defined as

$$
\mathbb{E}(Y) = \int_{\mathcal{Y}} y \cdot f_Y(y) \mathrm{d}y.
$$

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Average", "blue")` or `r colourize("expected value", "blue")`.
::::
:::

### `r colourize("Measure of central tendency", "blue")` {.unnumbered}

Probabilistically, a `r colourize("measure of central tendency", "blue")` is defined as a metric that identifies a **central or typical value** of a given `r colourize("probability distribution", "blue")`. In other words, a `r colourize("measure of central tendency", "blue")` refers to a **central or typical value** that a given `r colourize("random variable", "blue")` might take when we observe various realizations of this variable over a long period.

### `r colourize("Measure of uncertainty", "blue")` {.unnumbered}

Probabilistically, a `r colourize("measure of uncertainty", "blue")` refers to the **spread** of a given `r colourize("random variable", "blue")` when we observe its different realizations in the long term. Note a **larger spread** indicates more variability in these realizations. On the other hand, a **smaller spread** denotes less variability in these realizations.

## N {.unnumbered}

### `r colourize("Null hypothesis", "blue")` {.unnumbered}

## O {.unnumbered}

### `r colourize("Observed effect", "blue")` {.unnumbered}

### `r colourize("Outcome", "magenta")` {.unnumbered}

In supervised learning, it is the main variable of interest we are trying to **learn** or **predict**, or equivalently, the variable we are trying **explain** in a statistical inference framework.

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Dependent variable", "blue")`, `r colourize("endogeneous variable", "magenta")`, `r colourize("response variable", "blue")`, `r colourize("output", "magenta")` or `r colourize("target", "magenta")`.
::::
:::

### `r colourize("Output", "magenta")` {.unnumbered}

In supervised learning, it is the main variable of interest we are trying to **learn** or **predict**, or equivalently, the variable we are trying **explain** in a statistical inference framework.

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Dependent variable", "blue")`, `r colourize("endogeneous variable", "magenta")`, `r colourize("response variable", "blue")`, `r colourize("outcome", "magenta")` or `r colourize("target", "magenta")`.
::::
:::

### `r colourize("Overdispersion", "blue")` {.unnumbered}

## P {.unnumbered}

### `r colourize("Parameter", "blue")` {.unnumbered}

It is a characteristic (**numerical** or even **non-numerical**, such as a **distinctive category**) that **summarizes** the state of our `r colourize("population", "blue")` or **system** of interest.

Note the **standard mathematical notation** for `r colourize("population parameters", "blue")` are **Greek letters**. Moreover, in practice, these `r colourize("population parameter(s)", "blue")` of interest will be **unknown** to the data scientist or researcher. Instead, they would use formal statistical inference to **estimate** them.

### `r colourize("Population", "blue")` {.unnumbered}

It is a **whole collection of individuals or items** that share **distinctive attributes**. As data scientists or researchers, we are interested in studying these attributes, which we assume are **governed** by `r colourize("parameters", "blue")`. In practice, we must be **as specific as possible** when defining our given `r colourize("population", "blue")` such that we would frame our entire data modelling process since its very early stages.

Note that the term `r colourize("population", "blue")` could be exchanged for the term **system**, given that certain contexts do not specifically refer to individuals or items. Instead, these contexts could refer to **processes** whose attributes are also governed by `r colourize("parameters", "blue")`.

### `r colourize("Power", "blue")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("True positive rate", "blue")`.
::::
:::

### `r colourize("Predictor", "magenta")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Attribute", "magenta")`, `r colourize("covariate", "blue")`, `r colourize("exogeneous variable", "magenta")`, `r colourize("explanatory variable", "blue")`, `r colourize("feature", "magenta")`, `r colourize("independent variable", "blue")`, `r colourize("input", "magenta")` or `r colourize("regressor", "blue")`.
::::
:::

### `r colourize("Probability", "blue")` {.unnumbered}

Let $A$ be an event of interest in a random phenomenon, in a `r colourize("population", "blue")` or **system** of interest, whose all possible outcomes belong to a given `r colourize("sample space", "blue")` $S$. Generally, the `r colourize("probability", "blue")` for this event $A$ happening can be mathematically depicted as $P(A)$. Moreover, **suppose we observe the random phenomenon $n$ times** such as we were running some class of experiment, then $P(A)$ is defined as the following ratio:

$$
P(A) = \frac{\text{Number of times event $A$ is observed}}{n},
$${#eq-probability-dictionary}

**as the $n$ times we observe the random phenomenon goes to infinity.**

@eq-probability-dictionary will always put $P(A)$ in the following numerical range:

$$
0 \leq P(A) \leq 1.
$$

### `r colourize("Probability distribution", "blue")` {.unnumbered}

When we set a `r colourize("random variable", "blue")` $Y$, we also set a new set of $v$ possible outcomes $\mathcal{Y} = \{ y_1, \dots, y_v\}$ coming from the `r colourize("sample space", "blue")` $S$. This new set of possible outcomes $\mathcal{Y}$ corresponds to the range of the `r colourize("random variable", "blue")` $Y$ (i.e., all the possible values that could be taken on once we execute a given random experiment involving $Y$). 

That said, let us suppose we have a `r colourize("sample space", "blue")` of $u$ elements defined as 

$$
S = \{ s_1, \dots, s_u \},
$$

where each one of these elements has a `r colourize("probability", "blue")` assigned via a function $P_S(\cdot)$ such that

$$
P(S) = \sum_{i = 1}^u P_S(s_i) = 1.
$$

which has to satisfy @eq-sample-space-app.

Then, the `r colourize("probability distribution", "blue")` of $Y$, i.e., $P_Y(\cdot)$ assigns a `r colourize("probability", "blue")` to each **observed value** $Y = y_j$ (with $j = 1, \dots, v$) if and only if the outcome of the random experiment belongs to the `r colourize("sample space", "blue")`, i.e., $s_i \in S$ (for $i = 1, \dots, u$) such that $Y(s_i) = y_j$:

$$
P_Y(Y = y_j) = P \left( \left\{ s_i \in S : Y(s_i) = y_j \right\} \right).
$$

### `r colourize("Probability density function", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("continuous random variable", "blue")` whose support is $\mathcal{Y}$. Furthermore, consider a function $f_Y(y)$ such that 

$$
f_Y(y) : \mathbb{R} \rightarrow \mathbb{R}
$$

with

$$
f_Y(y) \geq 0.
$$

Then, $f_Y(y)$ is considered a `r colourize("probability density function (PDF)", "blue")` if the `r colourize("probability", "blue")` of $Y$ taking on a value within the range represented by the subset $A \subset \mathcal{Y}$ is equal to

$$
P_Y(Y \in A) = \int_A f_Y(y) \mathrm{d}y
$$

with

$$
\int_{\mathcal{Y}} f_Y(y) \mathrm{d}y = 1.
$$

### `r colourize("Probability mass function", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("discrete random variable", "blue")` whose support is $\mathcal{Y}$. Moreover, suppose that $Y$ has a `r colourize("probability distribution", "blue")` such that

$$
P_Y(Y = y) : \mathbb{R} \rightarrow [0, 1]
$$

where, for all $y \notin \mathcal{Y}$, we have

$$
P_Y(Y = y) = 0 
$$

and

$$
\sum_{y \in \mathcal{Y}} P_Y(Y = y) = 1.
$$
Then, $P_Y(Y = y)$ is considered a `r colourize("probability mass function (PMF)", "blue")`.

### `r colourize("$p$-value", "blue")` {.unnumbered}

## R {.unnumbered}

### `r colourize("Random Sample", "blue")` {.unnumbered}

A `r  colourize("random sample", "blue")` is a collection of `r colourize("random variables", "blue")` $Y_1, \dots, Y_n$ of size $n$ coming from a given `r colourize("population", "blue")` or system of interest. Note that **the most elementary definition** of a `r  colourize("random sample", "blue")` assumes that these $n$ `r colourize("random variables", "blue")` are mutually `r colourize("independent", "blue")` and **identically distributed** (which is abbreviated as *iid*). 

The fact that these $n$ random variables are identically distributed indicates that they have the same mathematical form for their corresponding `r colourize("probability mass functions (PMFs)", "blue")` or `r colourize("probability density function (PDFs)", "blue")`, depending on whether they are `r colourize("discrete", "blue")` or `r colourize("continuous", "blue")` respectively. Hence, under a `r colourize("generative modelling", "blue")` approach in a `r colourize("population", "blue")` or system of interest governed by $k$ `r colourize("parameters", "blue")` contained in the vector

$$
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T,
$$

we can apply the *iid* property in an elementary `r colourize("random sample", "blue")` to obtain the following joint `r colourize("probability distributions", "blue")`:

- In the case of $n$ *iid* `r colourize("discrete random variables", "blue")` $Y_1, \dots, Y_n$ whose common standalone `r colourize("PMF", "blue")` is $P_Y(Y = y)$ with support $\mathcal{Y}$, the joint `r colourize("PMF", "blue")` is mathematically expressed as

$$
\begin{align*}
P_{Y_1, \dots, Y_n}(Y_1 = y_1, \dots, Y_n = y_n | \boldsymbol{\theta}) &= \prod_{i = 1}^n P_Y(Y = y_i | \boldsymbol{\theta}) \\
& \qquad \text{for all} \\
& \qquad \quad y_i \in \mathcal{Y}, i = 1, \dots, n.
\end{align*}
$${#eq-joint-PMF-iid-random-variables-app}

- In the case of $n$ *iid* `r colourize("continuous random variables", "blue")` $Y_1, \dots, Y_n$ whose common standalone `r colourize("PDF", "blue")` is $f_Y(y)$ with support $\mathcal{Y}$, the joint `r colourize("PDF", "blue")` is mathematically expressed as

$$
\begin{align*}
f_{Y_1, \dots, Y_n}(y_1, \dots, y_n | \boldsymbol{\theta}) &= \prod_{i = 1}^n f_Y(y_i | \boldsymbol{\theta}) \\
& \qquad \text{for all} \\
& \qquad \quad y_i \in \mathcal{Y}, i = 1, \dots, n.
\end{align*}
$${#eq-joint-PDF-iid-random-variables-app}


Unlike @eq-joint-PMF-ind-random-variables-app and @eq-joint-PDF-ind-random-variables-app, note that @eq-joint-PMF-iid-random-variables-app and @eq-joint-PDF-iid-random-variables-app indicate the subscript $Y$ in the corresponding `r colourize("probability distributions", "blue")` since we have identically distributed `r colourize("random variables", "blue")`. Furthermore, the joint distributions are conditioned on the `r colourize("population parameter", "blue")` vector $\boldsymbol{\theta}$ which reflects our `r colourize("generative modelling", "blue")` approach.

::: {.Equivalence}
::::{.Equivalence-header}
Somewhat equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Training dataset", "magenta")`.
::::
:::

### `r colourize("Random variable", "blue")` {.unnumbered}

A `r colourize("random variable", "blue")` is a function where the input values correspond to real numbers assigned to events belonging to the `r colourize("sample space", "blue")` $S$, and whose outcome is one of these real numbers after executing a given random experiment. For instance, a `r colourize("random variable", "blue")` (and its support, i.e., real numbers) is depicted with an uppercase such that 

$$Y \in \mathbb{R}.$$

### `r colourize("Regression analysis", "blue")` {.unnumbered}

### `r colourize("Regressor", "blue")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Attribute", "magenta")`, `r colourize("covariate", "blue")`, `r colourize("exogeneous variable", "magenta")`, `r colourize("explanatory variable", "blue")`, `r colourize("feature", "magenta")`, `r colourize("independent variable", "blue")`, `r colourize("input", "magenta")` or `r colourize("predictor", "magenta")`.
::::
:::

### `r colourize("Response variable", "blue")` {.unnumbered}

In supervised learning, it is the main variable of interest we are trying to **learn** or **predict**, or equivalently, the variable we are trying **explain** in a statistical inference framework.

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Dependent variable", "blue")`, `r colourize("endogeneous variable", "magenta")`, `r colourize("outcome", "magenta")`, `r colourize("output", "magenta")` or `r colourize("target", "magenta")`.
::::
:::

## S {.unnumbered}

### `r colourize("Sample space", "blue")` {.unnumbered}

Let $A$ be an event of interest in a random phenomenon in a `r colourize("population", "blue")` or **system** of interest. The `r colourize("sample space", "blue")` $S$ of event $A$ denotes the set of all the possible **random outcomes** we might encounter every time we randomly observe $A$ such as we were running some class of experiment.

Note each of these outcomes has a determined probability associated with them. If we add up all these probabilities, the probability of the sample $S$ will be one, i.e.,

$$
P(S) = 1.
$${#eq-sample-space-app}

### `r colourize("Significance level", "blue")` {.unnumbered}

### `r colourize("Standard error", "blue")` {.unnumbered}

## T {.unnumbered}

### `r colourize("Target", "magenta")` {.unnumbered}

In supervised learning, it is the main variable of interest we are trying to **learn** or **predict**, or equivalently, the variable we are trying **explain** in a statistical inference framework.

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Dependent variable", "blue")`, `r colourize("endogeneous variable", "magenta")`, `r colourize("response variable", "blue")`, `r colourize("outcome", "magenta")` or `r colourize("output", "magenta")`.
::::
:::

### `r colourize("Test statistic", "blue")` {.unnumbered}

### `r colourize("Training dataset", "magenta")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Somewhat equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Random sample", "blue")`.
::::
:::

### `r colourize("Type I error", "blue")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("False positive", "blue")`.
::::
:::

### `r colourize("Type II error", "blue")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("False negative", "blue")`.
::::
:::

## U {.unnumbered}

### `r colourize("Underdispersion", "blue")` {.unnumbered}

## V {.unnumbered}

### `r colourize("Variance", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("discrete", "blue")` or `r colourize("continuous random variable", "blue")` whose support is $\mathcal{Y}$ with a mean represented by $\mathbb{E}(Y)$. Then, the `r colourize("variance", "blue")` of $Y$ is the `r colourize("mean", "blue")` of the squared deviation from the corresponding `r colourize("mean", "blue")` as follows:

$$
\text{Var}(Y) = \mathbb{E}\left\{[ Y - \mathbb{E}(Y)]^2 \right\}. \\
$$

Note the expression above is equivalent to:

$$
\text{Var}(Y) = \mathbb{E}(Y^2) - \left[ \mathbb{E}(Y) \right]^2.
$$
