<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7PRVEBE1EF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7PRVEBE1EF');
</script>

# ML-Stats Dictionary {#sec-dictionary}

```{r}
#| include: false

colourize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

Machine learning and statistics comprise a **substantial synergy** that is reflected in data science. Thus, it is imperative to construct solid bridges between both disciplines to ensure everything is clear regarding their tremendous amount of jargon and terminology. This **ML-Stats dictionary** (*ML* stands for *Machine Learning*) aims to be one of these bridges, especially within supervised learning and regression analysis contexts.

![Image by [*Gerd Altmann*](https://pixabay.com/users/geralt-9301/) via [*Pixabay*](https://pixabay.com/photos/definition-books-library-bookshelf-4255486/).](img/definition.jpg){width="550"} 

Below, you will find definitions either highlighted in `r colourize("blue", "blue")` if they correspond to `r colourize("statistical", "blue")` terminology or `r colourize("magenta", "magenta")` if the terminology is `r colourize("machine learning-related", "magenta")`. These definitions come from all **definition admonitions** introduced throughout the fifteen main chapters of this textbook. This colour scheme strives to combine all terminology to switch from one field to another easily. With practice and time, you will be able to jump back and forth when using these concepts.

::: {.Equivalence}
::::{.Equivalence-header}
Attention!
::::
::::{.Equivalence-container}
Noteworthy terms (either `r colourize("statistical", "blue")` or `r colourize("machine learning-related", "magenta")`) will include a particular admonition identifying which terms (again, either `r colourize("statistical", "blue")` or `r colourize("machine learning-related", "magenta")`) are **equivalent** or **somewhat equivalent** (**or even not equivalent if that is the case**).
::::
:::

## A {.unnumbered}

### `r colourize("Alternative hypothesis", "blue")` {.unnumbered}

In a `r colourize("hypothesis", "blue")` testing, an `r colourize("alternative hypothesis", "blue")` is denoted by $H_1$. This `r colourize("hypothesis", "blue")` corresponds to the **complement** (i.e., the **opposite**) of the `r colourize("null hypothesis", "blue")` $H_0$. Since the whole inferential process is designed to assess the strength of the evidence in favour or against of $H_0$, any inferential conclusion against $H_0$ can be worded as "*rejecting $H_0$ **in favour** of $H_1$*." In plain words, $H_1$ is an **inferential statement** associated to a **non-status quo** in some `r colourize("population(s)", "blue")` or system(s) of interest, which might refer to **actual signal** for the researcher in question.

Let us assume `r colourize("random variable", "blue")` $Y$ from some `r colourize("population(s)", "blue")` or system(s) of interest is governed by $k$ `r colourize("parameters", "blue")` contained in the following vector:

$$
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T.
$$

Moreover, suppose random variable $Y$ follows certain `r colourize("probability distribution", "blue")` $\mathcal{D}(\cdot)$ in a `r colourize("generative model", "blue")` $m$ as in

$$
\text{$m$: } y \sim \mathcal{D}(\boldsymbol{\theta}).
$$

Let $\boldsymbol{\Theta}_0^c \subset \boldsymbol{\theta}$ denote the non-status quo for the `r colourize("parameter(s)", "blue")` to be tested. Then, the `r colourize("alternative hypothesis", "blue")` is mathematically defined as

$$
\text{$H_1$: } \boldsymbol{\theta} \in \boldsymbol{\Theta}_0^c \quad \text{where} \quad \boldsymbol{\Theta}_0^c \subset \boldsymbol{\theta}.
$$

### `r colourize("Attribute", "magenta")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Covariate", "blue")`, `r colourize("exogeneous variable", "magenta")`, `r colourize("explanatory variable", "blue")`, `r colourize("feature", "magenta")`, `r colourize("independent variable", "blue")`, `r colourize("input", "magenta")`, `r colourize("predictor", "magenta")` or `r colourize("regressor", "blue")`.
::::
:::

### `r colourize("Average", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("random variable", "blue")` whose support is $\mathcal{Y}$. In general, the `r colourize("expected  value", "blue")` or `r colourize("mean", "blue")` $\mathbb{E}(Y)$ of this `r colourize("random variable", "blue")` is defined as a **weighted** `r colourize("average", "blue")` according to its corresponding `r colourize("probability distribution", "blue")`. In other words, this `r colourize("measure of central tendency", "blue")` $\mathbb{E}(Y)$ aims to find the middle value of this `r colourize("random variable", "blue")` by weighting all its possible values in its support $\mathcal{Y}$ as dictated by its `r colourize("probability distribution", "blue")`.

Given the above definition, when $Y$ is a `r colourize("discrete random variable", "blue")` whose `r colourize("probability mass function (PMF)", "blue")` is $P_Y(Y = y)$, then its **weighted** `r colourize("average", "blue")` is mathematically defined as

$$
\mathbb{E}(Y) = \sum_{y \in \mathcal{Y}} y \cdot P_Y(Y = y).
$$

When $Y$ is a `r colourize("continuous random variable", "blue")` whose `r colourize("probability density function (PDF)", "blue")` is $f_Y(y)$, its **weighted** `r colourize("average", "blue")` is mathematically defined as

$$
\mathbb{E}(Y) = \int_{\mathcal{Y}} y \cdot f_Y(y) \mathrm{d}y.
$$

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Expected value", "blue")` or `r colourize("mean", "blue")`.
::::
:::

## B {.unnumbered}

### `r colourize("Bayesian statistics", "blue")` {.unnumbered}

This statistical school of thinking also relies on the **frequency of events** to estimate specific `r colourize("parameters", "blue")` of interest in a `r colourize("population", "blue")` or system. Nevertheless, unlike `r colourize("frequentist", "blue")` statistics, `r colourize("Bayesian", "blue")` statisticians use **prior knowledge** on the `r colourize("population parameters", "blue")` to update their estimations on them along with the **current evidence** they can gather. This evidence is in the form of the repetition of $n$ experiments involving a random phenomenon. All these ingredients allow `r colourize("Bayesian", "blue")` statisticians to make inference by conducting  appropriate `r colourize("hypothesis testings", "blue")`, which are designed differently from their mainstream `r colourize("frequentist", "blue")` counterpart.

Under the umbrella of this approach, we assume that our governing `r colourize("parameters", "blue")` are **random**; i.e., they have their own `r colourize("sample space", "blue")` and `r colourize("probabilities", "blue")` associated to their corresponding outcomes. The statistical process of inference is heavily backed up by probability theory mostly in the form of the `r colourize("Bayes' rule", "blue")` (named after Reverend Thomas Bayes, an English statistician from the 18th century). This rule uses our **current evidence** along with our **prior beliefs** to deliver a **posterior distribution** of our **random** `r colourize("parameter(s)", "blue")` of interest.

### `r colourize("Bayes' rule", "blue")` {.unnumbered}

Suppose you have two events of interest, $A$ and $B$, in a random phenomenon of a `r colourize("population", "blue")` or system of interest. From @eq-conditional-probability-app, we can state the following expression for the `r colourize("conditional probability", "blue")` of $A$ **given** $B$:

$$
P(A | B) = \frac{P(A \cap B)}{P(B)} \quad \text{if $P(B) > 0$.}
$${#eq-cond-prob-A-given-B-app}

Note the `r colourize("conditional probability", "blue")` of $B$ given $A$ can be stated as:

$$
\begin{align*}
P(B | A) &= \frac{P(B \cap A)}{P(A)} \quad \text{if $P(A) > 0$} \\
&= \frac{P(A \cap B)}{P(A)} \quad \text{since $P(B \cap A) = P(A \cap B)$.}
\end{align*}
$${#eq-cond-prob-B-given-A-app}

Then, we can manipulate @eq-cond-prob-B-given-A-app as follows:

$$
P(A \cap B) = P(B | A) \times P(A).
$$

The above result can be plugged into @eq-cond-prob-A-given-B-app:

$$
\begin{align*}
P(A | B) &= \frac{P(A \cap B)}{P(B)} \\
&= \frac{P(B | A) \times P(A)}{P(B)}.
\end{align*}
$${#eq-bayes-rule-app}

@eq-bayes-rule-app is called the `r colourize("Bayes' rule", "blue")`. We are basically flipping around `r colourize("conditional probabilities", "blue")`.

## C {.unnumbered}

### `r colourize("Critical value", "blue")` {.unnumbered}

The `r colourize("critical value", "blue")` of a hypothesis testing defines the region for which we might reject $H_0$ in favour of $H_1$. This `r colourize("critical value", "blue")` is in the function of the `r colourize("significance level", "blue")` $\alpha$ and **test flavour**. It is located on the corresponding $x$-axis of the `r colourize("probability distribution", "blue")` of $H_0$. Hence, this value acts as a threshold to decide either of the following:

- If the **observed** `r colourize("test statistic", "blue")` exceeds a given `r colourize("critical value", "blue")`, then we have enough statistical evidence to reject $H_0$ in favour of $H_1$. 
- If the **observed** `r colourize("test statistic", "blue")` does not exceed a given `r colourize("critical value", "blue")`, then we have enough statistical evidence to fail to reject $H_0$.

### `r colourize("Conditional probability", "blue")` {.unnumbered}

Suppose you have two events of interest, $A$ and $B$, in a random phenomenon, in a `r colourize("population", "blue")` or system of interest. These two events belong to the  `r colourize("sample space", "blue")` $S$. Moreover, assume that the  `r colourize("probability", "blue")` of event $B$ is such that

$$
P(B) > 0,
$$

which is considered the **conditioning event**.

Hence, the `r colourize("conditional probability", "blue")` event $A$ **given** event $B$ is defined as

$$
P(A | B) = \frac{P(A \cap B)}{P(B)},
$${#eq-conditional-probability-app}

where $P(A \cap B)$ is read as **the `r colourize("probability", "blue")` of the intersection of events $A$ and $B$**.

### `r colourize("Confidence interval", "blue")` {.unnumbered}

A `r colourize("confidence interval", "blue")` provides an estimated range of values within which the true `r colourize("population parameter", "blue")` is likely to fall, based on the sample data. It reflects the degree of uncertainty associated with the obtained `r colourize("estimate", "blue")`. 

For instance, a 95% `r colourize("confidence interval", "blue")` means that if the study were repeated many times using different `r colourize("random samples", "blue")` from the same `r colourize("population", "blue")` or **system** of interest, approximately 95% of the resulting intervals would contain the true `r colourize("parameter", "blue")`.

### `r colourize("Continuous random variable", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("random variable", "blue")` whose support is $\mathcal{Y}$. If this support $\mathcal{Y}$ corresponds to an uncountably infinite set of possible values, then $Y$ is considered a `r colourize("continuous random variable", "blue")`.

Note a `r colourize("continuous random variable", "blue")` could be 

- **completely unbounded** (i.e., its set of possible values goes from $-\infty$ to $\infty$ as in $-\infty < y < \infty$), 
- **positively unbounded** (i.e., its set of possible values goes from $0$ to $\infty$ as in $0 \leq y < \infty$), 
- **negatively unbounded** (i.e., its set of possible values goes from $-\infty$ to $0$ as in $-\infty < y \leq 0$), or
- **bounded** between two values $a$ and $b$ (i.e., its set of possible values goes from $a$ to $b$ as in $a \leq y \leq b$).

### `r colourize("Covariate", "blue")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Attribute", "magenta")`, `r colourize("exogeneous variable", "magenta")`, `r colourize("explanatory variable", "blue")`, `r colourize("feature", "magenta")`, `r colourize("independent variable", "blue")`, `r colourize("input", "magenta")`, `r colourize("predictor", "magenta")` or `r colourize("regressor", "blue")`.
::::
:::

### `r colourize("Cumulative distribution function", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("random variable", "blue")` either `r colourize("discrete", "blue")` or `r colourize("continuous", "blue")`. Its `r colourize("cumulative distribution function (CDF)", "blue")` $F_Y(y)  : \mathbb{R} \rightarrow [0, 1]$ refers to the `r colourize("probability", "blue")` that $Y$ is less or equal than an observed value $y$:

$$
F_Y(y) = P(Y \leq y).
$$

Then, we have the following by type of  `r colourize("random variable", "blue")`:

- When $Y$ is `r colourize("discrete", "blue")`, whose support is $\mathcal{Y}$, suppose it has a `r colourize("probability mass function (PMF)", "blue")` $P_Y(Y = y)$. Then, the `r colourize("CDF", "blue")` is mathematically represented as:

$$
F_Y(y) = \sum_{\substack{t \in \mathcal{Y} \\ t \leq y}} P_Y(Y = t).
$${#eq-cdf-discrete-app}

- When $Y$ is `r colourize("continuous", "blue")`, whose support is $\mathcal{Y}$, suppose it has a `r colourize("probability density function (PDF)", "blue")` $f_Y(y)$. Then, the `r colourize("CDF", "blue")` is mathematically represented as:

$$
F_Y(y) = \int_{-\infty}^y f_Y(t) \mathrm{d}t.
$${#eq-cdf-continuous-app}

Note that in @eq-cdf-discrete-app and @eq-cdf-continuous-app, we use the auxiliary variable $t$ since we do not compute the summation or integral over the observed $y$ given its role on either the `r colourize("PMF", "blue")` or `r colourize("PDF", "blue")`. Therefore, we use this auxiliary variable $t$.

## D {.unnumbered}
 
### `r colourize("Dependent variable", "blue")` {.unnumbered}

In supervised learning, it is the main variable of interest we are trying to **learn** or **predict**, or equivalently, the variable we are trying **explain** in a statistical inference framework.

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Endogeneous variable", "magenta")`, `r colourize("response variable", "blue")`, `r colourize("outcome", "magenta")`, `r colourize("output", "magenta")` or `r colourize("target", "magenta")`.
::::
:::

### `r colourize("Discrete random variable", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("random variable", "blue")` whose support is $\mathcal{Y}$. If this support $\mathcal{Y}$ corresponds to a finite set or a countably infinite set of possible values, then $Y$ is considered a `r colourize("discrete random variable", "blue")`. 

For instance, we can encounter `r colourize("discrete random variables", "blue")` which could be classified as

- **binary** (i.e., a finite set of two possible values),
- **categorical** (either **nominal** or **ordinal**, which have a finite set of three or more possible values), or 
- **counts** (which might have a finite set or a countably infinite set of possible values as integers).

### `r colourize("Dispersion", "blue")` {.unnumbered}

## E {.unnumbered}

### `r colourize("Endogeneous variable", "magenta")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Dependent variable", "blue")`, `r colourize("outcome", "magenta")`, `r colourize("output", "magenta")`, `r colourize("response variable", "blue")` or `r colourize("target", "magenta")`.
::::
:::

### `r colourize("Equidispersion", "blue")` {.unnumbered}

### `r colourize("Estimate", "blue")` {.unnumbered}

Suppose we have an **observed** `r colourize("random sample", "blue")` of size $n$ with values $y_1, \dots , y_n$. Then, we apply a given `r colourize("estimator", "blue")` mathematical rule to these $n$ observed values. Hence, this numerical computation is called an `r colourize("estimate", "blue")` of our `r colourize("population parameter", "blue")` of interest.

### `r colourize("Estimator", "blue")` {.unnumbered}

An `r colourize("estimator", "blue")` is a mathematical rule involving the random variables $Y_1, \dots, Y_n$ from our `r colourize("random sample", "blue")` of size $n$. As its name says, this rule allows us to estimate our `r colourize("population parameter", "blue")` of interest.

### `r colourize("Expected value", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("random variable", "blue")` whose support is $\mathcal{Y}$. In general, the `r colourize("expected  value", "blue")` or `r colourize("mean", "blue")` $\mathbb{E}(Y)$ of this `r colourize("random variable", "blue")` is defined as a **weighted** `r colourize("average", "blue")` according to its corresponding `r colourize("probability distribution", "blue")`. In other words, this `r colourize("measure of central tendency", "blue")` $\mathbb{E}(Y)$ aims to find the middle value of this `r colourize("random variable", "blue")` by weighting all its possible values in its support $\mathcal{Y}$ as dictated by its `r colourize("probability distribution", "blue")`.

Given the above definition, when $Y$ is a `r colourize("discrete random variable", "blue")` whose `r colourize("probability mass function (PMF)", "blue")` is $P_Y(Y = y)$, then its `r colourize("expected  value", "blue")` is mathematically defined as

$$
\mathbb{E}(Y) = \sum_{y \in \mathcal{Y}} y \cdot P_Y(Y = y).
$${#eq-expected-value-discrete-app}

When $Y$ is a `r colourize("continuous random variable", "blue")` whose `r colourize("probability density function (PDF)", "blue")` is $f_Y(y)$, its `r colourize("expected  value", "blue")` is mathematically defined as

$$
\mathbb{E}(Y) = \int_{\mathcal{Y}} y \cdot f_Y(y) \mathrm{d}y.
$${#eq-expected-value-continuous-app}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Average", "blue")` or `r colourize("mean", "blue")`.
::::
:::

### `r colourize("Exogeneous variable", "magenta")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Attribute", "magenta")`, `r colourize("covariate", "blue")`, `r colourize("explanatory variable", "blue")`, `r colourize("feature", "magenta")`, `r colourize("independent variable", "blue")`, `r colourize("input", "magenta")`, `r colourize("predictor", "magenta")` or `r colourize("regressor", "blue")`.
::::
:::

### `r colourize("Explanatory variable", "blue")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Attribute", "magenta")`, `r colourize("covariate", "blue")`, `r colourize("exogeneous variable", "magenta")`, `r colourize("feature", "magenta")`, `r colourize("independent variable", "blue")`, `r colourize("input", "magenta")`, `r colourize("predictor", "magenta")` or `r colourize("regressor", "blue")`.
::::
:::

## F {.unnumbered}

### `r colourize("False negative", "blue")` {.unnumbered}

A `r colourize("false negative", "blue")` is defined as **incorrectly** failing to reject the `r colourize("null hypothesis", "blue")` $H_0$ in favour of the `r colourize("alternative hypothesis", "blue")` $H_1$ when, in fact, **$H_0$ is false**.

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Type II error", "blue")`.
::::
:::

### `r colourize("False positive", "blue")` {.unnumbered}

A `r colourize("false positive", "blue")` is defined as **incorrectly** rejecting the `r colourize("null hypothesis", "blue")` $H_0$ in favour of the `r colourize("alternative hypothesis", "blue")` $H_1$ when, in fact, **$H_0$ is true**. @tbl-errors-app-1 summarizes the types of inferential conclusions in function on whether $H_0$ is true or not.

|       | **$H_0$ is true** | **$H_0$ is false** |
|:-----:|:-----:|:-----:|
| **Reject $H_0$** | Type I error (*False positive*) | Correct |
| **Fail to reject $H_0$** | Correct | Type II error (*False negative*) |

: Types of inferential conclusions in a frequentist hypothesis testing. {#tbl-errors-app-1 .hover}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Type I error", "blue")`.
::::
:::

### `r colourize("Feature", "magenta")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Attribute", "magenta")`, `r colourize("covariate", "blue")`, `r colourize("exogeneous variable", "magenta")`, `r colourize("explanatory variable", "blue")`, `r colourize("independent variable", "blue")`, `r colourize("input", "magenta")`, `r colourize("predictor", "magenta")` or `r colourize("regressor", "blue")`.
::::
:::

### `r colourize("Frequentist statistics", "blue")` {.unnumbered}

This statistical school of thinking heavily relies on the **frequency of events** to estimate specific `r colourize("parameters", "blue")` of interest in a `r colourize("population", "blue")` or system. This frequency of events is reflected in the repetition of $n$ experiments involving a random phenomenon within this `r colourize("population", "blue")` or system.

Under the umbrella of this approach, we assume that our governing `r colourize("parameters", "blue")` are **fixed**. Note that, within the philosophy of this school of thinking, we can only make **precise** and **accurate** predictions as long as we repeat our $n$ experiments as many times as possible, i.e., 

$$
n \rightarrow \infty.
$$

## G {.unnumbered}

### `r colourize("Generalized linear models", "blue")` {.unnumbered}

An umbrella of regression approaches that model the conditional `r colourize("expected value", "blue")` of the `r colourize("response variable", "blue")` $Y$ based on a set of observed `r colourize("regressors", "blue")` $x$. Unlike a traditional model such as the `r colourize("continuous", "blue")` Ordinary Least-squares (OLS) that relies solely on a Normal distribution **to make inference**, `r colourize("GLMs", "blue")` extend this distributional assumption, allowing for a variety of `r colourize("probability distributions", "blue")` for the `r colourize("response variable", "blue")`. Note that this umbrella encompasses approaches that accommodate `r colourize("continuous", "blue")` or `r colourize("discrete", "blue")` responses $Y$. According to @casella2024, a typical `r colourize("GLM", "blue")` consists of three key components:

- **Random Component:** The `r colourize("response variables", "blue")` in a `r colourize("training dataset", "magenta")` of size $n$ (i.e., the `r colourize("random variables", "blue")` $Y_1, Y_2, \ldots, Y_n$) are **statistically `r colourize("independent", "blue")` but not identically distributed**. Still, they do belong to the same family of `r colourize("probability distributions", "blue")` (e.g., Gamma, Beta, Poisson, Bernoulli, etc.).
- **Systematic Component:** For the $i$th observation, this component depicts how the $k$ `r colourize("regressors", "blue")` $x_{i, j}$ (for $j = 1, 2, \ldots, k$) come into the `r colourize("GLM", "blue")` as a linear combination involving $k + 1$ regression `r colourize("parameters", "blue")` $\beta_0, \beta_1, \ldots, \beta_k$. This relationship is expressed as

$$
\eta_i = \beta_0 + \beta_1 x_{i, 1} + \beta_2 x_{i, 2} + \ldots + \beta_k x_{i, k}.
$$

- **Link Function:** This component connects (or "*links*") the systematic component $\eta_i$ with the `r colourize("mean", "blue")` of the `r colourize("random variable", "blue")` $Y_i$, denoted as $\mu_i$. The link function is mathematically represented as

$$
g(\mu_i) = \eta_i.
$$

@nelder1972 introduced **this umbrella term called `r colourize("GLM", "blue")`** in the statistical literature and identified a set of distinct statistical models that shared the above three components.

### `r colourize("Generative model", "blue")` {.unnumbered}

Suppose you observe some data $y$ from a `r colourize("population", "blue")` or system of interest. Moreover, let us assume this `r colourize("population", "blue")` or system is governed by $k$ `r colourize("parameters", "blue")` contained in the following vector:

$$
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T.
$$

If we state that the `r colourize("random variable", "blue")` $Y$ follows certain `r colourize("probability distribution", "blue")` $\mathcal{D}(\cdot)$, then we will have a `r colourize("generative model", "blue")` $m$ such that

$$
\text{$m$: } Y \sim \mathcal{D}(\boldsymbol{\theta}).
$$

## H {.unnumbered}

### `r colourize("Hypothesis", "blue")` {.unnumbered}

Suppose you observe some data $y$ from some `r colourize("population(s)", "blue")` or system(s) of interest governed by $k$ `r colourize("parameters", "blue")` contained in the following vector:

$$
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T.
$$

Moreover, let us assume that `r colourize("random variable", "blue")` $Y$ follows certain `r colourize("probability distribution", "blue")` $\mathcal{D}(\cdot)$ in a `r colourize("generative model", "blue")` $m$ as in

$$
\text{$m$: } Y \sim \mathcal{D}(\boldsymbol{\theta}).
$$

Beginning from the fact that $\boldsymbol{\theta} \in \boldsymbol{\Theta}$ where $\boldsymbol{\Theta} \in \mathbb{R}^k$, a statistical `r colourize("hypothesis", "blue")` is a general statement about some `r colourize("parameter", "blue")` vector $\boldsymbol{\theta}$ in regards to specific values contained in vector $\boldsymbol{\Theta}^*$ such that

$$
\text{$H$: } \boldsymbol{\theta} \in \boldsymbol{\Theta}^* \quad \text{where} \quad \boldsymbol{\Theta}^* \subset \boldsymbol{\Theta}.
$$

### `r colourize("Hypothesis testing", "blue")` {.unnumbered}

A `r colourize("hypothesis testing", "blue")` is the **decision rule** we have to apply between the `r colourize("null", "blue")` and `r colourize("alternative hypotheses", "blue")`, via our sample data, to **fail to reject** or **reject** the `r colourize("null hypothesis", "blue")`.

## I {.unnumbered}

### `r colourize("Independence", "blue")` {.unnumbered}

Suppose you have two events of interest, $A$ and $B$, in a random phenomenon of a `r colourize("population", "blue")` or system of interest. These two events are **statistically** `r colourize("independent", "blue")` if event $B$ does not affect event $A$ and vice versa. Therefore, the `r colourize("probability", "blue")` of their corresponding intersection is given by:

$$
P(A \cap B) = P(A) \times P(B).
$$

Let us expand the above definition to a `r colourize("random variable", "blue")` framework:

- Suppose you have a set of $n$ `r colourize("discrete random variables", "blue")` $Y_1, \dots, Y_n$ whose supports are $\mathcal{Y_1}, \dots, \mathcal{Y_n}$ with `r colourize("probability mass functions (PMFs)", "blue")` $P_{Y_1}(Y_1 = y_1), \dots, P_{Y_n}(Y_n = y_n)$ respectively. That said, the joint `r colourize("PMF", "blue")` of these $n$ `r colourize("random variables", "blue")` is the multiplication of their corresponding standalone `r colourize("PMFs", "blue")`:

$$
\begin{align*}
P_{Y_1, \dots, Y_n}(Y_1 = y_1, \dots, Y_n = y_n) &= \prod_{i = 1}^n P_{Y_i}(Y_i = y_i) \\
& \qquad \text{for all} \\
& \qquad \quad y_i \in \mathcal{Y}_i, i = 1, \dots, n.
\end{align*}
$${#eq-joint-PMF-ind-random-variables-app}

- Suppose you have a set of $n$ `r colourize("continuous random variables", "blue")` $Y_1, \dots, Y_n$ whose supports are $\mathcal{Y_1}, \dots, \mathcal{Y_n}$ with `r colourize("probability density functions (PDFs)", "blue")` $f_{Y_1}(y_1), \dots, f_{Y_n}(y_n)$ respectively. That said, the joint `r colourize("PDF", "blue")` of these $n$ `r colourize("random variables", "blue")` is the multiplication of their corresponding standalone `r colourize("PDFs", "blue")`:

$$
\begin{align*}
f_{Y_1, \dots, Y_n}(y_1, \dots, y_n) &= \prod_{i = 1}^n f_{Y_i}(y_i) \\
& \qquad \text{for all} \\
& \qquad \quad y_i \in \mathcal{Y}_i, i = 1, \dots, n.
\end{align*}
$${#eq-joint-PDF-ind-random-variables-app}

### `r colourize("Independent variable", "blue")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Attribute", "magenta")`, `r colourize("covariate", "blue")`, `r colourize("exogeneous variable", "magenta")`, `r colourize("explanatory variable", "blue")`, `r colourize("feature", "magenta")`, `r colourize("input", "magenta")`, `r colourize("predictor", "magenta")` or `r colourize("regressor", "blue")`.
::::
:::

### `r colourize("Input", "magenta")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Attribute", "magenta")`, `r colourize("covariate", "blue")`, `r colourize("exogeneous variable", "magenta")`, `r colourize("explanatory variable", "blue")`, `r colourize("feature", "magenta")`, `r colourize("independent variable", "blue")`, `r colourize("predictor", "magenta")` or `r colourize("regressor", "blue")`.
::::
:::

## L {.unnumbered}

### `r colourize("Likelihood function", "blue")` {.unnumbered}

Suppose you observe some data $y$ from a `r colourize("population", "blue")` or system of interest which is governed by $k$ `r colourize("parameters", "blue")` contained in the following vector:

$$
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T.
$$

The corresponding `r colourize("random variable", "blue")` $Y$ has a given `r colourize("probability", "blue")` function $P_Y(Y = y | \boldsymbol{\theta})$, which can be either a `r colourize("probability mass function (PMF)", "blue")` in the `r colourize("discrete", "blue")` case or a `r colourize("probability density function (PDF)", "blue")` in the `r colourize("continuous", "blue")` case. Then, the `r colourize("likelihood function", "blue")` for the `r colourize("parameter", "blue")` vector $\boldsymbol{\theta}$ given the observed data $y$ is **mathematically equivalent** to the aforementioned `r colourize("probability", "blue")` function such that:

$$
L(\boldsymbol{\theta} | y) = P_Y(Y = y | \boldsymbol{\theta}).
$${#eq-likelihood-function-app}

It is important to note that the above `r colourize("likelihood", "blue")` is in function of the `r colourize("parameter", "blue")` vector $\boldsymbol{\theta}$ and conditioned on the observed data $y$. Additionally, **in many `r colourize("continuous", "blue")` cases**, the `r colourize("likelihood function", "blue")` may exceed $1$ given the definition of bounds we have already established for a `r colourize("PDF", "blue")` (see @eq-lower-bound-PDF-app).

### `r colourize("Log-likelihood function", "blue")` {.unnumbered}

Suppose you observe some data $y$ from a `r colourize("population", "blue")` or system of interest which is governed by $k$ `r colourize("parameters", "blue")` contained in the following vector:

$$
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T.
$$

The corresponding `r colourize("random variable", "blue")` $Y$ has a given `r colourize("probability", "blue")` function $P_Y(Y = y | \boldsymbol{\theta})$, which can be either a `r colourize("probability mass function (PMF)", "blue")` in the `r colourize("discrete", "blue")` case or a `r colourize("probability density function (PDF)", "blue")` in the `r colourize("continuous", "blue")` case. Moreover, the `r colourize("likelihood function", "blue")`, as described in @eq-likelihood-function-app, is defined as follows:

$$
L(\boldsymbol{\theta} | y) = P_Y(Y = y | \boldsymbol{\theta}).
$$

Then, the `r colourize("log-likelihood function", "blue")` is merely the logarithm of the above function:

$$
\log L(\boldsymbol{\theta} | y) = \log \left[ P_Y(Y = y | \boldsymbol{\theta}) \right].
$${#eq-log-likelihood-function-app}

Using a `r colourize("log-likelihood function", "blue")`, which is a monotonic transformation of the `r colourize("likelihood function", "blue")`, offers the following practical advantages:

1. The logarithmic properties convert products into sums. This is particularly useful in `r colourize("likelihood functions", "blue")` for `r colourize("random samples", "blue")` that involve multiplying `r colourize("probability", "blue")` functions (and its corresponding factors).
2. When estimating `r colourize("parameters", "blue")`, calculating the derivative of a sum is easier than that of a product.
3. In many cases, the `r colourize("likelihood functions", "blue")` for observed `r colourize("random samples", "blue")` can yield very small values, which may lead to computational instability. By working on a logarithmic scale, these computations become more stable. This stability is crucial for numerical optimization methods applied to a given `r colourize("log-likelihood function", "blue")`, in cases where a closed-form solution for an `r colourize("estimate", "blue")` is not mathematically feasible.

## M {.unnumbered}

### `r colourize("Maximum likelihood estimation (MLE)", "blue")` {.unnumbered}

Suppose you observe some data $y$ from a `r colourize("population", "blue")` or system of interest which is governed by $k$ `r colourize("parameters", "blue")` contained in the following vector:

$$
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T.
$$

The corresponding `r colourize("random variable", "blue")` $Y$ has a given `r colourize("probability", "blue")` function $P_Y(Y = y | \boldsymbol{\theta})$, which can be either a `r colourize("probability mass function (PMF)", "blue")` in the `r colourize("discrete", "blue")` case or a `r colourize("probability density function (PDF)", "blue")` in the `r colourize("continuous", "blue")` case. Furthermore, the `r colourize("log-likelihood function", "blue")` is defined as in @eq-log-likelihood-function-app:

$$
\log L(\boldsymbol{\theta} | y) = \log \left[ P_Y(Y = y | \boldsymbol{\theta}) \right].
$$

`r colourize("Maximum likelihood estimation (MLE)", "blue")` aims to find the `r colourize("estimate", "blue")` of $\boldsymbol{\theta}$ that maximizes the above `r colourize("log-likelihood function", "blue")` as in:

$$
\hat{\boldsymbol{\theta}}_{\text{MLE}} = \underset{\boldsymbol{\theta}}{\operatorname{arg max}} \log L(\boldsymbol{\theta} | y).
$$

In `r colourize("supervised learning", "magenta")`, MLE is analogous to minimizing any given `r colourize("loss function", "magenta")` during model training.

### `r colourize("Mean", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("random variable", "blue")` whose support is $\mathcal{Y}$. In general, the `r colourize("expected  value", "blue")` or `r colourize("mean", "blue")` $\mathbb{E}(Y)$ of this `r colourize("random variable", "blue")` is defined as a **weighted** `r colourize("average", "blue")` according to its corresponding `r colourize("probability distribution", "blue")`. In other words, this `r colourize("measure of central tendency", "blue")` $\mathbb{E}(Y)$ aims to find the middle value of this `r colourize("random variable", "blue")` by weighting all its possible values in its support $\mathcal{Y}$ as dictated by its `r colourize("probability distribution", "blue")`.

Given the above definition, when $Y$ is a `r colourize("discrete random variable", "blue")` whose `r colourize("probability mass function (PMF)", "blue")` is $P_Y(Y = y)$, then its `r colourize("mean", "blue")` is mathematically defined as

$$
\mathbb{E}(Y) = \sum_{y \in \mathcal{Y}} y \cdot P_Y(Y = y).
$$

When $Y$ is a `r colourize("continuous random variable", "blue")` whose `r colourize("probability density function (PDF)", "blue")` is $f_Y(y)$, its `r colourize("mean", "blue")` is mathematically defined as

$$
\mathbb{E}(Y) = \int_{\mathcal{Y}} y \cdot f_Y(y) \mathrm{d}y.
$$

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Average", "blue")` or `r colourize("expected value", "blue")`.
::::
:::

### `r colourize("Measure of central tendency", "blue")` {.unnumbered}

Probabilistically, a `r colourize("measure of central tendency", "blue")` is defined as a metric that identifies a **central or typical value** of a given `r colourize("probability distribution", "blue")`. In other words, a `r colourize("measure of central tendency", "blue")` refers to a central or typical value that a given `r colourize("random variable", "blue")` might take when we observe various realizations of this variable over a long period.

### `r colourize("Measure of uncertainty", "blue")` {.unnumbered}

Probabilistically, a `r colourize("measure of uncertainty", "blue")` refers to the **spread** of a given `r colourize("random variable", "blue")` when we observe its different realizations in the long term. Note a **larger spread** indicates more variability in these realizations. On the other hand, a **smaller spread** denotes less variability in these realizations.

## N {.unnumbered}

### `r colourize("Null hypothesis", "blue")` {.unnumbered}

In a `r colourize("hypothesis(s)", "blue")` testing, a `r colourize("null hypothesis", "blue")` is denoted by $H_0$. The whole inferential process is designed to assess the strength of the evidence in favour or against this `r colourize("null hypothesis", "blue")`. In plain words, $H_0$ is an **inferential statement** associated to the **status quo** in some `r colourize("population(s)", "blue")` or system(s) of interest, which might refer to **no signal** for the researcher in question.

Again, suppose `r colourize("random variable", "blue")` $Y$ from some `r colourize("population(s)", "blue")` or system(s) of interest is governed by $k$ `r colourize("parameters", "blue")` contained in the following vector:

$$
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T.
$$

Moreover, we assume that `r colourize("random variable", "blue")` $Y$ follows certain `r colourize("probability distribution", "blue")` $\mathcal{D}(\cdot)$ in a `r colourize("generative model", "blue")` $m$ as in

$$
\text{$m$: } Y \sim \mathcal{D}(\boldsymbol{\theta}).
$$

Let $\boldsymbol{\Theta}_0 \subset \boldsymbol{\theta}$ denote the status quo for the `r colourize("parameter(s)", "blue")` to be tested. Then, the `r colourize("null hypothesis", "blue")` is mathematically defined as

$$
\text{$H_0$: } \boldsymbol{\theta} \in \boldsymbol{\Theta}_0 \quad \text{where} \quad \boldsymbol{\Theta}_0 \subset \boldsymbol{\theta}.
$$

## O {.unnumbered}

### `r colourize("Observed effect", "blue")` {.unnumbered}

An `r colourize("observed effect", "blue")` is the difference between the `r colourize("estimate", "blue")` provided the **observed** `r colourize("random sample", "blue")` (of size $n$, as in $y_1, \dots, y_n$) to the hypothesized value(s) of the `r colourize("population parameter(s)", "blue")` depicted in the `r colourize("statistical hypotheses", "blue")`.

### `r colourize("Outcome", "magenta")` {.unnumbered}

In supervised learning, it is the main variable of interest we are trying to **learn** or **predict**, or equivalently, the variable we are trying **explain** in a statistical inference framework.

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Dependent variable", "blue")`, `r colourize("endogeneous variable", "magenta")`, `r colourize("response variable", "blue")`, `r colourize("output", "magenta")` or `r colourize("target", "magenta")`.
::::
:::

### `r colourize("Output", "magenta")` {.unnumbered}

In supervised learning, it is the main variable of interest we are trying to **learn** or **predict**, or equivalently, the variable we are trying **explain** in a statistical inference framework.

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Dependent variable", "blue")`, `r colourize("endogeneous variable", "magenta")`, `r colourize("response variable", "blue")`, `r colourize("outcome", "magenta")` or `r colourize("target", "magenta")`.
::::
:::

### `r colourize("Overdispersion", "blue")` {.unnumbered}

## P {.unnumbered}

### `r colourize("Parameter", "blue")` {.unnumbered}

It is a characteristic (**numerical** or even **non-numerical**, such as a **distinctive category**) that **summarizes** the state of our `r colourize("population", "blue")` or system of interest.

Note the **standard mathematical notation** for `r colourize("population parameters", "blue")` are **Greek letters** (for more insights, you can check @sec-greek-alphabet). Moreover, in practice, these `r colourize("population parameter(s)", "blue")` of interest will be **unknown** to the data scientist or researcher. Instead, they would use formal statistical inference to **estimate** them.

### `r colourize("Parametric model", "blue")` {.unnumbered}

A `r colourize("parametric model", "blue")` is a type of model that assumes a specific functional relationship between the `r colourize("response variable", "blue")` of interest, $Y$, which is considered a `r colourize("random variable", "blue")`, and one or more observed `r colourize("explanatory variables", "blue")`, $x$. This relationship is characterized by a finite set of `r colourize("parameters", "blue")` and can often be expressed as a linear combination of the observed $x$ variables, which favours **interpretability**.

Moreover, since $Y$ is a `r colourize("random variable", "blue")`, there is room to make further assumptions on it in the form of a `r colourize("probability distribution", "blue")`, `r colourize("independence", "blue")` or even homoscedasticity (the condition where all responses in the `r colourize("population", "blue")` have the same `r colourize("variance", "blue")`). It is essential to test these assumptions after fitting this type of models, as any deviations may result in **misleading or biased** `r colourize("estimates", "blue")`, predictions, and inferential conclusions.

### `r colourize("Point estimate", "blue")` {.unnumbered}

Let $\theta$ denote a `r colourize("population parameter", "blue")` of interest. Suppose you have observed a `r colourize("random sample", "blue")` of size $n$, represented as the vector:

$$
\boldsymbol{y} = (y_1, y_2, \ldots, y_n)^T.
$$

The `r colourize("point estimate", "blue")` $\hat{\theta}$ serves as a possible value for $\theta$ and is expressed as a function of the observed `r colourize("random sample", "blue")` contained in $\boldsymbol{y}$:

$$
\hat{\theta} = h(\boldsymbol{y}).
$$

### `r colourize("Population", "blue")` {.unnumbered}

It is a **whole collection of individuals or items** that share **distinctive attributes**. As data scientists or researchers, we are interested in studying these attributes, which we assume are **governed** by `r colourize("parameters", "blue")`. In practice, we must be **as specific as possible** when defining our given `r colourize("population", "blue")` such that we would frame our entire data modelling process since its very early stages.

Note that the term `r colourize("population", "blue")` could be exchanged for the term **system**, given that certain contexts do not specifically refer to individuals or items. Instead, these contexts could refer to processes whose attributes are also governed by `r colourize("parameters", "blue")`.

### `r colourize("Power", "blue")` {.unnumbered}

The statistical `r colourize("power", "blue")` of a test $1 -\beta$ is the complement of the `r colourize("conditional probability", "blue")` $\beta$ of failing to reject the `r colourize("null hypothesis", "blue")` $H_0$ given that $H_0$ is false, which is mathematically represented as

$$
P \left( \text{Failing to reject $H_0$} | \text{$H_0$ is false} \right) = \beta;
$$

yielding

$$
\text{Power} = 1 - \beta. 
$$

In plain words, $1 - \beta \in [0, 1]$ is the **probabilistic ability** of our `r colourize("hypothesis testing", "blue")` to detect any signal in our inferential process, **if there is any**. **The larger the `r colourize("power", "blue")` in our `r colourize("power analysis", "blue")`, the less prone we are to commit a `r colourize("type II error", "blue")`.**

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("True positive rate", "blue")`.
::::
:::

### `r colourize("Power analysis", "blue")` {.unnumbered}

The `r colourize("power analysis", "blue")` is a set of statistical tools used to compute the **minimum required sample size $n$** for any given inferential study. These tools require the `r colourize("significance level", "blue")`, `r colourize("power", "blue")`, and **effect size** (i.e., the **magnitude of the signal**) the researcher aims to detect via their inferential study. This analysis seeks to determine whether observed results are likely **due to chance** or represent a **true and meaningful effect**.

### `r colourize("Predictor", "magenta")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Attribute", "magenta")`, `r colourize("covariate", "blue")`, `r colourize("exogeneous variable", "magenta")`, `r colourize("explanatory variable", "blue")`, `r colourize("feature", "magenta")`, `r colourize("independent variable", "blue")`, `r colourize("input", "magenta")` or `r colourize("regressor", "blue")`.
::::
:::

### `r colourize("Probability", "blue")` {.unnumbered}

Let $A$ be an event of interest in a random phenomenon, in a `r colourize("population", "blue")` or system of interest, whose all possible outcomes belong to a given `r colourize("sample space", "blue")` $S$. Generally, the `r colourize("probability", "blue")` for this event $A$ happening can be mathematically depicted as $P(A)$. Moreover, suppose we observe the random phenomenon $n$ times such as we were running some class of experiment, then $P(A)$ is defined as the following ratio:

$$
P(A) = \frac{\text{Number of times event $A$ is observed}}{n},
$${#eq-probability-dictionary}

**as the $n$ times we observe the random phenomenon goes to infinity**.

@eq-probability-dictionary will always put $P(A)$ in the following numerical range:

$$
0 \leq P(A) \leq 1.
$$

### `r colourize("Probability distribution", "blue")` {.unnumbered}

When we set a `r colourize("random variable", "blue")` $Y$, we also set a new set of $v$ possible outcomes $\mathcal{Y} = \{ y_1, \dots, y_v\}$ coming from the `r colourize("sample space", "blue")` $S$. This new set of possible outcomes $\mathcal{Y}$ corresponds to the support of the `r colourize("random variable", "blue")` $Y$ (i.e., all the possible values that could be taken on once we execute a given random experiment involving $Y$). 

That said, let us suppose we have a `r colourize("sample space", "blue")` of $u$ elements defined as 

$$
S = \{ s_1, \dots, s_u \},
$$

where each one of these elements has a `r colourize("probability", "blue")` assigned via a function $P_S(\cdot)$ such that

$$
P(S) = \sum_{i = 1}^u P_S(s_i) = 1.
$$

which has to satisfy @eq-sample-space-app.

Then, the `r colourize("probability distribution", "blue")` of $Y$, i.e., $P_Y(\cdot)$ assigns a `r colourize("probability", "blue")` to each **observed value** $Y = y_j$ (with $j = 1, \dots, v$) if and only if the outcome of the random experiment belongs to the `r colourize("sample space", "blue")`, i.e., $s_i \in S$ (for $i = 1, \dots, u$) such that $Y(s_i) = y_j$:

$$
P_Y(Y = y_j) = P \left( \left\{ s_i \in S : Y(s_i) = y_j \right\} \right).
$$

### `r colourize("Probability density function (PDF)", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("continuous random variable", "blue")` whose support is $\mathcal{Y}$. Furthermore, consider a function $f_Y(y)$ such that 

$$
f_Y(y) : \mathbb{R} \rightarrow \mathbb{R}
$$

with

$$
f_Y(y) \geq 0.
$${#eq-lower-bound-PDF-app}

Then, $f_Y(y)$ is considered a `r colourize("probability density function (PDF)", "blue")` if the `r colourize("probability", "blue")` of $Y$ taking on a value within the range represented by the subset $A \subset \mathcal{Y}$ is equal to

$$
P_Y(Y \in A) = \int_A f_Y(y) \mathrm{d}y
$$

with

$$
\int_{\mathcal{Y}} f_Y(y) \mathrm{d}y = 1.
$$

### `r colourize("Probability mass function (PMF)", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("discrete random variable", "blue")` whose support is $\mathcal{Y}$. Moreover, suppose that $Y$ has a `r colourize("probability distribution", "blue")` such that

$$
P_Y(Y = y) : \mathbb{R} \rightarrow [0, 1]
$$

where, for all $y \notin \mathcal{Y}$, we have

$$
P_Y(Y = y) = 0 
$$

and

$$
\sum_{y \in \mathcal{Y}} P_Y(Y = y) = 1.
$$

Then, $P_Y(Y = y)$ is considered a `r colourize("probability mass function (PMF)", "blue")`.

### `r colourize("$p$-value", "blue")` {.unnumbered}

A `r colourize("$p$-value", "blue")` refers to the `r colourize("probability", "blue")` of obtaining a `r colourize("test statistic", "blue")` just as **extreme** or **more extreme** than the **observed** `r colourize("test statistic", "blue")` coming from our **observed** `r colourize("random sample", "blue")` of size $n$. This `r colourize("$p$-value", "blue")` is obtained via the `r colourize("probability distribution", "blue")` of $H_0$ and the **observed** `r colourize("test statistic", "blue")`.

Alternatively to a `r colourize("critical value", "blue")`, we can reject or fail to reject the `r colourize("null hypothesis", "blue")` $H_0$ using this `r colourize("$p$-value", "blue")` as follows:

- If the `r colourize("$p$-value", "blue")` associated to the **observed** `r colourize("test statistic", "blue")` exceeds a given significance level $\alpha$, then we have enough statistical evidence to reject $H_0$ in favour of $H_1$. 
- If the `r colourize("$p$-value", "blue")` associated to the **observed** `r colourize("test statistic", "blue")` does not exceed a given significance level $\alpha$, then we have enough statistical evidence to fail to reject $H_0$.

## R {.unnumbered}

### `r colourize("Random sample", "blue")` {.unnumbered}

A `r colourize("random sample", "blue")` is a collection of `r colourize("random variables", "blue")` $Y_1, \dots, Y_n$ of size $n$ coming from a given `r colourize("population", "blue")` or system of interest. Note that **the most elementary definition** of a `r  colourize("random sample", "blue")` assumes that these $n$ `r colourize("random variables", "blue")` are mutually **`r colourize("independent", "blue")` and identically distributed** (which is abbreviated as *iid*). 

The fact that these $n$ `r colourize("random variables", "blue")` are identically distributed indicates that they have the same mathematical form for their corresponding `r colourize("probability mass functions (PMFs)", "blue")` or `r colourize("probability density function (PDFs)", "blue")`, depending on whether they are `r colourize("discrete", "blue")` or `r colourize("continuous", "blue")` respectively. Hence, under a `r colourize("generative modelling", "blue")` approach in a `r colourize("population", "blue")` or system of interest governed by $k$ `r colourize("parameters", "blue")` contained in the vector

$$
\boldsymbol{\theta} = (\theta_1, \theta_2, \cdots, \theta_k)^T,
$$

we can apply the *iid* property in an elementary `r colourize("random sample", "blue")` to obtain the following joint `r colourize("probability distributions", "blue")`:

- In the case of $n$ *iid* `r colourize("discrete random variables", "blue")` $Y_1, \dots, Y_n$ whose common standalone `r colourize("PMF", "blue")` is $P_Y(Y = y | \boldsymbol{\theta})$ with support $\mathcal{Y}$, the joint `r colourize("PMF", "blue")` is mathematically expressed as

$$
\begin{align*}
P_{Y_1, \dots, Y_n}(Y_1 = y_1, \dots, Y_n = y_n | \boldsymbol{\theta}) &= \prod_{i = 1}^n P_Y(Y = y_i | \boldsymbol{\theta}) \\
& \qquad \text{for all} \\
& \quad \quad y_i \in \mathcal{Y}, i = 1, \dots, n.
\end{align*}
$${#eq-joint-PMF-iid-random-variables-app}

- In the case of $n$ *iid* `r colourize("continuous random variables", "blue")` $Y_1, \dots, Y_n$ whose common standalone `r colourize("PDF", "blue")` is $f_Y(y | \boldsymbol{\theta})$ with support $\mathcal{Y}$, the joint `r colourize("PDF", "blue")` is mathematically expressed as

$$
\begin{align*}
f_{Y_1, \dots, Y_n}(y_1, \dots, y_n | \boldsymbol{\theta}) &= \prod_{i = 1}^n f_Y(y_i | \boldsymbol{\theta}) \\
& \qquad \text{for all} \\
& \quad \quad y_i \in \mathcal{Y}, i = 1, \dots, n.
\end{align*}
$${#eq-joint-PDF-iid-random-variables-app}

Unlike @eq-joint-PMF-ind-random-variables-app and @eq-joint-PDF-ind-random-variables-app, note that @eq-joint-PMF-iid-random-variables-app and @eq-joint-PDF-iid-random-variables-app **do not** indicate a subscript for $Y$ in the corresponding `r colourize("probability distributions", "blue")` since we have identically distributed `r colourize("random variables", "blue")`. Furthermore, the joint distributions are conditioned on the `r colourize("population parameter", "blue")` vector $\boldsymbol{\theta}$ which reflects our `r colourize("generative modelling", "blue")` approach.

::: {.Equivalence}
::::{.Equivalence-header}
Somewhat equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Training dataset", "magenta")`.
::::
:::

### `r colourize("Random variable", "blue")` {.unnumbered}

A `r colourize("random variable", "blue")` is a function where the input values correspond to real numbers assigned to events belonging to the `r colourize("sample space", "blue")` $S$, and whose outcome is one of these real numbers after executing a given random experiment. For instance, a `r colourize("random variable", "blue")` (and its support, i.e., real numbers) is depicted with an uppercase such that 

$$Y \in \mathbb{R}.$$

### `r colourize("Regression analysis", "blue")` {.unnumbered}

### `r colourize("Regressor", "blue")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Attribute", "magenta")`, `r colourize("covariate", "blue")`, `r colourize("exogeneous variable", "magenta")`, `r colourize("explanatory variable", "blue")`, `r colourize("feature", "magenta")`, `r colourize("independent variable", "blue")`, `r colourize("input", "magenta")` or `r colourize("predictor", "magenta")`.
::::
:::

### `r colourize("Response variable", "blue")` {.unnumbered}

In supervised learning, it is the main variable of interest we are trying to **learn** or **predict**, or equivalently, the variable we are trying **explain** in a statistical inference framework.

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Dependent variable", "blue")`, `r colourize("endogeneous variable", "magenta")`, `r colourize("outcome", "magenta")`, `r colourize("output", "magenta")` or `r colourize("target", "magenta")`.
::::
:::

## S {.unnumbered}

### `r colourize("Sample space", "blue")` {.unnumbered}

Let $A$ be an event of interest in a random phenomenon in a `r colourize("population", "blue")` or system of interest. The `r colourize("sample space", "blue")` $S$ of event $A$ denotes the set of all the possible **random outcomes** we might encounter every time we randomly observe $A$ such as we were running some class of experiment.

Note each of these outcomes has a determined `r colourize("probability", "blue")` associated with them. If we add up all these `r colourize("probabilities", "blue")`, the `r colourize("probability", "blue")` of the sample $S$ will be one, i.e.,

$$
P(S) = 1.
$${#eq-sample-space-app}

### `r colourize("Semiparametric model", "blue")` {.unnumbered}

A semiparametric model is a statistical model that incorporates **both parametric and nonparametric parts**. In the context of linear regression, these parts can be described as follows:

- The **parametric part** includes the systematic component where $k$ observed `r colourize("regressors", "blue")` $x$ are modelled along with $k + 1$ regression `r colourize("parameters", "blue")` $\beta_0, \beta_1, \ldots, \beta_k$ in a linear combination.
- The **nonparametric part** does not impose specific assumptions on one or more modelling components, allowing the observed `r colourize("training dataset", "magenta")` to estimate these elements without requiring any `r colourize("probability distributions", "blue")`.

### `r colourize("Significance level", "blue")` {.unnumbered}

The `r colourize("significance level", "blue")` $\alpha$ is defined as the `r colourize("conditional probability", "blue")` of rejecting the `r colourize("null hypothesis", "blue")` $H_0$ given that $H_0$ is true. This can be mathematically represented as

$$
P \left( \text{Reject $H_0$} | \text{$H_0$ is true} \right) = \alpha.
$$

In plain words, $\alpha \in [0, 1]$ allows us to probabilistically control for `r colourize("type I error", "blue")` since we are dealing with `r colourize("random variables", "blue")` in our inferential process. The `r colourize("significance level", "blue")` can be thought as one of the main `r colourize("hypothesis testing", "blue")` and `r colourize("power analysis", "blue")` settings.

### `r colourize("Standard error", "blue")` {.unnumbered}

The `r colourize("standard error", "blue")` allows us to quantify the extent to which an `r colourize("estimate", "blue")` coming from an **observed** `r colourize("random sample", "blue")` (of size $n$, as in $y_1, \dots, y_n$) may deviate from the `r colourize("expected value", "blue")` under the assumption that the `r colourize("null hypothesis", "blue")` is true. 

It plays a critical role in determining whether an `r colourize("observed effect", "blue")` is likely attributable to **random variation** or represents a **statistically significant finding**. In the absence of the `r colourize("standard error", "blue")`, it would not be possible to rigorously assess the **reliability** or **precision** of an `r colourize("estimate", "blue")`.

### `r colourize("Supervised learning", "magenta")` {.unnumbered}

### `r colourize("Survival analysis", "blue")` {.unnumbered}

## T {.unnumbered}

### `r colourize("Target", "magenta")` {.unnumbered}

In supervised learning, it is the main variable of interest we are trying to **learn** or **predict**, or equivalently, the variable we are trying **explain** in a statistical inference framework.

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Dependent variable", "blue")`, `r colourize("endogeneous variable", "magenta")`, `r colourize("response variable", "blue")`, `r colourize("outcome", "magenta")` or `r colourize("output", "magenta")`.
::::
:::

### `r colourize("Test statistic", "blue")` {.unnumbered}

The `r colourize("test statistic", "blue")` is a function of the `r colourize("random sample", "blue")` of size $n$, i.e., it is in the function of the `r colourize("random variables", "blue")` $Y_1, \dots, Y_n$. Therefore, the `r colourize("test statistic", "blue")` will also be a random variable, whose **observed value** will describe how closely the `r colourize("probability distribution", "blue")` from which the `r colourize("random sample", "blue")` comes from matches the `r colourize("probability distribution", "blue")` of the `r colourize("null hypothesis", "blue")` $H_0$.

More specifically, once we have obtained the `r colourize("observed effect", "blue")` and `r colourize("standard error", "blue")` from our **observed** `r colourize("random sample", "blue")`, we can compute the corresponding **observed** `r colourize("test statistic", "blue")`. This `r colourize("test statistic", "blue")` computation will be placed on the corresponding $x$-axis of the `r colourize("probability distribution", "blue")` of $H_0$ so we can reject or fail to reject it accordingly.

### `r colourize("Training dataset", "magenta")` {.unnumbered}

::: {.Equivalence}
::::{.Equivalence-header}
Somewhat equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Random sample", "blue")`.
::::
:::

### `r colourize("True positive rate", "blue")` {.unnumbered}

The statistical `r colourize("true positive rate", "blue")` of a test $1 -\beta$ is the complement of the `r colourize("conditional probability", "blue")` $\beta$ of failing to reject the `r colourize("null hypothesis", "blue")` $H_0$ given that $H_0$ is false, which is mathematically represented as

$$
P \left( \text{Failing to reject $H_0$} | \text{$H_0$ is false} \right) = \beta;
$$

yielding

$$
\text{Power} = 1 - \beta. 
$$

In plain words, $1 - \beta \in [0, 1]$ is the **probabilistic ability** of our `r colourize("hypothesis testing", "blue")` to detect any signal in our inferential process, **if there is any**. **The larger the `r colourize("true positive rate", "blue")` in our `r colourize("power analysis", "blue")`, the less prone we are to commit a `r colourize("type II error", "blue")`.**

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("Power", "blue")`.
::::
:::

### `r colourize("Type I error", "blue")` {.unnumbered}

`r colourize("Type I error", "blue")` is defined as **incorrectly** rejecting the `r colourize("null hypothesis", "blue")` $H_0$ in favour of the `r colourize("alternative hypothesis", "blue")` $H_1$ when, in fact, **$H_0$ is true**.

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("False positive", "blue")`.
::::
:::

### `r colourize("Type II error", "blue")` {.unnumbered}

`r colourize("Type II error", "blue")` is defined as **incorrectly** failing to reject the `r colourize("null hypothesis", "blue")` $H_0$ in favour of the `r colourize("alternative hypothesis", "blue")` $H_1$ when, in fact, **$H_0$ is false**. @tbl-errors-app-2 summarizes the types of inferential conclusions in function on whether $H_0$ is true or not.

|       | **$H_0$ is true** | **$H_0$ is false** |
|:-----:|:-----:|:-----:|
| **Reject $H_0$** | Type I error (*False positive*) | Correct |
| **Fail to reject $H_0$** | Correct | Type II error (*False negative*) |

: Types of inferential conclusions in a frequentist hypothesis testing. {#tbl-errors-app-2 .hover}

::: {.Equivalence}
::::{.Equivalence-header}
Equivalent to:
::::
::::{.Equivalence-container}
`r colourize("False negative", "blue")`.
::::
:::

## U {.unnumbered}

### `r colourize("Underdispersion", "blue")` {.unnumbered}

## V {.unnumbered}

### `r colourize("Variance", "blue")` {.unnumbered}

Let $Y$ be a `r colourize("discrete", "blue")` or `r colourize("continuous random variable", "blue")` whose support is $\mathcal{Y}$ with a `r colourize("mean", "blue")` represented by $\mathbb{E}(Y)$. Then, the `r colourize("variance", "blue")` of $Y$ is the `r colourize("mean", "blue")` of the squared deviation from the corresponding `r colourize("mean", "blue")` as follows:

$$
\text{Var}(Y) = \mathbb{E}\left\{[ Y - \mathbb{E}(Y)]^2 \right\}. \\
$$

Note the expression above is equivalent to:

$$
\text{Var}(Y) = \mathbb{E}(Y^2) - \left[ \mathbb{E}(Y) \right]^2.
$$
Finally, to put the **spread** measurement on the same units of `r colourize("random variable", "blue")` $Y$, the **standard devation** of $Y$ is merely the square root of $\text{Var}(Y)$:

$$
\text{sd}(Y) = \sqrt{\text{Var}(Y)}.
$$
