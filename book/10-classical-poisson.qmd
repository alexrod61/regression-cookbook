<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7PRVEBE1EF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7PRVEBE1EF');
</script>

# Bubblarious Classical Poisson Regression {#sec-classical-poisson}

```{r}
#| include: false

colourize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

::: {.Warning}
::::{.Warning-header}
Fun fact!
::::
::::{.Warning-container}
**Bubblarious!** For all the boba, fizzy drinks, and seltzers that go pop!
::::
:::

::: {#fig-classical-poisson-regression}
```{mermaid}
mindmap
  root((Regression 
  Analysis)
    Continuous <br/>Outcome Y
      {{Unbounded <br/>Outcome Y}}
        )Chapter 3: <br/>Ordinary <br/>Least Squares <br/>Regression(
          (Normal <br/>Outcome Y)
      {{Nonnegative <br/>Outcome Y}}
        )Chapter 4: <br/>Gamma Regression(
          (Gamma <br/>Outcome Y)
      {{Bounded <br/>Outcome Y <br/> between 0 and 1}}
        )Chapter 5: Beta <br/>Regression(
          (Beta <br/>Outcome Y)
      {{Nonnegative <br/>Survival <br/>Time Y}}
        )Chapter 6: <br/>Parametric <br/> Survival <br/>Regression(
          (Exponential <br/>Outcome Y)
          (Weibull <br/>Outcome Y)
          (Lognormal <br/>Outcome Y)
        )Chapter 7: <br/>Semiparametric <br/>Survival <br/>Regression(
          (Cox Proportional <br/>Hazards Model)
            (Hazard Function <br/>Outcome Y)
    Discrete <br/>Outcome Y
      {{Binary <br/>Outcome Y}}
        {{Ungrouped <br/>Data}}
          )Chapter 8: <br/>Binary Logistic <br/>Regression(
            (Bernoulli <br/>Outcome Y)
        {{Grouped <br/>Data}}
          )Chapter 9: <br/>Binomial Logistic <br/>Regression(
            (Binomial <br/>Outcome Y)
      {{Count <br/>Outcome Y}}
        {{Equidispersed <br/>Data}}
          )Chapter 10: <br/>Classical Poisson <br/>Regression(
            (Poisson <br/>Outcome Y)
```
:::

```{r setup, include=FALSE}
# Load required libraries
library(devtools)
library(readr)
library(tibble)
library(tidyr)
library(dplyr)
library(magrittr)
library(ggplot2)
library(asbio) # provides `crabs` dataset
library(gridExtra) # combine ggplot2 figures
library(ggExtra) # draw marginal histograms
```

::: {.LO}
::::{.LO-header}
Learning Objectives
::::
::::{.LO-container}
By the end of this chapter, you will be able to:

- Describe the reason that ordinary linear models is inappropriate to use.
- Determine when Poisson regression is an appropriate modeling choice.
- Write down the likelihood function of a Poisson regression models.
- Understand the computation procedure of the Poisson regression coefficient estimation.
- Interpret the Poisson regression coefficients in the real scenarios.
- Evaluate model performance and construct confidence intervals.
::::
:::

## Introduction

This chapter introduces a generalized linear regression model that can be applied on Poisson-distributed count data. Compared to ordinary regression, which assume normality and constant variance---homoskedasticity, Poisson regression models count data that is skewed and heteroskedastic. 

Some research questions you might explore using Poisson regression:

- How is the number of insurance claims filed by policyholders in a year associated with ages, vehicle types, and regions?
- How can the number of complaints received by a telecom company from customers be explained by service types and contract lengths?
- How does the distribution of counts of satellite male horseshoe crabs residing around a female horseshoe crab nest vary by the phenotypic characteristics of the female horseshoe crabs?


### Poisson Regression Assumptions

- **Independence**: Poisson regression assumes the responses to be counts--(nonnegative integers: 0, 1, 2,...). Each response is mutually independent to each other with mean parameter $\lambda_i,i=1,\dots,n$. 

- **Log-linearity**: Poisson regression models the log-link function of the response as a linear combination of the explanatory variables, i.e., $\log(\lambda_i) = \beta_0 + \beta_1 X_{i1}+\dots+\beta_p X_{ip}$ with $\lambda_i> 0$ for all $i=1,\dots,n$. 

- **Heteroskedasticity**: Poisson regression assumes heteoroskedastic response, i.e., the variance of the response increases along with the mean increasing; in contrast to the ordinary regression with Gaussian noise, whose the variance is constant and independent to the mean. Poisson regression assumes **equidispersion**, i.e., the variance is the same as the expectation of the response; compared to overdispersion in negative binomial distribution where the variance is greater than the mean. 

### A Graphical Look

Below is a graphical demonstration of the comparison between the ordinary linear regression with Gaussian distributed response and Poisson regression with Poisson distributed response. The ordinary linear regression has a linear fitted blue line (in the left panel), while the Poisson regression, due to the use of log-link function, has a fitted blue curve (in the right panel). Segment the explanatory variable (in the x axis) in each scenario into five sections using the gray dashed vertical lines. The red horizontal segments represent the frequencies of binned response in each section, which represents the rotated histogram of the response. In ordinary linear regression, the response in each section is symmetrically distributed with similar variation across five sections (i.e., homoskedasticity); while in Poisson regression, the response is skewed with heteroskedasticity (specifically, increasing variances as the responses increase) across sections. 

::: {.panel-tabset}

## R Plot

```{r, fig.width=14, fig.height=10}
#| echo: false
#| message: false
#| warning: false

########### Simulated Poisson Data ###########
set.seed(1)
x <- runif(1000, 0, 20)
lambda <- exp(0.15 * x)
y_pois <- rpois(1000, lambda)
df_pois <- tibble(x = x, y = y_pois)

# Bin setup
n_bins <- 5
bin_edges <- seq(min(x), max(x), length.out = n_bins + 1)
bin_width <- diff(bin_edges)[1]

df_pois <- df_pois %>%
  mutate(
    x_bin = cut(x, breaks = bin_edges, include.lowest = TRUE),
    x_bin_mid = (as.numeric(x_bin) - 0.5) * bin_width + min(x)
  )

y_breaks_pois <- seq(floor(min(y_pois)), ceiling(max(y_pois)), by = 1)

hist_df_pois <- df_pois %>%
  mutate(y_bin = cut(y, breaks = y_breaks_pois, include.lowest = TRUE)) %>%
  group_by(x_bin, x_bin_mid, y_bin) %>%
  summarise(n = n(), .groups = "drop") %>%
  mutate(
    y_bin_mid = as.numeric(gsub("\\((.+),(.+)\\]", "\\1", y_bin)) + 0.5,
    n_scaled = n / max(n) * (bin_width * 0.9)
  )

fit_pois <- glm(y ~ x, data = df_pois, family = poisson())
x_seq <- seq(min(x), max(x), length.out = 200)
fit_df_pois <- tibble(
  x = x_seq,
  y_hat = predict(fit_pois, newdata = tibble(x = x_seq), type = "response")
)

plot_pois <- ggplot(df_pois, aes(x, y)) +
  geom_point(alpha = 0.2, size = 0.5) +
  geom_line(data = fit_df_pois, aes(x, y_hat), color = "blue", size = 1) +
  geom_segment(data = hist_df_pois,
               aes(x = x_bin_mid, 
                   xend = x_bin_mid - n_scaled-bin_width * 0.2,
                   y = y_bin_mid, yend = y_bin_mid),
               color = "red", size = .8) +
  geom_vline(xintercept = bin_edges, linetype = "dashed", color = "black", size = .3) +
  labs(title = "", x = "Width", y = "Counts") +
  theme_bw() +
  theme(axis.title = element_text(size = 18), axis.text = element_text(size = 16))

########### Simulated Normal Data ###########
set.seed(2)
x2 <- runif(1000, 0, 20)
mu <- 2 + 0.5 * x2
y_norm <- rnorm(1000, mean = mu, sd = 3)
df_norm <- tibble(x = x2, y = y_norm)

df_norm <- df_norm %>%
  mutate(
    x_bin = cut(x, breaks = bin_edges, include.lowest = TRUE),
    x_bin_mid = (as.numeric(x_bin) - 0.5) * bin_width + min(x)
  )

y_breaks_norm <- seq(floor(min(y_norm)), ceiling(max(y_norm)), by = 1)

hist_df_norm <- df_norm %>%
  mutate(y_bin = cut(y, breaks = y_breaks_norm, include.lowest = TRUE)) %>%
  group_by(x_bin, x_bin_mid, y_bin) %>%
  summarise(n = n(), .groups = "drop") %>%
  mutate(
    y_bin_mid = as.numeric(gsub("\\((.+),(.+)\\]", "\\1", y_bin)) + 0.5,
    n_scaled = n / max(n) * (bin_width * 0.9)
  )

fit_norm <- lm(y ~ x, data = df_norm)
fit_df_norm <- tibble(
  x = x_seq,
  y_hat = predict(fit_norm, newdata = tibble(x = x_seq))
)

plot_norm <- ggplot(df_norm, aes(x, y)) +
  geom_point(alpha = 0.2, size = 0.5) +
  geom_line(data = fit_df_norm, aes(x, y_hat), color = "blue", size = 1) +
  geom_segment(data = hist_df_norm,
               aes(x = x_bin_mid, xend = x_bin_mid - n_scaled,
                   y = y_bin_mid, yend = y_bin_mid),
               color = "red", size = .8) +
  geom_vline(xintercept = bin_edges, linetype = "dashed", color = "black", size = .3) +
  labs(title = "", x = "Days", y = "Price") +
  theme_bw() +
  theme(axis.title = element_text(size = 18), axis.text = element_text(size = 16))

########### Combine and Show Both ###########
grid.arrange(plot_norm, plot_pois, ncol = 2)
```

## Python Plot

```{python, fig.width=14, fig.height=10}
#| echo: false
#| message: false


```

:::

## Case Study: Horseshoe Crab Satellites

Let's take a closer look at the example in the third question mentioned in the Introduction---exploring the differential distribution of the number of satellite male horseshoe crabs residing around a female horseshoe crab nest across various phenotypic characteristics. Using the dataset `crabs` provided by [Brockmann, 1996](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1439-0310.1996.tb01099.x), let's load the dataset and do some data wrangling.

## Data Collection and Wrangling

There are 173 records of the female horseshoe crab nests and their male satellites with 5 features: `satell`: satellite size of each nest (i.e., the number of male horseshoe crabs around each female horseshoe crab nest), `width`: shell width in cm, `color`: 1 = medium light, 2 = medium, 3 = medium dark, 4 = dark, `spine`: spine condition: 1 = both good, 2 = one broken, 3 = both broken, and `weight`: weight in kg (referring to Section 3.3.3 in [Agresti, 2018](https://www.wiley.com/en-us/An+Introduction+to+Categorical+Data+Analysis%2C+3rd+Edition-p-9781119405283)). 

::: {.panel-tabset}

## R Code

```{.r}
library(asbio)
data(crabs)
crabs <- as_tibble(crabs)
# reorder the columns
crabs <- crabs %>%
  select(satell, width, everything())
# display the dataset with dimension and column types
crabs
```

## Python Code

```{.python}


```

:::

::: {.panel-tabset}

## R Output

```{r, fig.width=14, fig.height=8}
#| echo: false
#| message: false
#| warning: false
library(asbio)
data(crabs)
crabs <- as_tibble(crabs)
crabs <- crabs %>%
  select(satell, width, everything())
crabs
```

## Python Output

```{python, fig.width=14, fig.height=10}
#| echo: false
#| message: false
#| warning: false

```

:::

## Exploratory Data Analysis

Suppose that the relationship between the satellite size per nest (`satell`) and width of the female horseshoe crab (`width` in cm) is our main research interest. 
Let's explore their relationship in the following scatterplot with the distribution of `satell` on the margin. 

::: {.panel-tabset}

## R Code

```{.r}
# draw the scatterplot of satellite size versus width 
p <- crabs %>%
  ggplot(aes(y = satell, x = width)) + 
  geom_point(alpha = 0.8) + 
  labs(x = "Width (cm)", y = "Satellite Size") +
  scale_x_continuous(breaks = seq(floor(min(crabs$width)), ceiling(max(crabs$width)), by = 2)) + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  theme_bw()
# add a marginal histogram with binwidth of 1, i.e., bins for integers; equivalent to the barplot
ggMarginal(p, type = "histogram", margins = "y", binwidth = 1, boundary = -0.5) # start with `satell=-0.5` to avoid the first bins (0s and 1s) being combined in one bin
```

## Python Code

```{.python}


```

:::

::: {.panel-tabset}

## R Output

```{r}
#| echo: false
#| message: false
#| warning: false
p <- crabs %>%
  ggplot(aes(y = satell, x = width)) + 
  geom_point(alpha = 0.8) + 
  labs(x = "Width (cm)", y = "Satellite Size") +
  scale_x_continuous(breaks = seq(floor(min(crabs$width)), ceiling(max(crabs$width)), by = 2)) + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  theme_bw()
ggMarginal(p, type = "histogram", margins = "y", binwidth = 1, boundary = -0.5)
```

## Python Output

```{python}
#| echo: false
#| message: false


```

:::

The distribution of satellite sizes is highly right skewed, which violates the normality assumption of the ordinary linear regression. 

In the scatter plot above, we can tell that the satellite size gets more spread out as the width increases. The averaged satellite sizes turn to increase along the width increasing. Let's now split the width into a few intervals and compute the representative point for each interval to have a clear look at the trend. 

::: {.panel-tabset}

## R Code

```{.r}
# set up the number of intervals
n_intervals <- 10

# compute the average points for each interval of `width`
crabs_binned <- crabs %>%
  mutate(width_inl = cut(width, breaks = n_intervals)) %>%
  group_by(width_inl) %>%
  summarize(
    mean_width = mean(width),
    mean_satell = mean(satell),
    .groups = "drop"
  )
crabs_binned
```

## Python Code

```{.python}


```

:::

::: {.panel-tabset}

## R Output

```{r}
#| echo: false
#| message: false
#| warning: false
n_intervals <- 10
crabs_binned <- crabs %>%
  mutate(width_inl = cut(width, breaks = n_intervals)) %>%
  group_by(width_inl) %>%
  summarize(
    mean_width = mean(width),
    mean_satell = mean(satell),
    .groups = "drop"
  )
crabs_binned
```

## Python Output

```{python}
#| echo: false
#| message: false


```

:::

We've prepared the summarized dataset for the average points---each entry includes an interval of `width`, an averaged width, and an averaged satellite size per interval. Now, let's visualize the representative points on the scatter plot to take a closer look at the trend. 

::: {.panel-tabset}

## R Code

```{.r}
# add the average points onto the scatterplot, connect them, and mark intervals
breaks <- seq(min(crabs$width), max(crabs$width), length.out = n_intervals + 1)
p + 
  geom_vline(xintercept = breaks, linetype = "dashed", color = "gray50") +
  geom_point(data = crabs_binned, aes(x = mean_width, y = mean_satell), color = "red", size = 2) +
  geom_line(data = crabs_binned, aes(x = mean_width, y = mean_satell), color = "red", linewidth = 1)
```

## Python Code

```{.python}


```

:::

::: {.panel-tabset}

## R Output

```{r}
#| echo: false
#| message: false
#| warning: false
breaks <- seq(min(crabs$width), max(crabs$width), length.out = n_intervals + 1)
p + 
  geom_vline(xintercept = breaks, linetype = "dashed", color = "gray50") +
  geom_point(data = crabs_binned, aes(x = mean_width, y = mean_satell), color = "red", size = 2) +
  geom_line(data = crabs_binned, aes(x = mean_width, y = mean_satell), color = "red", linewidth = 1)
```

## Python Output

```{python}
#| echo: false
#| message: false


```

:::

We can see a general increasing trend of the satellite size as the nest width grows. 

## Data Modelling

The Poisson regression model assumes a random sample of $n$ count observations $Y_i$s, hence independent (but not identically distributed!), which have the following distribution:
$$Y_i \sim Poisson(\lambda_i)$$.
Each $i$th observation has its own $\mathbb{E}(Y_i)=\lambda_i>0$, which also implicates $Var(Y_i)=\lambda_i>0$.


The Poisson regression model with regressors is written as:
$$h(\lambda_i) = \log(\lambda_i)=\beta_0+\beta_1 X_{i,1}+\dots+\beta_k X_{i,k}.$$

The general idea to solve this function is that solving the coefficients by maximizing the likelihood is equivalent to minimizing the log-likelihood. We solve the second-order Taylor expansion of the log-likelihood, which is in turn a squared norm, that is much easier to solve than the original objective with a closed-form solution. Then, the approximated problem is solved iteratively until the convergence criteria is satisfied. 

The iteration procedure is as follows.

- 1. Choose a set of initial value of the coefficients $\beta^0 = (\beta_0^0, \dots,\beta_k^0)$, which can be a vector of zeros.
- 2. Compute the fitted value 


## Estimation

## Goodness of Fit

## Results

## Storytelling
