<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7PRVEBE1EF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7PRVEBE1EF');
</script>

<!-- MathJax configuration -->
<script>
  MathJax = {
    tex: {
      tags: 'ams',  // enable numbering
      useLabelIds: true
    }
  };
</script>

# Bubblarious Classical Poisson Regression {#sec-classical-poisson}

```{r}
#| include: false

colourize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

::: {.Warning}
::::{.Warning-header}
Fun fact!
::::
::::{.Warning-container}
**Bubblarious!** For all the boba, fizzy drinks, and seltzers that go pop!
::::
:::

::: {#fig-classical-poisson-regression}
```{mermaid}
mindmap
  root((Regression 
  Analysis)
    Continuous <br/>Outcome Y
      {{Unbounded <br/>Outcome Y}}
        )Chapter 3: <br/>Ordinary <br/>Least Squares <br/>Regression(
          (Normal <br/>Outcome Y)
      {{Nonnegative <br/>Outcome Y}}
        )Chapter 4: <br/>Gamma Regression(
          (Gamma <br/>Outcome Y)
      {{Bounded <br/>Outcome Y <br/> between 0 and 1}}
        )Chapter 5: Beta <br/>Regression(
          (Beta <br/>Outcome Y)
      {{Nonnegative <br/>Survival <br/>Time Y}}
        )Chapter 6: <br/>Parametric <br/> Survival <br/>Regression(
          (Exponential <br/>Outcome Y)
          (Weibull <br/>Outcome Y)
          (Lognormal <br/>Outcome Y)
        )Chapter 7: <br/>Semiparametric <br/>Survival <br/>Regression(
          (Cox Proportional <br/>Hazards Model)
            (Hazard Function <br/>Outcome Y)
    Discrete <br/>Outcome Y
      {{Binary <br/>Outcome Y}}
        {{Ungrouped <br/>Data}}
          )Chapter 8: <br/>Binary Logistic <br/>Regression(
            (Bernoulli <br/>Outcome Y)
        {{Grouped <br/>Data}}
          )Chapter 9: <br/>Binomial Logistic <br/>Regression(
            (Binomial <br/>Outcome Y)
      {{Count <br/>Outcome Y}}
        {{Equidispersed <br/>Data}}
          )Chapter 10: <br/>Classical Poisson <br/>Regression(
            (Poisson <br/>Outcome Y)
```
:::

```{r setup-r, include=FALSE}
# Load required libraries
library(devtools)
library(readr)
library(tibble)
library(tidyr)
library(dplyr)
library(magrittr)
library(ggplot2)
library(asbio) # provides `crabs` dataset
library(gridExtra) # combine ggplot2 figures
library(ggExtra) # draw marginal histograms
library(broom)

library(reticulate)
py_config()

py_require("pandas")
py_require("numpy")
py_require("seaborn")
py_require("pyreadr")
py_require("plotnine")
py_require("matplotlib")
py_require("statsmodels")
py_require("scikit-learn")
py_require("spicy")

pandas <- import("pandas")
numpy <- import("numpy")
seaborn <- import("seaborn")
pyreadr <- import("pyreadr")
matplotlib <- import("matplotlib")
statsmodels <- import("statsmodels")
sklearn <- import("sklearn")
scipy <- import("scipy")
```

```{python setup-py, include=FALSE}
import numpy as np
import pandas as pd
from plotnine import *
import statsmodels.api as sm
from statsmodels.formula.api import glm, ols
from statsmodels.genmod.families import Poisson
from matplotlib import pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error
import scipy.stats as stats
```

::: {.LO}
::::{.LO-header}
Learning Objectives
::::
::::{.LO-container}
By the end of this chapter, you will be able to:

- Describe the reason that ordinary linear models is inappropriate to use.
- Determine when Poisson regression is an appropriate modeling choice.
- Write down the likelihood function of a Poisson regression models.
- Understand the computation procedure of the Poisson regression coefficient estimation.
- Interpret the Poisson regression coefficients in the real scenarios.
- Evaluate model performance and construct confidence intervals.
::::
:::

## Introduction

This chapter introduces a generalized linear regression model that can be applied on Poisson-distributed count data. Compared to ordinary regression, which assume normality and constant variance---homoskedasticity, Poisson regression models count data that is skewed and heteroskedastic. 

Some research questions you might explore using Poisson regression:

- How is the number of insurance claims filed by policyholders in a year associated with ages, vehicle types, and regions?
- How can the number of complaints received by a telecom company from customers be explained by service types and contract lengths?
- How does the distribution of counts of satellite male horseshoe crabs residing around a female horseshoe crab nest vary by the phenotypic characteristics of the female horseshoe crabs?


### Poisson Regression Assumptions

- **Independence**: Poisson regression assumes the responses to be counts--(nonnegative integers: 0, 1, 2,...). Each response is mutually independent to each other with mean parameter $\lambda_i,i=1,\dots,n$. 

- **Log-linearity**: Poisson regression models the log-link function of the response as a linear combination of the explanatory variables, i.e., $\log(\lambda_i) = \beta_0 + \beta_1 X_{i1}+\dots+\beta_p X_{ip}$ with $\lambda_i> 0$ for all $i=1,\dots,n$. 

- **Heteroskedasticity**: Poisson regression assumes heteoroskedastic response, i.e., the variance of the response increases along with the mean increasing; in contrast to the ordinary regression with Gaussian noise, whose the variance is constant and independent to the mean. Poisson regression assumes **equidispersion**, i.e., the variance is the same as the expectation of the response; compared to overdispersion in negative binomial distribution where the variance is greater than the mean. 

### A Graphical Look

Below is a graphical demonstration of the comparison between the ordinary linear regression with Gaussian distributed response and Poisson regression with Poisson distributed response. The left panel illustrates the price of smartphones (in 100 USD) increase with the number of days listed on an online marketplace. The right panel illustrates the number of bacteria colonies on a petri dish versus the hours of incubation. 

The ordinary linear regression has a linear fitted blue line (in the left panel), while the Poisson regression, due to the use of log-link function, has a fitted blue curve (in the right panel). Segment the explanatory variable (in the x axis) in each scenario into five sections using the gray dashed vertical lines. The red horizontal segments represent the frequencies of binned response in each section, which represents the rotated histogram of the response. In ordinary linear regression, the response in each section is symmetrically distributed with similar variation across five sections (i.e., homoskedasticity); while in Poisson regression, the response is skewed with heteroskedasticity (specifically, increasing variances as the responses increase) across sections. 

```{r, fig.width=14, fig.height=10}
#| echo: false
#| message: false
#| warning: false

########### Simulated Poisson Data ###########
set.seed(1)
x <- runif(1000, 0, 20)
lambda <- exp(0.15 * x)
y_pois <- rpois(1000, lambda)
df_pois <- tibble(x = x, y = y_pois)

# Bin setup
n_bins <- 5
bin_edges <- seq(min(x), max(x), length.out = n_bins + 1)
bin_width <- diff(bin_edges)[1]

df_pois <- df_pois %>%
  mutate(
    x_bin = cut(x, breaks = bin_edges, include.lowest = TRUE),
    x_bin_mid = (as.numeric(x_bin) - 0.5) * bin_width + min(x)
  )

y_breaks_pois <- seq(floor(min(y_pois)), ceiling(max(y_pois)), by = 1)

hist_df_pois <- df_pois %>%
  mutate(y_bin = cut(y, breaks = y_breaks_pois, include.lowest = TRUE)) %>%
  group_by(x_bin, x_bin_mid, y_bin) %>%
  summarise(n = n(), .groups = "drop") %>%
  mutate(
    y_bin_mid = as.numeric(gsub("\\((.+),(.+)\\]", "\\1", y_bin)) + 0.5,
    n_scaled = n / max(n) * (bin_width * 0.9)
  )

fit_pois <- glm(y ~ x, data = df_pois, family = poisson())
x_seq <- seq(min(x), max(x), length.out = 200)
fit_df_pois <- tibble(
  x = x_seq,
  y_hat = predict(fit_pois, newdata = tibble(x = x_seq), type = "response")
)

plot_pois <- ggplot(df_pois, aes(x, y)) +
  geom_point(alpha = 0.2, size = 0.5) +
  geom_line(data = fit_df_pois, aes(x, y_hat), color = "blue", size = 1) +
  geom_segment(data = hist_df_pois,
               aes(x = x_bin_mid, 
                   xend = x_bin_mid - n_scaled-bin_width * 0.2,
                   y = y_bin_mid, yend = y_bin_mid),
               color = "red", size = .8) +
  geom_vline(xintercept = bin_edges, linetype = "dashed", color = "black", size = .3) +
  labs(title = "", x = "Hours of Incubation", y = "Numbers of bacteria colonies") +
  theme_bw() +
  theme(axis.title = element_text(size = 18), axis.text = element_text(size = 16))

########### Simulated Normal Data ###########
set.seed(2)
x2 <- runif(1000, 0, 20)
mu <- 2 + 0.5 * x2
y_norm <- rnorm(1000, mean = mu, sd = 3)
df_norm <- tibble(x = x2, y = y_norm)

df_norm <- df_norm %>%
  mutate(
    x_bin = cut(x, breaks = bin_edges, include.lowest = TRUE),
    x_bin_mid = (as.numeric(x_bin) - 0.5) * bin_width + min(x)
  )

y_breaks_norm <- seq(floor(min(y_norm)), ceiling(max(y_norm)), by = 1)

hist_df_norm <- df_norm %>%
  mutate(y_bin = cut(y, breaks = y_breaks_norm, include.lowest = TRUE)) %>%
  group_by(x_bin, x_bin_mid, y_bin) %>%
  summarise(n = n(), .groups = "drop") %>%
  mutate(
    y_bin_mid = as.numeric(gsub("\\((.+),(.+)\\]", "\\1", y_bin)) + 0.5,
    n_scaled = n / max(n) * (bin_width * 0.9)
  )

fit_norm <- lm(y ~ x, data = df_norm)
fit_df_norm <- tibble(
  x = x_seq,
  y_hat = predict(fit_norm, newdata = tibble(x = x_seq))
)

plot_norm <- ggplot(df_norm, aes(x, y)) +
  geom_point(alpha = 0.2, size = 0.5) +
  geom_line(data = fit_df_norm, aes(x, y_hat), color = "blue", size = 1) +
  geom_segment(data = hist_df_norm,
               aes(x = x_bin_mid, xend = x_bin_mid - n_scaled,
                   y = y_bin_mid, yend = y_bin_mid),
               color = "red", size = .8) +
  geom_vline(xintercept = bin_edges, linetype = "dashed", color = "black", size = .3) +
  labs(title = "", x = "Days", y = "Prices (in 100 USD)") +
  theme_bw() +
  theme(axis.title = element_text(size = 18), axis.text = element_text(size = 16))

########### Combine and Show Both ###########
grid.arrange(plot_norm, plot_pois, ncol = 2)
```

## Case Study: Horseshoe Crab Satellites

Let's take a closer look at the example in the third question mentioned in the Introduction---exploring the differential distribution of the number of satellite male horseshoe crabs residing around a female horseshoe crab nest across various phenotypic characteristics. Using the dataset `crabs` provided by [Brockmann, 1996](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1439-0310.1996.tb01099.x), let's load the dataset and do some data wrangling.

## Data Collection and Wrangling

There are 173 records of the female horseshoe crab nests and their male satellites with 5 features: `satell`: satellite size of each nest (i.e., the number of male horseshoe crabs around each female horseshoe crab nest), `width`: shell width in cm, `color`: 1 = medium light, 2 = medium, 3 = medium dark, 4 = dark, `spine`: spine condition: 1 = both good, 2 = one broken, 3 = both broken, and `weight`: weight in kg (referring to Section 3.3.3 in [Agresti, 2018](https://www.wiley.com/en-us/An+Introduction+to+Categorical+Data+Analysis%2C+3rd+Edition-p-9781119405283)). 

::: {.panel-tabset}

## R Code

```{.r}
library(asbio)
data(crabs)
crabs <- as_tibble(crabs)
# reorder the columns
crabs <- crabs %>%
  select(satell, width, everything())
# display the dataset with dimension and column types
crabs
```

## Python Code

```{.python}
import pandas as pd
print(crabs.dtypes)
crabs
```

:::

::: {.panel-tabset}

## R Output

```{r, fig.width=14, fig.height=8}
#| echo: false
#| message: false
#| warning: false
library(asbio)
data(crabs)
crabs <- as_tibble(crabs)
crabs <- crabs %>%
  select(satell, width, everything())
crabs
```

## Python Output

```{python, fig.width=14, fig.height=10}
#| echo: false
#| message: false
#| warning: false
import pandas as pd
crabs = pd.DataFrame({
  "satell": [8, 0, 9, 0, 4, 0, 0, 0, 0, 0, 0, 0, 11, 0, 14, 8, 1, 1, 0, 5, 4, 3, 1, 2, 3, 0, 3, 5, 0, 0, 4, 0, 0, 8, 5, 0, 0, 6, 0, 6, 3, 5, 6, 5, 9, 4, 6, 4, 3, 3, 5, 5, 6, 4, 5, 15, 3, 3, 0, 0, 0, 5, 3, 5, 1, 8, 10, 0, 0, 3, 7, 1, 0, 6, 0, 0, 3, 4, 0, 5, 0, 0, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 2, 4, 3, 6, 0, 2, 2, 0, 12, 0, 5, 6, 6, 2, 0, 2, 3, 0, 3, 4, 2, 6, 6, 0, 4, 10, 7, 0, 5, 5, 6, 6, 7, 3, 3, 0, 0, 8, 4, 4, 10, 9, 4, 0, 0, 0, 0, 4, 0, 2, 0, 4, 4, 3, 8, 0, 7, 0, 0, 2, 3, 4, 0, 0, 0],
  "width": [28.3, 22.5, 26, 24.8, 26, 23.8, 26.5, 24.7, 23.7, 25.6, 24.3, 25.8, 28.2, 21, 26, 27.1, 25.2, 29, 24.7, 27.4, 23.2, 25, 22.5, 26.7, 25.8, 26.2, 28.7, 26.8, 27.5, 24.9, 29.3, 25.8, 25.7, 25.7, 26.7, 23.7, 26.8, 27.5, 23.4, 27.9, 27.5, 26.1, 27.7, 30, 28.5, 28.9, 28.2, 25, 28.5, 30.3, 24.7, 27.7, 27.4, 22.9, 25.7, 28.3, 27.2, 26.2, 27.8, 25.5, 27.1, 24.5, 27, 26, 28, 30, 29, 26.2, 26.5, 26.2, 25.6, 23, 23, 25.4, 24.2, 22.9, 26, 25.4, 25.7, 25.1, 24.5, 27.5, 23.1, 25.9, 25.8, 27, 28.5, 25.5, 23.5, 24, 29.7, 26.8, 26.7, 28.7, 23.1, 29, 25.5, 26.5, 24.5, 28.5, 28.2, 24.5, 27.5, 24.7, 25.2, 27.3, 26.3, 29, 25.3, 26.5, 27.8, 27, 25.7, 25, 31.9, 23.7, 29.3, 22, 25, 27, 23.8, 30.2, 26.2, 24.2, 27.4, 25.4, 28.4, 22.5, 26.2, 24.9, 24.5, 25.1, 28, 25.8, 27.9, 24.9, 28.4, 27.2, 25, 27.5, 33.5, 30.5, 29, 24.3, 25.8, 25, 31.7, 29.5, 24, 30, 27.6, 26.2, 23.1, 22.9, 24.5, 24.7, 28.3, 23.9, 23.8, 29.8, 26.5, 26, 28.2, 25.7, 26.5, 25.8, 24.1, 26.2, 26.1, 29, 28, 27, 24.5],
  "color": [2, 3, 1, 3, 3, 2, 1, 3, 2, 3, 3, 2, 2, 4, 2, 1, 2, 2, 4, 2, 2, 1, 2, 3, 4, 4, 2, 2, 4, 2, 1, 1, 2, 2, 2, 4, 2, 2, 4, 2, 3, 1, 1, 2, 3, 3, 2, 2, 2, 2, 4, 2, 1, 2, 2, 2, 2, 3, 2, 4, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 4, 3, 3, 2, 4, 2, 4, 4, 2, 2, 2, 4, 2, 3, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 4, 4, 3, 2, 3, 3, 1, 3, 2, 2, 2, 3, 4, 2, 2, 1, 2, 2, 4, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 4, 2, 2, 3, 2, 2, 2, 2, 2, 4, 2, 2, 2, 3, 3, 2, 2, 2, 4, 2, 2, 3, 3, 3, 3, 1, 4, 2],
  "spine": [3, 3, 1, 3, 3, 3, 1, 2, 1, 3, 3, 3, 3, 2, 1, 1, 3, 3, 3, 3, 2, 2, 1, 3, 3, 3, 3, 1, 3, 3, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 1, 3, 3, 1, 3, 1, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 3, 1, 3, 3, 3, 1, 3, 2, 1, 1, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 2, 3, 3, 3, 1, 2, 3, 1, 3, 3, 3, 1, 3, 2, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 2],
  "weight": [3.05, 1.55, 2.3, 2.1, 2.6, 2.1, 2.35, 1.9, 1.95, 2.15, 2.15, 2.65, 3.05, 1.85, 2.3, 2.95, 2, 3, 2.2, 2.7, 1.95, 2.3, 1.6, 2.6, 2, 1.3, 3.15, 2.7, 2.6, 2.1, 3.2, 2.6, 2, 2, 2.7, 1.85, 2.65, 3.15, 1.9, 2.8, 3.1, 2.8, 2.5, 3.3, 3.25, 2.8, 2.6, 2.1, 3, 3.6, 2.1, 2.9, 2.7, 1.6, 2, 3, 2.7, 2.3, 2.75, 2.25, 2.55, 2.05, 2.45, 2.15, 2.8, 3.05, 3.2, 2.4, 1.3, 2.4, 2.8, 1.65, 1.8, 2.25, 1.9, 1.6, 2.2, 2.25, 1.2, 2.1, 2.25, 2.9, 1.65, 2.55, 2.3, 2.25, 3.05, 2.75, 1.9, 1.7, 3.85, 2.55, 2.45, 3.2, 1.55, 2.8, 2.25, 1.967, 2.2, 3, 2.867, 1.6, 2.55, 2.55, 2, 2.9, 2.4, 3.1, 1.9, 2.3, 3.25, 2.5, 2.1, 2.1, 3.325, 1.8, 3.225, 1.4, 2.4, 2.5, 1.8, 3.275, 2.225, 1.65, 2.9, 2.3, 3.2, 1.475, 2.025, 2.3, 1.95, 1.8, 2.9, 2.25, 3.05, 2.2, 3.1, 2.4, 2.25, 2.625, 5.2, 3.325, 2.925, 2, 2.4, 2.1, 3.725, 3.025, 1.9, 3, 2.85, 2.3, 2, 1.6, 1.9, 1.95, 3.2, 1.85, 1.8, 3.5, 2.35, 2.275, 3.05, 2.15, 2.75, 2.2, 1.8, 2.175, 2.75, 3.275, 2.625, 2.625, 2]
})
crabs = crabs.astype({
    'color': 'category',
    'spine': 'category'
})
print(crabs.dtypes)
crabs
```

:::

Let's now split the data into training and test sets.

::: {.panel-tabset}

## R Code

```{.r}
# Use 70% data for training and 30% for testing
par <- 0.7

# Set seed for reproducible splitting
set.seed(1046)
n <- nrow(crabs)
train_indices <- sample(seq_len(n), size = floor(par * n))

# Create training and test sets
train_set <- crabs[train_indices, ]
test_set <- crabs[-train_indices, ]

# Print the number of training and test samples
cat("Number of training samples:", nrow(train_set), "\n")
cat("Number of test samples:", nrow(test_set), "\n")
```

## Python Code

```{.python}
# Use 70% data for training and 30% for testing
par = 0.7

# Set seed for reproducible splitting
np.random.seed(1046)
n = len(crabs)
train_indices = np.random.choice(n, size=int(par * n), replace=False)

# Create training and test sets
train_set = crabs.iloc[train_indices]
test_set = crabs.drop(index=train_indices)

# Print the number of training and test samples
print(f"Number of training samples: {len(train_set)}")
print(f"Number of test samples: {len(test_set)}")
```

:::

::: {.panel-tabset}

## R Output

```{r, fig.width=14, fig.height=8}
#| echo: false
#| message: false
#| warning: false
n <- nrow(crabs)
#RNGkind(kind = "Mersenne-Twister", normal.kind = "Inversion") # use Mersenne Twister (MT19937) PRNG to generate random seeds
set.seed(1046)
test_indices <- sample(seq_len(n), size = floor(0.3 * n))
train_set <- crabs[-test_indices, ]
test_set <- crabs[test_indices, ]
cat("Number of training samples:", nrow(train_set), "\n")
cat("Number of test samples:", nrow(test_set), "\n")
```

## Python Output

```{python}
#| echo: false
#| message: false
#| warning: false
n = len(crabs)
#rng = np.random.default_rng(np.random.MT19937(seed=1046))
#train_indices = rng.choice(np.arange(1, n + 1), size=int(0.3 * n), replace=False)
test_indices = np.array([60,127,21,76,164,42,18,109,121,5,85,115,123,144,120,54,40,122,84,168,139,12,138,105,43,106,150,110,75,147,116,49,159,173,163,129,51,167,79,107,20,13,81,72,77,97,52,47,59,96,46]) - 1
test_set = crabs.iloc[test_indices]
train_set = crabs.drop(index=test_indices)
print(f"Number of training samples: {len(train_set)}")
print(f"Number of test samples: {len(test_set)}")
```

:::

## Exploratory Data Analysis

Suppose that the relationship between the satellite size per nest (`satell`) and width of the female horseshoe crab (`width` in cm) is our main research interest. 
Let's explore their relationship in the following scatterplot with the distribution of `satell` on the margin. 

::: {.panel-tabset}

## R Code

```{.r}
# draw the scatterplot of satellite size versus width 
p <- crabs %>%
  ggplot(aes(y = satell, x = width)) + 
  geom_point(alpha = 0.8) + 
  labs(x = "Width (cm)", y = "Satellite Size") +
  scale_x_continuous(breaks = seq(floor(min(crabs$width)), ceiling(max(crabs$width)), by = 2)) + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  theme_bw()
# add a marginal histogram with binwidth of 1, i.e., bins for integers; equivalent to the barplot
ggMarginal(p, type = "histogram", margins = "y", binwidth = 1, boundary = -0.5) # start with `satell=-0.5` to avoid the first bins (0s and 1s) being combined in one bin
```

## Python Code

```{.python}
import seaborn as sns
sns.set(style = "whitegrid")
# create bins of width 1 for the marginal histogram
bin_edges = np.arange(crabs['satell'].min(), crabs['satell'].max() + 2, 1)  # bins of width 1
# draw the scatterplot of satellite size versus width with marginal distributions
g = sns.jointplot(
    data = crabs, 
    x = "width", 
    y = "satell", 
    color = "black",
    kind = "scatter",
    marginal_ticks = False,
    marginal_kws = dict(bins = bin_edges, fill = True, color="black", alpha=0.6)
)
g.ax_marg_x.remove() # remove the marginal histogram on x axis
# add labels
g.ax_joint.set_xlabel("Width (cm)") 
g.ax_joint.set_ylabel("Satellite Size")
# add axes' limits
ymin, ymax = crabs['satell'].min(), crabs['satell'].max()
xmin, xmax = crabs['width'].min(), crabs['width'].max()
g.ax_joint.set_xlim(xmin - 1, xmax + 1);
g.ax_joint.set_ylim(ymin - 0.5, ymax + 0.5);
g.ax_marg_y.set_ylim(ymin - 0.5, ymax + 0.5);
```

:::

::: {.panel-tabset}

## R Output

```{r}
#| echo: false
#| message: false
#| warning: false
p <- crabs %>%
  ggplot(aes(y = satell, x = width)) + 
  geom_point(alpha = 0.8) + 
  labs(x = "Width (cm)", y = "Satellite Size") +
  scale_x_continuous(breaks = seq(floor(min(crabs$width)), ceiling(max(crabs$width)), by = 2)) + 
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  theme_bw()
ggMarginal(p, type = "histogram", margins = "y", binwidth = 1, boundary = -0.5)
```

## Python Output

```{python}
#| echo: false
#| message: false
#| warning: false
import numpy as np
import seaborn as sns
sns.set(style = "whitegrid")
bin_edges = np.arange(crabs['satell'].min(), crabs['satell'].max() + 2, 1)  # bins of width 1
g = sns.jointplot(
    data = crabs, 
    x = "width", 
    y = "satell", 
    color = "black",
    kind = "scatter",
    marginal_ticks = False,
    marginal_kws = dict(bins = bin_edges, fill = True, color="black", alpha=0.6)
)
g.ax_marg_x.remove()
g.ax_joint.set_xlabel("Width (cm)")
g.ax_joint.set_ylabel("Satellite Size")
ymin, ymax = crabs['satell'].min(), crabs['satell'].max()
xmin, xmax = crabs['width'].min(), crabs['width'].max()
g.ax_joint.set_xlim(xmin - 1, xmax + 1);
g.ax_joint.set_ylim(ymin - 0.5, ymax + 0.5);
g.ax_marg_y.set_ylim(ymin - 0.5, ymax + 0.5);
```

:::

The distribution of satellite sizes is highly right skewed, which violates the normality assumption of the ordinary linear regression. 

In the scatter plot above, we can tell that the satellite size gets more spread out as the width increases. The averaged satellite sizes turn to increase along the width increasing. Let's now split the width into a few intervals and compute the representative point for each interval to have a clear look at the trend. 

::: {.panel-tabset}

## R Code

```{.r}
# set up the number of intervals
n_intervals <- 10

# compute the average points for each interval of `width`
crabs_binned <- crabs %>%
  mutate(width_inl = cut(width, breaks = n_intervals)) %>%
  group_by(width_inl) %>%
  summarize(
    mean_width = mean(width),
    mean_satell = mean(satell),
    .groups = "drop"
  )
crabs_binned
```

## Python Code

```{.python}
# set up the number of intervals
n_intervals = 10

# compute the average points for each interval of `width`
crabs['width_inl'] = pd.cut(crabs['width'], bins=n_intervals)
crabs_binned = (
    crabs
    .groupby('width_inl', observed=True)
    .agg(mean_width=('width', 'mean'), mean_satell=('satell', 'mean'))
    .reset_index()
    .round(2)
)
crabs_binned
```

:::

::: {.panel-tabset}

## R Output

```{r}
#| echo: false
#| message: false
#| warning: false
n_intervals <- 10
crabs_binned <- crabs %>%
  mutate(width_inl = cut(width, breaks = n_intervals)) %>%
  group_by(width_inl) %>%
  summarize(
    mean_width = mean(width),
    mean_satell = mean(satell),
    .groups = "drop"
  )
crabs_binned
```

## Python Output

```{python}
#| echo: false
#| message: false
#| warning: false
n_intervals = 10
crabs['width_inl'] = pd.cut(crabs['width'], bins=n_intervals).round(2)
crabs_binned = (
    crabs
    .groupby('width_inl', observed=True)
    .agg(mean_width=('width', 'mean'), mean_satell=('satell', 'mean'))
    .reset_index()
    .round(2)
)
crabs_binned
```

:::

We've prepared the summarized dataset for the average points---each entry includes an interval of `width`, an averaged width, and an averaged satellite size per interval. Now, let's visualize the representative points on the scatter plot to take a closer look at the trend. 

::: {.panel-tabset}

## R Code

```{.r}
# add the average points onto the scatterplot, connect them, and mark intervals
breaks <- seq(min(crabs$width), max(crabs$width), length.out = n_intervals + 1)
p + 
  geom_vline(xintercept = breaks, linetype = "dashed", color = "gray50") +
  geom_point(data = crabs_binned, aes(x = mean_width, y = mean_satell), color = "red", size = 2) +
  geom_line(data = crabs_binned, aes(x = mean_width, y = mean_satell), color = "red", linewidth = 1)
```

## Python Code

```{.python}
# draw the scatterplot of satellite size versus width
p = (
    ggplot(crabs, aes(x='width', y='satell')) +
    geom_point(alpha=0.7, color='black') +  # points in gray
    labs(x="Width (cm)", y="Satellite Size") +
    scale_x_continuous(breaks=range(int(np.floor(crabs['width'].min())), int(np.ceil(crabs['width'].max())) + 1, 2)) +
    scale_y_continuous(breaks=range(int(np.floor(crabs['satell'].min())), int(np.ceil(crabs['satell'].max())) + 1, 2)) +
    theme_bw()
)
# add the average points onto the scatterplot, connect them, and mark intervals
breaks = np.linspace(crabs['width'].min(), crabs['width'].max(), n_intervals + 1)
p_final = (
    p +
    geom_vline(xintercept=breaks, linetype='dashed', color='gray') +
    geom_point(data=crabs_binned, mapping=aes(x='mean_width', y='mean_satell'), color='red', size=2) +
    geom_line(data=crabs_binned, mapping=aes(x='mean_width', y='mean_satell'), color='red', size=1)
)
p_final.show()
```

:::

::: {.panel-tabset}

## R Output

```{r}
#| echo: false
#| message: false
#| warning: false
breaks <- seq(min(crabs$width), max(crabs$width), length.out = n_intervals + 1)
p + 
  geom_vline(xintercept = breaks, linetype = "dashed", color = "gray50") +
  geom_point(data = crabs_binned, aes(x = mean_width, y = mean_satell), color = "red", size = 2) +
  geom_line(data = crabs_binned, aes(x = mean_width, y = mean_satell), color = "red", linewidth = 1)
```

## Python Output

```{python}
#| echo: false
#| message: false
breaks = np.linspace(crabs['width'].min(), crabs['width'].max(), n_intervals + 1)

p = (
    ggplot(crabs, aes(x='width', y='satell')) +
    geom_point(alpha=0.7, color='black') +  # points in gray
    labs(x="Width (cm)", y="Satellite Size") +
    scale_x_continuous(breaks=range(int(np.floor(crabs['width'].min())), int(np.ceil(crabs['width'].max())) + 1, 2)) +
    scale_y_continuous(breaks=range(int(np.floor(crabs['satell'].min())), int(np.ceil(crabs['satell'].max())) + 1, 2)) +
    theme_bw()
)
p_final = (
    p +
    geom_vline(xintercept=breaks, linetype='dashed', color='gray') +
    geom_point(data=crabs_binned, mapping=aes(x='mean_width', y='mean_satell'), color='red', size=2) +
    geom_line(data=crabs_binned, mapping=aes(x='mean_width', y='mean_satell'), color='red', size=1)
)
p_final.show()
```

:::

We can see a general increasing trend of the satellite size as the nest width grows. 

## Data Modelling

The Poisson regression model assumes a random sample of $n$ count observations $Y_i$s, hence independent (but not identically distributed!), which have the following distribution:
$$Y_i \sim \mathrm{Poisson}(\lambda_i).$$ 
Each $i$th observation has its own $\mathbb{E}(Y_i)=\lambda_i>0$, which also implicates $Var(Y_i)=\lambda_i>0$.

Parameter $\lambda_i$ is the risk of an event occurrence, coming from the definition of the Poisson random variable, in a given timeframe or even a space. It is a **continuous** distributional parameter! For the `crabs` dataset, the events are the number of satellite male horseshoe crabs around a space: the female breeding nest. 

Since the Poisson Regression model is also a GLM, we need to set up a link function for the mean. Let $X_{i,\text{width}}$ be the $i$th value for the regressor `width` in our dataset. An easy modelling solution would be an **identity** link function as in
$$
h(\lambda_i)=\lambda_i=\beta_0+\beta_1 X_{i,\text{width}}. \label{eq:pois-uni-iden}
$$

However, **we have a response range issue!**

The model @eq:pois-uni-iden for has a significant **drawback**: the right-hand side is allowed to take on even negative values, which does not align with the nature of the parameter $\lambda_i$ (that always has to be **non-negative**).

Recall the essential characteristic of a GLM that should come into play for a link function. In this class of models, the direct relationship between the original response and the regressors may be **non-linear** as in $h(\lambda_i)$. Hence, instead of using the identity link function, we will use the natural logarithm of the mean: $\log(\lambda_i)$.

Before continuing with the crabs dataset, let us generalize the Poisson regression model with $k$ regressors as:
$$
  h(\lambda_i) = \log(\lambda_i)=\beta_0+\beta_1 X_{i,1}+\dots+\beta_k X_{i,k}. \label{eq:pois-k}
$$
We could make more sense in the interpretation by exponentiating @eq:pois-k: 
$$
  \lambda_i = \exp(\beta_0+\beta_1 X_{i,1}+\dots+\beta_k X_{i,k}), \label{eq:pois-k-exp}
$$
where an increase in one unit in any of the $k$ regressors (while keeping the rest of them constant) multiplies the mean $\lambda_i$ by a factor $\exp(\beta_j)$, for all $j=i,\dots,k$.

In the `crabs` dataset with width as a regressor, the Poisson regression model is depicted as:
$$
  \log(\lambda_i)=\lambda_i=\beta_0+\beta_1 X_{i,\text{width}}. \label{eq:pois-uni}
$$


## Estimation

In generalized linear models (GLMs), parameter estimation is typically performed using maximum likelihood estimation (MLE). A widely used algorithm to compute the MLE in GLMs is the iteratively reweighted least squares (IRLS) method, which learns the regression coefficients by solving the weighted least squares problems through iteration. Below we learn the essential elements of the IRLS method and the MLE that it solves. We use Poisson regression as an example, but the procedure applies to other distributions straightforwardly. 

The general idea to solve the generalized linear regression is to estimate the parameters by maximizing the likelihood. For Poisson regression in @eq:pois-k specifically, it is written as: 
$$
  \prod_{i=1}^n p_{Y_i}(y_i) = \prod_{i=1}^n \frac{e^{-\lambda_i}{\lambda_i}^{y_i}}{y_i!}, 
$$
where $p_{Y_i}(y_i)$ is the probability mass function of $Y_i$, for $i=1,\dots,n$.

Let $\boldsymbol{\beta}:=(\beta_0,\dots,\beta_k)^{\top}$ be the coefficient vector, and then @eq:pois-k-exp can be rewritten as $\lambda_i=\exp(X_{i}^{\top}\boldsymbol{\beta})$, where $X_{i}^{\top}$ is the row $i$ in the design matrix $X$. Maximizing the likelihood is equivalent to minimizing the log-likelihood in terms of solving for the parameters $\beta_j,j=1,\dots,k$:
$$
  {\arg\min}_{\boldsymbol{\lambda}} - \prod_{i=1}^n\frac{e^{-\lambda_i}{\lambda_i}^{y_i}}{y_i!} = {\arg\min}_{\boldsymbol{\lambda}} \sum_{i=1}^n \lambda_i - y_i \log(\lambda_i).
$$
Plugging in $\lambda_i=\exp(X_{i}^{\top}\boldsymbol{\beta})$ gives the minimization problem with respect to the Poisson regression coefficients: 
$$
  {\arg\min}_{\boldsymbol{\beta}} \sum_{i=1}^n \exp(X_{i}^{\top}\boldsymbol{\beta}) - y_i X_{i}^{\top}\boldsymbol{\beta}. \label{eq:pois-log}
$$

Since the objective in @eq:pois-log is not quadratic and there is no closed form solution, we could not solve the equation exactly. IRLS solves an approximated solution with a specified accuracy or tolerance. The basic idea is that IRLS uses Fisher scoring (or Newton-Raphson) to solve an approximated problem of the original objective @eq:pois-log and solve it iteratively with weights updated through iteration until the specified accuracy is achieved, i.e., the objective is converged. 

The MLE $\hat{\boldsymbol{\beta}}$ satisfies the score function $U(\boldsymbol{\beta}) = \nabla_{\boldsymbol{\beta}} \ell(\boldsymbol{\beta}) = 0$. Applying the Fisher scoring update gives: 
$$
  \boldsymbol{\beta}^{t+1} \leftarrow \boldsymbol{\beta}^{t} + {\mathcal{I}(\boldsymbol{\beta}^t)}^{-1} U(\boldsymbol{\beta}^{t}),
$$
where $\mathcal{I}(\boldsymbol{\beta}^t) := \mathbb{E}\big[- \nabla^2_{\boldsymbol{\beta}} \ell(\boldsymbol{\beta}^t)\big]$ is the Fisher information, which is the expectation of the negative second-order derivative of the log-likelihood with respect to the parameters. 

The Fisher scoring update corresponds to solve the following approximated problem of the original objective at iterate $t+1$, which is a weighted least squares problem written as: 
$$
  \begin{align}
    \boldsymbol{\beta}^{t+1} &= {\arg\min}_{\boldsymbol{\beta}} \sum_{i=1}^n w_i^t (y_i^t - X_i^{\top}\boldsymbol{\beta})^2 \\ 
    &= {\arg\min}_{\boldsymbol{\beta}} \sum_{i=1}^n w_i^t (y_i^t - \beta_0 - \beta_1 x_{i,\text{width}})^2 \label{eq:irls-approx}
  \end{align}
$$
given the estimates at time $t$ for the univariate case using `crabs` dataset. The approximated problem has a closed-form solution and is much easier to solve than the original objective. The observations $y_i$ and weights $w_i$ should be updated per iterate. The arbitrary observations $y_i^{t+1}$ is a function of the original observations $y_i^0$ and the coefficient estimates. 
The weights is updated by 
$$w_i^{t+1} = \Big(\frac{1}{Var(Y_i)}\Big)\Big(\frac{1}{g'(\lambda_i)}\Big)^2,$$ 
where $\lambda_i$ is the mean of the response $y_i$, $\eta_i=g(\lambda_i)$ is the link function of the mean. 
In our case, the weights at iterate $t+1$ can be updated given the coefficient estimates at $t$ using the following formula:
$$
  w_i^{t+1} = \exp(X_i^{\top} \boldsymbol{\beta}^t), \forall i=1,\dots,n.
$$

The IRLS iteration procedure is as follows. 

1. Choose a set of initial coefficients $\boldsymbol{\lambda}^0 = (\lambda_0^0,\dots,\lambda_k^0)$, which can be a vector of zeros.
2. Compute the weights based on the estimates from the previous iterate, or the inital coefficients at the first iterate.
3. Solve the approximated problem @irls-approx.
4. Check te convergence condition.
5. Return estimates if the objective is converged; go back to step 2 if not.

Let's now fit the model on the training data.

::: {.panel-tabset}

## R Code

```{.r}
# Fit Poisson regression
poisson_model <- glm(satell ~ width, family = poisson(link = "log"), data = train_data)

# View summary
summary(poisson_model)
```

## Python Code

```{.python}
# Fit Poisson regression
poisson_model = glm(formula='satell ~ width', data=train_data, family=sm.families.Poisson(link=sm.families.links.log())).fit()

# View summary
print(poisson_model.summary())
```

:::

::: {.panel-tabset}

## R Output

```{r, fig.width=14, fig.height=8}
#| echo: false
#| message: false
#| warning: false
poisson_model <- glm(satell ~ width, family = poisson(link = "log"), data = train_set)
summary(poisson_model)
```

## Python Output

```{python}
#| echo: false
#| message: false
#| warning: false
poisson_model = glm(formula='satell ~ width', data=train_set, family=sm.families.Poisson(link=sm.families.links.log())).fit()
print(poisson_model.summary())
```

:::

## Goodness of Fit

In GLMs, we can use residual deviance and Chi-squared test to check the goodness of fit of the model. The residual deviance measures how well the fitted model explains the observed outcomes, compared to a perfect model (the saturated model) that explains the data exactly. A smaller residual deviance means a better fit of the data. It is computed as
$$\text{Residual Deviance } = 2 (\ell_{\mathrm{saturated}} - \ell_{\mathrm{fitted}}),$$
where $\ell_A$ is the log-likelihood of model $A$. For Poisson regression specifically, the residual deviance is $2\sum_{i=1}^n \Big[y_i\log\big(\frac{y_i}{\hat{\lambda}_i}\big) - (y_i-\hat{\lambda}_i)\Big]$.

When the model is a good fit, the residual deviance is expected to follow a chi-squared distribution with degrees of freedom $df = n - p$, where $n$ is the number of observations and $p$ is the number of parameters, asymptotically for a large enough sample size. We then can compute the $p$-value using the chi-squared distribution:
$$
  p\text{-value} = 1-P(\chi^2_{\mathrm{df}} \leq \text{residual deviance}).
$$

$p$-value is the probability of observing a residual deviance as large as (or larger than) what we got, if the model is truly correct. A large p-value (e.g., > 0.05) means that the observed deviance is not surprisingly large. Out model is a good fit and could plausibly have generated the data. A small p-value (e.g., < 0.05) means that the deviance is too large to be due to chance. The model is a poor fit and likely missing something important. 

::: {.panel-tabset}

## R Code

```{.r}
# Residual deviance and degrees of freedom
res_dev <- deviance(poisson_model)
df_res <- df.residual(poisson_model)

# Chi-squared goodness-of-fit test
p_value <- 1 - pchisq(res_dev, df_res)

cat("Residual Deviance:", round(res_dev, 4), "\n")
cat("Degrees of Freedom:", df_res, "\n")
cat("Goodness-of-fit p-value:", round(p_value, 4), "\n")
```

## Python Code

```{.python}
res_dev = poisson_model.deviance
df_res = poisson_model.df_resid

p_value = 1 - stats.chi2.cdf(res_dev, df_res)

print(f"Residual Deviance: {res_dev:.4f}")
print(f"Degrees of Freedom: {df_res}")
print(f"Goodness-of-fit p-value: {p_value:.4f}")
```

:::

::: {.panel-tabset}

## R Output

```{r, fig.width=14, fig.height=8}
#| echo: false
#| message: false
#| warning: false
res_dev <- deviance(poisson_model)
df_res <- df.residual(poisson_model)
p_value <- 1 - pchisq(res_dev, df_res)
cat("Residual Deviance:", round(res_dev, 4), "\n")
cat("Degrees of Freedom:", df_res, "\n")
cat("Goodness-of-fit p-value:", round(p_value, 4), "\n")
```

## Python Output

```{python}
#| echo: false
#| message: false
#| warning: false
res_dev = poisson_model.deviance
df_res = poisson_model.df_resid
p_value = 1 - stats.chi2.cdf(res_dev, df_res)
print(f"Residual Deviance: {res_dev:.4f}")
print(f"Degrees of Freedom: {df_res}")
print(f"Goodness-of-fit p-value: {p_value:.4f}")
```

:::

The probability of observing a deviance as large as this if the model is truly correct (i.e., the goodness-of-fit $p$-value) is esentially 0, saying that there is significant evidence of lack-of-fit. 
There can be several reasons for lack-of-fit. The model is misspecified, e.g., it is missing important covariates or nonlinear effects. The link function might be incorrect, so that the model is systematically overestimate or underestimate the data. There can be outliers or influential points that inflate the deviance. The data might be overdispersed, so that the residual deviance is inflated by the large variance. 

## Inference

The estimated model can be used for two purposes: inference and prediction. In terms of inference, we use the fitted model to identify the relationship between the response and regressors. 
Wald’s test is a general method for hypothesis testing in maximum likelihood estimation (MLE), including generalized linear models (GLMs) like Poisson regression. It tests the hypotheses of the form:
$$
  \begin{align}
    H_0 &: \beta_j = 0, \\
    H_a &: \beta_j \neq 0.
  \end{align}
$$
using the fact that $\frac{\hat{\beta}_j}{SE(\hat{\beta}_j)} \sim \mathcal{N}(0,1)$ asymptotically under $H_0$, where $\beta_j$ is the $j$th estimated regression coefficient and $SE(\hat{\beta}_j)$ is its corresponding variability which is reflected in the standard error of the estimate. 
This ratio is the Wald's test statistic
$$z_j = \frac{\hat{\beta}_j}{SE(\hat{\beta}_j)}$$
that is used to determine the statistical significance of $\hat{\beta}_j$ using the fact that the squared statistic asymptotically follows a one-degree-of-freedom chi-squared test, i.e., 
$$W_j = \Big(\frac{\hat{\beta}_j}{SE(\hat{\beta}_j)}\Big)^2\sim \chi_1^2.$$

*Remark 1: A statistic like $t_j=\frac{\hat{\beta}_j}{SE(\hat{\beta}_j)}$ in t test is referred to as a $t$-value. It assumes finite-sample normality---a student's $t$-distribution under the null hypothesis with $H_0$ with $n-k-1$ degrees of freedom. It can only be used for linear models with normal errors. While the Wald's test applies to any MLE (e.g., Poisson or logistic) to be used in any GLMs, and assumes asymptotic normality---standard normal or $\chi^2$ distribution under $H_0$.*

*Remark 2: Wald's test is validate under several conditions: 1-large sample size, so that the test statistic is asymptotically normal, 2-regularity conditions for MLE, including differentiable likelihood, positive definite Fisher information, correctly specified model and parameter space, 3-well-estimated parameters, i.e., parameters are not near the bounaries, 4-stable and finite standard errors, and 5-no outliers or high leverage points.*

We can obtain the corresponding $p$-values for each $\beta_j$ associated to the Wald's test statistic under the null hypothesis $H_0$. **The smaller the $p$-value, the stronger the evidence against the null hypothesis $H_0$ in our sample.** Hence, small $p$-values (less than the significance level $\alpha$) indicate that the data provides evidence in favour of association (or **causation** if that is the case) between the response variable and the $j$th regressor.

Similarly, given a specified $(1-\alpha)\times 100%$ level of confidence, we can construct **confidence intervals** for the corresponding true value of $\beta_j$:
$$\hat{\beta}_j \pm t_{\alpha/2,n-k-1} SE(\hat{\beta}_j),$$
where $t_{\alpha/2,n-k-1}$ is the upper $\alpha/2$ quantile of the $t$-distribution with $n-k-1$ degrees of freedom.

Let's now compute the 95% confidence interval. 

::: {.panel-tabset}

## R Code

```{.r}
tidy(poisson_model, conf.int = TRUE) %>% mutate_if(is.numeric, round, 3)
```

## Python Code

```{.python}
# Get coefficient table
summary_df = poisson_model.summary2().tables[1]

# Compute confidence intervals
conf_int = poisson_model.conf_int()
conf_int.columns = ['conf_low', 'conf_high']

# Combine with coefficient table
summary_df = summary_df.join(conf_int)

# Round all numeric columns to 3 decimals
summary_df = summary_df.round(3)

print(summary_df)
```

:::

::: {.panel-tabset}

## R Output

```{r, fig.width=14, fig.height=8}
#| echo: false
#| message: false
#| warning: false
tidy(poisson_model, conf.int = TRUE) %>% mutate_if(is.numeric, round, 3)
```

## Python Output

```{python}
#| echo: false
#| message: false
#| warning: false
summary_df = poisson_model.summary2().tables[1]
conf_int = poisson_model.conf_int()
conf_int.columns = ['conf_low', 'conf_high']
summary_df = summary_df.join(conf_int)
summary_df = summary_df.round(3)
print(summary_df)
```

:::

Our sample gives us evidence to reject $H_0$ (with a nearly-zero $p$ value), which suggests that carapace width is statistically associated to the logarithm of the satellite size. Now, it is time to plot the fitted values coming from poisson_model to check whether it provides a positive relationship between `width` and the original scale of the response `satell`.

::: {.panel-tabset}

## R Code

```{.r}
p +
  geom_smooth(
    data = train_set, aes(width, satell),
    method = "glm", formula = y ~ x,
    method.args = list(family = poisson), se = FALSE
  ) +
  labs(title="Poisson Regression Fitted Curve")
```

## Python Code

```{.python}
# Compute the fitted values
width_range = np.linspace(train_set['width'].min(), train_set['width'].max(), 100)
predict_df = pd.DataFrame({'width': width_range})
predict_df['predicted'] = poisson_model.predict(predict_df)

# Draw the scatterplot
sns.scatterplot(data=train_set, x='width', y='satell', label='Observed')

# Add the Poisson regression line
sns.lineplot(data=predict_df, x='width', y='predicted', color='red', label='Poisson fit')

# Add title
plt.title('Poisson Regression Fitted Curve')
plt.xlabel('Width')
plt.ylabel('Satellite size')
plt.legend()
plt.show()
```

:::

::: {.panel-tabset}

## R Output

```{r, fig.width=14, fig.height=8}
#| echo: false
#| message: false
#| warning: false
p +
  geom_smooth(
    data = train_set, aes(width, satell),
    method = "glm", formula = y ~ x,
    method.args = list(family = poisson), se = FALSE
  ) +
  labs(title="Poisson Regression Fitted Curve") +
  theme(
    plot.title = element_text(size = 20, face = "bold"),
    axis.title.x = element_text(size = 16),
    axis.title.y = element_text(size = 16),
    axis.text = element_text(size = 14)
  )
```

## Python Output

```{python}
#| echo: false
#| message: false
#| warning: false
width_range = np.linspace(train_set['width'].min(), train_set['width'].max(), 100)
predict_df = pd.DataFrame({'width': width_range})
predict_df['predicted'] = poisson_model.predict(predict_df)
sns.scatterplot(data=train_set, x='width', y='satell', label='Observed')
sns.lineplot(data=predict_df, x='width', y='predicted', color='red', label='Poisson fit')
plt.title('Poisson Regression Fitted Curve')
plt.xlabel('Width')
plt.ylabel('Satellite size')
plt.legend()
plt.show()
```

:::

The blue line in the plot above is the fitted Poisson regression of `satell` versus `width`. The positive relationship is now clear with this regression line.

## Results

Let's now predict on the test set. 

::: {.panel-tabset}

## R Code

```{.r}
# Predict
predictions <- predict(poisson_model, newdata = test_set, type = "response")
actuals <- test_set$satell
# Mean Squared Error
mse <- mean((actuals - predictions)^2)

cat("MSE:", mse, "\n")
```

## Python Code

```{.python}
# Predict
predictions = poisson_model.predict(test_set)
actuals = test_set['satell']
# Mean Squared Error
mse = mean_squared_error(actuals, predictions)

print(f"MSE: {mse:.4f}")
```

:::

::: {.panel-tabset}

## R Output

```{r, fig.width=14, fig.height=8}
#| echo: false
#| message: false
#| warning: false
predictions <- predict(poisson_model, newdata = test_set, type = "response")
actuals <- test_set$satell
mse <- mean((actuals - predictions)^2)
cat("MSE:", round(mse, 4), "\n")
```

## Python Output

```{python}
#| echo: false
#| message: false
#| warning: false
predictions = poisson_model.predict(test_set)
actuals = test_set['satell']
mse = mean_squared_error(actuals, predictions)
print(f"MSE: {mse:.4f}")
```

:::

The mean squared error (MSE) is 

## Storytelling
