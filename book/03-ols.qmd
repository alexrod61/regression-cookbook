<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7PRVEBE1EF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7PRVEBE1EF');
</script>

# Ordinary Least-squares {#sec-ols}

```{r}
#| include: false

colourize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

```{r setup, include=FALSE}
# Load required libraries
library(devtools)
library(readr)
library(ggplot2)
library(ggcorrplot)
library(cookbook)
library(tidyverse)
library(car)
library(lmtest)

data_path <- system.file("data", "ols_data.rda", package = "cookbook")

if (file.exists(data_path)) {
  load(data_path)  # Loads the dataset
} else {
  stop("Error: 'ols_data.rda' not found in the cookbook package.")
}

loaded_objects <- ls()
data <- OLS
```

## Learning Objectives

- Understand how OLS estimates relationships by minimizing the sum of squared residuals.
- Identify when OLS is an appropriate modeling choice.
- Fit an OLS model using Python or R.
- Interpret regression coefficients to understand the impact of predictors on the response variable.
- Check key OLS assumptions (linearity, independence, homoscedasticity, and normality of residuals).
- Evaluate model fit using R-squared and error metrics (MAE, MSE, RMSE).

## Introduction

**Ordinary Least Squares (OLS)** is a foundational method in regression analysis that estimates the relationship between a dependent variable and one or more independent variables. At its core, OLS aims to minimize the difference, or "error," between the observed values and the values predicted by the model. It does this by finding the best-fitting line (or hyperplane in the case of multiple variables) that minimizes the sum of the squared differences between the observed and predicted values.

The importance of OLS lies in its simplicity, interpretability, and wide applicability. As one of the earliest and most widely used techniques in statistics and machine learning, OLS provides a straightforward approach to understanding how variables influence each other. In areas such as economics, biology, and social sciences, OLS has enabled researchers to model and quantify relationships, making it a powerful tool for both inference and prediction.

The usefulness of OLS extends beyond its ease of use. Its ability to provide clear parameter estimates—coefficients that describe the strength and direction of relationships between variables—makes it a valuable tool for decision-making. Whether predicting financial outcomes, assessing medical treatments, or understanding consumer behavior, OLS offers a framework for transforming raw data into actionable insights.

In this chapter, we will explore how OLS works, its assumptions, and how we can apply it to real-world data to solve problems, while also understanding its limitations. Through this, we aim to build a strong foundation in regression analysis that will serve as a stepping stone to more advanced techniques.

## Case Study: Understanding Financial Behaviors

Before we dive into the math behind Ordinary Least Squares (OLS), let's start by framing the problem we're trying to solve and the data we'll be working with. We’ll be using a toy dataset to understand the financial behaviors of students and identify the factors that influence their net money—the amount of money left over at the end of each month.

### The Dataset

In this case study, we have collected data on various aspects of students' financial lives. Each row in our dataset represents a single student, and the columns capture different characteristics of the student. Here's a breakdown of the variables:

| **Variable Name**         | **Description**                                                       |
|---------------------------|-----------------------------------------------------------------------|
| **Has_Job**               | Whether the student has a job (0 = No, 1 = Yes).                      |
| **Year_of_Study**         | The student’s current year of study (e.g., 1st year, 2nd year, etc.). |
| **Financially_Dependent** | Whether the student is financially dependent on someone else (0 = No, 1 = Yes). |
| **Monthly_Allowance**     | The amount of financial support the student receives each month.       |
| **Cooks_at_Home**         | Whether the student prepares their own meals (0 = No, 1 = Yes).        |
| **Living_Situation**      | The student's living arrangement (e.g., living with family, in a shared apartment, etc.). |
| **Housing_Type**          | The type of housing the student lives in (e.g., rented, owned, dormitory). |
| **Goes_Out_Spends_Money** | How frequently the student goes out and spends money (1 = rarely, 5 = very often). |
| **Drinks_Alcohol**        | Whether the student drinks alcohol (0 = No, 1 = Yes).                 |
| **Net_Money**             | The amount of money the student has left at the end of the month after income and expenses. |
| **Monthly_Earnings**      | The student’s earnings from any part-time jobs or other income sources. |

Here’s a sample of what the data looks like:

<div style="overflow-x: auto; white-space: nowrap;">
| **Has_Job** | **Year_of_Study** | **Financially_Dependent** | **Monthly_Allowance** | **Cooks_at_Home** | **Living_Situation** | **Housing_Type** | **Goes_Out_Spends_Money** | **Drinks_Alcohol** | **Net_Money** | **Monthly_Earnings** |
|-------------|--------------------|---------------------------|-----------------------|-------------------|----------------------|------------------|---------------------------|--------------------|---------------|----------------------|
| 0           | 1                  | 0                         | 658.99                | 0                 | 3                    | 1                | 6                           | 0                  | 529.34        | 0.00                 |
| 1           | 3                  | 0                         | 592.55                | 0                 | 3                    | 2                | 3                           | 1                  | 992.72        | 941.92               |
| 1           | 4                  | 1                         | 602.54                | 0                 | 2                    | 2                | 2                           | 1                  | 557.30        | 876.57               |
</div>

### The Problem We’re Trying to Solve

Our goal in this case study is to understand which factors impact a student’s Net Money. We’re interested in identifying which characteristics (like having a job, monthly earnings, or receiving financial support) explain why some students have more money left over at the end of the month than others.

The key question we want to answer is: Which factors have the biggest influence on how much net money a student has?

By analyzing this dataset using OLS, we can explore how each factor contributes to the variation in Net Money and quantify the relationships between them.

### Why This Matters

This case study is a simple but practical example of how regression analysis can be used to study financial behavior. In real-world situations, understanding such relationships can help individuals or institutions make informed decisions—like how much financial aid to offer or what type of part-time job is most effective for students.

By the end of this analysis, we’ll be able to:

- Identify which factors (like having a job or monthly earnings) most strongly affect students' net money.
- Estimate how much each factor contributes to changes in net money.
- Understand whether these relationships are statistically significant.

## Study Design

With our case study and dataset introduced, the next step is to define the **main statistical inquiries** we aim to address. In this stage, we will clarify the focus of our analysis: **What factors influence a student’s _Net Money_ at the end of the month?** To answer this, we need to determine whether we’re aiming to understand relationships between variables or make accurate predictions. This distinction between **inferential** and **predictive** analysis shapes the methods we will use, including the application of Ordinary Least Squares (OLS).

### Inferential vs. Predictive Analysis

As a data scientist, your first task is to decide whether the analysis is inferential or predictive.

- **Inferential Analysis** aims to explore and quantify relationships between explanatory variables (e.g., student characteristics) and the response variable (_Net Money_). For instance, we might ask: **Does having a part-time job significantly affect a student’s _net money_, and by how much?** The goal is to measure and understand these effects, assessing their statistical significance.
- **Predictive Analysis**, on the other hand, focuses on making accurate predictions about the response variable based on new data. Here, the question could be: **Can we predict a student’s _net money_ based on their _monthly earnings_, _living situation_, and _spending habits_?** The emphasis is on creating a model that delivers precise forecasts, even if we aren’t focused on explaining the underlying relationships.

### Applying Study Design to Our Case Study

For our case study, we are primarily interested in understanding the relationships between variables like _Has_Job_, _Monthly_Earnings_, and _Spending Habits_, and how they affect a student’s _Net Money_. This leads us toward an inferential approach. We aim to answer questions such as:

- **Does having a part-time job lead to significantly higher _net money_?**
- **How much does a student’s _monthly earnings_ affect their financial situation?**
- **Do _spending habits_, like going out frequently, decrease a student’s _net money_?**

Using OLS, we will estimate the impact of each factor and assess whether these effects are statistically significant. This inferential analysis will help us understand which variables have the greatest influence on students’ financial outcomes.

Alternatively, if our goal were to build a tool that predicts future _Net Money_ for new students based on their characteristics, we would adopt a predictive approach. OLS would be used to minimize prediction errors and make reliable forecasts. Although our focus here is inferential, it’s important to recognize that OLS is versatile and can be applied in both contexts.

## Data Collection and Wrangling

With the statistical questions clearly defined, the next step is to ensure that the data is appropriately prepared for analysis. Although we already have the dataset, it’s worth considering how this data could have been collected to better understand the context of the information.

### Data Collection

For a study like ours, data on students’ financial behaviors could have been collected through several methods. **Surveys** might have been distributed to students, asking about their work status, earnings, and spending habits. Alternatively, **administrative data** from universities or employers could provide reliable information about student income and employment. **Financial tracking apps** could also be a rich source of detailed, real-time spending and income data. Regardless of the method used, each brings its own set of challenges—such as missing data or potential biases—that need to be accounted for in the cleaning process.

### Data Wrangling

Now that we have our dataset, the primary focus is on cleaning and organizing it for analysis using Ordinary Least Squares (OLS). The first step is ensuring **data integrity**. This involves checking for missing values or inconsistencies. For example, if some students don’t have recorded earnings or net money, we need to decide how to handle those gaps. Removing incomplete records is one option, but in some cases, it may be more appropriate to impute missing values using logical estimates or averages from the dataset. In our toy dataset, we can see that we have no missing values.

```{r check_missing_value}
colSums(is.na(data))
```

Next, we need to transform certain variables so they are ready for regression analysis. For example, categorical variables like *Has_Job* and *Drinks_Alcohol* need to be encoded into numerical values (e.g., 1 for "Yes" and 0 for "No"). Additionally, continuous variables such as *Monthly_Earnings* and *Net_Money* should be examined for extreme outliers—students with unusually high incomes or spending might distort the model, so it’s important to assess whether these values should be adjusted or removed.

```{r convert_binary_to_factor}
# Convert binary categorical variables to factors
data <- data |>
  mutate(Has_Job = as.factor(Has_Job),
         Drinks_Alcohol = as.factor(Drinks_Alcohol),
         Financially_Dependent = as.factor(Financially_Dependent),
         Cooks_at_Home = as.factor(Cooks_at_Home))
```

```{r outliers}
# Using IQR method to filter out extreme values in continuous variables
remove_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  x[x < (Q1 - 1.5 * IQR) | x > (Q3 + 1.5 * IQR)] <- NA
  return(x)
}

data <- data |>
  mutate(across(c(Monthly_Earnings, Net_Money), remove_outliers))

# Remove rows with newly introduced NAs due to outlier handling
data <- na.omit(data)
```
After cleaning and transforming the data, the final preparation step is splitting the dataset. Typically, we divide the data into two sets: one for training the model and the other for testing its performance. The training set is used to build the OLS model, while the test set allows us to assess how well the model generalizes to unseen data. This ensures that our model doesn’t just perform well on the current dataset but is also robust enough to make reliable predictions or insights beyond this specific sample.

```{r train_test_split}
# Splitting the dataset into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(seq_len(nrow(data)), size = 0.8 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
```

## Exploratory Data Analysis (EDA)

Before diving into data modeling, it is crucial to develop a deep understanding of the relationships between variables in the dataset. This stage, known as Exploratory Data Analysis (EDA), helps us visualize and summarize the data, uncover patterns, detect anomalies, and test key assumptions that will inform our modeling decisions.

### Classifying Variables

The first step in EDA is to classify variables according to their types. This classification guides the selection of appropriate visualization techniques and modeling strategies. In our toy dataset, we categorize variables as follows:

Response Variable:

- `Net_Money`: Continuous (bounded, as it is limited by realistic income and expenses)

Regressors:

- **Binary**: `Has_Job`, `Financially_Dependent`, `Cooks_at_Home`, `Drinks_Alcohol`
- **Categorical**: `Living_Situation`, `Housing_Type`
- **Ordinal**: `Year_of_Study`, `Goes_Out_Spends_Money` (scale 1-5)
- **Continuous**: `Monthly_Allowance`, `Monthly_Earnings`

This classification ensures that we apply the correct statistical methods and visualizations in the next steps.

### Visualizing Variable Distributions

Once variables are classified, the next step is to explore their distributions to understand their characteristics. We employ different visualizations depending on the variable type:

#### Continuous Variables

- **Histograms**: Used to inspect the distribution of numerical variables.
- **Boxplots**: Useful for identifying outliers and comparing distributions.

```{r eda-histogram}
# Histogram of Net_Money
hist(train_data$Net_Money, 
     main = "Distribution of Net Money", 
     xlab = "Net Money", 
     col = "blue", 
     border = "white")
```

```{r eda-boxplot}
# Boxplot of Net Money
boxplot(train_data$Net_Money, 
        main = "Boxplot of Net Money", 
        ylab = "Net Money", 
        col = "lightblue")
```

#### Categorical and Ordinal Variables

- **Bar charts**: Display frequency counts for categorical variables.

```{r eda-barcharts}
# Bar plot of Living Situation
barplot(table(train_data$Living_Situation), 
        main = "Living Situation Distribution", 
        xlab = "Living Situation", 
        ylab = "Frequency", 
        col = "purple")
```

#### Exploring Relationships Between Variables

- **Correlation Matrices**: Help identify strong relationships between continuous predictors and the response variable.
- **Scatter Plots**: Assess linear relationships between numerical variables.
- **Boxplots**: Compare how different categories influence a continuous response variable.

```{r eda-corr-matrix}
# Correlation matrix
cor_matrix <- cor(train_data[, c("Net_Money", "Monthly_Allowance", "Monthly_Earnings")], use = "complete.obs")
print(cor_matrix)
```

```{r eda-scatterplot}
# Scatter plot of Monthly Allowance vs. Net Money
plot(train_data$Monthly_Allowance, train_data$Net_Money, 
     main = "Net Money vs. Monthly Allowance", 
     xlab = "Monthly Allowance", 
     ylab = "Net Money", 
     col = "blue", 
     pch = 19)
abline(lm(Net_Money ~ Monthly_Allowance, data = train_data), col = "red", lwd = 2)
```

```{r eda-multiple-boxplots}
# Boxplot of Net Money by Living Situation
boxplot(Net_Money ~ Living_Situation, 
        data = train_data, 
        main = "Net Money by Living Situation", 
        xlab = "Living Situation", 
        ylab = "Net Money", 
        col = "lightgreen")
```


#### Summary Statistics
Alongside visualizations, we compute descriptive statistics to summarize key characteristics of the data:

```{r eda-summary-stats}
# Summary statistics for numerical variables
summary(train_data[, c("Net_Money", "Monthly_Allowance", "Monthly_Earnings")])
```

This step provides insights into the central tendency (mean, median) and dispersion (standard deviation, range) of each variable, which will inform our model selection and feature engineering.

## Data Modelling

Following Exploratory Data Analysis (EDA), we now move to data modeling, where we apply a structured approach to uncover relationships between variables and predict outcomes. In this section, we focus on Ordinary Least Squares (OLS) regression, a widely used statistical technique for modeling linear relationships.

### Choosing a Suitable Regression Model

The choice of regression model depends on the patterns identified in EDA and the objectives of the analysis. Below are common regression approaches:

- **Simple Linear Regression**: Suitable when modeling a single predictor’s linear effect on the response variable.
- **Multiple Linear Regression:** Used when multiple predictors contribute to the response variable.
- **Polynomial Regression**: Captures non-linear relationships by incorporating polynomial terms.
- **Log-Linear Models**: Applied when transforming skewed distributions improves interpretability.
- **Ridge and Lasso Regression**: Useful when dealing with multicollinearity or performing feature selection through regularization.

Since our analysis focuses on understanding financial habits with multiple contributing factors, we use Multiple Linear Regression via OLS to quantify the relationships between the response (Net_Money) and predictors.

### Defining Modeling Parameters

After selecting OLS as our regression model, we define key modeling components:

Response Variable (Y):

- `Net_Money`: The dependent variable representing financial balance.

Predictors (X):

- Continuous: `Monthly_Allowance`, `Monthly_Earnings`
- Categorical: `Living_Situation`
- Ordinal: `Year_of_Study`

Each predictor is selected based on EDA insights, ensuring relevance in explaining `Net_Money`. If interaction effects exist, such as `Living_Situation` modifying the effect of `Monthly_Allowance`, we incorporate interaction terms.

### Setting Up the Modeling Equation

The OLS regression equation expresses how predictors influence the response variable:

$$
\begin{align}
\text{Net_Money} = &\ \beta_0 \\
               & + \beta_1 \times \text{Monthly_Allowance} \\
               & + \beta_2 \times \text{Monthly_Earnings} \\
               & + \beta_3 \times \text{Year_of_Study} \\
               & + \beta_4 \times \text{Living_Situation} \\
               & + \epsilon
\end{align}
$$

where:

- $\beta_0$​ is the intercept (baseline Net_Money when all predictors are zero),
- $\beta_1, \beta_2, \beta_3, \beta_4$ are regression coefficients, quantifying how each predictor affects `Net_Money`,
- $\epsilon$ is the error term, capturing unexplained variance.

This equation provides a formal structure to estimate how financial behavior is influenced by multiple factors. The next step will involve estimation, where we compute these coefficients and assess their significance.

## Estimation

With the data modeling stage completed, we now move to estimation, where we fit the Ordinary Least Squares (OLS) regression model and interpret its coefficients. This step provides numerical estimates for how much each predictor contributes to the response variable, allowing us to quantify their effects.

### Fitting the Model

To estimate the regression coefficients, we fit the OLS model to the training data using Python (statsmodels) or R (lm).

```{r fitting-ols}
# Load necessary library
library(stats)

# Fit the OLS model
ols_model <- lm(Net_Money ~ Monthly_Allowance + Monthly_Earnings + Year_of_Study, data=train_data)

# Display summary of model results
summary(ols_model)
```

### Interpreting the Coefficients

After fitting the model, we examine the estimated coefficients to understand their impact. Each coefficient represents the effect of a predictor on Net_Money, assuming all other factors are constant:

- Intercept ($\beta_0 = -826.01$): If all predictors are zero, the expected Net_Money is -$826.01.
- Monthly_Allowance ($\beta_1 = 1.43$): Each $1 increase in Monthly_Allowance is associated with a $1.43 increase in Net_Money.
- Monthly_Earnings ($\beta_2 = 0.98$): Each $1 increase in Monthly_Earnings increases Net_Money by $0.98.
- Year_of_Study ($\beta_3 = -104.34$): Being in a later year of study decreases Net_Money by -$104.34 on average.

## Goodness of Fit

After estimating the regression coefficients, we now assess how well the model fits the data and whether it satisfies the assumptions of Ordinary Least Squares (OLS) regression. This step ensures that our conclusions are statistically valid and that our model generalizes well to new data.

### Checking Model Assumptions

OLS regression relies on several key assumptions. We use diagnostic plots and statistical tests to verify these assumptions:

#### Linearity

The relationship between predictors and the response should be linear.

- **Diagnostic Plot**: A residual vs. fitted values plot should show a random spread (no patterns).
- **Solution if Violated**: Apply polynomial terms or transformations (e.g., log transformation).

```{r model-assumption-linearity}
# Residuals vs Fitted plot (R)
plot(ols_model$fitted.values, residuals(ols_model), 
     main = "Residuals vs Fitted", xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")
```

#### Independence of Errors

Residuals should not be correlated (especially in time-series data).

- **Diagnostic Test**: Durbin-Watson test detects autocorrelation.

```{r model-assumption-indep-of-error}
dwtest(ols_model)
```

#### Homoscedasticity (Constant Variance of Errors)

The variance of residuals should remain constant across all fitted values.

- **Diagnostic Plot**: Residuals vs. fitted values plot.
- **Solution if Violated**: Use weighted least squares regression.

```{r model-assumption-homoscedasticity}
ncvTest(ols_model)  # Test for homoscedasticity
```

#### Normality of Residuals

Residuals should be normally distributed for valid hypothesis testing.

- **Diagnostic Plot**: Q-Q plot (Quantile-Quantile plot) shows if residuals follow a normal distribution.


```{r model-assumption-normality-residuals}
qqnorm(residuals(ols_model))
qqline(residuals(ols_model), col = "red")
```

### Evaluating Model Fit

A good model should explain a large proportion of variance in the response variable.

#### R-Squared

- Definition: Measures the proportion of variance in the response variable explained by predictors.
- Interpretation: An R-squared of 0.85 means that 85% of the variation in Net_Money is explained by the predictors.

```{r r-squared}
summary(ols_model)$r.squared  # R-squared value
```

Limitations:
- A high R-squared does not imply causation.
- Adding predictors always increases R-squared, even if they are irrelevant.
- Adjusted R-squared accounts for unnecessary predictors.


```{r adjusted-r-squared}
summary(ols_model)$adj.r.squared  # Adjusted R-squared
```

#### Identifying Outliers and Influential Points

Extreme values can distort regression estimates. We detect them using:

Residual Plots

- Large residuals may indicate outliers.

```{r residual-plots}
plot(residuals(ols_model), main = "Residual Plot", ylab = "Residuals")
abline(h = 0, col = "red")
```

Cook’s Distance

- Definition: Measures how much a data point influences regression results.
- Threshold: If Cook’s distance > 0.5, the point may be influential.

```{r cooks-distance}
cook_values <- cooks.distance(ols_model)
plot(cook_values, type = "h", main = "Cook's Distance")
```

By validating these assumptions and model fit, we ensure that our regression analysis is reliable and informative. Next, we move to interpreting and communicating the results.

## Results

After validating the model’s goodness of fit, we now assess its predictive performance and inferential insights. This step involves using the trained model to make predictions and evaluating how well it generalizes to unseen data.

### Predictive Analysis

To assess our model’s effectiveness, we generate predictions on the test set and compute evaluation metrics. In Python and R, we use the trained OLS model to predict Net_Money for the test set.

```{r predict-on-test}
# Generate predictions on the test set
y_pred <- predict(ols_model, newdata=test_data)
```

#### Performance Metrics

To measure the accuracy of predictions, we compute standard regression error metrics:

- Mean Absolute Error (MAE): Measures average absolute differences between predicted and actual values.
- Mean Squared Error (MSE): Penalizes larger errors more heavily.
- Root Mean Squared Error (RMSE): Similar to MSE but in the same units as Net_Money.
- R-squared: Proportion of variance in Net_Money explained by the model.

```{r prediction-results}
# Extract response variable from test data
y_test <- test_data$Net_Money

# Calculate metrics in R
mae <- mean(abs(y_test - y_pred))
mse <- mean((y_test - y_pred)^2)
rmse <- sqrt(mse)
r2 <- summary(ols_model)$r.squared

cat(sprintf("MAE: %.2f, MSE: %.2f, RMSE: %.2f, R-squared: %.2f", mae, mse, rmse, r2))
```

### Inferential Analysis

Even though our primary focus is prediction, we can derive meaningful insights from the estimated coefficients.

#### Insights from Regression Coefficients

- Monthly_Allowance has a positive coefficient, indicating that higher allowances lead to greater financial balance (Net_Money).
- Year_of_Study has a negative coefficient, suggesting that students in later years tend to have lower Net_Money, possibly due to increased expenses or reduced financial support.
- If Living_Situation is significant, it may indicate that housing type influences financial balance.

With these insights, we move to the final stage: storytelling, where we communicate findings in a clear and actionable manner.

## Storytelling

The final step in our data science workflow is storytelling, where we translate our analytical findings into actionable insights. This stage ensures that our results are clearly understood by both technical and non-technical audiences. Effective storytelling involves summarizing insights, using visuals for clarity, and making data-driven recommendations.





