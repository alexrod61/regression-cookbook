<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7PRVEBE1EF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7PRVEBE1EF');
</script>

# Ordinary Least-squares {#sec-ols}

```{r}
#| include: false

colourize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

```{r setup, include=FALSE}
# Load required libraries
library(devtools)
library(readr)
library(ggplot2)
library(ggcorrplot)
library(cookbook)
library(tidyverse)
library(car)
library(lmtest)

data_path <- system.file("data", "ols_data.rda", package = "cookbook")

if (file.exists(data_path)) {
  load(data_path)  # Loads the dataset
} else {
  stop("Error: 'ols_data.rda' not found in the cookbook package.")
}

loaded_objects <- ls()
data <- OLS
```

## Learning Objectives

By the end of this chapter, you will be able to:
- Explain how Ordinary Least Squares (OLS) estimates relationships by minimizing the sum of squared residuals.
- Determine when OLS is an appropriate modeling choice.
- Fit an OLS model using Python or R.
- Interpret regression coefficients to assess the impact of predictors on the response variable.
- Check key OLS assumptions, including linearity, independence, homoscedasticity, and normality of residuals.
- Evaluate model performance using R-squared and error metrics such as MAE, MSE, and RMSE.

## Introduction

**Ordinary Least Squares (OLS)** is a fundamental method in regression analysis for estimating the relationship between a dependent variable and one or more independent variables. OLS finds the best-fitting line (or hyperplane for multiple predictors) by minimizing the sum of squared differences between observed and predicted values, ensuring the smallest overall error.

OLS is widely used due to its simplicity, interpretability, and broad applicability. As one of the earliest and most fundamental techniques in statistics and machine learning, it provides a structured way to understand variable relationships. Fields such as economics, biology, and social sciences rely on OLS to quantify associations and make informed decisions.

Beyond ease of use, OLS offers clear parameter estimates—coefficients that describe the strength and direction of relationships between variables. This makes it valuable for applications ranging from financial forecasting and medical research to consumer behavior analysis.

In this chapter, we will explore how OLS works, its assumptions, and its real-world applications. We will also examine its limitations, providing a solid foundation in regression analysis as a stepping stone to more advanced techniques.

## The "Best Line"

When fitting a regression line using Ordinary Least Squares (OLS), the goal is to find the line that best captures the relationship between the dependent variable $Y$ and the independent variable $X$. But how do we define "best"?

Imagine we have a scatter plot of data points and we draw two different lines:

- **Line A**: A reasonably well-fitted line that follows the general trend of the data.
- **Line B**: A slightly worse line that doesn’t fit the data as closely.

```{r two_lines, echo=FALSE}
# Sample data
set.seed(42)
X <- c(1000, 1200, 1500, 1800, 2000)
Y <- c(200, 230, 250, 290, 310)

# Create a data frame
df <- data.frame(Size = X, Price = Y)

# Fit the correct OLS model
correct_model <- lm(Price ~ Size, data = df)

# Create predictions for the two lines
df$Predicted_Correct <- predict(correct_model, newdata = df)
df$Predicted_Wrong <- 110 + 0.08 * df$Size  # Adjusted manually

# Reshape data for ggplot (to add legend)
df_long <- data.frame(
  Size = rep(df$Size, 2),
  Price = c(df$Predicted_Correct, df$Predicted_Wrong),
  Line = rep(c("Line A (Best Fit)", "Line B (Worse Fit)"), each = nrow(df))
)

# Store the plot with a legend
plot <- ggplot() +
  geom_point(data = df, aes(x = Size, y = Price), size = 3, color = "black") +  # Scatter plot
  geom_line(data = df_long, aes(x = Size, y = Price, color = Line), linewidth = 1.2) +
  scale_color_manual(values = c("Line A (Best Fit)" = "blue", "Line B (Worse Fit)" = "red")) +
  labs(title = "Comparing Regression Line Fits",
       x = "House Size (sq ft)",
       y = "House Price (in $1000s)",
       color = "Regression Line") +  # Legend title
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray40"),
        legend.position = "bottom")  # Move legend to bottom

plot
```

Now, let’s compare them:

- For each data point, the residual is the vertical distance between the observed Y value ($Y$) and the predicted Y value ($\hat{Y}$) from the line.
- If a line fits well, these residuals will be smaller on average, meaning the predictions are closer to reality.
- OLS chooses the best line by minimizing the sum of squared residuals—this ensures that the chosen line has the smallest total error.

### Why Squared Errors?

Squaring the residuals achieves two things:

- It avoids cancelation. Some residuals are positive (the line underestimates the true value), and some are negative (the line overestimates). Squaring makes sure all errors are treated equally.
- It penalizes large errors more. A bad prediction (a large residual) contributes significantly more to the total error than a small one, which helps OLS focus on overall accuracy.

### The Mathematical Formulation of the OLS Model

Now that we understand how OLS determines the best-fitting line by minimizing residuals, let's express this formally with a mathematical equation.

For a simple linear regression model with one predictor, the equation of the regression line is:

$$
Y=\beta_0+\beta_1X+\epsilon
$$

where:

- $Y$ is the dependent variable (outcome of interest).
- $X$ is the independent variable (predictor).
- $\beta_0$ (intercept) represents the predicted value of $Y$ when $X=0$.
- $\beta_1(slope) quantifies the change in $Y$ for a one-unit increase in $X$.
- $\epsilon$ (error term) accounts for random variability not explained by the model.

#### Example: Predicting House Prices

To better understand these notations, let's take a look an example. Say we want to predict the price of a house ($Y$ in thousands of dollars) based on its size ($X$ in square feet). Suppose we have the following data:

| House Size ($X$ in sq ft) | Price ($Y$ in \$1000s) |
|----------------|---------------|
| 1000          | 200           |
| 1200          | 230           |
| 1500          | 250           |
| 1800          | 290           |
| 2000          | 310           |

Our regression equation would look like:

$$
\text{Price} = \beta_0 + \beta_1 (\text{Size}) + \epsilon
$$

where:  

- $\beta_0$ (intercept) represents the estimated price of a house when the size is zero (which may not be meaningful but is mathematically required).  
- $\beta_1$ (slope) represents the expected increase in price per additional square foot.  

For instance, if the regression model estimates that each additional square foot adds **$107** to the house price, and the intercept represents a base price (even at 0 sq ft), then we can interpret the model coefficients meaningfully.

## Case Study: Understanding Financial Behaviors

To demonstrate Ordinary Least Squares (OLS) in action, we will walk through a case study using a toy dataset. This case study will help us understand the financial behaviors of students and identify the factors that influence their `Net_Money`, the amount of money left over at the end of each month. We will approach this case study using the data science workflow described in a previous chapter, ensuring a structured approach to problem-solving and model building.

### The Dataset

Our dataset captures various aspects of students' financial lives. Each row represents a student, and the columns describe different characteristics. Below is a breakdown of the variables:

| **Variable Name**         | **Description**                                                       |
|---------------------------|-----------------------------------------------------------------------|
| **Has_Job**               | Whether the student has a job (0 = No, 1 = Yes).                      |
| **Year_of_Study**         | The student’s current year of study (e.g., 1st year, 2nd year, etc.). |
| **Financially_Dependent** | Whether the student is financially dependent on someone else (0 = No, 1 = Yes). |
| **Monthly_Allowance**     | The amount of financial support the student receives each month.       |
| **Cooks_at_Home**         | Whether the student prepares their own meals (0 = No, 1 = Yes).        |
| **Living_Situation**      | The student's living arrangement (e.g., living with family, in a shared apartment, etc.). |
| **Housing_Type**          | The type of housing the student lives in (e.g., rented, owned, dormitory). |
| **Goes_Out_Spends_Money** | How frequently the student goes out and spends money (1 = rarely, 5 = very often). |
| **Drinks_Alcohol**        | Whether the student drinks alcohol (0 = No, 1 = Yes).                 |
| **Net_Money**             | The amount of money the student has left at the end of the month after income and expenses. |
| **Monthly_Earnings**      | The student’s earnings from any part-time jobs or other income sources. |

Here’s a sample of the dataset:

<div style="overflow-x: auto; white-space: nowrap;">
| **Has_Job** | **Year_of_Study** | **Financially_Dependent** | **Monthly_Allowance** | **Cooks_at_Home** | **Living_Situation** | **Housing_Type** | **Goes_Out_Spends_Money** | **Drinks_Alcohol** | **Net_Money** | **Monthly_Earnings** |
|-------------|--------------------|---------------------------|-----------------------|-------------------|----------------------|------------------|---------------------------|--------------------|---------------|----------------------|
| 0           | 1                  | 0                         | 658.99                | 0                 | 3                    | 1                | 6                           | 0                  | 529.34        | 0.00                 |
| 1           | 3                  | 0                         | 592.55                | 0                 | 3                    | 2                | 3                           | 1                  | 992.72        | 941.92               |
| 1           | 4                  | 1                         | 602.54                | 0                 | 2                    | 2                | 2                           | 1                  | 557.30        | 876.57               |
</div>

This dataset provides a structured way to analyze the financial habits of students and determine which factors contribute most to their financial stability. In the next section, we will apply OLS to quantify these relationships within the data science workflow framework.

### The Problem We’re Trying to Solve

Our goal in this case study is to understand which factors impact a student’s net money. Specifically, we aim to identify which characteristics—such as having a job, monthly earnings, or financial support—explain why some students have more money left over at the end of the month than others.

The key question we want to answer is:

> **Which factors have the biggest influence on a student’s net money?**

By applying OLS to this dataset, we can quantify how each factor contributes to variations in net money and determine the strength of these relationships.

### Why This Matters

This case study provides a practical demonstration of how regression analysis can be used to study financial behavior. In real-world scenarios, understanding these relationships can help individuals and institutions make informed decisions—for example, determining how much financial aid to provide or identifying which types of part-time jobs are most beneficial for students.

By the end of this analysis, we will:

- Identify which factors (e.g., having a job, monthly earnings) most strongly influence a student’s net money.
- Estimate how much each factor contributes to changes in net money.
- Determine whether these relationships are statistically significant.

### Study Design

With our case study and dataset introduced, the next step is to define the **main statistical inquiries** we aim to address. In this stage, we will clarify the focus of our analysis: **What factors influence a student’s _Net Money_ at the end of the month?** To answer this, we need to determine whether we’re aiming to understand relationships between variables or make accurate predictions. This distinction between **inferential** and **predictive** analysis shapes the methods we will use, including the application of Ordinary Least Squares (OLS).

### Inferential vs. Predictive Analysis

As a data scientist, your first task is to decide whether the analysis is inferential or predictive.

- **Inferential Analysis** aims to explore and quantify relationships between explanatory variables (e.g., student characteristics) and the response variable (_Net Money_). For instance, we might ask: **Does having a part-time job significantly affect a student’s _net money_, and by how much?** The goal is to measure and understand these effects, assessing their statistical significance.
- **Predictive Analysis**, on the other hand, focuses on making accurate predictions about the response variable based on new data. Here, the question could be: **Can we predict a student’s _net money_ based on their _monthly earnings_, _living situation_, and _spending habits_?** The emphasis is on creating a model that delivers precise forecasts, even if we aren’t focused on explaining the underlying relationships.

### Applying Study Design to Our Case Study

For our case study, we are primarily interested in understanding the relationships between variables like _Has_Job_, _Monthly_Earnings_, and _Spending Habits_, and how they affect a student’s _Net Money_. This leads us toward an inferential approach. We aim to answer questions such as:

- **Does having a part-time job lead to significantly higher _net money_?**
- **How much does a student’s _monthly earnings_ affect their financial situation?**
- **Do _spending habits_, like going out frequently, decrease a student’s _net money_?**

Using OLS, we will estimate the impact of each factor and assess whether these effects are statistically significant. This inferential analysis will help us understand which variables have the greatest influence on students’ financial outcomes.

Alternatively, if our goal were to build a tool that predicts future _Net Money_ for new students based on their characteristics, we would adopt a predictive approach. OLS would be used to minimize prediction errors and make reliable forecasts. Although our focus here is inferential, it’s important to recognize that OLS is versatile and can be applied in both contexts.

## Data Collection and Wrangling

With the statistical questions clearly defined, the next step is to ensure that the data is appropriately prepared for analysis. Although we already have the dataset, it is valuable to consider how this data could have been collected to better understand its context and potential limitations.

### Data Collection

For a study like ours, data on students’ financial behaviors could have been collected through various methods:

- **Surveys**: Students might have been asked about their employment status, earnings, and spending habits through structured questionnaires. While surveys can capture self-reported financial behaviors, they may suffer from recall bias or social desirability bias.
- **Administrative Data**: Universities or employers may maintain records on student income and employment, providing a more objective source of financial information. However, access to such data may be limited due to privacy regulations.
- **Financial Tracking Apps**: Digital financial management tools can offer detailed, real-time data on student income and spending patterns. While these apps provide high granularity, they may introduce selection bias, as only students who use such apps would be represented in the dataset.

Regardless of the data collection method, each approach presents challenges, such as missing data, reporting errors, or sample biases. Addressing these issues is a critical aspect of data wrangling.

### Data Wrangling

With our dataset in hand, the primary focus now shifts to cleaning and organizing the data for analysis using Ordinary Least Squares (OLS).

#### Handling Missing Data

Ensuring **data integrity** is the first step. This involves identifying missing values and deciding how to handle them. If certain students lack recorded earnings or net money values, we must determine the best approach to address these gaps. One option is to remove incomplete records, but if missingness is systematic, imputing missing values using logical estimates or averages may be more appropriate. Fortunately, in our toy dataset, there are no missing values.

```{r check_missing_value}
colSums(is.na(data))
```

#### Encoding Categorical Variables

Categorical variables need to be transformed into numerical representations for regression analysis. Binary variables, such as Has_Job and Drinks_Alcohol, are converted into factors to ensure correct interpretation in the model.

```{r convert_binary_to_factor}
# Convert binary categorical variables to factors
data <- data |>
  mutate(Has_Job = as.factor(Has_Job),
         Drinks_Alcohol = as.factor(Drinks_Alcohol),
         Financially_Dependent = as.factor(Financially_Dependent),
         Cooks_at_Home = as.factor(Cooks_at_Home))
```

#### Detecting and Handling Outliers

Extreme values in continuous variables, such as Monthly_Earnings and Net_Money, can distort the regression model. We use the Interquartile Range (IQR) method to identify and handle outliers. Observations that fall outside 1.5 times the IQR below the first quartile (Q1) or above the third quartile (Q3) are treated as missing values and subsequently removed.

```{r outliers}
# Using IQR method to filter out extreme values in continuous variables
remove_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  x[x < (Q1 - 1.5 * IQR) | x > (Q3 + 1.5 * IQR)] <- NA
  return(x)
}

data <- data |>
  mutate(across(c(Monthly_Earnings, Net_Money), remove_outliers))

# Remove rows with newly introduced NAs due to outlier handling
data <- na.omit(data)
```

#### Splitting the Data for Model Training

Once the data is cleaned and transformed, the final preparation step is splitting the dataset into training and testing subsets. This ensures that our model generalizes beyond the current dataset and performs well on unseen data. The training set is used to estimate model parameters, while the test set evaluates model performance.

```{r train_test_split}
# Splitting the dataset into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(seq_len(nrow(data)), size = 0.8 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
```

By following these data preparation steps, we ensure that our dataset is well-structured, free of inconsistencies, and suitable for regression analysis using OLS.

## Exploratory Data Analysis (EDA)

Before diving into data modeling, it is crucial to develop a deep understanding of the relationships between variables in the dataset. This stage, known as Exploratory Data Analysis (EDA), helps us visualize and summarize the data, uncover patterns, detect anomalies, and test key assumptions that will inform our modeling decisions.

### Classifying Variables

The first step in EDA is to classify variables according to their types. This classification guides the selection of appropriate visualization techniques and modeling strategies. In our toy dataset, we categorize variables as follows:

- `Net_Money` serves as the response variable, representing a continuous outcome constrained by realistic income and expenses.

The regressors include a mix of binary, categorical, ordinal, and continuous variables.

- Binary variables, such as Has_Job and Drinks_Alcohol, take on only two values and need to be encoded for modeling.
- Categorical variables, like Living_Situation and Housing_Type, represent qualitative distinctions between different student groups.
- Some predictors, like Year_of_Study and Goes_Out_Spends_Money, follow an ordinal structure, meaning they have a meaningful ranking but no consistent numerical spacing.
- Finally, Monthly_Allowance and Monthly_Earnings are continuous variables, requiring attention to their distributions and potential outliers.

By classifying variables correctly at the outset, we ensure that they are analyzed and interpreted appropriately throughout the modeling process.

### Visualizing Variable Distributions

Once variables are classified, the next step is to explore their distributions. Understanding how variables are distributed is crucial for identifying potential issues such as skewness, outliers, or missing values. We employ different visualizations depending on the variable type:

#### Continuous Variables

We begin by examining continuous variables, where histograms reveal the overall shape of the data and boxplots help detect extreme values.

A histogram of Net_Money allows us to check for normality and skewness. If the distribution is heavily skewed, transformations such as logarithms may be needed to improve model performance.

```{r eda-histogram}
# Histogram of Net_Money
hist(train_data$Net_Money, 
     main = "Distribution of Net Money", 
     xlab = "Net Money", 
     col = "blue", 
     border = "white")
```

Boxplots highlight the presence of outliers, which could represent extreme financial behaviors among students. If outliers are significantly distorting the analysis, we may consider trimming or transforming the data.

```{r eda-boxplot}
# Boxplot of Net Money
boxplot(train_data$Net_Money, 
        main = "Boxplot of Net Money", 
        ylab = "Net Money", 
        col = "lightblue")
```

#### Categorical and Ordinal Variables

Categorical variables require a different approach. Bar charts display the frequency of each category, providing insight into the distribution of qualitative attributes. If certain categories are underrepresented, we might need to group them or apply techniques such as one-hot encoding to ensure they contribute effectively to the model.

```{r eda-barcharts}
# Bar plot of Living Situation
barplot(table(train_data$Living_Situation), 
        main = "Living Situation Distribution", 
        xlab = "Living Situation", 
        ylab = "Frequency", 
        col = "purple")
```

#### Exploring Relationships Between Variables

Beyond examining individual variables, it is crucial to explore relationships between predictors and the response variable. Correlation matrices help quantify the strength of associations between continuous variables.

If two predictors are highly correlated, they may introduce multicollinearity, which can distort regression estimates. In such cases, we might consider removing one of the variables or using dimensionality reduction techniques.

```{r eda-corr-matrix}
# Correlation matrix
cor_matrix <- cor(train_data[, c("Net_Money", "Monthly_Allowance", "Monthly_Earnings")], use = "complete.obs")
print(cor_matrix)
```

Scatter plots further illustrate how continuous variables interact. Plotting Monthly Allowance against Net Money, for example, allows us to visually assess whether students with higher allowances tend to save more or spend beyond their means. If a clear linear trend exists, this variable is likely a strong predictor in our OLS model. If the relationship appears nonlinear, we might need to explore polynomial transformations or interaction terms.

```{r eda-scatterplot}
# Scatter plot of Monthly Allowance vs. Net Money
plot(train_data$Monthly_Allowance, train_data$Net_Money, 
     main = "Net Money vs. Monthly Allowance", 
     xlab = "Monthly Allowance", 
     ylab = "Net Money", 
     col = "blue", 
     pch = 19)
abline(lm(Net_Money ~ Monthly_Allowance, data = train_data), col = "red", lwd = 2)
```

For categorical variables, boxplots can compare how different groups influence the response variable. Examining how Net Money varies by Living Situation can reveal financial disparities between students who live at home versus those who rent independently. A significant difference between categories may indicate that Living Situation should be included in the model as an important predictor.

```{r eda-multiple-boxplots}
# Boxplot of Net Money by Living Situation
boxplot(Net_Money ~ Living_Situation, 
        data = train_data, 
        main = "Net Money by Living Situation", 
        xlab = "Living Situation", 
        ylab = "Net Money", 
        col = "lightgreen")
```


#### Summary Statistics

In addition to visual exploration, descriptive statistics provide a numerical summary of the dataset. Key metrics such as the mean, median, and standard deviation help quantify central tendencies and variability.

Large discrepancies between the mean and median suggest skewed distributions, while high standard deviations indicate substantial variability. These insights guide decisions on whether data transformations or scaling are necessary.

```{r eda-summary-stats}
# Summary statistics for numerical variables
summary(train_data[, c("Net_Money", "Monthly_Allowance", "Monthly_Earnings")])
```

### Key Takeaways from EDA

By conducting a thorough Exploratory Data Analysis, we gain critical insights into the dataset before proceeding to regression modeling. We have identified key predictor variables, assessed their distributions, explored relationships, and flagged potential issues such as skewness, multicollinearity, and outliers. With this foundation, we are now prepared to move forward with Ordinary Least Squares (OLS) regression, ensuring that our model is built on well-understood and properly prepared data.

## Data Modelling

After conducting Exploratory Data Analysis (EDA), we transition to the modeling stage, where we apply a structured approach to uncover relationships between variables and predict outcomes. In this section, we focus on Ordinary Least Squares (OLS) regression, a widely used statistical technique for modeling linear relationships.

OLS aims to estimate the effect of multiple predictors on an outcome variable by minimizing the sum of squared differences between observed and predicted values. This approach helps quantify financial behaviors, allowing us to interpret the impact of various factors on students’ net financial balance.

### Choosing a Suitable Regression Model

The choice of regression model depends on the patterns identified in EDA and the objectives of our analysis. Regression techniques vary in complexity, with some handling simple linear relationships and others accounting for more nuanced effects. Below are common approaches:

- **Simple Linear Regression** models the relationship between a single predictor and the response variable. This approach is suitable when we suspect a dominant factor driving financial balance.
- **Multiple Linear Regression** extends simple regression by incorporating multiple predictors, allowing us to account for various financial influences simultaneously.
- **Polynomial Regression** captures non-linear relationships by introducing polynomial terms of predictors, useful when relationships observed in scatter plots are curved rather than strictly linear.
- **Log-Linear Models** transform skewed distributions to improve interpretability and meet regression assumptions.
- **Regularized Regression (Ridge and Lasso)** applies penalties to regression coefficients to handle multicollinearity and enhance model generalization by reducing overfitting.

Given that our goal is to examine how multiple factors—such as income, expenses, and living arrangements—affect students’ financial balance, we select Multiple Linear Regression via OLS. This method allows us to quantify the influence of each predictor while controlling for confounding effects.

### Defining Modeling Parameters

Once we select OLS regression, we define the key modeling components: the response variable (dependent variable) and the predictor variables (independent variables).

#### Response Variable (Y):

The response variable, also known as the dependent variable, represents the financial outcome we aim to explain:

- `Net_Money`: The dependent variable representing financial balance.

#### Predictor Variables (X):

Each predictor variable is chosen based on its theoretical and statistical relevance in explaining financial behavior:

- `Has_Job` (Binary) – Indicates whether the student has a job (1 = Yes, 0 = No).
- `Financially_Dependent` (Binary) – Identifies students who rely on external financial support.
- `Year_of_Study` (Ordinal) – Represents academic seniority (higher values indicate later years).
- `Goes_Out_Spends_Money` (Ordinal) – Measures spending behavior on a scale from 1 to 6.
- `Drinks_Alcohol` (Binary) – Identifies whether a student consumes alcohol, which may impact discretionary spending.
- `Monthly_Allowance` (Continuous) – Represents financial support received from family or scholarships.
- `Monthly_Earnings` (Continuous) – Reflects the student’s personal income from work.
- `Living_Situation` (Categorical) – Encodes different living arrangements (e.g., dormitory, shared apartment, living with family).
- `Housing_Type` (Categorical) – Further distinguishes between different types of housing situations.
- `Cooks_at_Home` (Binary) – Indicates whether the student regularly prepares meals at home.

These predictors capture a mix of economic, behavioral, and lifestyle factors, providing a comprehensive view of the drivers of student financial balance.

### Setting Up the Modeling Equation

With all predictors defined, the OLS regression equation models the relationship between Net_Money and the predictor variables:

$$
\begin{align}
\text{Net_Money} = \beta_0 \\
               & + \beta_1 \times \text{Has_Job} \\
               & + \beta_2 \times \text{Financially_Dependent} \\
               & + \beta_3 \times \text{Year_of_Study} \\
               & + \beta_4 \times \text{Goes_Out_Spends_Money} \\
               & + \beta_5 \times \text{Drinks_Alcohol} \\
               & + \beta_6 \times \text{Monthly_Allowance} \\
               & + \beta_7 \times \text{Monthly_Earnings} \\
               & + \beta_8 \times \text{Living_Situation} \\
               & + \beta_9 \times \text{Housing_Type} \\
               & + \beta_{10} \times \text{Cooks_at_Home} \\
               & + \epsilon
\end{align}
$$

where:

- $\beta_0$ represents the intercept, or the baseline Net Money when all predictors are set to zero.
- $\beta_1, \beta_2, ..., \beta_{10}$ are the regression coefficients, quantifying the impact of each predictor on financial balance.
- $\epsilon$ is the error term, accounting for unexplained variability and random noise.

Each coefficient provides insight into how `Net_Money` changes when a specific predictor increases by one unit, holding all other factors constant. For example:

- $\beta_5$ (Drinks Alcohol) measures the financial impact of alcohol consumption, which may reflect higher discretionary spending.
- $\beta_6$ (Monthly Allowance) quantifies the increase in Net_Money per additional dollar of allowance.
- $\beta_10$ (Cooks at Home) indicates how much more (or less) financially stable students are when they cook at home instead of eating out.

If significant interaction effects exist—such as students who live independently having a different financial impact from increased earnings compared to those living with family—we can extend the model by adding interaction terms.

## Estimation

With the data modeling stage completed, we now move to estimation, where we fit the Ordinary Least Squares (OLS) regression model to the data and obtain numerical estimates for the regression coefficients. These estimates quantify how much each predictor contributes to the response variable, allowing us to measure their individual effects on Net Money.

The goal of estimation is to determine the best-fitting regression line by minimizing the sum of squared residuals—the differences between the observed and predicted values. This step provides a mathematical basis for analyzing financial behaviors in students.

### Fitting the Model

To estimate the regression coefficients, we fit the OLS model to the training data using Python (`statsmodels`) or R (`lm`). The model is trained using least squares estimation, which finds the coefficients that minimize the total squared error between observed values and predictions.

In R, we can fit the regression model using the `lm()` function:

```{r fitting-ols}
# Load necessary library
library(stats)

# Fit the OLS model
ols_model <- lm(Net_Money ~ Has_Job + Financially_Dependent + Year_of_Study + Goes_Out_Spends_Money + Drinks_Alcohol + Monthly_Allowance + Monthly_Earnings + Living_Situation + Housing_Type + Cooks_at_Home, data = train_data)

# Display summary of model results
summary(ols_model)
```

### Interpreting the Coefficients

After fitting the model, we examine the estimated coefficients to understand their impact. Each coefficient obtained from the OLS regression represents the expected change in Net_Money for a one-unit increase in the corresponding predictor, holding all other variables constant. The estimated regression equation can be expressed as:

$$
\begin{align}
\text{Net_Money} = -595.37 \\
               & + 73.93 \times \text{Has_Job} \\
               & - 499.36 \times \text{Financially_Dependent} \\
               & - 96.52 \times \text{Year_of_Study} \\
               & - 54.60 \times \text{Goes_Out_Spends_Money} \\
               & - 145.11 \times \text{Drinks_Alcohol} \\
               & + 1.47 \times \text{Monthly_Allowance} \\
               & + 0.95 \times \text{Monthly_Earnings} \\
               & + 102.95 \times \text{Living_Situation} \\
               & + 56.65 \times \text{Housing_Type} \\
               & - 96.31 \times \text{Cooks_at_Home} \\
               & + \epsilon
\end{align}
$$

For example:

- The intercept ($\beta_0=-595.37$) represents the expected financial balance for a student who has zero income, allowance, and falls at the baseline category for all categorical variables.
- A $1 increase in Monthly Allowance ($\beta_6=1.47$) is associated with a $1.47 increase in Net Money, meaning students with higher allowances tend to have a higher financial balance.

These estimates provide an initial understanding of the direction and magnitude of relationships between predictors and financial balance. However, before drawing conclusions, we need to validate model assumptions and evaluate the statistical significance of each coefficient.

## Goodness of Fit

After estimating the regression coefficients, the next step is to assess how well the model fits the data and whether it satisfies the assumptions of Ordinary Least Squares (OLS) regression. This evaluation ensures that the model is not only statistically valid but also generalizes well to unseen data. A well-fitting model should explain a substantial proportion of variation in the response variable while adhering to key statistical assumptions. If these assumptions are violated, model estimates may be biased, leading to misleading conclusions.

### Checking Model Assumptions

OLS regression is built on several fundamental assumptions:

- linearity
- independence of errors
- homoscedasticity
- normality of residuals

If these assumptions hold, OLS provides unbiased, efficient, and consistent estimates. We assess each assumption through diagnostic plots and statistical tests.

#### Linearity

A core assumption of OLS is that the relationship between each predictor and the response variable is linear. If this assumption is violated, the model may systematically under- or overestimate Net_Money, leading to biased predictions. The Residuals vs. Fitted values plot is a common diagnostic tool for checking linearity. In a well-specified linear model, residuals should be randomly scattered around zero, without any discernible patterns. If the residuals exhibit a U-shaped or curved pattern, this suggests a non-linear relationship, indicating that transformations such as logarithmic, square root, or polynomial terms may be necessary.

To visualize linearity, we plot the residuals against the fitted values:

```{r model-assumption-linearity}
# Residuals vs Fitted plot (R)
plot(ols_model$fitted.values, residuals(ols_model), 
     main = "Residuals vs Fitted", xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")
```
If the residual plot displays a clear trend, polynomial regression or feature engineering may be required to better capture the underlying data structure.

#### Independence of Errors

The residuals, or errors, in an OLS model should be independent of one another. This assumption is particularly relevant in time-series or sequential data, where errors from one observation might influence subsequent observations, leading to autocorrelation. If the errors are correlated, the estimated standard errors will be biased, making hypothesis testing unreliable.

The Durbin-Watson test is commonly used to detect autocorrelation. This test produces a statistic that ranges between 0 and 4, where values close to 2 indicate no significant autocorrelation, while values near 0 or 4 suggest positive or negative correlation in the residuals.

```{r model-assumption-indep-of-error}
dwtest(ols_model)
```

If the test suggests autocorrelation, a possible solution is to use time-series regression models such as Autoregressive Integrated Moving Average (ARIMA) or introduce lagged predictors to account for dependencies in the data.

#### Homoscedasticity (Constant Variance of Errors)

OLS regression assumes that the variance of residuals remains constant across all fitted values. If this assumption is violated, the model exhibits heteroscedasticity, where the spread of residuals increases or decreases systematically. This can result in inefficient coefficient estimates, making some predictors appear statistically significant when they are not.

To check for heteroscedasticity, we plot residuals against the fitted values and conduct a Breusch-Pagan test, which formally tests whether residual variance is constant.

```{r model-assumption-homoscedasticity}
ncvTest(ols_model)  # Test for homoscedasticity
```

If heteroscedasticity is detected, solutions include applying weighted least squares (WLS) regression, transforming the dependent variable (e.g., using a log transformation), or computing robust standard errors to correct for variance instability.

#### Normality of Residuals

For valid hypothesis testing and confidence interval estimation, OLS assumes that residuals follow a normal distribution. If residuals deviate significantly from normality, statistical inference may be unreliable, particularly for small sample sizes.

A Q-Q plot (Quantile-Quantile plot) is used to assess normality. If residuals are normally distributed, the points should lie along the reference line.

```{r model-assumption-normality-residuals}
qqnorm(residuals(ols_model))
qqline(residuals(ols_model), col = "red")
```

If the plot reveals heavy tails or skewness, potential solutions include applying log or Box-Cox transformations to normalize the distribution. In cases where normality is severely violated, using a non-parametric model or bootstrapping confidence intervals may be appropriate.

### Evaluating Model Fit

A good model should explain a large proportion of variance in the response variable.

#### R-Squared

Beyond checking assumptions, it is essential to assess how well the model explains variability in the response variable. One of the most commonly used metrics is R-Squared ($R^2$), which measures the proportion of variance in Net_Money that is explained by the predictors. An $R^2$ value close to 1 indicates a strong model fit, whereas a low value suggests that important predictors may be missing or that the model is poorly specified.

We can retrieve the R-squared and Adjusted R-squared values from the model summary:

```{r r-squared}
summary(ols_model)$r.squared  # R-squared value
summary(ols_model)$adj.r.squared  # Adjusted R-squared
```

While $R^2$ provides insight into model fit, it has limitations. Adding more predictors will always increase $R^2$, even if those predictors have little explanatory power. Adjusted R-squared accounts for this by penalizing unnecessary variables, making it a better indicator of true model performance.

A high $R^2$ does not imply causation, nor does it confirm that the model is free from omitted variable bias or multicollinearity. Therefore, a strong goodness-of-fit measure should be complemented by careful assessment of residual behavior and coefficient significance.

#### Identifying Outliers and Influential Points

Outliers and influential observations can distort regression estimates, making it crucial to detect and address them appropriately. One way to identify extreme residuals is through residual plots, where large deviations from zero may indicate problematic data points.

```{r residual-plots}
plot(residuals(ols_model), main = "Residual Plot", ylab = "Residuals")
abline(h = 0, col = "red")
```

Another important diagnostic tool is Cook’s Distance, which measures the influence of each observation on the regression results. Data points with Cook’s Distance values greater than 0.5 may significantly impact model estimates.

```{r cooks-distance}
cook_values <- cooks.distance(ols_model)
plot(cook_values, type = "h", main = "Cook's Distance")
```

If influential points are identified, the next steps involve investigating data quality, testing robust regression techniques, or applying Winsorization, which involves replacing extreme values with more moderate ones to reduce their impact.

## Results

After validating the goodness of fit, we now assess how well the model performs in both predictive analysis and inferential analysis. This step involves using the trained model to generate predictions on unseen data and evaluating how well it generalizes beyond the training set. Additionally, we analyze the estimated regression coefficients to draw meaningful conclusions about student financial behaviors.

### Predictive Analysis

A key objective of regression modeling is to generate reliable predictions. To assess how well our model generalizes, we apply it to the test dataset—a portion of the original data that was not used for model training. If the model’s predictions align closely with actual outcomes, we can conclude that it has strong predictive power.

In R, we use the predict() function to apply the trained OLS model to the test dataset:

```{r predict-on-test}
# Generate predictions on the test set
y_pred <- predict(ols_model, newdata=test_data)
```

Once predictions are generated, we evaluate their accuracy using common regression error metrics.

#### Performance Metrics

Model accuracy is assessed using four standard error metrics:

1. **Mean Absolute Error (MAE)** measures the average absolute differences between predicted and actual values. A lower MAE indicates better model accuracy.
2. **Mean Squared Error (MSE)** calculates the average squared differences between predicted and actual values, penalizing larger errors more heavily.
3. **Root Mean Squared Error (RMSE)** is the square root of MSE, making it easier to interpret since it retains the same units as the dependent variable (Net_Money).
4. **R-squared ($R^2$)** quantifies the proportion of variance in Net_Money explained by the model. A higher $R^2$ value indicates better model performance.

These metrics can be computed as follows:
```{r prediction-results}
# Extract response variable from test data
y_test <- test_data$Net_Money

# Calculate metrics in R
mae <- mean(abs(y_test - y_pred))
mse <- mean((y_test - y_pred)^2)
rmse <- sqrt(mse)
r2 <- summary(ols_model)$r.squared

cat(sprintf("MAE: %.2f, MSE: %.2f, RMSE: %.2f, R-squared: %.2f", mae, mse, rmse, r2))
```

If the RMSE is significantly larger than MAE, it suggests that the model is highly sensitive to large prediction errors, meaning that certain extreme values are having a disproportionate impact on the model’s performance. If the R-squared value is low, it may indicate that important predictors are missing from the model or that the relationship between predictors and Net_Money is more complex than a linear relationship can capture.

### Inferential Analysis

Beyond prediction, OLS regression allows us to interpret the estimated coefficients to uncover patterns in students’ financial behaviors. Each coefficient represents the expected change in Net_Money for a one-unit increase in the corresponding predictor, assuming all other variables remain constant.

#### Insights from Regression Coefficients

Here are a few insights that we can extract from the regression model result:

- The intercept (-595.37) represents the estimated financial balance for a baseline student (someone with Has_Job = 0, Financially_Dependent = 0, Year_of_Study = 0, etc.). Since a zero value for Year_of_Study is not meaningful in our context, the intercept itself has limited interpretability but serves as the starting point for estimating Net_Money based on predictor values.
- Financial Dependency (Financially_Dependent) has a strong negative effect (-499.36, p < 2e-16), meaning that students who rely financially on others (e.g., parents, guardians) tend to have significantly lower Net_Money. This could be due to a lack of independent income sources or higher expenses associated with dependence on external financial support.
- Social Spending Habits (Goes_Out_Spends_Money) also have a negative impact (-54.60, p < 2e-16), meaning that students who frequently go out and spend money experience a decline in Net_Money. This result aligns with expectations, as higher discretionary spending directly reduces available financial balance.
- Cooking at Home (Cooks_at_Home) has a negative effect (-96.31, p = 1.83e-09), which might seem counterintuitive at first. One possible explanation is that students who cook at home may already have lower budgets, leading them to adopt cost-saving habits, rather than the habit itself directly causing financial decline.
- Overall, the model explains 90.18% of the variance in Net_Money ($R^2$ = 0.9018, Adjusted $R^2$ = 0.9006), suggesting a strong predictive capability. The F-statistic (722.1, p < 2.2e-16) confirms that the model as a whole is statistically significant.

These findings provide a comprehensive view of student financial behaviors, reinforcing expected relationships (such as the role of allowances and earnings) while also highlighting unexpected trends (such as the negative effect of cooking at home). The next step is to integrate these results into a cohesive narrative, translating statistical insights into actionable recommendations.

## Storytelling

The final step in our data science workflow is storytelling, where we translate our analytical findings into actionable insights. This stage ensures that our results are clearly understood by both technical and non-technical audiences. Effective storytelling involves summarizing insights, using visuals for clarity, and making data-driven recommendations.





