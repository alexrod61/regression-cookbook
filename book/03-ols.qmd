<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7PRVEBE1EF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7PRVEBE1EF');
</script>

# Ordinary Least-squares {#sec-ols}

```{r}
#| include: false

colourize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

```{r setup, include=FALSE}
# Load required libraries
library(devtools)
library(readr)
library(ggplot2)
library(ggcorrplot)
library(cookbook)
library(tidyverse)
library(car)
library(lmtest)

data_path <- system.file("data", "ols_data.rda", package = "cookbook")

if (file.exists(data_path)) {
  load(data_path)  # Loads the dataset
} else {
  stop("Error: 'ols_data.rda' not found in the cookbook package.")
}

loaded_objects <- ls()
data <- OLS
```

## Learning Objectives

By the end of this chapter, you will be able to:
- Explain how Ordinary Least Squares (OLS) estimates relationships by minimizing the sum of squared residuals.
- Determine when OLS is an appropriate modeling choice.
- Fit an OLS model using Python or R.
- Interpret regression coefficients to assess the impact of predictors on the response variable.
- Check key OLS assumptions, including linearity, independence, homoscedasticity, and normality of residuals.
- Evaluate model performance using R-squared and error metrics such as MAE, MSE, and RMSE.

## Introduction

**Ordinary Least Squares (OLS)** is a fundamental method in regression analysis for estimating the relationship between a dependent variable and one or more independent variables. OLS finds the best-fitting line (or hyperplane for multiple predictors) by minimizing the sum of squared differences between observed and predicted values, ensuring the smallest overall error.

OLS is widely used due to its simplicity, interpretability, and broad applicability. As one of the earliest and most fundamental techniques in statistics and machine learning, it provides a structured way to understand variable relationships. Fields such as economics, biology, and social sciences rely on OLS to quantify associations and make informed decisions.

Beyond ease of use, OLS offers clear parameter estimates—coefficients that describe the strength and direction of relationships between variables. This makes it valuable for applications ranging from financial forecasting and medical research to consumer behavior analysis.

In this chapter, we will explore how OLS works, its assumptions, and its real-world applications. We will also examine its limitations, providing a solid foundation in regression analysis as a stepping stone to more advanced techniques.

## The "Best Line"

When fitting a regression line using Ordinary Least Squares (OLS), the goal is to find the line that best captures the relationship between the dependent variable $Y$ and the independent variable $X$. But how do we define "best"?

Imagine we have a scatter plot of data points and we draw two different lines:

- **Line A**: A reasonably well-fitted line that follows the general trend of the data.
- **Line B**: A slightly worse line that doesn’t fit the data as closely.

```{r two_lines, echo=FALSE}
# Sample data
set.seed(42)
X <- c(1000, 1200, 1500, 1800, 2000)
Y <- c(200, 230, 250, 290, 310)

# Create a data frame
df <- data.frame(Size = X, Price = Y)

# Fit the correct OLS model
correct_model <- lm(Price ~ Size, data = df)

# Create predictions for the two lines
df$Predicted_Correct <- predict(correct_model, newdata = df)
df$Predicted_Wrong <- 110 + 0.08 * df$Size  # Adjusted manually

# Reshape data for ggplot (to add legend)
df_long <- data.frame(
  Size = rep(df$Size, 2),
  Price = c(df$Predicted_Correct, df$Predicted_Wrong),
  Line = rep(c("Line A (Best Fit)", "Line B (Worse Fit)"), each = nrow(df))
)

# Store the plot with a legend
plot <- ggplot() +
  geom_point(data = df, aes(x = Size, y = Price), size = 3, color = "black") +  # Scatter plot
  geom_line(data = df_long, aes(x = Size, y = Price, color = Line), linewidth = 1.2) +
  scale_color_manual(values = c("Line A (Best Fit)" = "blue", "Line B (Worse Fit)" = "red")) +
  labs(title = "Comparing Regression Line Fits",
       x = "House Size (sq ft)",
       y = "House Price (in $1000s)",
       color = "Regression Line") +  # Legend title
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5, color = "gray40"),
        legend.position = "bottom")  # Move legend to bottom

plot
```

Now, let’s compare them:

- For each data point, the residual is the vertical distance between the observed Y value ($Y$) and the predicted Y value ($\hat{Y}$) from the line.
- If a line fits well, these residuals will be smaller on average, meaning the predictions are closer to reality.
- OLS chooses the best line by minimizing the sum of squared residuals—this ensures that the chosen line has the smallest total error.

### Why Squared Errors?

Squaring the residuals achieves two things:

- It avoids cancelation. Some residuals are positive (the line underestimates the true value), and some are negative (the line overestimates). Squaring makes sure all errors are treated equally.
- It penalizes large errors more. A bad prediction (a large residual) contributes significantly more to the total error than a small one, which helps OLS focus on overall accuracy.

### The Mathematical Formulation of the OLS Model

Now that we understand how OLS determines the best-fitting line by minimizing residuals, let's express this formally with a mathematical equation.

For a simple linear regression model with one predictor, the equation of the regression line is:

$$
Y=\beta_0+\beta_1X+\epsilon
$$

where:

- $Y$ is the dependent variable (outcome of interest).
- $X$ is the independent variable (predictor).
- $\beta_0$ (intercept) represents the predicted value of $Y$ when $X=0$.
- $\beta_1(slope) quantifies the change in $Y$ for a one-unit increase in $X$.
- $\epsilon$ (error term) accounts for random variability not explained by the model.

#### Example: Predicting House Prices

To better understand these notations, let's take a look an example. Say we want to predict the price of a house ($Y$ in thousands of dollars) based on its size ($X$ in square feet). Suppose we have the following data:

| House Size ($X$ in sq ft) | Price ($Y$ in \$1000s) |
|----------------|---------------|
| 1000          | 200           |
| 1200          | 230           |
| 1500          | 250           |
| 1800          | 290           |
| 2000          | 310           |

Our regression equation would look like:

$$
\text{Price} = \beta_0 + \beta_1 (\text{Size}) + \epsilon
$$

where:  

- $\beta_0$ (intercept) represents the estimated price of a house when the size is zero (which may not be meaningful but is mathematically required).  
- $\beta_1$ (slope) represents the expected increase in price per additional square foot.  

For instance, if the regression model estimates that each additional square foot adds **$107** to the house price, and the intercept represents a base price (even at 0 sq ft), then we can interpret the model coefficients meaningfully.

## Case Study: Understanding Financial Behaviors

To demonstrate Ordinary Least Squares (OLS) in action, we will walk through a case study using a toy dataset. This case study will help us understand the financial behaviors of students and identify the factors that influence their `Net_Money`, the amount of money left over at the end of each month. We will approach this case study using the data science workflow described in a previous chapter, ensuring a structured approach to problem-solving and model building.

### The Dataset

Our dataset captures various aspects of students' financial lives. Each row represents a student, and the columns describe different characteristics. Below is a breakdown of the variables:

| **Variable Name**         | **Description**                                                       |
|---------------------------|-----------------------------------------------------------------------|
| **Has_Job**               | Whether the student has a job (0 = No, 1 = Yes).                      |
| **Year_of_Study**         | The student’s current year of study (e.g., 1st year, 2nd year, etc.). |
| **Financially_Dependent** | Whether the student is financially dependent on someone else (0 = No, 1 = Yes). |
| **Monthly_Allowance**     | The amount of financial support the student receives each month.       |
| **Cooks_at_Home**         | Whether the student prepares their own meals (0 = No, 1 = Yes).        |
| **Living_Situation**      | The student's living arrangement (e.g., living with family, in a shared apartment, etc.). |
| **Housing_Type**          | The type of housing the student lives in (e.g., rented, owned, dormitory). |
| **Goes_Out_Spends_Money** | How frequently the student goes out and spends money (1 = rarely, 5 = very often). |
| **Drinks_Alcohol**        | Whether the student drinks alcohol (0 = No, 1 = Yes).                 |
| **Net_Money**             | The amount of money the student has left at the end of the month after income and expenses. |
| **Monthly_Earnings**      | The student’s earnings from any part-time jobs or other income sources. |

Here’s a sample of the dataset:

<div style="overflow-x: auto; white-space: nowrap;">
| **Has_Job** | **Year_of_Study** | **Financially_Dependent** | **Monthly_Allowance** | **Cooks_at_Home** | **Living_Situation** | **Housing_Type** | **Goes_Out_Spends_Money** | **Drinks_Alcohol** | **Net_Money** | **Monthly_Earnings** |
|-------------|--------------------|---------------------------|-----------------------|-------------------|----------------------|------------------|---------------------------|--------------------|---------------|----------------------|
| 0           | 1                  | 0                         | 658.99                | 0                 | 3                    | 1                | 6                           | 0                  | 529.34        | 0.00                 |
| 1           | 3                  | 0                         | 592.55                | 0                 | 3                    | 2                | 3                           | 1                  | 992.72        | 941.92               |
| 1           | 4                  | 1                         | 602.54                | 0                 | 2                    | 2                | 2                           | 1                  | 557.30        | 876.57               |
</div>

This dataset provides a structured way to analyze the financial habits of students and determine which factors contribute most to their financial stability. In the next section, we will apply OLS to quantify these relationships within the data science workflow framework.

### The Problem We’re Trying to Solve

Our goal in this case study is to understand which factors impact a student’s net money. Specifically, we aim to identify which characteristics—such as having a job, monthly earnings, or financial support—explain why some students have more money left over at the end of the month than others.

The key question we want to answer is:

> **Which factors have the biggest influence on a student’s net money?**

By applying OLS to this dataset, we can quantify how each factor contributes to variations in net money and determine the strength of these relationships.

### Why This Matters

This case study provides a practical demonstration of how regression analysis can be used to study financial behavior. In real-world scenarios, understanding these relationships can help individuals and institutions make informed decisions—for example, determining how much financial aid to provide or identifying which types of part-time jobs are most beneficial for students.

By the end of this analysis, we will:

- Identify which factors (e.g., having a job, monthly earnings) most strongly influence a student’s net money.
- Estimate how much each factor contributes to changes in net money.
- Determine whether these relationships are statistically significant.

### Study Design

With our case study and dataset introduced, the next step is to define the **main statistical inquiries** we aim to address. In this stage, we will clarify the focus of our analysis: **What factors influence a student’s _Net Money_ at the end of the month?** To answer this, we need to determine whether we’re aiming to understand relationships between variables or make accurate predictions. This distinction between **inferential** and **predictive** analysis shapes the methods we will use, including the application of Ordinary Least Squares (OLS).

### Inferential vs. Predictive Analysis

As a data scientist, your first task is to decide whether the analysis is inferential or predictive.

- **Inferential Analysis** aims to explore and quantify relationships between explanatory variables (e.g., student characteristics) and the response variable (_Net Money_). For instance, we might ask: **Does having a part-time job significantly affect a student’s _net money_, and by how much?** The goal is to measure and understand these effects, assessing their statistical significance.
- **Predictive Analysis**, on the other hand, focuses on making accurate predictions about the response variable based on new data. Here, the question could be: **Can we predict a student’s _net money_ based on their _monthly earnings_, _living situation_, and _spending habits_?** The emphasis is on creating a model that delivers precise forecasts, even if we aren’t focused on explaining the underlying relationships.

### Applying Study Design to Our Case Study

For our case study, we are primarily interested in understanding the relationships between variables like _Has_Job_, _Monthly_Earnings_, and _Spending Habits_, and how they affect a student’s _Net Money_. This leads us toward an inferential approach. We aim to answer questions such as:

- **Does having a part-time job lead to significantly higher _net money_?**
- **How much does a student’s _monthly earnings_ affect their financial situation?**
- **Do _spending habits_, like going out frequently, decrease a student’s _net money_?**

Using OLS, we will estimate the impact of each factor and assess whether these effects are statistically significant. This inferential analysis will help us understand which variables have the greatest influence on students’ financial outcomes.

Alternatively, if our goal were to build a tool that predicts future _Net Money_ for new students based on their characteristics, we would adopt a predictive approach. OLS would be used to minimize prediction errors and make reliable forecasts. Although our focus here is inferential, it’s important to recognize that OLS is versatile and can be applied in both contexts.

## Data Collection and Wrangling

With the statistical questions clearly defined, the next step is to ensure that the data is appropriately prepared for analysis. Although we already have the dataset, it is valuable to consider how this data could have been collected to better understand its context and potential limitations.

### Data Collection

For a study like ours, data on students’ financial behaviors could have been collected through various methods:

- **Surveys**: Students might have been asked about their employment status, earnings, and spending habits through structured questionnaires. While surveys can capture self-reported financial behaviors, they may suffer from recall bias or social desirability bias.
- **Administrative Data**: Universities or employers may maintain records on student income and employment, providing a more objective source of financial information. However, access to such data may be limited due to privacy regulations.
- **Financial Tracking Apps**: Digital financial management tools can offer detailed, real-time data on student income and spending patterns. While these apps provide high granularity, they may introduce selection bias, as only students who use such apps would be represented in the dataset.

Regardless of the data collection method, each approach presents challenges, such as missing data, reporting errors, or sample biases. Addressing these issues is a critical aspect of data wrangling.

### Data Wrangling

With our dataset in hand, the primary focus now shifts to cleaning and organizing the data for analysis using Ordinary Least Squares (OLS).

#### Handling Missing Data

Ensuring **data integrity** is the first step. This involves identifying missing values and deciding how to handle them. If certain students lack recorded earnings or net money values, we must determine the best approach to address these gaps. One option is to remove incomplete records, but if missingness is systematic, imputing missing values using logical estimates or averages may be more appropriate. Fortunately, in our toy dataset, there are no missing values.

```{r check_missing_value}
colSums(is.na(data))
```

#### Encoding Categorical Variables

Categorical variables need to be transformed into numerical representations for regression analysis. Binary variables, such as Has_Job and Drinks_Alcohol, are converted into factors to ensure correct interpretation in the model.

```{r convert_binary_to_factor}
# Convert binary categorical variables to factors
data <- data |>
  mutate(Has_Job = as.factor(Has_Job),
         Drinks_Alcohol = as.factor(Drinks_Alcohol),
         Financially_Dependent = as.factor(Financially_Dependent),
         Cooks_at_Home = as.factor(Cooks_at_Home))
```

#### Detecting and Handling Outliers

Extreme values in continuous variables, such as Monthly_Earnings and Net_Money, can distort the regression model. We use the Interquartile Range (IQR) method to identify and handle outliers. Observations that fall outside 1.5 times the IQR below the first quartile (Q1) or above the third quartile (Q3) are treated as missing values and subsequently removed.

```{r outliers}
# Using IQR method to filter out extreme values in continuous variables
remove_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  x[x < (Q1 - 1.5 * IQR) | x > (Q3 + 1.5 * IQR)] <- NA
  return(x)
}

data <- data |>
  mutate(across(c(Monthly_Earnings, Net_Money), remove_outliers))

# Remove rows with newly introduced NAs due to outlier handling
data <- na.omit(data)
```

#### Splitting the Data for Model Training

Once the data is cleaned and transformed, the final preparation step is splitting the dataset into training and testing subsets. This ensures that our model generalizes beyond the current dataset and performs well on unseen data. The training set is used to estimate model parameters, while the test set evaluates model performance.

```{r train_test_split}
# Splitting the dataset into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(seq_len(nrow(data)), size = 0.8 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
```

By following these data preparation steps, we ensure that our dataset is well-structured, free of inconsistencies, and suitable for regression analysis using OLS.

## Exploratory Data Analysis (EDA)

Before diving into data modeling, it is crucial to develop a deep understanding of the relationships between variables in the dataset. This stage, known as Exploratory Data Analysis (EDA), helps us visualize and summarize the data, uncover patterns, detect anomalies, and test key assumptions that will inform our modeling decisions.

### Classifying Variables

The first step in EDA is to classify variables according to their types. This classification guides the selection of appropriate visualization techniques and modeling strategies. In our toy dataset, we categorize variables as follows:

Response Variable:

- `Net_Money`: Continuous (bounded, as it is limited by realistic income and expenses)

Regressors:

- **Binary**: `Has_Job`, `Financially_Dependent`, `Cooks_at_Home`, `Drinks_Alcohol`
- **Categorical**: `Living_Situation`, `Housing_Type`
- **Ordinal**: `Year_of_Study`, `Goes_Out_Spends_Money` (scale 1-5)
- **Continuous**: `Monthly_Allowance`, `Monthly_Earnings`

This classification ensures that we apply the correct statistical methods and visualizations in the next steps.

### Visualizing Variable Distributions

Once variables are classified, the next step is to explore their distributions to understand their characteristics. We employ different visualizations depending on the variable type:

#### Continuous Variables

- **Histograms**: Used to inspect the distribution of numerical variables.
- **Boxplots**: Useful for identifying outliers and comparing distributions.

```{r eda-histogram}
# Histogram of Net_Money
hist(train_data$Net_Money, 
     main = "Distribution of Net Money", 
     xlab = "Net Money", 
     col = "blue", 
     border = "white")
```

```{r eda-boxplot}
# Boxplot of Net Money
boxplot(train_data$Net_Money, 
        main = "Boxplot of Net Money", 
        ylab = "Net Money", 
        col = "lightblue")
```

#### Categorical and Ordinal Variables

- **Bar charts**: Display frequency counts for categorical variables.

```{r eda-barcharts}
# Bar plot of Living Situation
barplot(table(train_data$Living_Situation), 
        main = "Living Situation Distribution", 
        xlab = "Living Situation", 
        ylab = "Frequency", 
        col = "purple")
```

#### Exploring Relationships Between Variables

- **Correlation Matrices**: Help identify strong relationships between continuous predictors and the response variable.
- **Scatter Plots**: Assess linear relationships between numerical variables.
- **Boxplots**: Compare how different categories influence a continuous response variable.

```{r eda-corr-matrix}
# Correlation matrix
cor_matrix <- cor(train_data[, c("Net_Money", "Monthly_Allowance", "Monthly_Earnings")], use = "complete.obs")
print(cor_matrix)
```

```{r eda-scatterplot}
# Scatter plot of Monthly Allowance vs. Net Money
plot(train_data$Monthly_Allowance, train_data$Net_Money, 
     main = "Net Money vs. Monthly Allowance", 
     xlab = "Monthly Allowance", 
     ylab = "Net Money", 
     col = "blue", 
     pch = 19)
abline(lm(Net_Money ~ Monthly_Allowance, data = train_data), col = "red", lwd = 2)
```

```{r eda-multiple-boxplots}
# Boxplot of Net Money by Living Situation
boxplot(Net_Money ~ Living_Situation, 
        data = train_data, 
        main = "Net Money by Living Situation", 
        xlab = "Living Situation", 
        ylab = "Net Money", 
        col = "lightgreen")
```


#### Summary Statistics
Alongside visualizations, we compute descriptive statistics to summarize key characteristics of the data:

```{r eda-summary-stats}
# Summary statistics for numerical variables
summary(train_data[, c("Net_Money", "Monthly_Allowance", "Monthly_Earnings")])
```

This step provides insights into the central tendency (mean, median) and dispersion (standard deviation, range) of each variable, which will inform our model selection and feature engineering.

## Data Modelling

Following Exploratory Data Analysis (EDA), we now move to data modeling, where we apply a structured approach to uncover relationships between variables and predict outcomes. In this section, we focus on Ordinary Least Squares (OLS) regression, a widely used statistical technique for modeling linear relationships.

### Choosing a Suitable Regression Model

The choice of regression model depends on the patterns identified in EDA and the objectives of the analysis. Below are common regression approaches:

- **Simple Linear Regression**: Suitable when modeling a single predictor’s linear effect on the response variable.
- **Multiple Linear Regression:** Used when multiple predictors contribute to the response variable.
- **Polynomial Regression**: Captures non-linear relationships by incorporating polynomial terms.
- **Log-Linear Models**: Applied when transforming skewed distributions improves interpretability.
- **Ridge and Lasso Regression**: Useful when dealing with multicollinearity or performing feature selection through regularization.

Since our analysis focuses on understanding financial habits with multiple contributing factors, we use Multiple Linear Regression via OLS to quantify the relationships between the response (Net_Money) and predictors.

### Defining Modeling Parameters

After selecting OLS as our regression model, we define key modeling components:

Response Variable (Y):

- `Net_Money`: The dependent variable representing financial balance.

Predictors (X):

- Continuous: `Monthly_Allowance`, `Monthly_Earnings`
- Categorical: `Living_Situation`
- Ordinal: `Year_of_Study`

Each predictor is selected based on EDA insights, ensuring relevance in explaining `Net_Money`. If interaction effects exist, such as `Living_Situation` modifying the effect of `Monthly_Allowance`, we incorporate interaction terms.

### Setting Up the Modeling Equation

The OLS regression equation expresses how predictors influence the response variable:

$$
\begin{align}
\text{Net_Money} = &\ \beta_0 \\
               & + \beta_1 \times \text{Monthly_Allowance} \\
               & + \beta_2 \times \text{Monthly_Earnings} \\
               & + \beta_3 \times \text{Year_of_Study} \\
               & + \beta_4 \times \text{Living_Situation} \\
               & + \epsilon
\end{align}
$$

where:

- $\beta_0$​ is the intercept (baseline Net_Money when all predictors are zero),
- $\beta_1, \beta_2, \beta_3, \beta_4$ are regression coefficients, quantifying how each predictor affects `Net_Money`,
- $\epsilon$ is the error term, capturing unexplained variance.

This equation provides a formal structure to estimate how financial behavior is influenced by multiple factors. The next step will involve estimation, where we compute these coefficients and assess their significance.

## Estimation

With the data modeling stage completed, we now move to estimation, where we fit the Ordinary Least Squares (OLS) regression model and interpret its coefficients. This step provides numerical estimates for how much each predictor contributes to the response variable, allowing us to quantify their effects.

### Fitting the Model

To estimate the regression coefficients, we fit the OLS model to the training data using Python (statsmodels) or R (lm).

```{r fitting-ols}
# Load necessary library
library(stats)

# Fit the OLS model
ols_model <- lm(Net_Money ~ Monthly_Allowance + Monthly_Earnings + Year_of_Study, data=train_data)

# Display summary of model results
summary(ols_model)
```

### Interpreting the Coefficients

After fitting the model, we examine the estimated coefficients to understand their impact. Each coefficient represents the effect of a predictor on Net_Money, assuming all other factors are constant:

- Intercept ($\beta_0 = -826.01$): If all predictors are zero, the expected Net_Money is -$826.01.
- Monthly_Allowance ($\beta_1 = 1.43$): Each $1 increase in Monthly_Allowance is associated with a $1.43 increase in Net_Money.
- Monthly_Earnings ($\beta_2 = 0.98$): Each $1 increase in Monthly_Earnings increases Net_Money by $0.98.
- Year_of_Study ($\beta_3 = -104.34$): Being in a later year of study decreases Net_Money by -$104.34 on average.

## Goodness of Fit

After estimating the regression coefficients, we now assess how well the model fits the data and whether it satisfies the assumptions of Ordinary Least Squares (OLS) regression. This step ensures that our conclusions are statistically valid and that our model generalizes well to new data.

### Checking Model Assumptions

OLS regression relies on several key assumptions. We use diagnostic plots and statistical tests to verify these assumptions:

#### Linearity

The relationship between predictors and the response should be linear.

- **Diagnostic Plot**: A residual vs. fitted values plot should show a random spread (no patterns).
- **Solution if Violated**: Apply polynomial terms or transformations (e.g., log transformation).

```{r model-assumption-linearity}
# Residuals vs Fitted plot (R)
plot(ols_model$fitted.values, residuals(ols_model), 
     main = "Residuals vs Fitted", xlab = "Fitted Values", ylab = "Residuals")
abline(h = 0, col = "red")
```

#### Independence of Errors

Residuals should not be correlated (especially in time-series data).

- **Diagnostic Test**: Durbin-Watson test detects autocorrelation.

```{r model-assumption-indep-of-error}
dwtest(ols_model)
```

#### Homoscedasticity (Constant Variance of Errors)

The variance of residuals should remain constant across all fitted values.

- **Diagnostic Plot**: Residuals vs. fitted values plot.
- **Solution if Violated**: Use weighted least squares regression.

```{r model-assumption-homoscedasticity}
ncvTest(ols_model)  # Test for homoscedasticity
```

#### Normality of Residuals

Residuals should be normally distributed for valid hypothesis testing.

- **Diagnostic Plot**: Q-Q plot (Quantile-Quantile plot) shows if residuals follow a normal distribution.


```{r model-assumption-normality-residuals}
qqnorm(residuals(ols_model))
qqline(residuals(ols_model), col = "red")
```

### Evaluating Model Fit

A good model should explain a large proportion of variance in the response variable.

#### R-Squared

- Definition: Measures the proportion of variance in the response variable explained by predictors.
- Interpretation: An R-squared of 0.85 means that 85% of the variation in Net_Money is explained by the predictors.

```{r r-squared}
summary(ols_model)$r.squared  # R-squared value
```

Limitations:
- A high R-squared does not imply causation.
- Adding predictors always increases R-squared, even if they are irrelevant.
- Adjusted R-squared accounts for unnecessary predictors.


```{r adjusted-r-squared}
summary(ols_model)$adj.r.squared  # Adjusted R-squared
```

#### Identifying Outliers and Influential Points

Extreme values can distort regression estimates. We detect them using:

Residual Plots

- Large residuals may indicate outliers.

```{r residual-plots}
plot(residuals(ols_model), main = "Residual Plot", ylab = "Residuals")
abline(h = 0, col = "red")
```

Cook’s Distance

- Definition: Measures how much a data point influences regression results.
- Threshold: If Cook’s distance > 0.5, the point may be influential.

```{r cooks-distance}
cook_values <- cooks.distance(ols_model)
plot(cook_values, type = "h", main = "Cook's Distance")
```

By validating these assumptions and model fit, we ensure that our regression analysis is reliable and informative. Next, we move to interpreting and communicating the results.

## Results

After validating the model’s goodness of fit, we now assess its predictive performance and inferential insights. This step involves using the trained model to make predictions and evaluating how well it generalizes to unseen data.

### Predictive Analysis

To assess our model’s effectiveness, we generate predictions on the test set and compute evaluation metrics. In Python and R, we use the trained OLS model to predict Net_Money for the test set.

```{r predict-on-test}
# Generate predictions on the test set
y_pred <- predict(ols_model, newdata=test_data)
```

#### Performance Metrics

To measure the accuracy of predictions, we compute standard regression error metrics:

- Mean Absolute Error (MAE): Measures average absolute differences between predicted and actual values.
- Mean Squared Error (MSE): Penalizes larger errors more heavily.
- Root Mean Squared Error (RMSE): Similar to MSE but in the same units as Net_Money.
- R-squared: Proportion of variance in Net_Money explained by the model.

```{r prediction-results}
# Extract response variable from test data
y_test <- test_data$Net_Money

# Calculate metrics in R
mae <- mean(abs(y_test - y_pred))
mse <- mean((y_test - y_pred)^2)
rmse <- sqrt(mse)
r2 <- summary(ols_model)$r.squared

cat(sprintf("MAE: %.2f, MSE: %.2f, RMSE: %.2f, R-squared: %.2f", mae, mse, rmse, r2))
```

### Inferential Analysis

Even though our primary focus is prediction, we can derive meaningful insights from the estimated coefficients.

#### Insights from Regression Coefficients

- Monthly_Allowance has a positive coefficient, indicating that higher allowances lead to greater financial balance (Net_Money).
- Year_of_Study has a negative coefficient, suggesting that students in later years tend to have lower Net_Money, possibly due to increased expenses or reduced financial support.
- If Living_Situation is significant, it may indicate that housing type influences financial balance.

With these insights, we move to the final stage: storytelling, where we communicate findings in a clear and actionable manner.

## Storytelling

The final step in our data science workflow is storytelling, where we translate our analytical findings into actionable insights. This stage ensures that our results are clearly understood by both technical and non-technical audiences. Effective storytelling involves summarizing insights, using visuals for clarity, and making data-driven recommendations.





