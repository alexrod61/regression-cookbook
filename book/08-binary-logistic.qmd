<!--#
# ======================================================
# Quarto Document Setup
# ======================================================
# This section defines metadata and execution rules for the chapter.
# - "title" sets the chapter title displayed in the rendered book.
# - "execute: eval: false" prevents R/Python code from being run 
#   during rendering (useful if code is illustrative, or if setup 
#   is handled separately).
# ====================================================== -->

---
title: "Binary Logistic Regression"
execute:
  eval: false
---


<!-- ======================================================
Google Analytics Tracking Snippet
------------------------------------------------------
- This loads the Google Analytics "gtag.js" script asynchronously 
  so it does not block page rendering.
- The `id` value ("G-7PRVEBE1EF") is your unique measurement ID.
- `window.dataLayer` is initialized if it doesn’t exist already.
- The `gtag` function pushes tracking events into the dataLayer.
- `gtag('js', new Date());` records the time the analytics script 
  was loaded.
- `gtag('config', 'G-7PRVEBE1EF');` activates tracking for this ID.
------------------------------------------------------
You can safely move this block into the YAML `include-in-header` 
if you want it injected site-wide, rather than repeating it per 
chapter.
====================================================== -->


<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7PRVEBE1EF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7PRVEBE1EF');
</script>


```{r}
#| include: false # Do not display this chunk in the rendered output

# The `colourize` function is a small utility that applies text color 
# formatting depending on the output format (LaTeX, HTML, or other).
# - If rendering to LaTeX/PDF, it wraps text in a LaTeX `\textcolor{}` command.
# - If rendering to HTML, it wraps text in a <span> with inline CSS color.
# - If rendering to anything else (e.g., Word), it simply returns the text unchanged.
# This allows text highlighting in multiple output formats with one function.

colourize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

<!-- 
This block creates a **Mermaid mindmap diagram** that shows how different 
chapters in the book fit into the broader context of regression analysis.  

- It organizes regression models by the type of outcome variable (Continuous vs. Discrete).  
- Each branch links to a chapter in the book (e.g., OLS, Gamma Regression, Beta Regression).  
- This diagram provides readers with a "map" of where Binary Logistic Regression fits in.  
-->


::: {#fig-binary-logistic-regression}
```{mermaid}
mindmap
  root((Regression 
  Analysis)
    Continuous <br/>Outcome Y
      {{Unbounded <br/>Outcome Y}}
        )Chapter 3: <br/>Ordinary <br/>Least Squares <br/>Regression(
          (Normal <br/>Outcome Y)
      {{Nonnegative <br/>Outcome Y}}
        )Chapter 4: <br/>Gamma Regression(
          (Gamma <br/>Outcome Y)
      {{Bounded <br/>Outcome Y <br/> between 0 and 1}}
        )Chapter 5: Beta <br/>Regression(
          (Beta <br/>Outcome Y)
      {{Nonnegative <br/>Survival <br/>Time Y}}
        )Chapter 6: <br/>Parametric <br/> Survival <br/>Regression(
          (Exponential <br/>Outcome Y)
          (Weibull <br/>Outcome Y)
          (Lognormal <br/>Outcome Y)
        )Chapter 7: <br/>Semiparametric <br/>Survival <br/>Regression(
          (Cox Proportional <br/>Hazards Model)
            (Hazard Function <br/>Outcome Y)
    Discrete <br/>Outcome Y
      {{Binary <br/>Outcome Y}}
        {{Ungrouped <br/>Data}}
          )Chapter 8: <br/>Binary Logistic <br/>Regression(
            (Bernoulli <br/>Outcome Y)
```
:::


```{r setup, include=FALSE}
# ======================================================
# Setup Chunk
# ------------------------------------------------------
# Purpose:
# - Load all R libraries used throughout this chapter
# - Import the logistic regression dataset
# - Share the dataset with Python using reticulate
# - Ensure Python has the required packages installed
# - Set seeds for reproducibility
# ------------------------------------------------------
# Note:
# - `include=FALSE` means this code runs during rendering,
#   but is hidden from the final output.
# ======================================================

# ---- Load R libraries ----
library(devtools)      # Tools for R package development (not always essential here)
library(readr)         # Fast CSV and text data import
library(ggplot2)       # Grammar of Graphics for plots
library(ggcorrplot)    # Correlation matrix plots
library(cookbook)      # Provides the "Logistic_Regression" example dataset
library(tidyverse)     # Core data science packages (dplyr, tidyr, etc.)
library(car)           # Companion to Applied Regression (diagnostics, tests)
library(lmtest)        # Tests for linear/nonlinear models (e.g., likelihood ratio tests)
library(reticulate)    # R ↔ Python interoperability 

# ---- Set random seeds (reproducibility) ----
set.seed(1234)         # Fix R's random number generator
reticulate::py_run_string("import numpy as np; np.random.seed(1234)")  # Fix Python's RNG


# ---- Load dataset ----
# Load the built-in example dataset "Logistic_Regression" from the cookbook package.
# This dataset will be used consistently across all examples in this chapter.
data("Logistic_Regression")
BLR <- Logistic_Regression   # Assign dataset to BLR (shorter, convenient name)
data <- BLR                  # Duplicate assignment for compatibility with examples

# ---- Setup Python dependencies ----
# Ensure Python has the required libraries for logistic regression analysis.
reticulate::py_require(c(
  "pandas", 
  "numpy", 
  "matplotlib", 
  "scikit-learn", 
  "statsmodels"
))

# Share the dataset from R into the Python environment
py$data <- reticulate::r_to_py(BLR)
```

::: {.LO}

::: {.LO-header}
**Learning Objectives**
:::

::: {.LO-container}
By the end of this chapter, you will be able to:

- Explain how **Binary Logistic Regression** models the probability of a binary outcome using **log-odds**.  
- Describe why **Ordinary Least Squares** is not appropriate for binary outcomes, and how the **logistic function** addresses this.  
- Fit a binary logistic regression model in both `R` and `Python`.  
- Interpret **log-odds**, **odds ratios**, and **predicted probabilities** in real-world contexts.  
- Evaluate model fit using metrics such as **accuracy**, **confusion matrices**, and **ROC curves**.  
- Identify when binary logistic regression is the **appropriate tool**, and understand its **limitations**.  
:::

:::



## Introduction

In many real-world problems, the outcome we're trying to predict is **not a number** — it's a **yes or no**, **success or failure**, **clicked or didn’t click**. For example:

- Will a student **default on a loan** given their credit score and income?
- Will a student **pass a course** based on whether they have a part-time job and how many hours they studied?
- Will a student **likely graduate within 4 years**, based on their academic record and declared major?

These outcomes are **binary**: they can take on only two possible values, typically coded as `1` (event occurs) and `0` (event does not occur).

To model such outcomes, we need a regression approach that produces **predicted probabilities** between 0 and 1 — not arbitrary numbers on the real line. This is where **Binary Logistic Regression** comes in.

In this chapter, we’ll see how logistic regression:
- Links input variables to the **log-odds** of the outcome,
- Produces interpretable coefficients (like **odds ratios**), and
- Helps us make informed predictions in binary classification problems.

## Why Ordinary Least Squares Fails for Binary Outcomes

To understand the need for logistic regression, consider applying Ordinary Least Squares (OLS) to a **binary outcome**.

Suppose we are trying to predict whether a student **defaulted** on a loan (`1`) or **did not default** (`0`) using a continuous predictor like `credit_score`.

Before diving into the technical reasons why OLS is inappropriate for binary outcomes, let’s look at a simplified version of the dataset we’ll use throughout this chapter:

```r
# Prepare data: numeric default variable and select predictor
blr_data <- BLR %>%
  select(credit_score, defaulted) %>%
  mutate(defaulted = as.numeric(defaulted))
```

Below is a sample of this dataset:

| **credit\_score** | **income** | **education\_years** | **married** | **owns\_home** | **age**  | **defaulted** | **successes** | **trials** |
| ------: | ---------: | -------------------: | ----------: | -------------: | ----------------: | ------------: | ------------: | ---------: |
|          787| 118911|              17|       0|         0|  47|         0|         9|     10|
|          441|  53122|              13|       0|         1|  50|         1|         2|     10|
|          407|  39174|              10|       1|         0|  50|         1|         5|     10|
|          812| 148982|              12|       1|         1|  32|         0|         8|     10|
|          825| 154293|              16|       1|         0|  41|         0|         7|     10|
|          729| 111562|              15|       1|         0|  64|         0|         7|     10|
|          700|  70000|              15|       0|         1|  46|         0|         8|     10|
|          451|  40311|              12|       0|         1|  59|         1|         4|     10|
|          404|  45944|              13|       0|         0|  53|         1|         4|     10|
|          374|  43167|              11|       0|         0|  34|         1|         4|     10|



Now, let’s narrow in on the key variables of interest for this section.


| **credit_score** | **defaulted** |
| ---------------- | ------------- |
| 787              | 0             |
| 441              | 1             |
| 407              | 1             |
| 812              | 0             |
| 825              | 0             |
| 729              | 0             |
| 700              | 0             |
| 451              | 1             |
| 404              | 1             |
| 374              | 1             |


This dataset allows us to examine whether a student defaulted on a loan (`defaulted = 1`) based on their `credit_score`. It’s a classic binary outcome: a "yes" or "no" event.


If we use OLS, we estimate:

$$
\mathbb{E}(Y_i \mid X_i) = p_i = \beta_0 + \beta_1 X_i
$$

This means we’re treating the **probability of default** as a linear function of credit score.

However, there are **two fundamental issues** with using OLS here:

1. The linear model can produce predicted values **less than 0** or **greater than 1** — which doesn’t make sense when modeling a **probability**.
2. OLS assumes **constant variance** of the residuals, but binary outcomes inherently violate this assumption, because their variance depends on the mean:

$$
\text{Var}(Y_i) = p_i(1 - p_i)
$$

To see this issue in action, let’s try fitting an OLS regression model using our binary outcome and a continuous predictor. While OLS is designed to minimize squared error, it doesn't account for the discrete nature of the response — which means it can return predicted probabilities below 0 or above 1. In the example below, we use a customer's credit score to predict the probability of default, and you’ll see how the linear model fails to respect the fundamental constraints of probability.


![OLS fails to constrain predictions between 0 and 1, despite binary outcomes.](img/ols-failure-credit-score2.png){#fig-ols-failure-binary width="90%"}

These limitations make OLS unsuitable for binary classification tasks.  
To properly model binary outcomes, we need a method that:

- Produces predictions strictly between 0 and 1,
- Accounts for the fact that the variance depends on the mean, and
- Connects our predictors to the probability of the event in a way that is easy to interpret.

The key idea is to **transform the probability** so that it can be modeled as a linear function without breaking these rules.  
In the next section, we’ll see how the **logit transformation** does exactly that.

::: {.panel-tabset}

## R Code
```{.r}
# ⚠️ This OLS fit is not appropriate for binary outcomes.
# We include it here only to illustrate why logistic regression is needed.

# Load necessary libraries
library(ggplot2)
library(dplyr)

# Prepare data: numeric default variable and select predictor
blr_data <- BLR %>%
  select(credit_score, defaulted) %>%
  mutate(defaulted = as.numeric(defaulted))

# Fit an OLS model (not ideal for binary outcome)
ols_model <- lm(defaulted ~ credit_score, data = blr_data)

# Add predicted values
blr_data$predicted <- predict(ols_model, newdata = blr_data)

# Plot actual data and OLS-fitted line
plot <- ggplot(blr_data, aes(x = credit_score, y = defaulted)) +
  geom_jitter(height = 0.05, width = 0, alpha = 0.4, color = "black") +
  geom_line(aes(y = predicted), color = "blue", linewidth = 1.2) +
  labs(
    title = "OLS Fitted Line on Binary Data",
    x = "Credit Score",
    y = "Predicted Probability of Default"
  ) +
  coord_cartesian(ylim = c(-0.2, 1.2)) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

plot
```

## Python Code
```{.python}
# ⚠️ This OLS fit is not appropriate for binary outcomes.
# We include it here only to illustrate why logistic regression is needed.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm

# Extract relevant columns from the BLR dataset
blr = r.data['BLR']
df = blr[['credit_score', 'defaulted']].copy()
df['defaulted'] = df['defaulted'].astype(int)

# Fit an OLS model
X = sm.add_constant(df['credit_score'])
model = sm.OLS(df['defaulted'], X).fit()
df['predicted'] = model.predict(X)

# Plot the actual binary outcomes and OLS predictions
fig, ax = plt.subplots(figsize=(6, 4))
ax.scatter(df['credit_score'], df['defaulted'], alpha=0.4, color='black', label='Actual Data', s=20)
ax.plot(df['credit_score'], df['predicted'], color='blue', label='OLS Fit', linewidth=2)
ax.set_title("OLS Fitted Line on Binary Data", fontsize=14, fontweight='bold')
ax.set_xlabel("Credit Score")
ax.set_ylabel("Predicted Probability of Default")
ax.set_ylim(-0.2, 1.2)
ax.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

```

:::

::: {.panel-tabset}

## R Output TODO
```{r}
#| echo: false
#| message: false

# Sample data
set.seed(42)
X <- c(1000, 1200, 1500, 1800, 2000)
Y <- c(200, 230, 250, 290, 310)

# Create a data frame
df <- data.frame(Size = X, Price = Y)

# Fit the correct OLS model
correct_model <- lm(Price ~ Size, data = df)

# Create predictions for the two lines
df$Predicted_Correct <- predict(correct_model, newdata = df)
df$Predicted_Wrong <- 110 + 0.08 * df$Size  # Adjusted manually

# Reshape data for ggplot (to add legend)
df_long <- data.frame(
  Size = rep(df$Size, 2),
  Price = c(df$Predicted_Correct, df$Predicted_Wrong),
  Line = rep(c("Line A (Best Fit)", "Line B (Worse Fit)"), each = nrow(df))
)

# Store the plot with a legend
library(ggplot2)
plot <- ggplot() +
  geom_point(data = df, aes(x = Size, y = Price), size = 3, color = "black") +
  geom_line(data = df_long, aes(x = Size, y = Price, color = Line), linewidth = 1.2) +
  scale_color_manual(values = c("Line A (Best Fit)" = "blue", "Line B (Worse Fit)" = "red")) +
  labs(title = "Comparing Regression Line Fits",
       x = "House Size (sq ft)",
       y = "House Price (in $1000s)",
       color = "Regression Line") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        legend.position = "bottom")

plot
```

## Python Output TODO
```{python}
#| echo: false
#| message: false

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm

# Sample data
np.random.seed(42)
X = np.array([1000, 1200, 1500, 1800, 2000])
Y = np.array([200, 230, 250, 290, 310])
df = pd.DataFrame({'Size': X, 'Price': Y})

# Fit the correct OLS model
X_sm = sm.add_constant(df['Size'])
model = sm.OLS(df['Price'], X_sm).fit()
df['Predicted_Correct'] = model.predict(X_sm)

# Manually add the incorrect line
df['Predicted_Wrong'] = 110 + 0.08 * df['Size']

# Reshape for plotting
df_long = pd.concat([
    df[['Size', 'Predicted_Correct']].rename(columns={'Predicted_Correct': 'Price'}).assign(Line='Line A (Best Fit)'),
    df[['Size', 'Predicted_Wrong']].rename(columns={'Predicted_Wrong': 'Price'}).assign(Line='Line B (Worse Fit)')
])

# Plot
fig, ax = plt.subplots(figsize=(6, 4))
ax.scatter(df['Size'], df['Price'], color='black', label='Actual Data')
for label, group in df_long.groupby('Line'):
    ax.plot(group['Size'], group['Price'], label=label)
ax.set_title("Comparing Regression Line Fits", fontsize=14, fontweight='bold')
ax.set_xlabel("House Size (sq ft)")
ax.set_ylabel("House Price (in $1000s)")
ax.legend(title="Regression Line", loc='lower right')
plt.grid(True)
plt.show()
```

:::

## The Logit Function

In Section 8.2, we saw that modeling probability directly with a linear equation can produce impossible values (less than 0 or greater than 1) and ignores the way variance changes with the mean.

The fix is to apply a transformation that:

- Expands the (0, 1) probability range to the entire real line,
- Is reversible, so we can convert back to probabilities, and
- Preserves the ordering of probabilities.

One transformation that checks all these boxes is the **logit function**.

### Logit: A Link Between Probability and Linear Predictors

The **logit function** transforms the probability $p_i$ into a value on the entire real line:
$$
\text{logit}(p_i) = \log\!\left(\frac{p_i}{1 - p_i}\right) = \beta_0 + \beta_1 X_i
$$

This transformation:

- Is **monotonic** (it preserves order),
- Maps probabilities $p_i \in (0, 1)$ to $(-\infty, \infty)$, and
- Solves the range issue of OLS by letting us fit a linear model in the transformed space.

By modeling the **log-odds** as a linear function of predictors and then inverting the transformation, we obtain valid predicted **probabilities** that remain within $[0,1]$.

::: {.callout-note}
The **logit function** is defined as:
$$
\text{logit}(p_i) = \log\!\left(\frac{p_i}{1 - p_i}\right)
$$
It models the **log-odds** of the outcome as a linear function of predictors.
:::

### From Log-Odds Back to Probability

We can invert the logit function to get predicted probabilities:
$$
p_i = \frac{\exp\!\left(\beta_0 + \beta_1 X_i\right)}{1 + \exp\!\left(\beta_0 + \beta_1 X_i\right)} \in (0,1)
$$

This **sigmoid function** outputs values strictly between 0 and 1 for any real-valued linear predictor, making it ideal for modeling probabilities.

This transformation lets us use a linear predictor on the **log-odds** scale and then recover valid probabilities.  
This idea forms the basis of **binary logistic regression**, introduced next.



::: {.callout-note}
## TODO:
Add an annotated diagram showing how increasing `X` impacts odds, log-odds, and probabilities.
:::

---

### The Logistic Function and Its Shape

```{.r}
#| fig-cap: "The logistic (sigmoid) curve maps any real-valued linear predictor to probabilities in [0,1]. A linear fit can wander outside [0,1]."
#| fig-width: 7
#| fig-height: 4

set.seed(1)
x <- seq(-4, 4, length.out = 200)
eta <- -1 + 1.2 * x                    # linear predictor
p   <- 1 / (1 + exp(-eta))             # sigmoid

# A naive linear "probability" for comparison (clipped display range only)
p_lin <- scales::rescale(eta, to = c(-0.4, 1.4))

library(ggplot2)
df <- data.frame(x = x, sigmoid = p, linear = p_lin)

ggplot(df, aes(x = x)) +
  geom_line(aes(y = sigmoid), linewidth = 1.1) +
  geom_line(aes(y = linear), linetype = 2) +
  coord_cartesian(ylim = c(-0.1, 1.1)) +
  labs(x = "Linear predictor  (η = β0 + β1 X)",
       y = "Mapped value",
       caption = "Solid: logistic (valid probabilities).  Dashed: naive linear (can leave [0,1]).") +
  theme_minimal(base_size = 12)
```

- Introduce the **sigmoid** function:

$$
p_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_i)}}
$$

- It maps real-valued input to the [0, 1] interval.
- Shape: S-curve — slow rise at ends, steep middle.
- Contrast visually with a linear line from OLS.

::: {.panel-tabset}
## R Code
```{r}
# Logistic vs Linear fit demo
set.seed(1)
x <- seq(-4, 4, length.out = 200)
eta <- -1 + 1.2 * x
p   <- 1 / (1 + exp(-eta))   # logistic sigmoid

# Linear "probability" rescaled to [−0.4, 1.4] just for comparison
p_lin <- scales::rescale(eta, to = c(-0.4, 1.4))

df <- data.frame(x = x, sigmoid = p, linear = p_lin)

ggplot(df, aes(x = x)) +
  geom_line(aes(y = sigmoid), linewidth = 1.2, color = "blue") +
  geom_line(aes(y = linear), linetype = "dashed", color = "red") +
  coord_cartesian(ylim = c(-0.1, 1.1)) +
  labs(x = "Linear predictor  (η = β0 + β1 X)",
       y = "Mapped value",
       caption = "Blue: logistic sigmoid (valid probabilities).  Red dashed: naive linear fit (can leave [0,1]).") +
  theme_minimal(base_size = 12)

```

## Python Code
```{python}
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(1)
x = np.linspace(-4, 4, 200)
eta = -1 + 1.2 * x
p = 1 / (1 + np.exp(-eta))  # logistic sigmoid

# Linear "probability" rescaled
p_lin = (eta - eta.min()) / (eta.max() - eta.min()) * (1.4 - (-0.4)) + (-0.4)

plt.figure(figsize=(7,4))
plt.plot(x, p, label="Logistic sigmoid", linewidth=2, color="blue")
plt.plot(x, p_lin, "--", label="Naive linear fit", color="red")
plt.ylim(-0.1, 1.1)
plt.xlabel("Linear predictor  (η = β0 + β1 X)")
plt.ylabel("Mapped value")
plt.legend()
plt.title("Logistic curve vs. linear fit")
plt.show()

```
:::

---

## Case Study: Understanding Financial Behaviors

Statistical models are most useful when they connect to a real-world problem.
Here, we’ll use binary logistic regression to study **financial behavior** — specifically, the likelihood that a student **defaults** on a loan.

---

### The Dataset

We’ll use the `Logistic_Regression` dataset (packaged in `cookbook`).

* **Predictors**:

  * `credit_score` (numeric, higher = lower risk)
  * `income` (numeric, annual income in dollars)
  * `age` (numeric)
  * `loan_amount` (numeric, loan size)
  * `student` (categorical: yes/no)

* **Outcome**:

  * `defaulted` (binary: `1 = default`, `0 = no default`)

::: {.callout-note}
This is a **binary classification problem**: given a mix of continuous and categorical predictors, we want to predict whether a student defaults.
:::

---

### The Problem We’re Trying to Solve

Banks and lenders routinely face the question:

> *Given a customer’s information, what is the probability that they will default on a loan?*

Why it matters:

* Accurate predictions reduce **financial risk**.
* Misclassification can be costly (approving high-risk loans or rejecting low-risk ones).
* Logistic regression provides both **probability estimates** and interpretable **odds ratios** for each predictor.

---

### Study Design

* **Predictor types**: both **continuous** (income, age, credit\_score, loan\_amount) and **categorical** (student status).
* **Outcome**: binary (defaulted).
* **Sample size**: dataset includes a few hundred observations (enough to illustrate the method).
* **Assumptions**:

  * Observations are independent.
  * Predictors have a linear relationship with the **log-odds** of default.
  * No perfect collinearity among predictors.

---

### Applying Study Design to Our Case Study

Before modeling, we:

* **Split data** into training and testing sets.
* **Encode categorical variables** (e.g., `student` → 0/1).
* **Standardize or rescale predictors** if needed for interpretation.
* **Preview** the structure of the data:

```{r}
# Peek at dataset
head(BLR)
```

```{python}
# Python equivalent
import pandas as pd
pd.DataFrame(py$data).head()
```


With the dataset understood and prepared, we’re ready to fit a **binary logistic regression model** to predict loan default.

---


## Fitting the Binary Logistic Regression Model

Now that we’ve introduced the dataset and design, we can fit a **binary logistic regression model**.
Our goal is to estimate how predictors like `credit_score`, `income`, and `student` status influence the likelihood of **default**.

---

### Model Specification

The model assumes:

$$
\begin{aligned}
\text{logit}(p_i) &= \log\!\left(\frac{p_i}{1 - p_i}\right) \\
&= \beta_0
  + \beta_1\,\text{credit\_score}_i
  + \beta_2\,\text{income}_i \\
&\quad + \beta_3\,\text{age}_i
  + \beta_4\,\text{loan\_amount}_i
  + \beta_5\,\text{student}_i
\end{aligned}
$$



where:

* $p\_i =$ probability of default for customer $i$
* $\beta\_j =$ regression coefficients

---

### Fitting in R

```{r}
# Fit logistic regression model
fit_r <- glm(defaulted ~ credit_score + income + age + loan_amount + student,
             data = BLR,
             family = binomial(link = "logit"))

summary(fit_r)
```

---

### Fitting in Python

```{python}
import statsmodels.api as sm

# Define predictors (X) and outcome (y)
X = py$data[['credit_score', 'income', 'age', 'loan_amount', 'student']]
X = sm.add_constant(X)  # add intercept
y = py$data['defaulted']

# Fit logistic regression
model = sm.Logit(y, X).fit()
print(model.summary())
```

---

### Model Output

* Each coefficient ($\beta\_j$) is expressed on the **log-odds scale**.
* Positive coefficients → higher log-odds of default (greater risk).
* Negative coefficients → lower log-odds of default (less risk).

We will later translate these log-odds into **odds ratios** and **predicted probabilities** for interpretation.

---

## Fitting the Logistic Regression Model

We model the **log‑odds** of default as a linear function of predictors. In practice:

* **R:** `glm(..., family = binomial(link = "logit"))`
* **Python:** `statsmodels.Logit(...)`

::: {.panel-tabset}

## R Code

```{r}
#| eval: false
#| message: false
#| warning: false

# Fit a simple, interpretable model
fit_glm <- glm(
  defaulted ~ credit_score + income,
  data = BLR,
  family = binomial(link = "logit")
)

# Coefficients (log-odds) and odds ratios
coef_table <- broom::tidy(fit_glm, conf.int = TRUE, conf.level = 0.95)
coef_table$odds_ratio <- exp(coef_table$estimate)
coef_table$or_low     <- exp(coef_table$conf.low)
coef_table$or_high    <- exp(coef_table$conf.high)

knitr::kable(
  coef_table[, c("term","estimate","std.error","statistic","p.value","odds_ratio","or_low","or_high")],
  digits = 3,
  col.names = c("Term","Log-Odds (β)","SE","z","p","Odds Ratio","OR 2.5%","OR 97.5%")
)
```

## Python Code

```{python}
#| eval: false
#| message: false
#| warning: false

import pandas as pd
import statsmodels.api as sm
import numpy as np

# BLR was exported from R in the setup; use it directly
df = BLR.copy()
df['defaulted'] = df['defaulted'].astype(int)

X = df[['credit_score','income']]
X = sm.add_constant(X)
y = df['defaulted']

logit_model = sm.Logit(y, X).fit(disp=False)

# Build a coefficient table with odds ratios and CIs
params = logit_model.params
bse    = logit_model.bse
zvals  = params / bse
pvals  = logit_model.pvalues
conf   = logit_model.conf_int()
or_    = np.exp(params)
or_lo  = np.exp(conf[0])
or_hi  = np.exp(conf[1])

coef_table = pd.DataFrame({
    "Term": params.index,
    "Log-Odds (β)": params.values,
    "SE": bse.values,
    "z": zvals.values,
    "p": pvals.values,
    "Odds Ratio": or_.values,
    "OR 2.5%": or_lo.values,
    "OR 97.5%": or_hi.values
})

coef_table
```

:::

**Notes**

* Coefficients are in **log‑odds** units.
* Exponentiating a coefficient gives the **odds ratio**: $\text{OR} = e^{\beta}$.
* E.g., $\beta_{\text{credit\_score}}=-0.01\Rightarrow \text{OR}\approx 0.99$: each 1‑point increase in credit score multiplies the odds of default by \~0.99 (≈1% decrease).

---

## From Log‑Odds to Probabilities

The inverse‑logit (a.k.a. logistic/sigmoid) maps any linear predictor $\eta$ back to a valid probability:

$$
p = \text{logit}^{-1}(\eta) = \frac{1}{1 + e^{-\eta}}
$$

Below we:

1. get **linear predictors** ($\eta$),
2. transform to **probabilities** $p$, and
3. compute example probabilities at low/median/high credit scores (holding income at its mean).

::: {.panel-tabset}

## R Code

```{r}
#| eval: false
#| message: false
#| warning: false

# 1) Linear predictor (η) and probability (p) for observed rows
BLR$eta_hat <- predict(fit_glm, type = "link")         # η = Xβ
BLR$p_hat   <- predict(fit_glm, type = "response")     # p = logistic(η)

# 2) Example scenarios at low / median / high credit score
qs <- quantile(BLR$credit_score, probs = c(0.25, 0.5, 0.75), na.rm = TRUE)
newdat <- tibble::tibble(
  credit_score = as.numeric(qs),
  income = mean(BLR$income, na.rm = TRUE),
  label = c("Low (25th %)","Median (50th %)","High (75th %)")
)

newdat$eta   <- predict(fit_glm, newdata = newdat, type = "link")
newdat$prob  <- predict(fit_glm, newdata = newdat, type = "response")

knitr::kable(
  newdat[, c("label","credit_score","income","eta","prob")],
  digits = 4,
  col.names = c("Scenario","Credit Score","Income (mean)","η (linear predictor)","Probability")
)
```

## Python Code

```{python}
#| eval: false
#| message: false
#| warning: false

import numpy as np
import pandas as pd

# 1) Linear predictor and probabilities for observed rows
eta_hat = np.dot(X, logit_model.params)      # η = Xβ
p_hat   = 1 / (1 + np.exp(-eta_hat))         # logistic(η)

df_out = df[['credit_score','income','defaulted']].copy()
df_out['eta_hat'] = eta_hat
df_out['p_hat']   = p_hat

# 2) Example scenarios at low / median / high credit score (income at mean)
qs = df['credit_score'].quantile([0.25, 0.5, 0.75]).values
inc_mean = df['income'].mean()

newX = pd.DataFrame({
    'const': 1.0,
    'credit_score': qs,
    'income': [inc_mean, inc_mean, inc_mean]
}, index=["Low (25th %)","Median (50th %)","High (75th %)"])

eta = np.dot(newX, logit_model.params)
prob = 1 / (1 + np.exp(-eta))

pd.DataFrame({
    "Scenario": newX.index,
    "Credit Score": newX['credit_score'].values,
    "Income (mean)": newX['income'].values,
    "η (linear predictor)": eta,
    "Probability": prob
})
```

:::


**Takeaway**

* The model fits a straight line on the **log‑odds** scale, then the **inverse‑logit** maps it to valid probabilities in $[0,1]$.
* You can read coefficients as changes in **log‑odds**, or exponentiate to **odds ratios**; but to **communicate**, show **probabilities** for realistic scenarios (like low/median/high credit score).

---

## Data Collection and Wrangling

Before building a logistic regression model, we need to ensure our data is **reliable, clean, and structured** for analysis. Poor data preparation can undermine even the best statistical methods.

### Data Collection

In our case, we’re working with a **loan default dataset**. Each row represents a borrower, and the key outcome is whether they **defaulted on their loan** (`defaulted = 1`) or not (`defaulted = 0`).
Predictors include:

* **credit\_score** (continuous)
* **income** (continuous)
* **education\_years** (continuous)
* **married** (binary categorical)
* **owns\_home** (binary categorical)
* **age** (continuous)

The dataset also contains `successes` and `trials` (grouped-binomial form), but in this chapter we’ll treat outcomes as **individual-level Bernoulli trials**.

### Data Wrangling

To prepare the dataset:

* **Handling Missing Data:** If borrowers have missing credit scores or income values, we must decide whether to impute them (mean, median, regression-based) or drop those cases. Missingness should be checked systematically.
* **Encoding Categorical Variables:** Variables like `married` and `owns_home` need to be encoded as 0/1 indicators. In R and Python, logistic regression functions handle factors/dummies automatically, but it’s important to confirm coding.
* **Dealing with Class Imbalance:** Loan defaults are usually much rarer than non-defaults. A dataset with 90% non-defaults and 10% defaults might lead a model to always predict “no default” and still achieve 90% accuracy. Strategies include:

  * Adjusting the decision threshold (not always 0.5),
  * Using resampling methods (oversampling defaults, undersampling non-defaults), or
  * Applying class weights in the model.
* **Splitting Data for Training/Testing:** To fairly assess predictive performance, we split the dataset into **training** (used to fit the model) and **testing** (used to evaluate it). A common split is 70/30. This prevents overfitting and ensures the model generalizes to new borrowers.

---

**TODOs for this section:**

* Check for missing values and document how they were handled.
* Ensure categorical variables (`married`, `owns_home`) are encoded properly.
* Assess class balance in `defaulted` and consider strategies if imbalance is severe.
* Split dataset into training and test sets, keeping class balance in mind.

---

::: {.panel-tabset}

## R Code

```{.r}
library(dplyr)
library(caret)

# Check missing data
colSums(is.na(BLR))

# Encode categorical variables (ensure they are factors)
BLR <- BLR %>%
  mutate(
    married = as.factor(married),
    owns_home = as.factor(owns_home)
  )

# Check class balance
table(BLR$defaulted)
prop.table(table(BLR$defaulted))

# Split into training/testing sets
set.seed(123)
trainIndex <- createDataPartition(BLR$defaulted, p = .7, list = FALSE)
train <- BLR[ trainIndex, ]
test  <- BLR[-trainIndex, ]
```

## Python Code

```{.python}
import pandas as pd
from sklearn.model_selection import train_test_split

# Check missing data
print(df.isnull().sum())

# Encode categorical variables
df['married'] = df['married'].astype('category')
df['owns_home'] = df['owns_home'].astype('category')

# Check class balance
print(df['defaulted'].value_counts(normalize=True))

# Split into train/test sets (stratify to preserve class balance)
train, test = train_test_split(df, test_size=0.3, random_state=123,
                               stratify=df['defaulted'])
```

:::



---

## Exploratory Data Analysis (EDA)

Before fitting a logistic regression, we need to understand the structure of our data. Exploratory analysis helps us spot patterns, distributions, and potential issues.

### Classifying Variables

Our dataset contains predictors such as `credit_score`, `income`, `education_years`, `married`, `owns_home`, and `age`. The response variable `defaulted` is binary (`0` = no default, `1` = default).

### Visualizing Distributions

* **Continuous predictors** (e.g., credit score, income) can be visualized with histograms or density plots, split by default status.
* **Categorical predictors** (e.g., married, owns\_home) can be compared with barplots of default rates.

### Exploring Relationships with Binary Outcome

Boxplots or violin plots of credit score by default status, or barplots of default rates by home ownership, help reveal potential predictors. Correlations among continuous predictors can also be checked to watch for multicollinearity.

**TODOs for this section:**

* Plot distribution of credit scores for defaulters vs non-defaulters.
* Plot default rates by categorical predictors (`married`, `owns_home`).
* Correlation heatmap for continuous predictors.
* Summarize key EDA takeaways (e.g., lower credit scores → higher default rates).

---

::: {.panel-tabset}

## R Code

```{.r}
library(ggplot2)

# Boxplot of credit score by default
ggplot(BLR, aes(x = as.factor(defaulted), y = credit_score)) +
  geom_boxplot() +
  labs(x = "Defaulted", y = "Credit Score", 
       title = "Credit Score Distribution by Default Status")

# Barplot of default by home ownership
ggplot(BLR, aes(x = as.factor(owns_home), fill = as.factor(defaulted))) +
  geom_bar(position = "fill") +
  labs(x = "Owns Home", y = "Proportion", 
       fill = "Defaulted",
       title = "Default Rates by Home Ownership")

# Correlation heatmap
library(ggcorrplot)
corr <- cor(BLR[,c("credit_score","income","education_years","age")])
ggcorrplot(corr, lab = TRUE)
```

## Python Code

```{.python}
import seaborn as sns
import matplotlib.pyplot as plt

# Boxplot credit score by default
sns.boxplot(data=df, x="defaulted", y="credit_score")
plt.title("Credit Score Distribution by Default Status")
plt.show()

# Barplot default by home ownership
sns.barplot(data=df, x="owns_home", y="defaulted", ci=None)
plt.title("Default Rates by Home Ownership")
plt.show()

# Correlation heatmap
corr = df[['credit_score','income','education_years','age']].corr()
sns.heatmap(corr, annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap of Continuous Predictors")
plt.show()
```

:::

---

## Data Modelling

With EDA complete, we’re ready to specify our logistic regression model.

### Choosing a Logistic Regression Model

The response variable is binary: `defaulted ∈ {0,1}`. Logistic regression is appropriate because it ensures predictions stay between 0 and 1.

### Defining Modeling Parameters

* **Response Variable (Y):** `defaulted` (1 = default, 0 = no default).
* **Predictors (X):** Key predictors like `credit_score` and `income`. Categorical variables (e.g., `married`) can be included as factors/dummies.

### Setting Up the Logistic Regression Equation

The logistic regression model is:

$$
\log\left(\frac{p_i}{1 - p_i}\right) = \beta_0 + \beta_1 \cdot \text{credit\_score}_i + \beta_2 \cdot \text{income}_i + \cdots
$$

where $p_i$ is the probability that customer $i$ defaults.

This means coefficients describe changes in the **log-odds of default** for a one-unit change in the predictor, holding other predictors constant.

**TODOs for this section:**

* Write down the full logistic regression equation for your chosen predictors.
* Decide whether categorical variables should be included (dummy-coded).
* Check multicollinearity before finalizing model.

---

::: {.panel-tabset}

## R Code

```{.r}
# Fit base model
logit_model <- glm(defaulted ~ credit_score + income + married + owns_home,
                   data = BLR, family = binomial)
summary(logit_model)
```

## Python Code

```{.python}
import statsmodels.api as sm

X = df[['credit_score','income','married','owns_home']]
X = sm.add_constant(X)
y = df['defaulted']

logit_model = sm.Logit(y, X).fit()
print(logit_model.summary())
```

:::

---

## Estimation

Once the model is specified, we need to estimate its parameters. Unlike OLS, which minimizes squared errors, logistic regression uses **Maximum Likelihood Estimation (MLE)**.

### Maximum Likelihood Estimation

MLE finds the coefficients that maximize the probability of observing the data we actually have. Intuitively, the model adjusts coefficients until the predicted probabilities align as closely as possible with observed defaults.

### Fitting the Model in Practice

In practice, estimation is done using iterative numerical algorithms (e.g., Newton-Raphson). Software (R, Python) handles this behind the scenes, but it’s important to understand that coefficients are chosen to maximize the likelihood, not minimize error.

For our loan dataset, fitting the model gives us coefficients for credit score, income, and other predictors. These are the estimates we’ll later test and interpret.

**TODOs for this section:**

* Report log-likelihood value for the fitted model.
* Show convergence message (iteration process).
* Provide estimated coefficients with standard errors.

---

::: {.panel-tabset}

## R Code

```{.r}
# Logistic regression fit
logit_model <- glm(defaulted ~ credit_score + income,
                   data = BLR, family = binomial)

summary(logit_model) # shows estimates, SE, z, p-values, log-likelihood
```

## Python Code

```{.python}
X = sm.add_constant(df[['credit_score','income']])
y = df['defaulted']

logit_model = sm.Logit(y, X).fit()
print(logit_model.summary())  # includes estimates, SE, z, p-values, log-likelihood
```

:::



---

## Inference

After estimating a logistic regression model, we often want to know whether predictors are **statistically significant** — i.e., whether they have a meaningful relationship with the probability of default. In logistic regression, inference is based on the **likelihood framework**.

### Wald Tests

The **Wald test** checks whether an individual coefficient is significantly different from zero. For example, we can test whether `credit_score` has a nonzero effect on the odds of default. The test statistic is the ratio of the estimated coefficient to its standard error.

### Likelihood Ratio Tests

We can also compare **nested models** (e.g., model with `credit_score` only vs. model with `credit_score + income`) using the **likelihood ratio (LR) test**. This evaluates whether adding predictors significantly improves model fit.

### Confidence Intervals

Finally, we often report **confidence intervals for odds ratios**. For example, if the odds ratio for credit score is 0.99 with a 95% CI \[0.98, 0.995], we can say with confidence that higher credit scores reduce the odds of default.

**TODOs for this section:**

* Wald test output for credit score and income.
* Likelihood ratio test comparing one- vs. two-predictor models.
* Confidence intervals for odds ratios, reported in plain language.

---

::: {.panel-tabset}

## R Code

```{.r}
# Logistic regression
logit_model <- glm(defaulted ~ credit_score + income,
                   data = BLR, family = binomial)

# Wald test results are included in summary
summary(logit_model)

# Confidence intervals for odds ratios
exp(cbind(OR = coef(logit_model), confint(logit_model)))

# Likelihood ratio test for nested models
model1 <- glm(defaulted ~ credit_score, data = BLR, family = binomial)
anova(model1, logit_model, test = "Chisq")
```

## Python Code

```{.python}
import statsmodels.api as sm
import numpy as np
from scipy.stats import chi2

df = r.data['BLR'].copy()
df['defaulted'] = df['defaulted'].astype(int)

# Full model
X_full = sm.add_constant(df[['credit_score','income']])
y = df['defaulted']
model_full = sm.Logit(y, X_full).fit()

# Wald test (coeff / SE)
wald_stats = (model_full.params / model_full.bse)**2
print("Wald test chi2 values:\n", wald_stats)

# Confidence intervals for odds ratios
conf = model_full.conf_int()
odds_ratios = np.exp(model_full.params)
conf_exp = np.exp(conf)
print("Odds Ratios:\n", pd.DataFrame({"OR": odds_ratios,
                                      "2.5%": conf_exp[0],
                                      "97.5%": conf_exp[1]}))

# Likelihood ratio test vs simpler model
X_simple = sm.add_constant(df[['credit_score']])
model_simple = sm.Logit(y, X_simple).fit()

LR_stat = 2 * (model_full.llf - model_simple.llf)
df_diff = model_full.df_model - model_simple.df_model
p_value = chi2.sf(LR_stat, df_diff)
print(f"LR Test: chi2={LR_stat:.2f}, df={df_diff}, p={p_value:.4f}")
```

:::

---

## Coefficient Interpretation

Once we’ve established that predictors matter, the next step is to **interpret the coefficients** in a meaningful way.

### Odds Ratios and Their Meaning

Logistic regression coefficients are expressed in **log-odds units**. To make them interpretable, we exponentiate them to obtain **odds ratios**.

* Example: If the coefficient for credit score is -0.01, the odds ratio is about 0.99.
* Interpretation: each 1-point increase in credit score reduces the odds of default by about 1%.

Scaling makes interpretation clearer:

* A **50-point increase** in credit score reduces the odds of default by roughly 40%.
* For income, if the odds ratio is 1.00001, it means that each additional dollar increases odds only slightly — so we might instead interpret per **\$10,000 increase**.

### Pitfalls in Interpretation

It’s important to remember that odds ratios are **multiplicative**, not additive. This means the effect on probability depends on the baseline.
For example:

* Going from a 40% chance of default to 30% is a big shift,
* But the same odds ratio may translate into a much smaller change if the baseline probability is already low (e.g., from 5% to 4%).

Clear communication requires translating odds ratios back into **probability changes** for meaningful scenarios.

### Example from Loan Default Dataset

Suppose our model finds:

* OR for credit score = 0.99 → each point decrease in odds of default by 1%.
* OR for income = 0.95 per \$10,000 → higher income slightly reduces default risk.

We can then present this in plain English:

> “A borrower with a credit score of 700 has about a 10% chance of default, but with a score of 600, their probability rises to nearly 30%, holding income constant.”

**TODOs for this section:**

* Report odds ratios with 95% CI for credit score and income.
* Translate coefficients into real-world terms (per 50-point change in credit score, per \$10k change in income).
* Add figure showing predicted probability curve vs. credit score.

---

::: {.panel-tabset}

## R Code

```{.r}
# Logistic regression
logit_model <- glm(defaulted ~ credit_score + income,
                   data = BLR, family = binomial)

# Odds ratios and CI
odds_ratios <- exp(cbind(OR = coef(logit_model), confint(logit_model)))
odds_ratios

# Example: probability at credit_score = 600 vs 700
new_data <- data.frame(credit_score = c(600, 700),
                       income = mean(BLR$income))
predict(logit_model, newdata = new_data, type = "response")
```

## Python Code

```{.python}
# Odds ratios
params = model_full.params
conf = model_full.conf_int()
odds_ratios = np.exp(params)
conf_exp = np.exp(conf)
print(pd.DataFrame({"OR": odds_ratios,
                    "2.5%": conf_exp[0],
                    "97.5%": conf_exp[1]}))

# Example: probability at 600 vs 700 credit score
test_data = pd.DataFrame({
    "const": 1,
    "credit_score": [600, 700],
    "income": [df['income'].mean(), df['income'].mean()]
})
print(model_full.predict(test_data))
```

:::


---

## Predictions

Once we’ve fit a logistic regression model, we can use it to generate **predicted probabilities** of default for each customer. These probabilities fall between 0 and 1 and tell us how likely the model thinks it is that a customer will default given their predictors.

### Predicted Probabilities vs. Predicted Classes

Predicted probabilities can be turned into **class predictions** (default vs. no default) by applying a threshold, usually 0.5. Customers with probability ≥ 0.5 are classified as “default,” and those below as “no default.”

But in practice, the choice of threshold matters. If we lower the threshold to 0.3, we’ll catch more actual defaulters (higher **sensitivity**) but at the cost of more false alarms (lower **specificity**).

### Evaluating Performance

To judge prediction quality, we use metrics such as:

* **Accuracy:** proportion of correct predictions.
* **Sensitivity (recall):** proportion of true defaults correctly identified.
* **Specificity:** proportion of true non-defaults correctly identified.
* **ROC curve & AUC:** performance across all thresholds, not just one.

For our loan dataset, we might find that the model predicts non-defaults very well (high specificity) but misses some defaults (lower sensitivity). This trade-off is a central theme in logistic regression applications.

**TODOs for this section:**

* Insert confusion matrix for the loan dataset at threshold 0.5.
* Add ROC curve and report AUC.
* Discuss trade-offs between sensitivity and specificity with an example.

---

::: {.panel-tabset}

## R Code

```{.r}
# Logistic regression model
logit_model <- glm(defaulted ~ credit_score + income,
                   data = BLR, family = binomial)

# Predictions
BLR$pred_prob <- predict(logit_model, type = "response")
BLR$pred_class <- ifelse(BLR$pred_prob > 0.5, 1, 0)

# Confusion matrix
library(caret)
confusionMatrix(as.factor(BLR$pred_class), as.factor(BLR$defaulted))

# ROC curve
library(pROC)
roc_curve <- roc(BLR$defaulted, BLR$pred_prob)
plot(roc_curve, main="ROC Curve for Loan Default Model")
auc(roc_curve)
```

## Python Code

```{.python}
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import statsmodels.api as sm

df = r.data['BLR'].copy()
df['defaulted'] = df['defaulted'].astype(int)

X = sm.add_constant(df[['credit_score','income']])
y = df['defaulted']
logit_model = sm.Logit(y, X).fit()
df['pred_prob'] = logit_model.predict(X)
df['pred_class'] = (df['pred_prob'] > 0.5).astype(int)

# Confusion matrix & metrics
print(confusion_matrix(y, df['pred_class']))
print(classification_report(y, df['pred_class']))

# ROC curve
fpr, tpr, thresholds = roc_curve(y, df['pred_prob'])
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}")
plt.plot([0,1], [0,1], linestyle="--", color="grey")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve for Loan Default Model")
plt.legend()
plt.show()
```

:::

---

## Goodness of Fit & Model Selection

Evaluating whether our model is a “good” fit is just as important as making predictions. For logistic regression, the diagnostics differ from OLS.

### Pseudo R-Squared Measures

Because we don’t have the same notion of variance explained as in OLS, we use **pseudo R² measures** (e.g., McFadden’s R²). These are useful for comparison, but don’t carry the same interpretation as R² in linear regression.

### Analysis of Deviance

We can compare models using the **deviance** statistic, which measures how well the model fits relative to a saturated model. Lower deviance indicates better fit. Nested models (e.g., one with `credit_score` only vs. one with `credit_score + income`) can be compared using a likelihood ratio test.

### Information Criteria

Another approach is to use **information criteria** such as:

* **AIC (Akaike Information Criterion)**
* **BIC (Bayesian Information Criterion)**

Both balance fit and complexity: lower AIC or BIC means a better trade-off. AIC tends to favor more complex models; BIC penalizes complexity more heavily.

For the loan default dataset, we might find that adding `income` improves fit according to AIC but not BIC, suggesting we need to decide between **predictive accuracy** and **parsimony**.

**TODOs for this section:**

* Report pseudo R² for the loan model.
* Compare single-predictor vs. two-predictor models with deviance test.
* Report AIC and BIC values.
* Provide plain-language interpretation (e.g., “Adding income slightly improves model fit”).  

---

::: {.panel-tabset}

## R Code

```{.r}
# Fit models
model1 <- glm(defaulted ~ credit_score, data = BLR, family = binomial)
model2 <- glm(defaulted ~ credit_score + income, data = BLR, family = binomial)

# Compare deviance (likelihood ratio test)
anova(model1, model2, test = "Chisq")

# Pseudo R-squared
library(pscl)
pR2(model2)

# AIC and BIC
AIC(model1, model2)
BIC(model1, model2)
```

## Python Code

```{.python}
import statsmodels.api as sm

df = r.data['BLR'].copy()
df['defaulted'] = df['defaulted'].astype(int)

# Model 1: credit score only
X1 = sm.add_constant(df[['credit_score']])
model1 = sm.Logit(df['defaulted'], X1).fit()

# Model 2: credit score + income
X2 = sm.add_constant(df[['credit_score','income']])
model2 = sm.Logit(df['defaulted'], X2).fit()

# Likelihood ratio test
LR_stat = 2 * (model2.llf - model1.llf)
df_diff = model2.df_model - model1.df_model
from scipy.stats import chi2
p_value = chi2.sf(LR_stat, df_diff)

print("Likelihood Ratio Test:", LR_stat, "df:", df_diff, "p:", p_value)

# AIC & BIC
print("Model 1 AIC/BIC:", model1.aic, model1.bic)
print("Model 2 AIC/BIC:", model2.aic, model2.bic)
```

:::


---


## Model Diagnostics

Once we’ve estimated our logistic regression model, it’s important to check whether the model is **well-specified** and whether there are any **problematic observations** influencing the results. Diagnostics help us assess whether our predictions are trustworthy and whether model assumptions are being violated.

### Deviance Residuals

In logistic regression, we don’t have “raw residuals” like in OLS. Instead, we use **deviance residuals**, which measure how far off each predicted probability is from the actual outcome. Large residuals may indicate observations the model struggles to predict — for instance, a borrower with a very high credit score who still defaulted.

Plotting deviance residuals can help detect such outliers.

### Binned Residual Plots

Another way to check fit is with **binned residual plots**. Here, predicted probabilities are grouped (binned), and we compare average predicted probabilities with observed default rates in each bin. A well-calibrated model should show points lying close to the diagonal line (predicted = observed).

For our loan dataset, if the model predicts a 20% default rate for customers in a bin, then about 20% of those customers should actually have defaulted.

### Detecting Influential Points

Finally, some individual cases may **exert outsized influence** on the model — often measured using statistics like **Cook’s distance** or **leverage**. For example, a single customer with an unusually low credit score but very high income may skew the coefficient estimates. Identifying such cases ensures that no single observation is disproportionately driving conclusions.

---

**TODOs for this section:**

* Add plot of deviance residuals vs. predicted probabilities.
* Add binned residual plot for calibration.
* Add influence plot (highlighting high-leverage or influential cases).
* Provide a short interpretation using the loan default dataset.

---

::: {.panel-tabset}

## R Code

```{.r}
# Logistic regression model
logit_model <- glm(defaulted ~ credit_score + income,
                   data = BLR, family = binomial)

# Deviance residuals
residuals_dev <- residuals(logit_model, type = "deviance")

# Plot deviance residuals vs. predicted probabilities
BLR$pred_prob <- predict(logit_model, type = "response")
plot(BLR$pred_prob, residuals_dev,
     xlab = "Predicted Probability",
     ylab = "Deviance Residuals",
     main = "Deviance Residuals vs Predicted Probability")
abline(h = 0, col = "red", lty = 2)

# Binned residual plot (using arm package)
library(arm)
binnedplot(BLR$pred_prob, residuals_dev,
           xlab = "Predicted Probability",
           ylab = "Average Residual",
           main = "Binned Residual Plot")

# Influence measures
influence_measures <- influence.measures(logit_model)
summary(influence_measures)
```

## Python Code

```{.python}
import statsmodels.api as sm
import matplotlib.pyplot as plt
import numpy as np

# Logistic regression
df = r.data['BLR'].copy()
df['defaulted'] = df['defaulted'].astype(int)
X = sm.add_constant(df[['credit_score','income']])
y = df['defaulted']
logit_model = sm.Logit(y, X).fit()
df['pred_prob'] = logit_model.predict(X)

# Deviance residuals (via statsmodels residuals)
resid_dev = logit_model.resid_dev

# Plot deviance residuals vs predicted probabilities
plt.scatter(df['pred_prob'], resid_dev, alpha=0.6)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel("Predicted Probability")
plt.ylabel("Deviance Residuals")
plt.title("Deviance Residuals vs Predicted Probability")
plt.show()

# Influence plot
sm.graphics.influence_plot(logit_model, criterion="cooks")
plt.show()
```

:::


---

## Results

Once we’ve fit our logistic regression model, the next step is to **present the results**. Results can be organized into two complementary perspectives:

1. **Predictive Analysis** — How well does our model classify customers into “default” vs. “non-default”?
2. **Inferential Analysis** — What do the model’s coefficients tell us about the relationship between predictors (e.g., credit score, income) and the probability of default?

### Predictive Analysis

From a predictive standpoint, the model provides **predicted probabilities** for each customer. By applying a threshold (commonly 0.5), we can classify customers as predicted to default or not.

We then evaluate performance using:

* **Accuracy:** The proportion of correct predictions.
* **Sensitivity (Recall):** How well the model detects actual defaults.
* **Specificity:** How well the model detects non-defaults.
* **ROC Curve / AUC:** Performance across all possible thresholds.

For example, in our loan dataset, the model correctly classifies a high proportion of non-defaulters, but sensitivity may be lower if defaults are relatively rare.

### Inferential Analysis

From an inferential perspective, the coefficients give us **odds ratios** that describe how predictors affect default risk.

For instance, a coefficient of `-0.01` for credit score corresponds to an **odds ratio of 0.99** — meaning that for every 1-point increase in credit score, the odds of defaulting decrease by about 1%. Scaled up, a **50-point increase** lowers the odds by roughly 40%.

By converting odds ratios into **changes in probability** at meaningful ranges of credit score, we provide a more intuitive interpretation for readers.

---

**TODOs for this section:**

* Insert table summarizing model coefficients with odds ratios and confidence intervals.
* Add classification table (confusion matrix) showing accuracy, sensitivity, specificity.
* Include ROC curve for predictive storytelling.
* Provide plain-language interpretation of at least one coefficient (credit score).

---

::: {.panel-tabset}

## R Code

```{.r}
# Fit logistic regression model
logit_model <- glm(defaulted ~ credit_score + income,
                   data = BLR, family = binomial)

# Summary for inference
summary(logit_model)

# Odds ratios with confidence intervals
exp(cbind(OR = coef(logit_model), confint(logit_model)))

# Predictive evaluation
library(caret)
pred_prob <- predict(logit_model, type = "response")
pred_class <- ifelse(pred_prob > 0.5, 1, 0)
confusionMatrix(as.factor(pred_class), as.factor(BLR$defaulted))

# ROC curve
library(pROC)
roc_curve <- roc(BLR$defaulted, pred_prob)
plot(roc_curve, main="ROC Curve for Loan Default Model")
auc(roc_curve)
```

## Python Code

```{.python}
import pandas as pd
import statsmodels.api as sm
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt

# Logistic regression
df = r.data['BLR'].copy()
df['defaulted'] = df['defaulted'].astype(int)

X = sm.add_constant(df[['credit_score','income']])
y = df['defaulted']
logit_model = sm.Logit(y, X).fit()

# Inference: odds ratios
params = logit_model.params
conf = logit_model.conf_int()
odds_ratios = pd.DataFrame({
    "OR": params.apply(lambda x: np.exp(x)),
    "2.5%": conf[0].apply(lambda x: np.exp(x)),
    "97.5%": conf[1].apply(lambda x: np.exp(x))
})
print(odds_ratios)

# Predictions
df['pred_prob'] = logit_model.predict(X)
df['pred_class'] = (df['pred_prob'] > 0.5).astype(int)

# Confusion matrix & report
print(confusion_matrix(y, df['pred_class']))
print(classification_report(y, df['pred_class']))

# ROC Curve
fpr, tpr, thresholds = roc_curve(y, df['pred_prob'])
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,4))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0,1], [0,1], color='grey', linestyle='--')
plt.title("ROC Curve for Loan Default Model", fontsize=14, fontweight='bold')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.show()
```

:::

---





## Storytelling

::: {.callout-tip}
Effective storytelling connects statistical results to real-world meaning. Translate log-odds into “lenders are X times more likely to...” language.
:::


The final step of our analysis is not just running the model, but **communicating the findings** clearly. Logistic regression results are often presented to stakeholders like lenders, policy makers, or managers, who may not be trained in statistics. For them, **the story matters more than the math.**

Let’s revisit our loan default dataset. Suppose our logistic regression showed that **credit score** is strongly predictive of default:

* A 50-point increase in credit score reduces the odds of defaulting by about 40%.
* In probability terms, this means that increasing credit score from 400 to 450 lowers the chance of default from roughly 70% to about 50%.

Notice how we moved from the technical language of **odds ratios** to a **plain-language probability story**. This makes the model’s results accessible to a wider audience.

Visuals amplify the story:

* A plot of predicted probabilities vs. credit score shows the **smooth S-shaped curve** replacing the jagged OLS line.
* A confusion matrix or ROC curve shows how well our model actually classifies defaulters.
* Calibration plots show whether predicted probabilities line up with observed rates of default.

Finally, good storytelling means acknowledging limitations. For example, our dataset may suffer from **class imbalance** (fewer defaults than non-defaults), which could bias results. We should be clear about what the model can and cannot do.

**TODOs for this section:**

* Add figure of predicted probabilities vs. actual defaults (using `credit_score`).
* Show a ROC curve for the loan default model.
* Add plain-language interpretations of coefficients (esp. odds ratio for `credit_score`).
* Add a note about limitations (e.g., income and education may be correlated with credit score).

---

::: {.panel-tabset}

## R Code

```{.r}
# Logistic regression on loan default data
logit_model <- glm(defaulted ~ credit_score + income,
                   data = BLR, family = binomial)

# Predicted probabilities
BLR$pred_prob <- predict(logit_model, type = "response")

# Storytelling visualization
ggplot(BLR, aes(x = credit_score, y = pred_prob, color = as.factor(defaulted))) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess", se = FALSE, color = "black") +
  labs(
    title = "Predicted Probability of Default vs. Credit Score",
    x = "Credit Score",
    y = "Predicted Probability",
    color = "Actual Default"
  ) +
  theme_minimal()
```

## Python Code

```{.python}
import matplotlib.pyplot as plt
import pandas as pd
import statsmodels.api as sm

# Extract loan default data
df = r.data['BLR'].copy()
df['defaulted'] = df['defaulted'].astype(int)

# Fit logistic regression
X = sm.add_constant(df[['credit_score','income']])
y = df['defaulted']
logit_model = sm.Logit(y, X).fit()
df['pred_prob'] = logit_model.predict(X)

# Storytelling visualization
fig, ax = plt.subplots(figsize=(6,4))
scatter = ax.scatter(df['credit_score'], df['pred_prob'],
                     c=df['defaulted'], cmap='coolwarm', alpha=0.6)
ax.set_title("Predicted Probability of Default vs. Credit Score", fontsize=14, fontweight='bold')
ax.set_xlabel("Credit Score")
ax.set_ylabel("Predicted Probability")
legend1 = ax.legend(*scatter.legend_elements(), title="Actual Default")
ax.add_artist(legend1)
plt.show()
```

:::








