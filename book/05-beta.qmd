<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7PRVEBE1EF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7PRVEBE1EF');
</script>

# Soup-erb Beta Regression {#sec-beta}

```{r}
#| include: false

colourize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

```{r setup, include=FALSE}
# Load required libraries
library(devtools)
library(readr)
library(ggplot2)
library(ggcorrplot)
library(cookbook)
library(tidyverse)
library(car)
library(lmtest)
library(fastDummies)

# Setting up Python dependencies
library(reticulate)
py_require("pandas") 
py_require("numpy")
py_require("matplotlib")
py_require("statsmodels")
py_require("pyreadr")
py_require("scikit-learn")
py_require("seaborn")
py_require("scipy")

pandas <- import("pandas")
numpy <- import("numpy")
matplotlib <- import("matplotlib")
statsmodels <- import("statsmodels")
pyreadr <- import("pyreadr")
sklearn <- import("sklearn")
seaborn <- import("seaborn")
scipy <- import("scipy")

# Load data in R
data <- Beta

py$data <- r_to_py(Beta)
```

::: {.LO}
::::{.LO-header}
Learning Objectives
::::
::::{.LO-container}
By the end of this chapter, you will be able to:

::::
:::

## Introduction

Many real world problems involve modeling proportions. A common example is a website conversion rate, where each page is associated with a value between zero and one that represents the fraction of visitors who complete a purchase. Some pages convert only a small share of visitors, while others perform much better. Although these values are numerical and continuous, they are also restricted to a fixed range. A suitable model must respect these limits and reflect the fact that variability often changes depending on the average level of the proportion.

Traditional linear regression is not well suited for this type of data. It can produce predictions outside the valid range and assumes that variability is constant across all observations. Problems like this call for a regression approach that is designed specifically for proportion outcomes. This is exactly where Beta regression comes to shine.

## What is Beta Regression

Beta regression is a regression model designed for situations where the response variable is a proportion that takes values on the open interval between zero and one. Instead of modeling the response directly as a straight line, Beta regression assumes that the response follows a Beta distribution. This distribution is defined only on the zero to one interval and is flexible enough to represent many different shapes.

Because of this flexibility, Beta regression is well suited for data such as conversion rates, completion rates, coverage percentages, or any outcome that represents a fraction of a whole. The model naturally respects the bounds of the data and avoids unrealistic predictions.

## The Beta Distribution

The Beta distribution is a continuous probability distribution defined on the interval between zero and one. A key feature of this distribution is its flexibility. Depending on its parameter values, it can take many shapes, including symmetric, left skewed, or right skewed forms, as well as distributions that place more mass near the boundaries.

```{python beta-distribution}
#| echo: false
#| message: false

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import beta

# Create values between 0 and 1
x = np.linspace(0.001, 0.999, 500)

# Different (alpha, beta) combinations
parameters = [
    (0.5, 0.5),
    (2, 2),
    (2, 5),
    (5, 2),
    (1, 3),
    (3, 1),
]

# Plot Beta distributions
plt.figure(figsize=(9, 5))

for a, b in parameters:
    y = beta.pdf(x, a, b)
    plt.plot(x, y, label=f"alpha = {a}, beta = {b}")

plt.title("Examples of Beta Distributions on (0, 1)")
plt.xlabel("Proportion")
plt.ylabel("Density")
plt.xlim(0, 1);   # Suppress additional output
plt.legend(frameon=False)
plt.tight_layout()
plt.show()
```

The figure above shows several examples of Beta distributions with different parameter choices. Although the shapes vary widely, all distributions remain within the zero to one range, making the Beta distribution a natural choice for modeling proportion data.

The shape of a Beta distribution is controlled by two parameters, commonly called alpha and beta. When alpha is larger than beta, values tend to be closer to one. When beta is larger than alpha, values tend to be closer to zero. When the two parameters are similar, values cluster around the middle of the range. Together, alpha and beta also influence how spread out the data are, with smaller values producing more variability and larger values leading to tighter concentration around the average.

In practice, working directly with alpha and beta is not very convenient for regression modeling. These parameters describe the shape of the distribution, but they do not directly tell us how the average proportion changes when a predictor changes. To address this, Beta regression rewrites the distribution in terms of quantities that are easier to interpret, such as the **average proportion** and **how tightly the data cluster around that average**. This reexpression allows the model to relate the average proportion to predictor variables while still ensuring that all fitted values remain between zero and one. We explore this idea in more detail in the next section.

## Linking the Average Proportion to Predictors

In the Gamma regression chapter, we introduced generalized linear models as a way to extend linear regression to response variables that do not follow a normal distribution. Beta regression follows the same framework and is another example of a generalized linear model, designed specifically for proportion data.

As with other generalized linear models, Beta regression does not model the response directly. Instead, it models the average of the response through a transformation that allows predictor variables to be related to the outcome using a familiar linear structure. This approach is especially important for proportion data, since the average must always remain between zero and one.

::: {.Tip}
::::{.Tip-header}
A Quick Refresh on GLMs
::::
::::{.Tip-container}
Generalized linear models are built from three components.

- **Random component** describes the distribution of the response variable  
- **Systematic component** combines predictor variables through a linear predictor  
- **Link function** connects the linear predictor to the average of the response  

These three pieces work together to extend linear regression to a wide range of response types.
::::
:::

For Beta regression, the three components of GLM take the following form.

- **Random component**: The response variable is assumed to follow a Beta distribution. This choice reflects the fact that the outcome is continuous and bounded between zero and one.

- **Systematic component**: Predictor variables are combined through a linear predictor. This part of the model describes how changes in the predictors are associated with changes in the average proportion.

- **Link function**: The link function connects the linear predictor to the average proportion. Its role is to ensure that the modeled average remains between zero and one. A commonly used choice is the logit function, which maps proportions to the real line and is also used in logistic regression.

## The Beta Regression Model

Putting the three components together, the Beta regression model can be written in mathematical form.

For each observation $i$, the response variable is assumed to follow a Beta distribution,

$$
Y_i \sim \text{Beta}(\mu_i \phi,\ (1 - \mu_i)\phi)
$$
Here, $\mu_i$ represents the average proportion for observation $i$, and $\phi$ is a parameter that controls how tightly the data cluster around that average. Larger values of $\phi$ correspond to less variability, while smaller values allow for more spread.

The average proportion $\mu_i$ is then linked to the predictors through a linear predictor using a link function. With the logit link, this takes the form

$$
\begin{aligned}
\text{logit}(\mu_i) &= \log\left(\frac{\mu_i}{1-\mu_i}\right) \\
                    &= \eta_i \\
                    &= \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}
\end{aligned}
$$

Here, $\beta_0$ is the intercept and represents the baseline value of the transformed average proportion when all predictors are set to zero. The remaining coefficients $\beta_1, ..., \beta_p$ describe the association between each predictor and the average proportion on the logit scale, holding all other predictors constant. A positive coefficient indicates that larger values of the predictor are associated with a higher average proportion, while a negative coefficient indicates the opposite.

This formulation follows the same generalized linear model structure introduced in the Gamma regression chapter. The random component specifies the distribution of the response, the systematic component combines predictors through a linear predictor, and the link function connects the linear predictor to the average of the response. Together, these components allow Beta regression to model proportion data in a flexible and interpretable way, while ensuring that fitted values always remain between zero and one.

## Case Study: Modeling Student Academic Performance

To demonstrate Beta regression in action, we will walk through a case study using a toy dataset that captures student academic performance. In this case study, our response variable is a final course grade, expressed as a percentage.

Because grades represent proportions that lie between zero and one hundred percent, they are naturally suited for Beta regression after being rescaled to the unit interval. Our goal is to understand how different aspects of a student’s academic life and daily habits are associated with their overall performance.

We will approach this case study using the data science workflow introduced in the first chapter. This ensures a structured approach to framing the problem, exploring the data, selecting an appropriate model, and interpreting the results.

### The Dataset

The toy dataset used in this case study represents a group of university students. Each row corresponds to a single student and includes information about study habits, lifestyle factors, and academic support. Below is a summary of the variables included in the dataset.

| **Variable Name**             | **Description**                                          |
| ----------------------------- | -------------------------------------------------------- |
| **Final Grade (%)**           | Final course grade expressed as a percentage.            |
| **Café Location**             | Primary café or study location used by the student.      |
| **Study Time**                | Self-reported amount of time spent studying.             |
| **Year in School**            | Academic year of the student (e.g., 1st Year, 4th Year). |
| **Hobbies**                   | Primary hobby outside of school.                         |
| **Primary Sport**             | Main sport played by the student, if any.                |
| **Living Arrangement**        | Living situation (e.g., alone, with friends).            |
| **Part-time Work Hours**      | Number of hours worked per week in a part-time job.      |
| **Study Group Participation** | Whether the student participates in a study group.       |
| **Hours of Sleep per Night**  | Average number of hours of sleep per night.              |
| **Has a Tutor**               | Whether the student receives tutoring support.           |


Here is a snapshot of the dataset:

<div style="overflow-x: auto;">

| **Final Grade (%)** | **Café Location**       | **Study Time** | **Year in School** | **Hobbies** | **Primary Sport** | **Living Arrangement** | **Part-time Work Hours** | **Study Group Participation** | **Hours of Sleep per Night** | **Has a Tutor** |
|---------------------|--------------------------|----------------|---------------------|-------------|-------------------|------------------------|---------------------------|-------------------------------|------------------------------|------------------|
| 96.99               | Koerner's Pub            | 5+ hours       | 1st Year            | Sports      | Soccer            | With Friends           | 15                        | Yes                           | 5.4                          | Yes              |
| 94.62               | Bean Around the World    | 5+ hours       | 4th Year            | Gaming      | Soccer            | Alone                  | 7                         | Yes                           | 5.4                          | No               |
| 90.30               | The Loop Café            | 5+ hours       | 2nd Year            | Sports      | None              | Alone                  | 6                         | No                            | 5.6                          | Yes              |
| 86.77               | Bean Around the World    | 5+ hours       | 1st Year            | Reading     | Basketball        | Alone                  | 10                        | No                            | 7.8                          | No               |

</div>


This dataset allows us to examine how academic behaviors and lifestyle choices are associated with student performance, while respecting the proportional nature of the outcome.

### The Problem We’re Trying to Solve

In this case study, our goal is to understand what factors are associated with higher or lower final grades. Since grades are continuous, bounded, and naturally interpreted as proportions, Beta regression is a suitable modeling choice.

The key question we aim to answer is:

> **Which academic and lifestyle factors are associated with differences in students’ final grades?**

Rather than focusing only on prediction, we are interested in understanding how study habits, support systems, and daily routines relate to academic outcomes. In the next section, we clarify our study design and begin applying the data science workflow to this problem.

## Study Design

In this case study, our goal is to explore how academic and lifestyle factors are associated with students’ final grades. The outcome of interest is a student’s final course grade, expressed as a percentage.

At this stage of the analysis, our focus is on clearly defining the problem and the type of question we want to answer, rather than committing to a specific model. In particular, we are interested in understanding how different variables relate to students’ overall performance, and whether certain study habits or forms of academic support are associated with higher or lower grades.

Our emphasis is inferential rather than predictive. We aim to describe and interpret relationships between variables, not to build a model optimized solely for forecasting future grades. A predictive approach would be possible, but interpretability and insight are the primary objectives in this case study.

## Data Collection and Wrangling

With our statistical question clearly defined, the next step is to ensure that the data is suitable for analysis. Although the dataset is already provided, it is still useful to reflect briefly on how this type of data might have been collected. Doing so helps clarify what the data represents and highlights potential limitations before moving on to preparation and modeling.

### Data Collection

For a study examining student academic performance, data like this could reasonably be collected through several common sources:

- **Academic Records**: Final grades are typically recorded by instructors or academic departments at the end of a course. These records provide a standardized and reliable measure of student performance.

- **Student Surveys**: Information on study habits, sleep, part-time work, tutoring, and study group participation is often collected through self-reported surveys. While this data provides valuable context, it may be subject to reporting bias or measurement error.

- **Institutional Data Systems**: Universities often combine academic records with student support and enrollment information, such as year in school or participation in academic programs.

Each of these sources comes with trade-offs. Academic records are objective but limited to performance outcomes, while survey-based variables may be noisy but offer insight into behaviors and lifestyle factors. Recognizing these limitations helps guide both exploratory analysis and interpretation later on.

### Data Wrangling

Once the data has been collected, the next step is to prepare it for modeling. Even small, well-structured datasets often require preprocessing before they can be analyzed effectively.

For Beta regression, data wrangling typically involves:

- Validating that the response variable represents a proportion.
- Rescaling the response variable so that it lies strictly between zero and one.
- Checking for boundary values such as exact zeros or ones.
- Converting categorical variables into numerical formats suitable for modeling.
- Inspecting missing values and potential inconsistencies in self-reported data.

Based on the data, there can, of course, be other things you may have to do but for now, let's walk through these steps using our student performance dataset, demonstrating each step in both R and Python.

#### 1. Preparing and Validating the Response Variable

In this case study, the response variable is the final course grade, recorded as a percentage. While this variable is continuous, it is currently measured on a scale from 0 to 100. Since Beta regression models proportions, the response must first be rescaled to lie between zero and one.

Before rescaling, we begin by examining the distribution of final grades to confirm that the values are within a reasonable range.

::: {.panel-tabset}
## R Code
```{r}
summary(data$`Final Grade (%)`)
```

## Python Code
```{python}
# Summary statistics for the response variable
print(data["Final Grade (%)"].describe())
```
:::

The summary confirms that all grades lie between 0 and 100, making them suitable for conversion to proportions. We therefore rescale the response by dividing each value by 100.

::: {.panel-tabset}
## R Code
```{r}
data$final_grade_prop <- data$`Final Grade (%)` / 100
summary(data$final_grade_prop)
```

## Python Code
```{python}
# Rescale grades to proportions
data["final_grade_prop"] = data["Final Grade (%)"] / 100
print(data["final_grade_prop"].describe())
```
:::

::: {.Heads-up}
::::{.Heads-up-header}
Heads-up!
::::
::::{.Heads-up-container}
At this stage, the structure of the response variable already suggests that a proportion-based model, such as Beta regression, will be a good fit. With that in mind, we perform the preprocessing needed for such a model here. In practice, this decision is often informed by exploratory analysis and may be revisited later, but introducing it now helps keep the workflow clear and focused.
::::
:::

After rescaling, the response variable now lies on the unit interval. The next step is to check whether any values are exactly zero or one, since standard Beta regression assumes outcomes are strictly between these boundaries.

#### 2. Checking for Boundary Values

Beta regression requires that the response variable take values strictly between zero and one. We therefore check whether any rescaled grades are exactly equal to zero or one.

::: {.panel-tabset}
## R Code
```{r}
sum(data$final_grade_prop <= 0)
sum(data$final_grade_prop >= 1)
```

## Python Code
```{python}
# Check for boundary values
print((data["final_grade_prop"] <= 0).sum())
print((data["final_grade_prop"] >= 1).sum())
```
:::

Shoot! The checks reveal that there are several observations with values equal to one. While this is not unusual for percentage-based data, it poses a problem for standard Beta regression, which assumes the response lies strictly between zero and one.

::: {.Heads-up}
::::{.Heads-up-header}
Heads-up!
::::
::::{.Heads-up-container}
When proportion data includes values exactly equal to zero or one, the assumptions of standard Beta regression are violated. This situation is common in practice, especially when proportions are derived from percentages or rounded measurements. There are several principled ways to handle boundary values, each with different trade-offs.

One approach is **boundary adjustment**, where values equal to zero or one are shifted slightly inward by a small amount. This method is simple, preserves the ordering of observations, and allows standard Beta regression to be applied without adding model complexity. It is often appropriate when boundary values arise from rounding or measurement limits rather than representing a fundamentally different process.

A second approach, proposed by Smithson and Verkuilen (2006), **applies a systematic transformation to the response variable**. Instead of only adjusting values at the boundaries, this method gently shifts all observations so that none of them are exactly zero or one.

Intuitively, the transformation treats the observed proportions as slightly uncertain and pulls extreme values a small distance toward the interior of the interval. This avoids hard cutoffs at zero and one while preserving the relative ordering of the data.

Mathematically, the transformation can be written as
$$
x' = \frac{x (N - 1) + s}{N}
$$

where $N$ is the sample size and $s$ is a small constant between 0 and 1, often chosen as $s=0.5$.

One way to think about this adjustment is that it spreads the data evenly across the open interval $(0,1)$, ensuring that no observation lies exactly on the boundaries. From a Bayesian perspective (for the Bayesianists out there), the constant $s$ can be interpreted as introducing a weak prior that prevents extreme values from being taken at face value. In practice, this approach is often viewed as more principled than manually nudging boundary values, though it also adds an extra layer of abstraction.

Another option is to use **zero–one–inflated Beta models**, which explicitly treat boundary values as arising from a separate process. These models are more flexible and appropriate when zeros or ones occur frequently and carry substantive meaning, but they also introduce additional parameters and complexity.

Finally, boundary observations can be **excluded**, though this is generally discouraged unless there is a strong substantive justification, as it removes potentially informative data.
::::
:::

In this case study, our goal is to focus on the core ideas of Beta regression and to keep the workflow accessible. Because the number of boundary values is small and likely driven by rounding of high grades, we proceed with a simple boundary adjustment and continue with standard Beta regression.

::: {.panel-tabset}
## R Code
```{r}
epsilon <- 0.001
data$final_grade_prop[data$final_grade_prop >= 1] <- 1 - epsilon
data$final_grade_prop[data$final_grade_prop <= 0] <- epsilon

summary(data$final_grade_prop)
```

## Python Code
```{python}
epsilon = 0.001
data.loc[data["final_grade_prop"] >= 1, "final_grade_prop"] = 1 - epsilon
data.loc[data["final_grade_prop"] <= 0, "final_grade_prop"] = epsilon

print(data["final_grade_prop"].describe())
```
:::

After this adjustment, all response values lie strictly between zero and one, making the data suitable for Beta regression.

#### 3. Encoding Categorical Variables

Several variables in the dataset are categorical, such as café location, year in school, hobbies, living arrangement, study group participation, and tutoring status. Regression models, including Beta regression, require numerical inputs, so these variables must be converted into a suitable numerical representation. We use one-hot encoding for this purpose.

As a quick refresh, one-hot encoding represents each category as a binary indicator variable. For a given categorical feature, each level becomes its own column that takes the value 1 when an observation belongs to that category and 0 otherwise. To avoid redundancy, one category is typically omitted and treated as the reference level.

::: {.panel-tabset}
## R Code
```{r}
# Convert categorical variables to factors
categorical_vars <- c(
"Café Location",
"Study Time",
"Year in School",
"Hobbies",
"Primary Sport",
"Living Arrangement",
"Study Group Participation",
"Has a Tutor"
)

data[categorical_vars] <- lapply(data[categorical_vars], as.factor)

# Create design matrix with dummy variables
design_matrix <- model.matrix(
final_grade_prop ~ .,
data = data
)

head(design_matrix)
```

## Python Code
```{python}
import pandas as pd

# One-hot encode categorical variables
categorical_vars = [
"Café Location",
"Study Time",
"Year in School",
"Hobbies",
"Primary Sport",
"Living Arrangement",
"Study Group Participation",
"Has a Tutor"
]

data_encoded = pd.get_dummies(
data,
columns=categorical_vars,
drop_first=True
)

data_encoded.head()
```
:::

After encoding, each categorical variable is represented numerically, making the dataset suitable for regression modeling. The resulting design matrix includes one column per predictor, with categorical effects interpreted relative to a reference category. We return to this point when interpreting the fitted model.

#### 4. Checking for Missing Values and Inconsistencies

A useful step before any analysis is to check for missing data. Missing values can affect model fitting and interpretation, especially when the dataset is small.

::: {.panel-tabset}
## R Code
```{r}
# Check for missing values in each column
colSums(is.na(data))
```

## Python Code
```{python}
# Check for missing values in each column
print(data.isna().sum())
```
:::

Fortunately, there are no missing values in this dataset. If missing values were present, several options would be available, including removing affected observations, imputing values, or revisiting how the data was collected. The appropriate choice depends on the extent and nature of the missingness, as well as the goals of the analysis.

::: {.Heads-up}
::::{.Heads-up-header}
Heads-up!
::::
::::{.Heads-up-container}
In addition to missing values, it is also good practice to scan for obvious inconsistencies, such as unexpected category labels or implausible numeric values. Earlier steps have already verified that the response variable lies within the required range. In a larger or messier dataset, it would also be sensible to check whether predictors such as hours of sleep or part-time work hours fall within reasonable bounds. We omit these additional checks here to keep the example focused, but the same principles apply in more complex analyses.
::::
:::

With these checks complete, the dataset is clean, numerically encoded, and ready for exploratory data analysis.

## Exploratory Data Analysis (EDA)


::: {.Warning}
::::{.Warning-header}
Fun fact!
::::
::::{.Warning-container}
**Soup-erb!** Soup that’s so heartwarming it feels like a cozy hug.
::::
:::

::: {#fig-beta-regression}
```{mermaid}
mindmap
  root((Regression 
  Analysis)
    Continuous <br/>Outcome Y
      {{Unbounded <br/>Outcome Y}}
        )Chapter 3: <br/>Ordinary <br/>Least Squares <br/>Regression(
          (Normal <br/>Outcome Y)
      {{Nonnegative <br/>Outcome Y}}
        )Chapter 4: <br/>Gamma Regression(
          (Gamma <br/>Outcome Y)
      {{Bounded <br/>Outcome Y <br/> between 0 and 1}}
        )Chapter 5: Beta <br/>Regression(
          (Beta <br/>Outcome Y)
    Discrete <br/>Outcome Y
```
:::
